"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Supporting program comprehension with source code summarization","S. Haiduc; J. Aponte; A. Marcus","Wayne State University, Detroit, MI; Universidad Nacional de Colombia, Bogot&#x0E1;, Colombia; Wayne State University, Detroit, MI","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","2","","223","226","One of the main challenges faced by today's developers is keeping up with the staggering amount of source code that needs to be read and understood. In order to help developers with this problem and reduce the costs associated with it, one solution is to use simple textual descriptions of source code entities that developers can grasp easily, while capturing the code semantics precisely. We propose an approach to automatically determine such descriptions, based on automated text summarization technology.","1558-1225;0270-5257","978-1-60558-719","10.1145/1810295.1810335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062165","program comprehension;summary;text summarization","Semantics;Natural languages;Large scale integration;Software engineering;Tagging;Software systems","reverse engineering;software cost estimation;software maintenance","program comprehension;source code summarization;cost reduction;textual description;code semantics;automated text summarization;software maintenance","","37","","12","","","","","IEEE","IEEE Conferences"
"Examining Software Design Projects in a First-Year Engineering Course: : The Impact of the Project Type on Programming Complexity and Programming","K. M. Kcskemety; Z. Dix; B. Kott","Department of Engineering Education, The Ohio State University, Columbus, OH, USA; Department of Engineering Education, The Ohio State University, Columbus, OH, USA; Department of Engineering Education, The Ohio State University, Columbus, OH, USA","2018 IEEE Frontiers in Education Conference (FIE)","","2018","","","1","5","This Innovative Practice work-in-progress paper examines a game software design project implemented at a large Midwestern university to replace prior end of term projects. The use of end of term projects for introductory software courses can increase student engagement and provide a way to summarize and synthesize topics learned in the course. Previous research has shown student's perceptions of the game project to be positive. This current work examines a sample set of programming source code submissions for the Source Lines of Code metric and counts of the instances of certain programming fundamentals (input/output, repetition structures, selection structures). The results show that the game projects had significantly higher effort required when examining these metrics compared to the prior end of term projects. This preliminary work will lead to additional analysis of the full set of source code submissions, as well as, different metrics to measure complexity of the submissions.","2377-634X;1539-4565","978-1-5386-1174-6978-1-5386-1175","10.1109/FIE.2018.8659033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659033","software design project;source code analysis;first-year engineering","Games;Complexity theory;Matlab;Measurement;Programming profession;Software design","computer aided instruction;computer science education;educational courses;engineering education;software metrics","game project;programming source code submissions;programming complexity;game software design project;Midwestern university;student engagement;source lines;code metric","","","","16","","","","","IEEE","IEEE Conferences"
"Source code comprehension strategies and metrics to predict comprehension effort in software maintenance and evolution tasks - an empirical study with industry practitioners","K. Nishizono; S. Morisakl; R. Vivanco; K. Matsumoto","Graduate School, of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, 630-0192 JAPAN; Faculty of Informatics, Shizuoka University, 3-5-1 Johoku, Naka, Hamamatsu, 432-8011 JAPAN; Department of Computer Science, University of Manitoba, Winnipeg, Canada R3T 2N2; Graduate School, of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, 630-0192 JAPAN","2011 27th IEEE International Conference on Software Maintenance (ICSM)","","2011","","","473","481","The goal of this research was to assess the consistency of source code comprehension strategies and comprehension effort estimation metrics, such as LOC, across different types of modification tasks in software maintenance and evolution. We conducted an empirical study with software development practitioners using source code from a small paint application written in Java, along with four semantics-preserving modification tasks (refactoring, defect correction) and four semantics-modifying modification tasks (enhancive and modification). Each task has a change specification and corresponding source code patch. The subjects were asked to comprehend the original source code and then judge whether each patch meets the corresponding change specification in the modification task. The subjects recorded the time to comprehend and described the comprehension strategies used and their reason for the patch judgments. The 24 subjects used similar comprehension strategies. The results show that the comprehension strategies and effort estimation metrics are not consistent across different types of modification tasks. The recorded descriptions indicate the subjects scanned through the original source code and the patches when trying to comprehend patches in the semantics-modifying tasks while the subjects only read the source code of the patches in semantics-preserving tasks. An important metric for estimating comprehension efforts of the semantics-modifying tasks is the Code Clone Subtracted from LOC(CCSLOC), while that of semantics-preserving tasks is the number of referred variables.","1063-6773;1063-6773","978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662","10.1109/ICSM.2011.6080814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080814","source code comprehension;comprehension effort estimation metrics;semantics-preserving;semantics-modifying;software maintenance and evolution","Analytical models;Measurement","software maintenance;software metrics","source code comprehension;software maintenance;evolution task;comprehension effort estimation metrics;software evolution;software development practitioner;semantics-preserving modification task;refactoring;defect correction;semantics-modifying modification task;source code patch;code clone subtracted from LOC","","4","","17","","","","","IEEE","IEEE Conferences"
"Cognitive Load Comparison of Traditional and Distributed Pair Programming on Visual Programming Language","C. Tsai; Y. Yang; C. Chang","NA; NA; NA","2015 International Conference of Educational Innovation through Technology (EITT)","","2015","","","143","146","Research results from computer science education show that pair programming is an effective teaching strategy on learning performance for computer science education in K-12. However, pair programming is not popularly used in K-12. One important issue is because sometimes pair programming by K-12 students is not as effective as expected. This study examines whether pair programming is effective from cognitive load perspective in distributed situation. We compares the cognitive loads for the visual programming language, called Star Logo TNG, under three circumstances that is alone, pair programming, and distributed pair programming. The experimental results show that learners prefer traditional pair programming to alone. Moreover, the pair programming can reduce germane cognitive load significantly. However, there is no significant difference between the single and distributed pair programming. Meanwhile, there is no significant difference on learning performance between traditional and distributed pair programming. Those results indicate the learning support and knowledge sharing tools for distributed pair programming could be improved.","","978-1-4673-8038-6978-1-4673-8037","10.1109/EITT.2015.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446165","computer science education;distributed pair programming;visual programming language;cognitive load;Star Logo TNG","Programming profession;Computer languages;Visualization;Load modeling;Computer science education","computer aided instruction;computer science education;distributed programming;programming languages;teaching;visual programming","cognitive load perspective;traditional pair programming;distributed pair programming;visual programming language;computer science education;teaching strategy;learning performance;K-12 education;Star Logo TNG language;learning support;knowledge sharing","","1","","10","","","","","IEEE","IEEE Conferences"
"From Program Comprehension to People Comprehension","A. Begel","NA","2010 IEEE 18th International Conference on Program Comprehension","","2010","","","190","191","Large-scale software engineering requires many teams to collaborate together to create software products. The problems these teams suffer trying to coordinate their joint work can be addressed through tools inspired by social networking. Social networking tools help people to more easily discover and more efficiently maintain relationships than is feasible using one-to-one or face-to-face interactions. Applying these ideas to the software domain requires new kinds and combinations of software program and process analyses that overcome intrinsic limitations in the accuracy of the underlying data sources and the ambiguity inherent in human relationships.","1092-8138;1092-8138","978-1-4244-7603-9978-1-4244-7604-6978-0-7695-4113","10.1109/ICPC.2010.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521747","software process;human aspects","Collaborative software;Humans;Software engineering;Collaborative work;Social network services;Computer bugs;Software testing;Data engineering;Data mining;USA Councils","social aspects of automation;social networking (online);software engineering","program comprehension;people comprehension;social networking tools;software program;software products;human aspects","","1","","9","","","","","IEEE","IEEE Conferences"
"Code complexity estimation for Java programs","M. Simon; Z. Porkoláb; G. Horváth","Department of Programming Languages and Compilers, Eötvös Loránd University, Pázmány Péter sétány 1/C H-1117 Budapest, Hungary; Department of Programming Languages and Compilers, Eötvös Loránd University Pázmány Péter sétány 1/C H-1117 Budapest, Hungary; Department of Programming Languages and Compilers, Eötös Loránd University, Pázmány Péter sétány 1/C H-1117 Budapest, Hungary","2015 IEEE 13th International Scientific Conference on Informatics","","2015","","","232","235","Estimating algorithmic complexity of complex software systems is important as based on such estimations we can approximate certain run-time properties of the software, like worst-case execution time or latency of critical components. These estimations are also necessary e.g. to plan with the required hardware capacity to run the software. While software design usually deals with algorithmic complexity, the actual implementation may vary from the plans. In case of industry-scale complex systems such differences are very hard to detect with manual code inspections. In this paper we propose a new approach for source level worst-case time estimation for Java programs. Knowing the presumed complexity, we try to prove whether the worst-case execution of the program suits the complexity requirements. These requirements are given in a specific DSL embedded into the code. When the supposed upper limit does not hold we identify the place in the source code where the assumption failed and emit diagnostic messages. Thus the programmer gets hint what went wrong and how to fix the problem (if possible). Complexity requirements may depend on parameters, e.g. the size of a data structure, etc., therefore our DSL interacts with the source code. The DSL is given in form of Java attributes, this way our solution is highly portable. We implemented a proof of concept prototype version of the idea and we show our results.","","978-1-4673-9868-8978-1-4673-9867","10.1109/Informatics.2015.7377838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377838","","Complexity theory;Estimation;Java;DSL;Dispersion;Program processors","computational complexity;Java;program diagnostics;source code (software)","Java programs;code complexity estimation;algorithmic complexity;complex software systems;worst-case execution time;critical components;software run-time properties;software design;source level worst-case time estimation;complexity requirements;DSL;source code;diagnostic messages;Java attributes","","","","7","","","","","IEEE","IEEE Conferences"
"Reduced-Complexity Linear Programming Decoding Based on ADMM for LDPC Codes","H. Wei; X. Jiao; J. Mu","NA; NA; NA","IEEE Communications Letters","","2015","19","6","909","912","The Euclidean projection onto check polytopes is the most time-consuming operation in the linear programming (LP) decoding based on alternating direction method of multipliers (ADMM) for low-density parity-check (LDPC) codes. In this letter, instead of reducing the complexity of Euclidean projection itself, we propose a new method to reduce the decoding complexity of ADMM-based LP decoder by decreasing the number of Euclidean projections. In particular, if all absolute values of the element-wise differences between the input vector of Euclidean projection in the current iteration and that in the previous iteration are less than a predefined value, then the Euclidean projection at the current iteration will be no longer performed. Simulation results show that the proposed decoder can still save roughly 20% decoding time even if both the over-relaxation and early termination techniques are used.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2418261","National Nature Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073564","Linear programming decoding;alternating direction method of multipliers (ADMM);reduce-complexity;Linear programming decoding;alternating direction method of multipliers (ADMM);reduce-complexity","Decoding;Standards;Signal to noise ratio;Vectors;Iterative decoding;Complexity theory","computational complexity;computational geometry;iterative methods;linear programming;parity check codes","reduced-complexity linear programming decoding;ADMM-based LP decoder;LDPC codes;low-density parity-check codes;Euclidean projection;check polytopes;alternating direction method-of-multipliers;absolute values;current iteration;decoding time;early termination technique;over-relaxation technique;decoding complexity reduction","","17","","9","","","","","IEEE","IEEE Journals & Magazines"
"A Code Complexity Model of Object Oriented Programming (OOP)","H. Hourani; H. Wasmi; T. Alrawashdeh","Faculty of Science and IT, Al Zaytoonah University of Jordan, Amman, Jordan; Faculty of Science and IT, Al Zaytoonah University of Jordan, Amman, Jordan; Faculty of Science and IT, Al Zaytoonah University of Jordan, Amman, Jordan","2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT)","","2019","","","560","564","The Code Complexity and Object Oriented Programming (OOP) is an import topic due to the role of OOP playing in most of the software design and architectures nowadays. In OOP there are key design concepts like Encapsulation, Polymorphisms and Inheritance that affect the coding design, structure and style. The challenge is how to minimize the Complexity in OOP and complying with the key concepts of OOP design. This paper reviews the literature on current solutions for code complexity and proposes a new model for OOP code complexity. The new model has added into OOP complexity metrics the following characteristics: abstraction and class details complexity. The proposed model is based on the following attributes selection criteria: Readability, Understandability, Maintainability, Reusability, Extensibility and Consistency of the programming code.","","978-1-5386-7942-5978-1-5386-7941-8978-1-5386-7943","10.1109/JEEIT.2019.8717448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717448","Code complexity;Object Oriented Programming;OOP;OOP Metrics;2O2C Metrics","Complexity theory;Object oriented modeling;Couplings;Object oriented programming;Software;Size measurement","","","","","","19","","","","","IEEE","IEEE Conferences"
"Low-Complexity Software Stack Decoding of Polar Codes","H. Aurora; C. Condo; W. J. Gross","McGill University, Department of Electrical and Computer Engineering, Montreal, Quebec, Canada; McGill University, Department of Electrical and Computer Engineering, Montreal, Quebec, Canada; McGill University, Department of Electrical and Computer Engineering, Montreal, Quebec, Canada","2018 IEEE International Symposium on Circuits and Systems (ISCAS)","","2018","","","1","5","Polar codes are a recent class of linear error-correcting codes that asymptotically achieve the channel capacity at infinite code length. The Successive Cancellation List (SCL) algorithm yields very good error-correction performance, at the cost of high implementation complexity. The Stack (SCS) decoding algorithm provides similar error-correction performance at a lower complexity. In this work, we propose an efficient software implementation of the SCS decoding algorithm, along with techniques to further reduce its computational complexity. In particular, we reduce the SCS memory requirements through efficient path switching, replace the stack sorting with a linear search, and explore the use of a partial CRC along with an early termination criterion. Using the proposed methods, we are able to reduce the computational complexity of the SCS decoder, reducing the number of estimated bits up to 97% with respect to SCL, while maintaining similar error-correction performance as SCL.","2379-447X","978-1-5386-4881-0978-1-5386-4882","10.1109/ISCAS.2018.8351832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8351832","","Indexes;Decoding;Complexity theory;Switches;Software;Arrays;Measurement","channel capacity;channel coding;computational complexity;decoding;error correction codes;search problems","channel capacity;SCL;SCS decoding algorithm;computational complexity;stack sorting;linear search;low-complexity software Stack decoding;polar codes;linear error-correcting codes;successive cancellation list algorith","","1","","15","","","","","IEEE","IEEE Conferences"
"A survey on impact of lines of code on software complexity","S. Bhatia; J. Malhotra","Dept. of CSE, GNDU, RC, jalandhar, India; Dept. of CSE, GNDU, RC, Jalandhar, India","2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014)","","2014","","","1","4","Size is one of the important attributes of a software product. Lines of Code (SLOC or LOC) is one of the most widely used sizing metrics in industry. The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. Estimation of effort is complicated and challenging task in software industry. Project size is a measure of problem complexity in terms of effort and time required to develop the products. This paper mainly focuses on Impact of LOC on complexity. SLOC is typically used to predict the amount of effort that will be required to develop a program, as well as to estimate programming productivity or complexity once the software is produced.","2347-9337","978-1-4799-6393-5978-1-4799-6392","10.1109/ICAETR.2014.7012875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012875","Complexity;Effort;Lines of code component","Software;Complexity theory;Estimation;Software measurement;Size measurement;Biological system modeling","computational complexity;software metrics;source code (software)","software complexity;lines of code;SLOC;software system;source code;software industry;project size;problem complexity;programming productivity","","2","","13","","","","","IEEE","IEEE Conferences"
"Normalizing source code vocabulary to support program comprehension and software quality","L. Guerrouj","DGIGL - SOCCER Lab, Ptidej Team, Polytechnique Montr&#x00E9;al, Qu&#x00E9;bec, Canada","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","1385","1388","The literature reports that source code lexicon plays a paramount role in program comprehension, especially when software documentation is scarce, outdated or simply not available. In source code, a significant proportion of vocabulary can be either acronyms and-or abbreviations or concatenation of terms that can not be identified using consistent mechanisms such as naming conventions. It is, therefore, essential to disambiguate concepts conveyed by identifiers to support program comprehension and reap the full benefit of Information Retrieval-based techniques (e.g., feature location and traceability) whose linguistic information (i.e., source code identifiers and comments) used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. To this aim, we propose source code vocabulary normalization approaches that exploit contextual information to align the vocabulary found in the source code with that found in other software artifacts. We were inspired in the choice of context levels by prior works and by our findings. Normalization consists of two tasks: splitting and expansion of source code identifiers. We also investigate the effect of source code vocabulary normalization approaches on software maintenance tasks. Results of our evaluation show that our contextual-aware techniques are accurate and efficient in terms of computation time than state of the art alternatives. In addition, our findings reveal that feature location techniques can benefit from vocabulary normalization when no dynamic information is available.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606723","Source code linguistic analysis;information retrieval;program comprehension;software quality","Context;Vocabulary;Software quality;Software maintenance;Dictionaries","reverse engineering;software maintenance;software quality;system documentation;ubiquitous computing;vocabulary","software quality;source code lexicon;program comprehension;software documentation;information retrieval-based techniques;linguistic information;source code comments;software artifacts;source code vocabulary normalization approaches;contextual information;context levels;source code identifiers splitting;source code identifiers expansion;software maintenance tasks;contextual-aware techniques;feature location","","5","","26","","","","","IEEE","IEEE Conferences"
"The Role of Method Chains and Comments in Software Readability and Comprehension—An Experiment","J. Börstler; B. Paech","Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, Heidelberg University, Heidelberg, Germany","IEEE Transactions on Software Engineering","","2016","42","9","886","898","Software readability and comprehension are important factors in software maintenance. There is a large body of research on software measurement, but the actual factors that make software easier to read or easier to comprehend are not well understood. In the present study, we investigate the role of method chains and code comments in software readability and comprehension. Our analysis comprises data from 104 students with varying programming experience. Readability and comprehension were measured by perceived readability, reading time and performance on a simple cloze test. Regarding perceived readability, our results show statistically significant differences between comment variants, but not between method chain variants. Regarding comprehension, there are no significant differences between method chain or comment variants. Student groups with low and high experience, respectively, show significant differences in perceived readability and performance on the cloze tests. Our results do not show any significant relationships between perceived readability and the other measures taken in the present study. Perceived readability might therefore be insufficient as the sole measure of software readability or comprehension. We also did not find any statistically significant relationships between size and perceived readability, reading time and comprehension.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2016.2527791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404062","Software readability;software comprehension;software measurement;comments;method chains;experiment","Software;Guidelines;Software measurement;Software engineering;Programming;Complexity theory;Object oriented modeling","software maintenance;software metrics","software readability;software comprehension;software maintenance;software measurement;method chains;code comments;cloze tests","","3","","57","","","","","IEEE","IEEE Journals & Magazines"
"Automatically Assessing Code Understandability","S. Scalabrino; G. Bavota; C. Vendome; M. Linares-V?squez; D. Poshyvanyk; R. Oliveto","Biosciences and Territory, Universita degli Studi del Molise, 18960 Pesche, IS Italy (e-mail: simone.scalabrino@unimol.it); Faculty of Informatics, Universita della Svizzera Italiana, 27216 Lugano, Lugano Switzerland 6904 (e-mail: gabriele.bavota@usi.ch); Computer Science and Software Engineering, Miami University, 6403 Oxford, Ohio United States (e-mail: cgvendome@email.wm.edu); Systems Engineering and Computing, Universidad de los Andes, 27991 Bogota, Bogota Colombia (e-mail: m.linaresv@uniandes.edu.co); Computer Science Department, College of William and Mary, 8604 Williamsburg, Virginia United States (e-mail: denys@cs.wm.edu); Biosciences and Territory, Universita degli Studi del Molise, 18960 Pesche, IS Italy (e-mail: rocco.oliveto@unimol.it)","IEEE Transactions on Software Engineering","","2019","PP","99","1","1","Understanding software is an inherent requirement for many maintenance and evolution tasks. Without a thorough understanding of the code, developers would not be able to fix bugs or add new features timely. Measuring code understandability might be useful to guide developers in writing better code, and could also help in estimating the effort required to modify code components. Unfortunately, there are no metrics designed to assess the understandability of code snippets. In this work, we perform an extensive evaluation of 121 existing as well as new code-related, documentation-related, and developer-related metrics. We try to (i) correlate each metric with understandability and (ii) build models combining metrics to assess understandability. To do this, we use 444 human evaluations from 63 developers and we obtained a bold negative result: none of the 121 experimented metrics is able to capture code understandability, not even the ones assumed to assess quality attributes apparently related, such as code readability and complexity. While we observed some improvements while combining metrics in models, their effectiveness is still far from making them suitable for practical applications. Finally, we conducted interviews with five professional developers to understand the factors that influence their ability to understand code snippets, aiming at identifying possible new metrics.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2019.2901468","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651396","Software metrics;Code understandability;Empirical study;Negative result","Complexity theory;Software;Computer bugs;Readability metrics;Software measurement;Indexes","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Supporting program comprehension with program summarization","Y. Liu; X. Sun; X. Liu; Y. Li","School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China","2014 IEEE/ACIS 13th International Conference on Computer and Information Science (ICIS)","","2014","","","363","368","A large amount of software maintenance effort is spent on program comprehension. How to accurately and quickly get the functional features in a program becomes a hot issue in program comprehension. Some studies in this area are focused on extracting the topics by analyzing linguistic information in the source code based on the textual mining techniques. However, the extracted topics are usually composed of some standalone words and difficult to understand. In this paper, we attempt to solve this problem based on a novel program summarization technique. First, we propose to use latent semantic indexing and clustering to group source artifacts with similar vocabulary to analyze the composition of each package in the program. Then, some topics composed of a vector of independent words can be extracted based on latent semantic indexing. Finally, we employ Minipar, a nature language parser, to help generate the summaries. The summaries can effectively organize the words from the topics in the form of the predefined sentence based on some rules. With such form of summaries, developers can understand what the features the program has and their corresponding source artifacts.","","978-1-4799-4860","10.1109/ICIS.2014.6912159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912159","","Pragmatics;Semantics;Large scale integration;Matrix decomposition;Indexing;Software systems","data mining;grammars;indexing;pattern clustering;software maintenance;source code (software);text analysis;vocabulary","program comprehension;software maintenance effort;functional features;topic extraction;linguistic information analysis;source code;textual mining techniques;program summarization technique;latent semantic indexing;program summarization technique;latent semantic clustering;source artifacts;independent word vector extraction;Minipar;language parser","","2","","20","","","","","IEEE","IEEE Conferences"
"Using wolfram software to improve reading comprehension in mathematics for software engineering students","L. R. Barba-Guaman; P. A. Quezada-Sarmiento; C. A. Calderon-Cordova; A. M. Sarmiento-Ochoa; L. Enciso; T. S. Luna-Briceno; L. E. Conde-Zhingre","Universidad Técnica Particular de Loja, Departamento de Ciencias de la Computación y Electrónica, Loja- Ecuador; Universidad Internacional del Ecuador, Escuela de Informática y Multimedia - Ingeniería en Tecnologías de la Información y Comunicación, Quito- Ecuador; Universidad Técnica Particular de Loja, Departamento de Ciencias de la Computación y Electrónica, Loja- Ecuador; Universidad Nacional de Loja, Facultad de la Educación, el Arte y la Comunicación, Loja- Ecuador; Universidad Técnica Particular de Loja, Departamento de Ciencias de la Computación y Electrónica, Loja- Ecuador; Universidad Internacional del Ecuador, Escuela de Derecho, Quito - Ecuador; Universidad Internacional del Ecuador, Escuela de Informática y Multimedia - Ingeniería en Tecnologías de la Información y Comunicación, Quito- Ecuador","2018 13th Iberian Conference on Information Systems and Technologies (CISTI)","","2018","","","1","4","The higher education through virtual environment is students centered oriented to the interactive learning, in situations to be closer to the real world; for that reason, teachers must have new communicative competence. students centered oriented to the interactive learning, in situations to be closer to the real world. This paper shows the use of Wolfram software as a supplementary educational resource in the process of reading comprehension in mathematics, in the first, and third semester of Information and Computer System Engineering as part of process of formation of future engineering. Mathematics is a fundamental subject in the normative process of educating the future professionals. For example, this can be applied by using the technology, and software by Wolfram. Students from experimental groups used together with the teacher the software Wolfram Mathematica. The experience in this research by teacher shows the necessity of include this math package as supplementary educational resource, which support the capacity of calculus, and interpretation of the non-trivial problems.","","978-989-98434-8-6978-1-5386-4885","10.23919/CISTI.2018.8399388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399388","Education;Mathematics;Mathematica wolfram software","Software;Tungsten;Calculus;Training;Tools","computer aided instruction;computer science education;educational courses;further education;mathematics computing;software engineering;symbol manipulation","Wolfram software;mathematics;software engineering students;higher education;virtual environment;interactive learning;communicative competence;supplementary educational resource;Computer System Engineering;normative process;software Wolfram Mathematica","","1","","30","","","","","IEEE","IEEE Conferences"
"Applying cognitive load theory to generate effective programming tutorials","K. J. Harms","Department of Computer Science &amp; Engineering, Washington University in St. Louis, St. Louis, Missouri, United States","2013 IEEE Symposium on Visual Languages and Human Centric Computing","","2013","","","179","180","In this paper, the author describe our prior work on how automatically generating walk-through tutorials helps novice programmers learn new programming concepts found in unfamiliar code. Following this, the author introduce his proposal to use Cognitive Load Theory to improve the effectiveness of learning new programming concepts from generated tutorials. To accomplish this, the author propose automatically generating personalized tutorials based on a user's programming expertise.","1943-6092;1943-6106","978-1-4799-0369","10.1109/VLHCC.2013.6645274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645274","","Tutorials;Programming profession;Current measurement;Load modeling;Educational institutions;Programming environments","cognition;computer science education;programming","cognitive load theory;programming tutorials;walk-through tutorial generation;programming concepts learning","","2","","10","","","","","IEEE","IEEE Conferences"
"qLow complexity LDPC coded IB-DFE for multilevel modulations and coded OFDM: comparison and complexity trade-offs","P. Montezuma; D. Marques; R. Dinis","DEE, FCT Universidade Nova de Lisboa, Portugal; DEE, FCT Universidade Nova de Lisboa, Portugal; DEE, FCT Universidade Nova de Lisboa, Portugal","2014 International Conference on Telecommunications and Multimedia (TEMU)","","2014","","","179","184","In this paper, we propose IB-DFE receiver (Iterative Block Decision Feedback Equalization) with iterative SISO LDPC decoding (soft-input soft-output - Low Density Parity Code) suitable for SC-FDE (Single- Carrier with Frequency-Domain Equalization) with multilevel modulations. This scheme can be implemented in a simple way using an analytical characterization where the any multilevel constellation is represented as a sum of BPSK (Bi-Phase Shift Keying) sub-constellations. This decomposition also allows energy efficient amplification compatible with grossly nonlinear amplifiers where BPSK component is amplified independently. The proposed system is compared with LDPC-coded OFDM (Orthogonal Frequency Division Multiplexing) with similar complexity. It is shown that the proposed system allows significant improvements on error performance without significant increase on decoding process' complexity. The simulation results show that the iterative equalization together with the iterative decoding of LDPC improves the performance significantly when compared with uncoded schemes, even for small block sizes. In addition, the proposed coded IB-DFE scheme outperforms coded OFDM and has similar performance to uncoded IB-DFE even for lower number of iterations in the iterative equalization process, which can contribute for reductions in the computational load at the receiver's side.","","978-1-4799-3200","10.1109/TEMU.2014.6917757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6917757","Multilevel constellation;SC-FDE with offset modulations;LDPC decoding;coded OFDM;computational load","OFDM;Receivers;Decoding;Iterative decoding;Transmitters;Phase shift keying","decision feedback equalisers;iterative decoding;OFDM modulation;parity check codes;phase shift keying","multilevel modulations;coded OFDM;IB DFE receiver;iterative block decision feedback equalization;iterative SISO LDPC decoding;soft input soft output low density parity code;SC FDE;single carrier with frequency domain equalization;BPSK;biphase shift keying;energy efficient amplification;orthogonal frequency division multiplexing","","","","12","","","","","IEEE","IEEE Conferences"
"Polar Codes for Low-Complexity Forward Error Correction in Optical Access Networks","Z. Wu; B. Lankl","NA; NA","Photonic Networks; 15. ITG Symposium","","2014","","","1","8","Optical communication systems operating at high data rates require forward error correction (FEC) to achieve reliable communication at improved power efficiency. However, telecommunications equipment in optical access networks can only accommodate restricted hardware complexity. In this contribution we study Polar codes, a recently proposed class of error-correcting codes with advantageous properties in terms of their structure, complexity and design method. We investigate if Polar codes can compete with existing ITU G.975.1 standard FEC schemes and identify the challenges which need to be tackled when implementing hardware decoders.","","978-3-8007-3604","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839983","","","","","","","","","","","","","VDE","VDE Conferences"
"On planning an evaluation of the impact of identifier names on the readability and quality of smalltalk programs","M. Lungu; J. Kurš","Software Composition Group University of Bern Switzerland; Software Composition Group University of Bern Switzerland","2013 2nd International Workshop on User Evaluations for Software Engineering Researchers (USER)","","2013","","","13","15","One of the long running debates between programmers is whether camelCaseldentifiers are better than underscore_identifiers. This is ultimately a matter of programming language culture and personal taste, and to our best knowledge none of the camps has won the argument yet. It is our intuition that a solution exists which is superior to both the previous ones from the point of view of usability: the solution we name sentence case identifiers allows phrases as nams for program entities such as classes or methods. In this paper we propose a study in which to evaluate the impact of sentence case identifiers in practice.","","978-1-4673-6433","10.1109/USER.2013.6603079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603079","user evaluation;programming languages;empirical software engineering","Computer languages;Syntactics;Programming;Grammar;Conferences;Planning;Software","object-oriented programming;planning (artificial intelligence);Smalltalk","planning;identifier names;readability;smalltalk programs;camelCaseldentifiers;underscore_identifiers;programming language","","1","","4","","","","","IEEE","IEEE Conferences"
"Constructing a Novel Chinese Readability Classification Model Using Principal Component Analysis and Genetic Programming","Y. Lee; H. Tseng; J. Chen; C. Peng; T. Chang; Y. Sung","NA; NA; NA; NA; NA; NA","2012 IEEE 12th International Conference on Advanced Learning Technologies","","2012","","","164","166","The studies of readability aim to measure the level of text difficulty. Although traditional formulae such as the Flesch-Kincaid formula can properly predict text readability, they are only effective for English text. Other formulae with very few features may result in inaccurate text classification. The study takes into account multiple linguistic features, and attempts to increase the level of accuracy in text classification by adopting a new model which integrates Principal Component Analysis (PCA) with Genetic Programming (GP). Empirical data are utilized to demonstrate the performance of the proposed model.","2161-3761;2161-377X","978-1-4673-1642-2978-0-7695-4702","10.1109/ICALT.2012.134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6268065","Readability;Principal component analysis;Genetic programming;Text analysis component","Predictive models;Principal component analysis;Mathematical model;Educational institutions;Genetic programming;Psychology;Support vector machines","genetic algorithms;natural language processing;pattern classification;principal component analysis;text analysis","novel Chinese readability classification model;principal component analysis;genetic programming;Flesch-Kincaid formula;text readability;English text;multiple linguistic features;text classification;GP;PCA","","3","","17","","","","","IEEE","IEEE Conferences"
"Reordering Program Statements for Improving Readability","Y. Sasaki; Y. Higo; S. Kusumoto","NA; NA; NA","2013 17th European Conference on Software Maintenance and Reengineering","","2013","","","361","364","In order to understand source code, humans sometimes execute the program in their mind. When they illustrate the program execution in their mind, it is necessary to memorize what values all the variables are along with the execution. If there are many variables in the program, it is hard to their memorization. However, it is possible to ease to memorize them by shortening the distance between the definition of a variable and its reference if they are separated in the source code. This paper proposes a technique reordering statements in a module by considering how far the definition of a variable is from its references. We applied the proposed technique to a Java OSS and collected human evaluations for the reordered methods. As a result, we could confirm that the reordered methods had better readability than their originals. Moreover, we obtained some knowledge of human consideration about the order of statements.","1534-5351","978-0-7695-4948-4978-1-4673-5833","10.1109/CSMR.2013.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498489","source code readability;source code analysis;software tool;arranging program statements","Java;Correlation;Software maintenance;Educational institutions;Software metrics;Europe","Java;public domain software;software maintenance","program statement reordering;program execution;program memorization;Java OSS;open-source software;software maintainance;source code readability","","2","","7","","","","","IEEE","IEEE Conferences"
"Proposal of multi-value cell structure for high-density two-dimensional codes and evaluation of readability using smartphones","N. Teraura; K. Sakurai","Terrara Code Research Institute, Tokai, Japan; Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan","2015 7th International Conference on New Technologies, Mobility and Security (NTMS)","","2015","","","1","5","In the now-popular two-dimensional code, bits are expressed using white and two black colors in the cell, which forms the element. However, such codes do not have confidentiality. Since confidentiality may be needed according to the use, a large-capacity and high-density two-dimensional code with compatibility and confidentiality can be realized by adding a secrecy part in addition to the existing part. To enlarge capacity, it is necessary to form a cell using many bits, which requires a multicolor method. The read verification of the compatible area and the additional area was carried out using a smartphone, and the code was verified to confirm its practicality.","2157-4952;2157-4960","978-1-4799-8784-9978-1-4799-8783","10.1109/NTMS.2015.7266514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266514","2d code;compatibility;confidentiality;identity;security;smartphone","Image color analysis;Smart phones;Software;Proposals;Image coding;Brightness","codes;smart phones","high-density two-dimensional code;multicolor method;smartphones;high density 2D codes;multivalue cell structure","","1","","13","","","","","IEEE","IEEE Conferences"
"Using Eye Tracking Technology to Analyze the Impact of Stylistic Inconsistency on Code Readability","Q. Mi; J. Keung; J. Huang; Y. Xiao","NA; NA; NA; NA","2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)","","2017","","","579","580","A number of research efforts have focused in the area of programming style. However, to the best of our knowledge, there is little sound and solid evidence of how and to what extent can stylistic inconsistency impact the readability and maintainability of the source code. To bridge the research gap, we design an empirical experiment in which eye tracking technology is introduced to quantitatively reflect developers' cog.nitive efforts and mental processes when encountering the inconsistency issue.","","978-1-5386-2072-4978-1-5386-2073","10.1109/QRS-C.2017.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004381","programming style;stylistic inconsistency;eye tracking technology;code readability;program comprehension","Gaze tracking;Visualization;Bridges;Software engineering;Indexes;Programming profession","software maintenance;source code (software)","eye tracking technology;stylistic inconsistency;code readability;programming style;source code;research gap;cognitive efforts;mental processes;inconsistency issue;maintainability","","","","3","","","","","IEEE","IEEE Conferences"
"Novel Repair-by-Transfer Codes and Systematic Exact-MBR Codes with Lower Complexities and Smaller Field Sizes","S. Lin; W. Chung","Research Center for Information Technology Innovation, Academia Sinica, Taipei City, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei City, Taiwan","IEEE Transactions on Parallel and Distributed Systems","","2014","25","12","3232","3241","The (n, k, d) regenerating code is a class of (n, k) erasure codes with the capability to recover a lost code fragment from other d existing code fragments. In this paper, we focus on the design of exact regenerating codes at minimum bandwidth regenerating (MBR) points. For d = n - 1, a class of (n, k, d = n - 1) exact-MBR codes, termed as repair-by-transfer codes, have been developed in prior work to avoid arithmetic operations in node repairing process. The first result of this paper presents a new class of repair-by-transfer codes via congruent transformations. As compared with prior works, the advantages of proposed codes include: i) the minimum of field size is significantly reduced from (<sub>2</sub><sup>n</sup>) ton; ii) the encoding complexity is decreased from n<sup>4</sup> to n<sup>3</sup>. Our simulation results 2 show that the proposed code achieves faster operations than the prior approach does under large n. The second result of this paper presents a new form of coding matrix for product-matrix exact-MBR codes. The proposed coding matrix includes the following advantages: i) the minimum of finite field size is reduced from n - k + d ton; ii) the fast Reed-Solomon erasure coding algorithms can be applied on the proposed exact-MBR codes to reduce the time complexities.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2013.2297109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714425","Distributed storage;maximum-distance-separable (MDS) codes;partial downloading;Reed-Solomon codes;repair-by-transfer","Encoding;Symmetric matrices;Systematics;Vectors;Maintenance engineering;Decoding;Reed-Solomon codes","computational complexity;Reed-Solomon codes","repair-by-transfer codes;systematic exact-MBR codes;erasure codes;code fragment;exact regenerating codes;minimum bandwidth regenerating points;MBR points;arithmetic operations;node repairing process;congruent transformations;encoding complexity;coding matrix;product matrix exact-MBR codes;fast Reed-Solomon erasure coding algorithms;time complexities","","12","","28","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity secure network coding against wiretapping using intra/inter-generation coding","G. Liu; B. Liu; X. Liu; F. Li; W. Guo","School of Information Engineering, Xi'an University, Xi'an 710065, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an 710071, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an 710071, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an 710071, Shaanxi, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an 710071, Shaanxi, China","China Communications","","2015","12","6","116","125","Existing solutions against wiretapping attacks for network coding either bring significant bandwidth overhead or incur a high computational complexity. In order to reduce the security overhead of the existing solutions for securing network coding, a novel securing network coding paradigm is presented relying on two coding models: intra-generation coding and inter-generation coding. The basic idea to secure network coding using intra-generation coding is to limit the encryption operations for each generation, and then subject the scrambled and the remaining original source vectors to a linear transformation. This method is then generalized seamlessly using inter-generation coding by further exploiting the algebraic structure of network coding. We show that the proposed schemes have properties of low-complexity security, little bandwidth consumption, and high efficiency in integrating with the existing security techniques effectively.","1673-5447","","10.1109/CC.2015.7122470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122470","network coding; security; wiretappingattacks; algebraic coding; encryption","Network coding;Encoding;Encryption;Receivers;Bandwidth","cryptography;network coding","low-complexity secure network coding;intrageneration coding;intergeneration coding;wiretapping attack;coding model;linear transformation","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Improving the accuracy of the code complexity calculation for automatically generated tasks with programming codes","E. Stankov; M. Jovanov; J. Andonov; A. M. Bogdanova","Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje, Macedonia; Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje, Macedonia; Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje, Macedonia; Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje, Macedonia","2016 IEEE Global Engineering Education Conference (EDUCON)","","2016","","","686","692","Teaching programming is an activity that becomes more and more popular. Assessment of students that attend introductory courses in programming can partly be done by presenting simple source code fragments. Students should be able to provide answer to the question: ""What is the output of the given code?"" When preparing the code segments, teachers should be aware of the complexity ('weight') of the code. Especially, when preparing many versions of the same test (to assess a huge number of students), they should try to provide same or similar complexity tasks for all students. A possible solution to this problem is to provide automatic generation of questions containing source code segments. In order to achieve complexity consistency in the process of automatic production of questions for programming courses, there must be a way to automatically measure the complexity of source codes. In our previous work, we have defined a source code metric that considers the source code complexity from a perspective of the student's effort required for manual calculation of the program output, if the input is known. The metric measures the complexity using user-specified weight values assigned to each of the operators and branch statements in the code. In this paper we present a new tool that will help improve the accuracy of the code complexity calculation for automatically generated tasks containing source codes. We also describe our preliminary findings from the research that we have conducted using this tool in order to determine appropriate weight values, and provide remarks for the future experiments on the subject.","2165-9567","978-1-4673-8633-3978-1-4673-8632","10.1109/EDUCON.2016.7474624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474624","source code complexity calculation;automatic generation of tasks containing source codes","Complexity theory;Testing;Programming profession;Measurement;Production;Databases","computer science education;educational courses;programming;teaching","source code complexity calculation;programming codes;programming teaching;student assessment;programming course;source code fragments;question generation;user-specified weight values","","","","9","","","","","IEEE","IEEE Conferences"
"Trend analysis on the metadata of program comprehension papers","M. Sulír; J. Porubän","Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Slovakia; Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Slovakia","2015 13th International Conference on Engineering of Modern Electric Systems (EMES)","","2015","","","1","4","As program comprehension is a vast research area, it is necessary to get an overview of its rising and falling trends. We performed an n-gram frequency analysis on titles, abstracts and keywords of 1885 articles about program comprehension from the years 2000-2014. According to this analysis, the most rising trends are feature location and open source systems, the most falling ones are program slicing and legacy systems.","","978-1-4799-7650-8978-1-4799-7649-2978-1-4799-7648","10.1109/EMES.2015.7158425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158425","Program comprehension;bibliography;trends;n-grams","Market research;Software;IEEE Xplore;Bibliographies;Software engineering;Visualization","meta data;program slicing;software maintenance","trend analysis;meta data;program comprehension;n-gram frequency analysis;program slicing;legacy systems","","","","12","","","","","IEEE","IEEE Conferences"
"IMMV: An interactive multi-matrix visualization for program comprehension","A. Abuthawabeh; D. Zeckzer","TU Kaiserslautern, Germany; TU Kaiserslautern, Germany","2013 First IEEE Working Conference on Software Visualization (VISSOFT)","","2013","","","1","4","Many visualization techniques are used by software engineers to understand and to analyze the static structure of software systems, with the static structure being extracted from the source code. However, the need for scalable visualizations, which take into account the increasing number of code entities (classes and interfaces) and different types of code couplings (relations between them) in software systems, is still not sufficiently fulfilled. In this paper, we extend the design of an existing multi-matrix visualization approach to represent the static structure of software systems in a scalable way. First, we extended the data model and the algorithms. Second, we added more visualization and interaction elements. Finally, we incorporated the folding (collapsing) and the unfolding (expanding) of the package hierarchy, which have quadratic time complexity and quadratic space complexity in the number of nodes. This extended approach can be applied to support program comprehension and to analyze the static structure of software systems.","","978-1-4799-1457","10.1109/VISSOFT.2013.6650549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650549","","Visualization;Software systems;Couplings;Algorithm design and analysis;Color;Complexity theory","computational complexity;data visualisation;matrix algebra;program diagnostics","IMMV;interactive multimatrix visualization;program comprehension;visualization techniques;software engineers;source code;scalable visualizations;code entities;code couplings;static software systems structure;interaction elements;visualization elements;package hierarchy;quadratic time complexity;quadratic space complexity","","4","","13","","","","","IEEE","IEEE Conferences"
"Artifact Driven Communication to Improve Program Comprehension","J. Kubelka","NA","2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)","","2017","","","457","460","Developer communication is an important factor during program comprehension. Live programming environments encourage developers to comprehend applications through manipulation of running instances-liveness. Such application exploration is interrupted whenever programmers need to communicate an issue with dislocated co-workers. Describing the issue becomes challenging as programmers use text based communication mediums, e.g., emails or chats. The issues are magnified during bug day events, where developers around the world meet together in order to improve a project. Communication and coordination need to be increased, but the infrastructure stays the same. We target the research gap by introducing the COLLABORATIVE PLAYGROUND, a tool that exposes liveness into developer communication during programming change tasks and integrates coordination needs during bug day events. We will evaluate our approach by deploying the COLLABORATIVE PLAYGROUND during a series of Bug Day events for Pharo, one of the most active live programming platforms in existence.","","978-1-5386-1589-8978-1-5386-1590","10.1109/ICSE-C.2017.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965383","Collaborative Programming;Live Programming Environment;Qualitative Study","Computer bugs;Collaboration;Tools;Programming;Libraries;Measurement;Electronic mail","groupware;program debugging","program comprehension;running instance manipulation;text based communication mediums;bug day events;collaborative playground;liveness;developer communication;programming change tasks;coordination needs integration;Pharo;live programming platforms;artifact driven communication","","","","13","","","","","IEEE","IEEE Conferences"
"Program comprehension: A method of generating visualized UML class diagram","GuHui; WangHui","College of Computer Science and Technology, Zhejiang University of Technology, ZJUT, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, ZJUT, Hangzhou, China","The 2nd International Conference on Information Science and Engineering","","2010","","","6775","6776","Program comprehension is an important research content of software engineering. This paper presents a Program comprehension visualization method of using UML class diagram. This method has two parts, first, The program source code is abstracted into database table, the form used to summarize and express program structure and key information, Then, the table was transformed into UML class diagrams, The class diagram can be effective to help understanding program structure and other key information.","2160-1283;2160-1291","978-1-4244-7618-3978-1-4244-7616-9978-1-4244-7617","10.1109/ICISE.2010.5690312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5690312","program comprehension;table abstract;UML class diagram;visualization","Unified modeling language;Visualization;Software engineering;Educational institutions;Software maintenance","","","","","","","","","","","IEEE","IEEE Conferences"
"Comprehending Studies on Program Comprehension","I. Schröter; J. Krüger; J. Siegmund; T. Leich","NA; NA; NA; NA","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","","2017","","","308","311","Program comprehension is an important aspect of developing and maintaining software, as programmers spend most of their time comprehending source code. Thus, it is the focus of many studies and experiments to evaluate approaches and techniques that aim to improve program comprehension. As the amount of corresponding work increases, the question arises how researchers address program comprehension. To answer this question, we conducted a literature review of papers published at the International Conference on Program Comprehension, the major venue for research on program comprehension. In this article, we i) present preliminary results of the literature review and ii) derive further research directions. The results indicate the necessity for a more detailed analysis of program comprehension and empirical research.","","978-1-5386-0535-6978-1-5386-0536","10.1109/ICPC.2017.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961527","Systematic Review;Study Comprehension;Empirical Research","Terminology;Bibliographies;Documentation;Software;Guidelines;Context;Testing","software maintenance;source code (software)","source code;program comprehension;software maintenance;software development","","","","22","","","","","IEEE","IEEE Conferences"
"Workshop on Program Comprehension through Dynamic Analysis (PCODA10)","A. Hamou-Lhadj; D. Rothlisberger; A. Zaidman; O. Greevy","NA; NA; NA; NA","2010 17th Working Conference on Reverse Engineering","","2010","","","279","280","Applying program comprehension techniques may render software maintenance and evolution easier. Understanding a software system typically requires a combination of static and dynamic analysis techniques. The aim of this workshop is to bring together researchers and practitioners working in the area of program comprehension with an emphasis on dynamic analysis. We are interested in investigating how dynamic analysis techniques are used or can be used to enable better comprehension of a software system. The objective is to compare existing techniques, and to identify common case studies and possible symbioses for existing solutions. Building upon four previous editions of the workshop, we aim to set up a forum for exchanging experiences, discussing solutions, and exploring new ideas.","2375-5369;1095-1350","978-1-4244-8911","10.1109/WCRE.2010.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645500","","Conferences;Performance analysis;Reverse engineering;IEEE Computer Society;Software systems;Software maintenance;Special issues and sections","","","","","","8","","","","","IEEE","IEEE Conferences"
"A study on the impact of emotional quotient on program comprehension","A. Savarimuthu; L. Arockiam; A. Aloysius","Human resource Management, St. Joseph's College, Tiruchirappalli - 2, India; Computer Science, St. Joseph's College, Tiruchirappalli - 2, India; Computer Science, St. Joseph's College, Tiruchirappalli - 2, India","2010 International Conference on Communication and Computational Intelligence (INCOCCI)","","2010","","","357","361","Emotional Quotient (aka. Emotional Intelligence Quotient (EIQ)) can be described as the capacity for recognizing our own feelings and those of others, for motivating ourselves, and for managing emotions well in us and in our relationships. The primary objective of this work is to study the impact of EQ on the primary cognitive processes namely comprehension in Object oriented Systems. Comprehension is an understanding of the program code. The PG students were involved in all the experiments conducted. Java programs with various features of OO programming were given for comprehension. From the experiments that were conducted to find the relation between the program comprehension ability and the EQ values, it is identified that the EQ values have an impact on the program comprehension in the various OO features such as Inheritance and Polymorphism. The results of the experiments conducted may be used by the recruiter to carefully select the candidates for OO software development.","","978-81-8371-369","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5738757","Emotional Quotient (aka. Emotional Intelligence Quotient (EIQ));Program Comprehension;OO Systems","Programming profession;Humans;Educational institutions;Psychology;Training;Debugging","cognition;human computer interaction;Java;object-oriented methods;object-oriented programming;software engineering","emotional quotient;program comprehension;emotional intelligence quotient;primary cognitive process;object oriented system;Java program;OO programming;program comprehension ability;polymorphism;OO software development","","","","26","","","","","IEEE","IEEE Conferences"
"Comprehension Effort and Programming Activities: Related? Or Not Related?","A. Rahman","NA","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","","2018","","","66","69","Researchers have observed programmers to allocate considerable amount of effort in program comprehension. But, how does program comprehension effort relate with programming activities? We answer this question by conducting an empirical study using the MSR 2018 Mining Challenge Dataset. We quantify programmers' comprehension effort, and investigate the relationship between program comprehension effort and four programming activities: navigating, editing, building projects, and debugging. We observe when programmers are involved in high comprehension effort they navigate and make edits at a significantly slower rate. However, we do not observe any significant differences in programmers' build and debugging behavior, when programmers are involved in high comprehension effort. Our findings suggest that the relationship between program comprehension effort and programming activities is nuanced, as not all programming activities associate with program comprehension effort.","2574-3864;2574-3848","978-1-4503-5716-6978-1-5386-6171","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595181","comprehension;Halstead's complexity;programming;behavior;programmer","Navigation;Programming;Debugging;Measurement;Mathematical model;Data mining;Buildings","program debugging;programming","program comprehension effort;programming activities;program debugging","","","","13","","","","","IEEE","IEEE Conferences"
"Improving program comprehension by answering questions (keynote)","B. A. Myers","Human Computer Interaction Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA","2013 21st International Conference on Program Comprehension (ICPC)","","2013","","","1","2","My Natural Programming Project is working on making software development easier to learn, more effective, and less error prone. An important focus over the last few years has been to discover what are the hard-to-answer questions that developers ask while they are trying to comprehend their programs, and then to develop tools to help answer those questions. For example, when studying programmers working on everyday bugs, we found that they continuously ask “Why” and “Why Not” questions as they try to comprehend what happened. We developed the “Whyline” debugging tool, which allows programmers to directly ask these questions of their programs and get a visualization of the answers. In a small lab study, Whyline increased productivity by a factor of about two. We studied professional programmers trying to understand unfamiliar code and identified over 100 questions they identified as hard-to-answer. In particular, we saw that programmers frequently had specific questions about the feasible execution paths, so we developed a new visualization tool to directly present this information. When trying to use unfamiliar APIs, such as the Java SDK and the SAP eSOA APIs, we discovered some common patterns that make programmers up to 10 times slower in finding and understanding how to use the appropriate methods, so we developed new tools to assist them. This talk will provide an overview of our studies and resulting tools that address program comprehension issues.","1092-8138","978-1-4673-3092","10.1109/ICPC.2013.6613827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613827","Natural Programming;Debugging;Application Programming Interfaces (APIs);Integrated development environments (IDEs);Documentation;Reverse Engineering;Programming Environments","Visualization;Programming;Educational institutions;User interfaces;Usability;Documentation;Software engineering","computer science education;program debugging;program visualisation;project management;reverse engineering","program comprehension;question answering;My Natural Programming Project;software development learning;hard-to-answer questions;tool development;Whyline debugging tool;program visualization;productivity;professional programmer;execution paths;visualization tool;API","","","","15","","","","","IEEE","IEEE Conferences"
"Fault comprehension for concurrent programs","S. Park","Georgia Institute of Technology, Atlanta, GA, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","1444","1446","Concurrency bugs are difficult to find because they occur with specific memory-access orderings between threads. Traditional bug-finding techniques for concurrent programs have focused on detecting raw-memory accesses representing the bugs, and they do not identify memory accesses that are responsible for the same bug. To address these limitations, we present an approach that uses memory-access patterns and their suspicious-ness scores, which indicate how likely they are to be buggy, and clusters the patterns responsible for the same bug. The evaluation on our prototype shows that our approach is effective in handling multiple concurrency bugs and in clustering patterns for the same bugs, which improves understanding of the bugs.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606739","","Concurrent computing;Computer bugs;Context;Clustering algorithms;Programming;Instruction sets;Facebook","concurrency control;parallel programming;pattern clustering;program debugging;storage management","memory-access ordering;concurrent bugs;concurrent programs;memory access identification;memory-access pattern;suspiciousness scores;prototype evaluation;concurrency bug handling;pattern clustering;fault comprehension","","1","","20","","","","","IEEE","IEEE Conferences"
"Industrial Program Comprehension Challenge 2011: Archeology and Anthropology of Embedded Control Systems","A. Begel; J. Quante","NA; NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","227","229","The Industrial Program Comprehension Challenge is a two-year-old track of the International Conference on Program Comprehension that provides a venue for researchers and industrial practitioners to communicate about new research directions that can help address real world problems. This year, 2011, a scenario-based challenge was created to inspire researchers to apply the best ""archaeological"" techniques for understanding the complexity of industrial software, and foster appreciation for the delicate ""anthropological"" scenario which drives the behavior of the software engineers, management, and customers. Participants had two months to work on the challenge and submit write-ups of their solutions. Acceptable submissions were exhibited as posters, while the best solutions were presented during the Industrial Challenge conference session. This new challenge format gives researchers the opportunity to present their novel techniques, tools and ideas to the community.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970193","","Software;Leg;Legged locomotion;Computer bugs;Electronic mail;Companies","embedded systems;production engineering computing;software engineering","archeology;anthropology;embedded control systems;Industrial Program Comprehension Challenge;industrial software","","1","","1","","","","","IEEE","IEEE Conferences"
"A Visualization Tool for 3D Graphics Program Comprehension and Debugging","S. Podila; Y. Zhu","NA; NA","2016 IEEE Working Conference on Software Visualization (VISSOFT)","","2016","","","111","115","Real-time 3D graphics programs are based on a 3D pipeline structure. In a 3D pipeline, data is loaded, prepared, transferred from CPU to GPU, and then processed on GPU to create images. Regular debuggers and special graphics debuggers primarily focus on displaying data but do not show how data is transferred from CPU to GPU. If there is an error in data transfer, programmers usually need to locate the error by reading the source code, which many of our students find difficult. We have developed a tool to visualize data flow between CPU and GPU in a 3D pipeline. The visualization helps students understand 3D graphics programs, especially some of the implicit data connections in 3D graphics APIs. It also helps students quickly locate bugs related to data transfer in a 3D pipeline. We demonstrate our tool with an example that shows how a subtle error can be quickly detected in our visualization.","","978-1-5090-3850-3978-1-5090-3851","10.1109/VISSOFT.2016.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780167","software visualization;software comprehension;debugging;3D graphics program","Three-dimensional displays;Pipelines;Graphics processing units;Data visualization;Debugging;Engines;Data transfer","application program interfaces;graphics processing units;microprocessor chips;pipeline processing;program debugging;program visualisation","3D graphic API;data flow visualization;GPU;CPU;3D pipeline structure;3D graphic program debugging;3D graphic program comprehension;visualization tool","","","","13","","","","","IEEE","IEEE Conferences"
"A retrospective view on: The role of concepts in program comprehension: (MIP award)","V. Rajlich; N. Wilde","Department of Computer Science, Wayne State University, Detroit, MI, U.S.A.; Department of Computer Science, University of West Florida, Pensacola, FL, U.S.A.","2012 20th IEEE International Conference on Program Comprehension (ICPC)","","2012","","","12","13","This retrospective briefly recapitulates highlights of the original paper that was published at IWPC 2002. Then it overviews research directions of the last 10 years: research in tools and techniques of concept location a that aim to support software developer, research of integrated model of software change, creation of software engineering course that emphasizes the role of software developer in iterative and agile software processes, and further basic research into the role and properties of concepts.","1092-8138","978-1-4673-1216-5978-1-4673-1213-4978-1-4673-1215","10.1109/ICPC.2012.6240480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240480","partial program comprehension;as needed comprehension;concepts and features;concept location;software as repository of knowledge;constructivist learning","Software engineering;Pragmatics;Software tools;Education;Ontologies","iterative methods;software engineering","retrospective view;program comprehension;MIP award;concept location;software developer;integrated model;software change;software engineering course;agile software processes;iterative processes","","","","12","","","","","IEEE","IEEE Conferences"
"Overview of Program Comprehension","W. Kechao; W. Tiantian; S. Xiaohong; M. Peijun","NA; NA; NA; NA","2012 International Conference on Computer Science and Electronics Engineering","","2012","1","","601","605","With the increasing of software requirements, software is becoming larger and larger. It becomes an important problem to maintain the software. Thus understanding the program exactly, rapidly and comprehensively plays an important role. This paper has presented the related research of program comprehension, summarized six typical program comprehension strategies and compared these strategies.","","978-0-7695-4647-6978-1-4673-0689","10.1109/ICCSEE.2012.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187918","software maintenance;program comprehension","Educational institutions;Knowledge based systems;Semantics;Programming;Software maintenance;Conferences","formal specification;reverse engineering;software maintenance","program comprehension overview;software requirements;software maintenance;program comprehension strategies","","","","15","","","","","IEEE","IEEE Conferences"
"User evaluation of a domain specific program comprehension tool","L. Moonen","Simula Research Laboratory, P.O. Box 134, N-1325 Lysaker, Norway","2012 First International Workshop on User Evaluation for Software Engineering Researchers (USER)","","2012","","","45","48","The user evaluation in this paper concerns a domain-specific tool to support the comprehension of large safety-critical component-based software systems for the maritime sector. We discuss the context and motivation of our research, and present the user-specific details of our tool, called FlowTracker. We include a walk-through of the system and present the profiles of our prospective users. Next, we discuss the design of an exploratory qualitative study that we have conducted to evaluate the usability and effectiveness of our tool. We conclude with a summary of lessons learned and challenges that we see for user evaluation of such domain-specific program comprehension tools.","","978-1-4673-1859-4978-1-4673-1858","10.1109/USER.2012.6226583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226583","user evaluation;domain specific tooling;program comprehension;software visualization","Actuators;Safety;Software;Navigation;Visualization;Sensor systems","marine engineering;object-oriented programming;reverse engineering;safety-critical software;software tools","user evaluation;domain specific program comprehension tool;large safety-critical component-based software system;maritime sector;FlowTracker;tool usability;tool effectiveness","","","","13","","","","","IEEE","IEEE Conferences"
"[Journal First] A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers","A. Armaly; P. Rodeghero; C. McMillan","NA; NA; NA","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","","2018","","","788","788","Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.","1558-1225","978-1-4503-5638-1978-1-5386-5293","10.1145/3180155.3182544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453151","Program comprehension;accessibility technology;blindness","Software engineering;Tools;Java;Measurement;Software maintenance;Visualization","handicapped aids;programming environments;public domain software;software engineering","program comprehension strategies;sighted programmers;source code;reading processes","","","","","","","","","IEEE","IEEE Conferences"
"Infusing Topic Modeling into Interactive Program Comprehension: An Empirical Study","T. Wang; Y. Liu","NA; NA","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","","2017","2","","260","261","Automatic and semi-automatic approaches supporting program comprehension are sought by researchers and practitioners to facilitate software engineering tasks, such as development, maintenance, extension and so on. Using topic modeling is a promising way to automatically discover feature and structure from textual software assets. However, there are gaps between knowing and doing when applying topic modeling to software engineering practice. In this paper, we explored how to infuse topic modeling into understanding Java programs in a generic level, and summarized the whole procedure as a methodology called MAT. MAT utilizes essential information automatically generated from Java projects to establish a project overview and to bring search capability for software engineers. Experiments on two open source Java projects suggest that MAT can support program comprehension for Java software engineers during carrying on software maintenance and extension tasks.","0730-3157","978-1-5386-0367-3978-1-5386-0368","10.1109/COMPSAC.2017.151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029929","Mining software assets;Java program comprehension;Topic models","Software;Java;Software engineering;Computational modeling;Maintenance engineering;Analytical models;Semantics","data mining;document handling;Java;project management;public domain software;software maintenance;software management","topic modeling;interactive program comprehension;software engineering tasks;feature discovery;textual software assets;Java programs;MAT;open source Java projects;software maintenance;document collection","","","","7","","","","","IEEE","IEEE Conferences"
"Robot ON!: A Serious Game for Improving Programming Comprehension","M. A. Miljanovic; J. S. Bradbury","NA; NA","2016 IEEE/ACM 5th International Workshop on Games and Software Engineering (GAS)","","2016","","","33","36","A number of educational games have been created to help students programming. Many of these games focus on problem solving and the actual act of writing programs, while very few focus on programming comprehension. We introduce a serious game called Robot ON! aimed at players who have never programmed before. Unlike other serious programming games, Robot ON! focuses on comprehension rather than problem-solving challenges; players do not actually write any programs, but are instead given the task of demonstrating their knowledge and understanding of a program's behavior. Robot ON! includes tools that allow players to demonstrate understanding of variable values, data types, program statements, and control flow. We include an evaluation plan to assess Robot ON!'s playability, enjoyment, and benefits to program comprehension.","","978-1-4503-4160-8978-1-5090-2203","10.1109/GAS.2016.014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809514","programming;software engineering;computer science;education;serious games;game-based learning;code walkthrough;code review;program comprehension","Games;Programming profession;Educational robots;Writing","computer aided instruction;computer science education;serious games (computing);software engineering","serious game;Robot ON!;programming comprehension;educational games;student programming;writing programs;program statements","","","","12","","","","","IEEE","IEEE Conferences"
"An Empirical Study of Blindness and Program Comprehension","A. Armaly","NA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","","2016","","","683","685","Blind programmers typically use a screen reader when reading code whereas sighted programmers are able to skim the code with their eyes. This difference has the potential to impact the generalizability of software engineering studies and approaches. We present a summary of a paper which will soon be under review at TSE that investigates how code comprehension of blind programmers differs from that of sighted programmers. Put briefly, we found no statistically-significant differences between the areas of code that the blind programmers found to be important and the areas of code that the sighted programmers found to be important.","","978-1-4503-4205-6978-1-5090-2245","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883372","","Software engineering;Conferences;Computer languages;Programming;Measurement;Software maintenance;Blindness","computer aided instruction;handicapped aids;software engineering","software engineering studies;reading code;screen reader;blind programmers;blindness;program comprehension","","","","17","","","","","IEEE","IEEE Conferences"
"Poster: Knowledge Transfer from Research to Industry: A Survey on Program Comprehension","I. von Nostitz-Wallwitz; J. Krüger; J. Siegmund; T. Leich","NA; NA; NA; NA","2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)","","2018","","","300","301","The number of scientific publications is continuously increasing, with most publications describing research that is also interesting for industrial software engineers. Program comprehension in particular is an essential and time consuming task in industry, but new approaches are rarely adopted.We conducted a survey with 89 participants from research and industry to investigate this problem. Our results indicate that researchers have to integrate other ways to communicate their work and make evaluations more practical.","2574-1934","978-1-4503-5663-3978-1-5386-6479","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449549","Program comprehension;survey;knowledge transfer","Tools;Knowledge transfer;Task analysis;Industries;Jacobian matrices;Software engineering;Programming","","","","","","","","","","","IEEE","IEEE Conferences"
"Influence of Synchronized Domain Visualizations on Program Comprehension","N. Oliveira; M. J. V. Pereira; D. da Cruz; M. Berón","NA; NA; NA; NA","2010 IEEE 18th International Conference on Program Comprehension","","2010","","","192","195","An effective program comprehension is reached when it is possible to view and relate what happens when the program is executed, synchronized with its effects in the real world concepts. This enables the interconnection of program's meaning at both problem and program domains. To sustain this statement we need (i) to develop a tool which provides and synchronizes views at both domains, and (ii) to perform an experiment to measure the actual impact of this approach. So, in this working session we aim at discussing the benefits of providing synchronized domain visualizations. We also envisage to discuss the preparation and conduction of appropriate experiments that will test that benefits. A case study will be used and the discussion will be supported by experimental material specially prepared for the occasion, but adapted from material already used in previous experiments.","1092-8138;1092-8138","978-1-4244-7603-9978-1-4244-7604-6978-0-7695-4113","10.1109/ICPC.2010.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521744","DSL comprehension;Program comprehension experiments;Program comprehension tools;Synchronized program visualizations","Visualization;Testing;DSL;Conducting materials;Animation;Performance evaluation;Cause effect analysis;Data mining;Humans;Graphics","data visualisation;synchronisation","synchronized domain visualizations;effective program comprehension","","","","11","","","","","IEEE","IEEE Conferences"
"Lost comments support program comprehension","T. Omori","Department of Computer Science, Ritsumeikan University, Kusatsu, Shiga, Japan","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","","2017","","","567","568","Source code comments are valuable to keep developers' explanations of code fragments. Proper comments help code readers understand the source code quickly and precisely. However, developers sometimes delete valuable comments since they do not know about the readers' knowledge and think the written comments are redundant. This paper describes a study of lost comments based on edit operation histories of source code. The experimental result shows that developers sometimes delete comments although their associated code fragments are not changed. Lost comments contain valuable descriptions that can be utilized as new data sources to support program comprehension.","","978-1-5090-5501-2978-1-5090-5502","10.1109/SANER.2017.7884680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884680","","History;Software;Encoding;Syntactics;Computer science;Writing;Data mining","software engineering;source code (software)","lost comments support program comprehension;source code comments;code fragments explanation;software developers;edit operation histories","","","","4","","","","","IEEE","IEEE Conferences"
"Novice comprehension of Object-Oriented OO programs: An empirical study","A. S. Alardawi; A. M. Agil","The College of Computer Technology, Tripoli-Libya; The College of Computer Technology, Tripoli-Libya","2015 World Congress on Information Technology and Computer Applications (WCITCA)","","2015","","","1","4","Class structure represents one of the essential concepts of Object-Oriented paradigm and therefore, a good understanding of this concept will positively affect the effectiveness of novice programmers. Comprehension underpins many programming activities such as program design and program implementation. Program comprehension represents in this context a mental model approach that involves interesting theoretical frameworks of program comprehension. Our starting point is Burkhardt cognitive model for OO program comprehension [1]. The model considers two distinct but interacting models: program and situation. Our focus does not rely primarily in distinguishing between these models, but use both of them to assess the influence on novices of class structure on program comprehension. We report on an empirical study that aims of to investigate the effect of class structure on program comprehension for novices using controlled experimentation in which the treatments were a simple program without class structure versus the same program with classes present; they are termed respectively as: Non-Class based program and as Class based program. Data was collected from three different sets of experiments comprising of a total of 211 undergraduate first year computer science students from different institutions. Preliminary findings of this investigation are reported, in particular results indicate that Class based programs were more understandable, readable, and accessible than the corresponding Non-Class based programs. Our findings align with and support those works that claim the cognitive benefits of the OO paradigm. Directions for future research are highlighted.","","978-1-4673-6636-6978-1-4673-6635","10.1109/WCITCA.2015.7367057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367057","empirical study;program comprehension;class structure;mental model","Object oriented modeling;Encapsulation;Cognitive science;Software;Programming profession;Object recognition","computer science education;object-oriented programming","object-oriented program comprehension;OO program comprehension;class structure;novice programmers;mental model approach;Burkhardt cognitive model;program model;situation model;nonclass based program;class based programs","","1","","21","","","","","IEEE","IEEE Conferences"
"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li","NA; NA; NA; NA; NA; NA","2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)","","2018","","","584","584","This paper is published in IEEE Transaction on Software Engineering (DOI: 10.1109/TSE.2017.2734091). Comparing with previous programming comprehension studies that are usually in controlled settings or have a small number of participants, we perform a more realistic investigation of program comprehension activities. To do this, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We collect 3,148 working hour data from 78 professional developers in a field study. We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. Then we measure comprehension time by calculating the time that developers spend on program comprehension. We find that on average developers spend ~58% of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension.","1558-1225","978-1-4503-5638-1978-1-5386-5293","10.1145/3180155.3182538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453126","Program Comprehension;Field Study;Inference Model","Computer science;Software;Software engineering;Australia;Conferences;Software measurement;Information technology","","","","","","","","","","","IEEE","IEEE Conferences"
"Context and Vision: Studying Two Factors Impacting Program Comprehension","Z. Soh","NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","258","261","Linguistic information derived from identifiers and comments has a paramount role in program comprehension. Indeed, very often, program documentation is scarce and when available, it is almost always outdated. Previous research works showed that program comprehension is often solely grounded on identifiers and comments and that, ultimately, it is the quality of comments and identifiers that impact the accuracy and efficiency of program comprehension. Previous works also investigated the factors influencing program comprehension. However, they are limited by the available tools used to establish relations between cognitive processes and program comprehension. The goal of our research work is to foster our understanding of program comprehension by better understanding its implied underlying cognitive processes. We plan to study vision as the fundamental mean used by developers to understand a code in the context of a given program. Vision is indeed the trigger mechanism starting any cognitive process, in particular in program comprehension. We want to provide supporting evidence that context guides the cognitive process toward program comprehension. Therefore, we will perform a series of empirical studies to collect observations related to the use of context and vision in program comprehension. Then, we will propose laws and then derive a theory to explain the observable facts and predict new facts. The theory could be used in future empirical studies and will provide the relation between program comprehension and cognitive processes.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970176","Program comprehension;cognitive process;vision science;program context","Context;Conferences;Visualization;Software;Software engineering;Maintenance engineering;Computers","software development management;system documentation","program comprehension;linguistic information;program documentation;cognitive processes","","1","","25","","","","","IEEE","IEEE Conferences"
"Case studies of optimized sequence diagram for program comprehension","M. Srinivasan; Jeong Yang; Young Lee","Electrical Engineering and Computer Science, Texas A&M University- Kingsville, USA; Electrical Engineering and Computer Science, Texas A&M University- Kingsville, USA; Electrical Engineering and Computer Science, Texas A&M University- Kingsville, USA","2016 IEEE 24th International Conference on Program Comprehension (ICPC)","","2016","","","1","4","In large project, source code becomes increasing complex and lengthy so program comprehension plays an important and significant role for developers. Sequence diagram generated using static source code or dynamic only approach provides limited execution coverage, additionally contains redundant, dead and fault driven methods, which increase the size of the diagram and complexity. In this paper, to address the problems, optimized sequence diagrams were developed by combining static source code and dynamic only approach, also incorporating various levels of abstraction in order to reduce complexity and provide complete behavior of the system. Case studies determined from the sequence diagram for three systems generated based on source code only and fully dynamic approach proved that the proposed optimized sequence diagrams were less complex and provided more detailed description of the functionality of the system.","","978-1-5090-1428","10.1109/ICPC.2016.7503734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503734","sequence diagram;polymorphism;object-oriented;visualization;program comprehension","Animals;Visualization;Measurement;Java;Complexity theory;Conferences;Reverse engineering","object-oriented methods;source code (software)","program comprehension;static source code;fault driven methods;complexity;optimized sequence diagrams","","5","","14","","","","","IEEE","IEEE Conferences"
"Enhancing Object-Oriented Programming Comprehension Using Optimized Sequence Diagram","M. Srinivasan; Y. Lee; J. Yang","NA; NA; NA","2016 IEEE 29th International Conference on Software Engineering Education and Training (CSEET)","","2016","","","81","85","This paper presents how to generate an optimized sequence diagram from static java source code and dynamic execution trace at a web-based educational programming environment. The aim of this research is to help student programmers better understand the dynamic behavior of a java program using optimized sequence diagram, therefore to enhance object-oriented programming learning experience.","2377-570X","978-1-5090-0765-3978-1-5090-0764","10.1109/CSEET.2016.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474469","sequence diagram;polymorphism;object-oriented;visualization;program comprehension","Shape;Visualization;Java;Unified modeling language;Grammar;Object oriented programming;Complexity theory","computer aided instruction;computer science education;Internet;Java;object-oriented programming;source code (software);Unified Modeling Language","optimized sequence diagram;static Java source code;dynamic execution trace;Web-based educational programming environment;object-oriented programming learning experience","","1","","18","","","","","IEEE","IEEE Conferences"
"Studying Onboarding to Improve Program Comprehension Tool Support","R. Yates","NA","2010 IEEE Symposium on Visual Languages and Human-Centric Computing","","2010","","","257","258","Gaining an understanding of unfamiliar software systems is hard. Existing support tools are based on studies of the information sought by software developers, but often the developers themselves do not know what to look for. Here an alternative is proposed: by studying the information `pushed' from software experts to new developers during onboarding, the information provided by the experts can be analysed and compared to the information `pulled' by the new developers. The content and presentation of this data will inform tool design for onboarding support.","1943-6106;1943-6092","978-1-4244-8485","10.1109/VLHCC.2010.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635249","onboarding;program comprehension","Software;Programming;Conferences;Data visualization;USA Councils;Companies;Computational modeling","software tools","program comprehension tool support;software system;software developer;tool design","","","","11","","","","","IEEE","IEEE Conferences"
"Making the comprehension of software architecture attractive","C. S. C. Rodrigues; C. M. L. Werner","Comp. Science, C OPPE/ UFRJ; Comp. Science, C OPPE/UFRJ","2011 24th IEEE-CS Conference on Software Engineering Education and Training (CSEE&T)","","2011","","","416","420","Visualization stimulates the cognitive capacity of humans and facilitates the understanding of a subject. It performs a crucial role in teaching software architecture. As systems become more complex, new education proposals have been introduced in the classroom, especially those that make teaching more attractive to students. This paper presents the VisAr3D approach which was designed to provide a 3D visualization of UML models, where the user should intuitively understand architectural elements in this 3D environment. It includes exploration, interaction and simulation resources to establish a practical and pleasant learning activity, focusing in large scale systems.","2377-570X;1093-0175;1093-0175","978-1-4577-0348-5978-1-4577-0349-2978-1-4577-0347","10.1109/CSEET.2011.5876116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5876116","","Unified modeling language;Three dimensional displays;Visualization;Software architecture;Education;Solid modeling;Computer architecture","computer aided instruction;computer science education;software architecture;Unified Modeling Language","software architecture attractive;software architecture teaching;VisAr3D approach;UML models;3D visualization","","2","","11","","","","","IEEE","IEEE Conferences"
"A Unified Framework for the Comprehension of Software's Time","O. Benomar; H. Sahraoui; P. Poulin","NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","603","606","The dimension of time in software appears in both program execution and software evolution. Much research has been devoted to the understanding of either program execution or software evolution, but these two research communities have developed tools and solutions exclusively in their respective context. In this paper, we claim that a common comprehension framework should apply to the time dimension of software. We formalize this as a meta-model that we instantiate and apply to the two different comprehension problems.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203023","","Software;Heating;Visualization;Collaboration;History;Measurement;Context","software maintenance","software time dimension;program execution;software evolution;meta-model;comprehension framework;software maintenance","","","","19","","","","","IEEE","IEEE Conferences"
"Software Oriented Fuzzy Comprehension Assessment Model for the Education Quality of Primary and Secondary School Teachers","B. Song","NA","2013 IEEE 13th International Conference on Advanced Learning Technologies","","2013","","","346","348","Education quality assessment of primary and secondary school teachers is an important part of teacher education. In this paper, a fuzzy comprehensive assessment model for the education quality of primary and secondary school teachers, which combines the experience of experts with quantitative analysis, is proposed. It reduces the influence of human arbitrariness on the assessment, and then enhances the accuracy and validity of the assessment. Since all of the calculation process of the fuzzy comprehension assessment can be done by software tools or computer programming, the model can be easily operated in software.","2161-3761;2161-377X","978-0-7695-5009","10.1109/ICALT.2013.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601948","fuzzy comprehension assessment modle;primary and secondary school teacher;education quality;software oriented","Educational institutions;Indexes;Vectors;Software;Statistical analysis;Computers","fuzzy set theory;software tools;teaching","software oriented fuzzy comprehension assessment model;primary school teacher;secondary school teacher;education quality assessment;teacher education;quantitative analysis;software tools;computer programming","","","","5","","","","","IEEE","IEEE Conferences"
"A Framework for Estimating Interest on Technical Debt by Monitoring Developer Activity Related to Code Comprehension","V. Singh; W. Snipes; N. A. Kraft","NA; NA; NA","2014 Sixth International Workshop on Managing Technical Debt","","2014","","","27","30","Evaluating technical debt related to code structure at a fine-grained level of detail is feasible using static code metrics to identify troublesome areas of a software code base. However, estimating the interest payments at a similar level of detail is a manual process relying on developers to submit their estimates as they encounter instances of technical debt. We propose a framework that continuously estimates interest payments using code comprehension metrics produced by a tool that monitors developer activities in the Integrated Development Environment. We describe the framework and demonstrate how it is used to evaluate the presence of technical debt and interest payments accumulated for code in an industrial software product.","","978-1-4799-6791","10.1109/MTD.2014.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974886","Technical Debt;program comprehension;static analysis;software maintenance;code metrics;code smells","Conferences","program diagnostics;software maintenance;software tools","technical debt interest estimation;developer activity monitoring;code comprehension metrics;integrated development environment;industrial software product;code maintainability;static analysis","","6","","12","","","","","IEEE","IEEE Conferences"
"Understanding the complexity embedded in large routine call traces with a focus on program comprehension tasks","A. Hamou-Lhadj; T. C. Lethbridge","Concordia University, Canada; University of Ottawa, Canada","IET Software","","2010","4","2","161","177","The analysis of execution traces has been shown to be useful in many software maintenance activities that require a certain understanding of the systems' behaviour. Traces, however, are extremely large, hence are difficult for humans to analyse without effective tools. These tools usually support some sort of trace abstraction techniques that can help users understand the essence of a trace despite the trace being massive. Designing such tools requires a good understanding of the amount of complexity embedded in traces. Trace complexity has traditionally been measured using the file size or the number of lines in the trace. In this study, the authors argue that such metrics provide limited indication of the complexity of a trace. The authors address this issue by presenting a catalogue of metrics for assessing the various facets of traces of routine calls, with the ultimate objective being to facilitate the development of tools for the exploration of lengthy traces. The authors show the effectiveness of our metrics by applying them to 35 traces generated from four software systems.","1751-8806;1751-8814","","10.1049/iet-sen.2009.0031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5440856","","","computational complexity;embedded systems;software maintenance;software metrics","complexity embedded understanding;large routine call traces;program comprehension tasks;software maintenance;software systems","","3","","","","","","","IET","IET Journals & Magazines"
"Design of Low Complexity Non-Binary LDPC Codes with an Approximated Performance-Complexity Tradeoff","Y. Yu; W. Chen","Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, and SKL for ISN, Xidian University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, and SKL for ISN, Xidian University, China","IEEE Communications Letters","","2012","16","4","514","517","By presenting an approximated performance-complexity tradeoff (PCT) algorithm, a low-complexity non-binary low density parity check (LDPC) code over q-ary-input symmetric-output channel is designed in this manuscript which converges faster than the threshold-optimized non-binary LDPC codes in the low error rate regime. We examine our algorithm by both hard and soft decision decoders. Moreover, simulation shows that the approximated PCT algorithm has accelerated the convergence process by 30% regarding the number of the decoding iterations.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2012.021612.112467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6155572","Nonbinary LDPC;EXIT chart;performance-complexity tradeoff;Gallager decoding algorithm b","Complexity theory;Decoding;Iterative decoding;Error probability;Algorithm design and analysis;Optimization","channel coding;communication complexity;decoding;error statistics;parity check codes","performance-complexity tradeoff algorithm;PCT algorithm;low-complexity nonbinary low density parity check code;q-ary-input symmetric-output channel;threshold-optimized nonbinary LDPC code;low error rate regime;hard decision decoder;soft decision decoder;convergence process;decoding iteration","","7","","13","","","","","IEEE","IEEE Journals & Magazines"
"A Gamification Technique for Motivating Students to Learn Code Readability in Software Engineering","Q. Mi; J. Keung; X. Mei; Y. Xiao; W. K. Chan","NA; NA; NA; NA; NA","2018 International Symposium on Educational Technology (ISET)","","2018","","","250","254","Code readability is one of the important software quality attributes that computer science students need to learn in their programming classes, unfortunately most of the students do not have the necessary work experience or background to appreciate the importance of code readability. Traditional methods of learning code readability tend to be less than interactive and practical in the classroom environment. With the advent of gamification technique, this study introduced a new interactive teaching method and implemented as GamiCRS, an online platform for students to learn code readability. The focus was on incorporating game-based mechanisms to enable students with positive attitudes towards a more interesting learning process. A complete incentive and reward model is proposed in the study together with a combination of both intrinsic and extrinsic motivators identified. To ensure its dynamic efficacy, a field experiment was carried out to compare GamiCRS with its non-gamified counterparts and to evaluate learning outcomes. The empirical results show a positive effect towards the application of GamiCRS in the classroom environment. As many learning activities in software engineering are typically challenging and seldom amusing, gamification can thus be applied as a compelling addition to supporting a wider variety of teaching tactics.","","978-1-5386-7209-9978-1-5386-7210","10.1109/ISET.2018.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456230","Code Readability;Gamification;Education;Crowdsourcing;Motivation;Technology Acceptance Model","Crowdsourcing;Education;Task analysis;Software engineering;Computer science;Urban areas;Software","computer aided instruction;computer games;computer science education;human factors;software engineering;software quality;teaching","gamification technique;code readability;software engineering;computer science students;student motivation;software quality attributes;interactive teaching method;GamiCRS;reward model;incentive model","","","","14","","","","","IEEE","IEEE Conferences"
"Software Power: A New Approach to Software Complexity Metrics","Q. Du; F. Wang","NA; NA","2010 Second World Congress on Software Engineering","","2010","2","","165","168","Software complexity Metrics is an important basis of the software process management. However, until now, there is not a very effective approach to measure the software complexity. In the paper, we proposed a new approach to measure software complexity, i.e. Software Power(SP). Firstly, the definition derived, then, some examples given to demonstrate the effectiveness of SP. Finally, we give a conclusion.","","978-1-4244-9287","10.1109/WCSE.2010.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5718370","Information entropy;SoftwareComplexity;Software Power","Complexity theory;Entropy;Software measurement;Flow graphs;Software systems","computational complexity;software management;software metrics;software process improvement","software power;software complexity metrics;software process management","","","","12","","","","","IEEE","IEEE Conferences"
"An Efficiency-Complexity Controllable Rate Adaptive Lossy Source Coding—Hybrid Majority Voting Code","W. Lin; T. Matsumoto","School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Japan","IEEE Communications Letters","","2018","22","12","2419","2422","This letter proposes a practical source coding scheme, so-called hybrid majority voting (HMV) code, for lossy compression with a discrete memoryless source. Inspired by the coding scheme used in the classic rate-distortion theorem, we find a series of basic MV codes and analyze their rate-distortion performance. We then present an algorithm to find two component MV codes and apply them to lossy compression, group by group, to construct HMV codes. Finally, we show an example of joint source-channel coding based on the HMV code. The performance evaluation indicates that the HMV code makes it possible to easily control efficiency and complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2874220","China Scholarship Council; Japan Advanced Institute of Science and Technology (JAIST) Core-to-Core Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485403","Lossy source coding;rate-distortion;discrete memoryless source;rate adaptation;non-linear compression","Distortion;Channel coding;Source coding;Rate-distortion;Decoding;Complexity theory","adaptive codes;combined source-channel coding;rate distortion theory","practical source coding scheme;lossy compression;discrete memoryless source;classic rate-distortion theorem;basic MV codes;rate-distortion performance;component MV codes;HMV code;joint source-channel coding;efficiency-complexity controllable rate adaptive lossy source coding;hybrid majority voting code;performance evaluation","","","","15","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity turbo matching coded optical transmission system based on code weight decision","S. Xu; B. Liu; L. Zhang; X. Xin; Rahat; L. Rao; F. Zhao; S. Ji; Z. Qu","School of Electronic Engineering, Beijing University of Posts and Telecommunications (BUPT), Xitucheng Road NO.10, Beijing, China, 100876; Institute of Optoelectronics, Nanjing University of Information Science &amp; Technology (NUIST), Nanjing, China, 210044; School of Electronic Engineering, Beijing University of Posts and Telecommunications (BUPT), Xitucheng Road NO.10, Beijing, China, 100876; School of Electronic Engineering, Beijing University of Posts and Telecommunications (BUPT), Xitucheng Road NO.10, Beijing, China, 100876; Institute of Optoelectronics, Nanjing University of Information Science &amp; Technology (NUIST), Nanjing, China, 210044; School of Electronic Engineering, Beijing University of Posts and Telecommunications (BUPT), Xitucheng Road NO.10, Beijing, China, 100876; School of Automation, Xi'an University of Posts &amp; Telecommunications, Xi'an, China, 710121; Institute of Optoelectronics, Nanjing University of Information Science &amp; Technology (NUIST), Nanjing, China, 210044; School of Electronic Engineering, Beijing University of Posts and Telecommunications (BUPT), Xitucheng Road NO.10, Beijing, China, 100876","2017 16th International Conference on Optical Communications and Networks (ICOCN)","","2017","","","1","3","Weight distribution of code is very important for error correction in the use of Turbo coding program in optical fiber communication. Input sequence need to pass convolutional encoder, Therefore, it is necessary that the weight of the output should be kept as large as possible. Although the interleaver can help to improve the output code weight, the higher the complexity of interleaving, the iterative decoding will be more complex. In consequence, we can judge the weight of the input data before encoding. Invert all bits when the weight of the code is less than a certain value of its length. It can improve the weight of the code through such a simple way and reduce the complexity of the interleaver at the same time.","","978-1-5386-3273-4978-1-5386-3271-0978-1-5386-3272-7978-1-5386-3274","10.1109/ICOCN.2017.8121491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8121491","Optical fiber communication;Code weight;Interleaving","Turbo codes;Decoding;Optical fiber communication;Optical fibers;Complexity theory","convolutional codes;error correction codes;interleaved codes;iterative decoding;optical fibre communication;turbo codes","code weight decision;weight distribution;Turbo coding program;optical fiber communication;convolutional encoder;output code weight;error correction code;low complexity turbo matching coded optical transmission system;interleaver codes;iterative decoding","","","","5","","","","","IEEE","IEEE Conferences"
"Low complexity construction of low density lattice codes based on array codes","R. A. P. Hernandez; B. M. Kurkoski","Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan 923-1292; Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa, Japan 923-1292","2014 International Symposium on Information Theory and its Applications","","2014","","","264","268","Recently a variety of lattices called low density lattices codes (LDLC) have been studied because they can be decoded efficiently using belief propagation, and can be seen as a Euclidean space codes analogue to low density parity check codes (LDPC). Previous LDLC lattice designs, like Latin square, are based on high-complexity computer search to eliminate 4-cycles. Array codes have been used to construct LDPC codes efficiently. This work describes the design of LDLC based on array codes. This construction is 4-cycle free and a systematic construction of the parity check matrix. For all cases considered, the LDLC based on array codes have a better symbol error rate performance than the latin square. For example, there is a 0.5 dB gain for dimension n = 91.","","978-4-8855-2292","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979845","","Lattices;Parity check codes;Arrays;Generators;Sparse matrices;Encoding;AWGN channels","matrix algebra;parity check codes","low complexity construction;low density lattice codes;array codes;LDLC;belief propagation;Euclidean space codes analogue;low density parity check codes;LDPC;parity check matrix","","","","15","","","","","IEEE","IEEE Conferences"
"Nonbinary LDPC codes constructed based on a cyclic MDS code and a low-complexity nonbinary message-passing decoding algorithm","C. Chen; B. Bai; X. Wang; M. Xu","State Key Lab. of ISN, Xidian University, Xi'an, China; State Key Lab. of ISN, Xidian University, Xi'an, China; State Key Lab. of ISN, Xidian University, Xi'an, China; Panasonic Research & Development Center China Co., Ltd.","IEEE Communications Letters","","2010","14","3","239","241","In this letter, we propose a construction of nonbinary quasi-cyclic low-density parity-check (QC-LDPC) codes based on a cyclic maximum distance separable (MDS) code. The parity-check matrices are significantly rank deficient square matrices and their Tanner graphs have a girth of at least 6. The minimum distances of the codes are very respectable as far as LDPC codes are concerned. Based on plurality voting and iterative mechanism, a low-complexity nonbinary massage-passing decoding algorithm is proposed. It only requires finite field operations, integer additions and integer comparisons. Simulation results show that the decoding algorithm is fit for the proposed codes, providing efficient trade-offs between performance and decoding complexity, which suggests that the coding scheme may find some applications in communication or storage systems with high-speed and low-power consumption requirements.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2010.03.092296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5426595","Nonbinary low-density parity-check (LDPC) code;maximum distance separable (MDS) code;plurality voting;iterative decoding;nonbinary message-passing decoding","Parity check codes;Iterative decoding;Iterative algorithms;Sparse matrices;Voting;Galois fields;Chaotic communication;Sum product algorithm;Fast Fourier transforms;Reed-Solomon codes","communication complexity;cyclic codes;decoding;matrix algebra;message passing;parity check codes","LDPC codes;cyclic MDS code;nonbinary message-passing decoding algorithm;quasicyclic low-density parity-check codes;maximum distance separable code;parity-check matrix;Tanner graphs;decoding complexity;coding scheme;low-complexity decoding algorithm","","17","","10","","","","","IEEE","IEEE Journals & Magazines"
"A Novel Low-Complexity Joint Coding and Decoding Algorithm for NB-LDPC Codes","S. Song; J. Tian; J. Lin; Z. Wang","School of Electronic, Science and Engineering, Nanjing University, P. R. China; School of Electronic, Science and Engineering, Nanjing University, P. R. China; School of Electronic, Science and Engineering, Nanjing University, P. R. China; School of Electronic, Science and Engineering, Nanjing University, P. R. China","2019 IEEE International Symposium on Circuits and Systems (ISCAS)","","2019","","","1","5","Non-binary low-density parity-check (NB-LDPC) codes exhibit a much better performance than their binary counterparts, especially for moderate codeword length and high-order modulation. However, their decoding algorithms suffer from very high computational complexity. In this paper, a low-complexity algorithm is proposed, named parity-check erased algorithm (PCEA), where an additional parity check bit is added to each symbol of the codeword when encoding and a series of simple operations are performed based on these bits during decoding. As a universal joint coding and decoding algorithm, the PCEA can be combined with arbitrary NB-LDPC encoding schemes and decoding algorithms based on message passing. The proposed algorithm facilitates significant improvement of decoding performance with a small decrease of the code rate. Additionally, it usually has an even better performance than a nearly same-rate code constructed by the original method, and requires much lower decoding complexity due to smaller size of the parity check matrix.","2158-1525;0271-4302","978-1-7281-0397-6978-1-7281-0398","10.1109/ISCAS.2019.8702165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8702165","Non-binary low-density parity-check codes;additional parity check bits;joint coding and decoding algorithm;low complexity","Decoding;Encoding;Iterative decoding;Simulation;Computational complexity","","","","","","15","","","","","IEEE","IEEE Conferences"
"Computational complexities and relative performance of LDPC codes and turbo codes","Jiancun Zuo; Qiudong Sun; Fangming Zhao","School of Electronic and Electrical Engineering, Shanghai Second Polytechnic University, 201209, China; School of Electronic and Electrical Engineering, Shanghai Second Polytechnic University, 201209, China; School of Electronic and Electrical Engineering, Shanghai Second Polytechnic University, 201209, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","251","254","This paper addresses the computational requirements and relative performance of low-density parity-check (LDPC) codes and Turbo codes. Detailed complexity analysis and exact formulas giving the required number of calculations are first presented for LDPC codes and Turbo codes, and then Monte Carlo simulations are performed to compare their relative performance at comparable computation cost. The numeral results show that even at moderate block length the optimized irregular LDPC codes can also beat Turbo codes with only 19% computational cost.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615299","complexity analysis;relative performance;irregular low-density parity-check codes;turbo codes","Educational institutions","computational complexity;Monte Carlo methods;parity check codes;turbo codes","moderate block length code;Monte Carlo simulation;low-density parity-check codes;turbo codes;LDPC codes;computational complexity","","","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity turbo product code for high-speed fiber-optic systems based on expurgated BCH codes","F. Paludi; D. A. Morera; T. Goette; M. Schnidrig; F. Ramos; M. R. Hueda","ClariPhy Argentina S.A. - Humberto Primo 680 - Córdoba (5000), Argentina; ClariPhy Argentina S.A. - Humberto Primo 680 - Córdoba (5000), Argentina; ClariPhy Argentina S.A. - Humberto Primo 680 - Córdoba (5000), Argentina; ClariPhy Argentina S.A. - Humberto Primo 680 - Córdoba (5000), Argentina; ClariPhy Argentina S.A. - Humberto Primo 680 - Córdoba (5000), Argentina; F.C.E.F.N - Universidad Nacional de Córdoba - CONICET - Av. Vélez Sarsfield 1611 - Córdoba (5016), Argentina","2016 IEEE International Symposium on Circuits and Systems (ISCAS)","","2016","","","429","432","We propose a low-complexity implementation architecture for turbo product code (TPC) suitable for next-generation fiber optic networks (e.g. ≳ 100 Gb/s). The proposed code makes use of expurgated Bose-Chaudhuri-Hocquenghem (BCH) codes to improve the performance and reduce implementation complexity. In comparison with existing solutions, our results show that the propos d TPC architecture is able to reduce approximately to a half the gate count and the memory size. This feature makes the new TPC an excellent candidate for practical VLSI implementation in commercial transceivers.","2379-447X","978-1-4799-5341-7978-1-4799-5340-0978-1-4799-5342","10.1109/ISCAS.2016.7527262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7527262","","Random access memory;Decoding;Computer architecture;Bit error rate;Complexity theory;Optical fiber networks;Adaptive optics","optical fibre networks;turbo codes","low complexity turbo product code;high-speed fiber optic systems;expurgated BCH codes;low complexity implementation architecture;next generation fiber optic networks;expurgated Bose-Chaudhuri-Hocquenghem;TPC architecture","","2","","7","","","","","IEEE","IEEE Conferences"
"Reduced-complexity entropy coding of transform coefficient levels using truncated golomb-rice codes in video compression","T. Nguyen; D. Marpe; H. Schwarz; T. Wiegand","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany","2011 18th IEEE International Conference on Image Processing","","2011","","","753","756","In hybrid video coding, the difference between the intra or inter prediction signal and the original signal is transmitted using block-based transform coding. The state-of-the-art in coding the quantized transform coefficients is the approach specified in H.264/AVC for context-adaptive binary arithmetic coding. It has, however, been shown that the number of binary symbols that have to be arithmetically coded for the transform coefficients can become very large, making the concept less attractive for high rate applications. To overcome this issue, we propose a combination of simple variable-length codes and context-adaptive binary coding, which yields the same coding efficiency as the H.264/AVC transform coefficient coding at a lower complexity level and which has been adopted into the HEVC test model (HM).","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116664","Transform coding;video compression","Transforms;Image coding;Context modeling;Complexity theory;Encoding;Context;Transform coding","block codes;data compression;entropy;transforms;video coding","reduced-complexity entropy coding;transform coefficient level;truncated Golomb-Rice code;video compression;hybrid video coding;interprediction signal;intraprediction signal;block-based transform coding;H.264-AVC transform coefficient coding;HEVC test model;binary symbol;context-adaptive binary arithmetic coding;quantized transform coefficient","","12","","8","","","","","IEEE","IEEE Conferences"
"PFN: A novel program feature network for program comprehension","X. Liu; X. Sun; B. Li; J. Zhu","School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China; School of Information Engineering, Yangzhou University, China","2014 IEEE/ACIS 13th International Conference on Computer and Information Science (ICIS)","","2014","","","349","354","Program comprehension is one of the most frequently performed activities during software maintenance and evolution. In order to facilitate program comprehension, a variety of graphical models have been proposed in software engineering community to construct relationships between program elements. These graphical models are mostly used for understanding the system based on structural syntax dependencies between program elements. However, these graphical models fail to extract the functional or semantic features of the system. Thus, developers still cannot effectively identify the functional part in source code fit for their needs. This paper tries to fill this gap, and proposes a novel representation, program feature network (PFN), to identify the semantic features of the program at class level. PFN is generated based on the relational topic model, a hierarchical probabilistic model of networks. Based on PFN, the semantic features and the links between pairs of two classes in the program can be clearly shown. In addition, PFN can predict the possible links between the newly change request in existing program feature network rather than reconstructing the representation from the start.","","978-1-4799-4860","10.1109/ICIS.2014.6912158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912158","","Object oriented modeling;Semantics;Graphical models;Syntactics;Graphics;Software systems","network theory (graphs);probability;program testing;software maintenance","software evolution;hierarchical probabilistic model;PFN;relational topic model;class level;program semantic features;program elements;software engineering community;software maintenance;program comprehension;program feature network","","2","","23","","","","","IEEE","IEEE Conferences"
"Evaluating software clustering algorithms in the context of program comprehension","A. Mahmoud; Nan Niu","Computer Science and Engineering, Mississippi State University, USA; Computer Science and Engineering, Mississippi State University, USA","2013 21st International Conference on Program Comprehension (ICPC)","","2013","","","162","171","We propose a novel approach for evaluating software clustering algorithms in the context of program comprehension. Based on the assumption that program comprehension is a task-driven activity, our approach utilizes interaction logs from previous maintenance sessions to automatically devise multiple comprehension-aware and task-sensitive decompositions of software systems. These decompositions are then used as authoritative figures to evaluate the effectiveness of various clustering algorithms. Our approach addresses several challenges associated with evaluating clustering algorithms externally using expert-driven authoritative decompositions. Such limitations include the subjectivity of human experts, the availability of such authoritative figures, and the decaying structure of software systems. We conduct an experimental analysis using two datasets, including an open-source system and a proprietary system, to test the applicability of our approach and validate our research claims.","1092-8138","978-1-4673-3092","10.1109/ICPC.2013.6613844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613844","Program comprehension;maintenance;software clustering","Clustering algorithms;Software algorithms;Software systems;Algorithm design and analysis;Maintenance engineering;Partitioning algorithms","pattern clustering;software maintenance","software clustering algorithms;program comprehension;task-driven activity;interaction logs;software systems;task-sensitive decompositions;multiple comprehension-awareness;software maintenance;expert-driven authoritative decompositions;decaying structure;authoritative figures;open-source system;proprietary system","","2","","57","","","","","IEEE","IEEE Conferences"
"Using background colors to support program comprehension in software product lines","J. Feigenspan; M. Schulze; M. Papendieck; C. Kästner; R. Dachselt; V. Köppen; M. Frisch","University of Magdeburg, Germany; University of Magdeburg, Germany; University of Magdeburg, Germany; Philipps University Marburg, Germany; University of Magdeburg, Germany; University of Magdeburg, Germany; University of Magdeburg, Germany","15th Annual Conference on Evaluation & Assessment in Software Engineering (EASE 2011)","","2011","","","66","75","Background: Software product line engineering provides an effective mechanism to implement variable software. However, the usage of preprocessors, which is typical in industry, is heavily criticized, because it often leads to obfuscated code. Using background colors to support comprehensibility has shown effective, however, scalability to large software product lines (SPLs) is questionable. Aim: Our goal is to implement and evaluate scalable usage of background colors for industrial-sized SPLs. Method: We designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, we conducted a controlled experiment with a large real-world SPL with over 160,000 lines of code and 340 features. We used a within-subjects design with treatments colors and no colors. We compared correctness and response time of tasks for both treatments. Results: For certain kinds of tasks, background colors improve program comprehension. Furthermore, subjects generally favor background colors. Conclusion: We show that background colors can improve program comprehension in large SPLs. Based on these encouraging results, we will continue our work improving program comprehension in large SPLs.","","978-1-84919-509","10.1049/ic.2011.0008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083163","","","data flow analysis;reverse engineering;software engineering","background colors;program comprehension;software product line engineering;preprocessors;obfuscated code;FeatureCommander","","10","","","","","","","IET","IET Conferences"
"On the Positive Effect of Reactive Programming on Software Comprehension: An Empirical Study","G. Salvaneschi; S. Proksch; S. Amann; S. Nadi; M. Mezini","Department of Computer Science, Reactive Systems Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany; Department of Computing Science, AB, University of AlbertaCanada; Department of Computer Science, Software Technology Group, Technische Universit&#x00E4;t Darmstadt, Darmstadt, Germany","IEEE Transactions on Software Engineering","","2017","43","12","1125","1143","Starting from the first investigations with strictly functional languages, reactive programming has been proposed as the programming paradigm for reactive applications. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages-including object-oriented languages-and applied reactive programming to several domains, such as GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research is that, beside other claimed advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible. This claim has never been evaluated. In this paper, we present the first empirical study that evaluates the effect of reactive programming on comprehension. The study involves 127 subjects and compares reactive programming to the traditional object-oriented style with the Observer design pattern. Our findings show that program comprehension is significantly enhanced by the reactive-programming paradigm-a result that suggests to further develop research in this field.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2655524","European Research Council; German Federal Ministry of Education and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827078","Reactive programming;empirical study;controlled experiment;software comprehension","Programming;Runtime;Software development;Robot sensing systems","functional languages;object-oriented languages;object-oriented programming","object-oriented languages;software comprehension;functional languages;observer design pattern;program comprehension;reactive languages;reactive programming","","3","","72","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Exploring Software Measures to Assess Program Comprehension","J. Feigenspan; S. Apel; J. Liebig; C. Kastner","NA; NA; NA; NA","2011 International Symposium on Empirical Software Engineering and Measurement","","2011","","","127","136","Software measures are often used to assess program comprehension, although their applicability is discussed controversially. Often, their application is based on plausibility arguments, which, however, is not sufficient to decide whether software measures are good predictors for program comprehension. Our goal is to evaluate whether and how software measures and program comprehension correlate. To this end, we carefully designed an experiment. We used four different measures that are often used to judge the quality of source code: complexity, lines of code, concern attributes, and concern operations. We measured how subjects understood two comparable software systems that differ in their implementation, such that one implementation promised considerable benefits in terms of better software measures. We did not observe a difference in program comprehension of our subjects as the software measures suggested it. To explore how software measures and program comprehension could correlate, we used several variants of computing the software measures. This brought them closer to our observed result, however, not as close as to confirm a relationship between software measures and program comprehension. Having failed to establish a relationship, we present our findings as an open issue to the community and initiate a discussion on the role of software measures as comprehensibility predictors.","1949-3789;1949-3770;1938-6451","978-1-4577-2203-5978-0-7695-4604","10.1109/ESEM.2011.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092561","Software measures;Program comprehension","Software measurement;Software;Java;Programming;Complexity theory;Size measurement;Time factors","software quality","software measurement;program comprehension assess;program comprehension;plausibility arguments;source code quality;code line;concern attributes;concern operations;software systems","","9","","40","","","","","IEEE","IEEE Conferences"
"Finding the right needles in hay helping program comprehension of large software systems","I. Şora","Department of Computer and Software Engineering, University Politehnica of Timisoara, Romania","2015 International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)","","2015","","","129","140","Maintenance of complex software systems can be done by software engineers only after they understand well the existing code. Program comprehension is supported by documentation - either developer documentation or reverse engineered. What is most often missing is a short document providing the new user with useful information to start with - an executive summary. In this work we propose a tool to automatically extract such a summary, by identifying the most important classes of a system. Our approach relies on techniques of static analysis of dependencies and graph-based ranking. Experiments on a set of real systems show good results.","","978-989-758-143","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320345","Reverse Engineering;Program Comprehension;Recommender System;Key Classes","Software systems;Documentation;Software algorithms;Libraries;Software engineering;Reverse engineering","graph theory;reverse engineering;software maintenance","helping program comprehension;large-software systems;complex software system maintenance;static dependency analysis;graph-based ranking","","","","17","","","","","IEEE","IEEE Conferences"
"Supporting program comprehension in large preprocessor-based software product lines","J. Feigenspan; M. Schulze; M. Papendieck; C. Kastner; R. Dachselt; V. Koppen; M. Frisch; G. Saake","School of Computer Science, University of Magdeburg, Germany; School of Computer Science, University of Magdeburg, Germany; School of Computer Science, University of Magdeburg, Germany; School of Mathematic and Computer Science, Philipps University Marburg, Germany; School of Computer Science, University of Magdeburg, Germany; School of Computer Science, University of Magdeburg, Germany; School of Computer Science, University of Magdeburg, Germany; School of Computer Science, University of Magdeburg, Germany","IET Software","","2012","6","6","488","501","Software product line (SPL) engineering provides an effective mechanism to implement variable software. However, using preprocessors to realise variability, which is typical in industry, is heavily criticised, because it often leads to obfuscated code. Using background colours to highlight code annotated with preprocessor statements to support comprehensibility has proved to be effective, however, scalability to large SPLs is questionable. The authors' aim is to implement and evaluate scalable usage of background colours for industrial-sized SPLs. They designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, the authors conducted a controlled experiment with a large real-world SPL with over 99'000 lines of code and 340 features. They used a within-subjects design with treatment colours and no colours. They compared correctness and response time of tasks for both treatments. For certain kinds of tasks, background colours improve program comprehension. Furthermore, the subjects generally favour background colours compared with no background colours. In addition, the subjects who worked with background colours had to use the search functions less frequently. The authors show that background colours can improve program comprehension in large SPLs. Based on these encouraging results, they continue their work on improving program comprehension in large SPLs.","1751-8806;1751-8814","","10.1049/iet-sen.2011.0172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353334","","","reverse engineering;software engineering","program comprehension support;preprocessor-based software product lines;SPL engineering;variable software implementation;background colours;code annotation;industrial-sized SPL;FeatureCommander tool;within-subjects design;treatment colours;no colours","","1","","","","","","","IET","IET Journals & Magazines"
"SMNLV: A small-multiples node-link visualization supporting software comprehension by displaying multiple relationships in software structure","A. Abuthawabeh; D. Zeckzer","TU Kaiserslautern, Germany; Leipzig University, Germany","2015 IEEE 3rd Working Conference on Software Visualization (VISSOFT)","","2015","","","175","179","Software engineering tasks like understanding of (legacy) software, checking guidelines, finding structure, or re-engineering of existing software require the analysis of the static software structure. The optimal visualization of this structure depends on the task at hand. In general, the software structure is mapped to a graph and graph drawing algorithms are used for displaying the structure. The task of drawing these graphs becomes more involved if not only one type of relations, e.g., call graphs, but many relation types should be analyzed at the same time. We propose a small-multiples node-link visualization, where each small visualization shows the graph related to one edge type. The visualizations are synchronized using selection and coordinated views. Using a separate view for each of the relations provides additional analysis capabilities. The main advantage of this approach is, that optimized drawing algorithms can be used for each type of relationship while being able to display multiple relationships at the same time.","","978-1-4673-7526-9978-1-4673-7525","10.1109/VISSOFT.2015.7332432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332432","","Visualization;Software;Layout;Software algorithms;Electronic mail;Guidelines;Algorithm design and analysis","data visualisation;software engineering","SMNLV;small-multiples node-link visualization;software comprehension;software engineering task;static software structure;structure optimal visualization;graph drawing algorithm;selection view;coordinated view","","","","29","","","","","IEEE","IEEE Conferences"
"Semantic-based extraction approach for generating source code summary towards program comprehension","R. Kadar; S. M. Syed-Mohamad; N. Abdul Rashid","School of Computer Sciences, Universiti Sains Malaysia, Penang, MALAYSIA; School of Computer Sciences, Universiti Sains Malaysia, Penang, MALAYSIA; School of Computer Sciences, Universiti Sains Malaysia, Penang, MALAYSIA","2015 9th Malaysian Software Engineering Conference (MySEC)","","2015","","","129","134","Program comprehension is a vital process that involves much effort in software maintenance. A key challenge for the developers is to comprehend a software system to be maintained since it is difficult and time consuming. Nowadays, software systems have grown in size causing the increased of developers' tasks in exploring and understanding source code. Source code is a crucial resource for developers to become familiar with a software system since some system documentation is often unavailable or outdated. Although many researchers have discussed different strategies and techniques to overcome the program comprehension problem, there are still many challenges that they have not yet discovered when trying to understand a software system through reading source code. Therefore, this study attempts to overcome the problem of source code comprehension by suggesting a suitable comprehension technique. We propose a semantic-based extraction approach of source code and generating it as a summary. This work aims to explain the concepts and relationships of program by integrating utilization of ontology and UML class-based modeling approaches. It will be easier for maintainer to understand source code as well as create a better way for improving program comprehension.","","978-1-4673-8227-4978-1-4673-8226","10.1109/MySEC.2015.7475208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475208","program comprehension;information extraction;semantic relationship;graphical representation;visualization","Unified modeling language;Ontologies;Semantics;Software systems;Natural languages;Java;Object oriented modeling","ontologies (artificial intelligence);software maintenance;source code (software);Unified Modeling Language","UML class-based modeling;ontology;software systems;software maintenance;program comprehension;source code summary;semantic-based extraction","","","","26","","","","","IEEE","IEEE Conferences"
"Integrating Runtime Values with Source Code to Facilitate Program Comprehension","M. Sulír","NA","2018 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2018","","","743","748","An inherently abstract nature of source code makes programs difficult to understand. In our research, we designed three techniques utilizing concrete values of variables and other expressions during program execution. RuntimeSearch is a debugger extension searching for a given string in all expressions at runtime. DynamiDoc generates documentation sentences containing examples of arguments, return values and state changes. RuntimeSamp augments source code lines in the IDE (integrated development environment) with sample variable values. In this post-doctoral article, we briefly describe these three approaches and related motivational studies, surveys and evaluations. We also reflect on the PhD study, providing advice for current students. Finally, short-term and long-term future work is described.","2576-3148;1063-6773","978-1-5386-7870-1978-1-5386-7871","10.1109/ICSME.2018.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530096","integrated development environment;documentation;debugging;dynamic analysis;variables","Documentation;Tools;Runtime;Debugging;Task analysis;Graphical user interfaces;Collaboration","Java;program compilers;program debugging;program diagnostics;programming environments;reverse engineering;software maintenance;software tools","documentation sentences;IDE;integrated development environment;program execution;debugger extension;program comprehension;string;runtimesamp augments source code lines;runtimesearch;DynamiDoc;abstract nature","","","","35","","","","","IEEE","IEEE Conferences"
"Analyzing Code Comments to Boost Program Comprehension","Y. Shinyama; Y. Arahori; K. Gondow","NA; NA; NA","2018 25th Asia-Pacific Software Engineering Conference (APSEC)","","2018","","","325","334","We are trying to find source code comments that help programmers understand a nontrivial part of source code. One of such examples would be explaining to assign a zero as a way to ""clear"" a buffer. Such comments are invaluable to programmers and identifying them correctly would be of great help. Toward this goal, we developed a method to discover explanatory code comments in a source code. We first propose 12 distinct categories of code comments. We then developed a decision-tree based classifier that can identify explanatory comments with 60% precision and 80% recall. We analyzed 2,000 GitHub projects that are written in two languages: Java and Python. This task is novel in that it focuses on a microscopic comment (""local comment"") within a method or function, in contrast to the prior efforts that focused on API-or method-level comments. We also investigated how different category of comments is used in different projects. Our key finding is that there are two dominant types of comments: preconditional and postconditional. Our findings also suggest that many English code comments have a certain grammatical structure that are consistent across different projects.","2640-0715;1530-1362","978-1-7281-1970-0978-1-7281-1971","10.1109/APSEC.2018.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719486","Program Comprehension;Source code comments;Natural language processing;decision tree;java;python;github","Syntactics;Java;Python;Natural languages;Semantics","","","","","","26","","","","","IEEE","IEEE Conferences"
"An Empirical Study of the Impact of Two Antipatterns, Blob and Spaghetti Code, on Program Comprehension","M. Abbes; F. Khomh; Y. Gueheneuc; G. Antoniol","NA; NA; NA; NA","2011 15th European Conference on Software Maintenance and Reengineering","","2011","","","181","190","Antipatterns are ""poor"" solutions to recurring design problems which are conjectured in the literature to make object-oriented systems harder to maintain. However, little quantitative evidence exists to support this conjecture. We performed an empirical study to investigate whether the occurrence of antipatterns does indeed affect the understandability of systems by developers during comprehension and maintenance tasks. We designed and conducted three experiments, with 24 subjects each, to collect data on the performance of developers on basic tasks related to program comprehension and assessed the impact of two antipatterns and of their combinations: Blob and Spaghetti Code. We measured the developers' performance with: (1) the NASA task load index for their effort, (2) the time that they spent performing their tasks, and, (3) their percentages of correct answers. Collected data show that the occurrence of one antipattern does not significantly decrease developers' performance while the combination of two antipatterns impedes significantly developers. We conclude that developers can cope with one antipattern but that combinations of antipatterns should be avoided possibly through detection and refactorings.","1534-5351;1534-5351","978-1-61284-259-2978-0-7695-4343","10.1109/CSMR.2011.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741260","Antipatterns;Blob;Spaghetti Code;Program Comprehension;Program Maintenance;Empirical Software Engineering","Maintenance engineering;Programming;Analysis of variance;Java;Indexes;NASA;Time measurement","object-oriented programming;software maintenance","antipatterns;blob;spaghetti code;program comprehension;recurring design problems;object-oriented systems;maintenance tasks;NASA task load index","","85","","37","","","","","IEEE","IEEE Conferences"
"Collective Code Bookmarks for Program Comprehension","A. Guzzi; L. Hattori; M. Lanza; M. Pinzger; A. v. Deursen","NA; NA; NA; NA; NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","101","110","The program comprehension research community has been developing useful tools and techniques to support developers in the time-consuming activity of understanding software artifacts. However, the majority of the tools do not bring collective benefit to the team: After gaining the necessary understanding of an artifact (e.g., using a technique based on visualization, feature localization, architecture reconstruction, etc.), developers seldom document what they have learned, thus not sharing their knowledge. We argue that code bookmarking can be effectively used to document a developer's findings, to retrieve this valuable knowledge later on, and to share the findings with other team members. We present a tool, called Pollicino, for collective code bookmarking. To gather requirements for our bookmarking tool, we conducted an online survey and interviewed professional software engineers about their current usage and needs of code bookmarks. We describe our approach and the tool we implemented. To assess the tool's effectiveness, adequacy, and usability, we present an exploratory pre-experimental user study we have performed with 11 participants.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970168","","Usability;Interviews;XML;Navigation;Documentation;Particle measurements","reverse engineering;software tools;system documentation","program comprehension;software artifact understanding;developer findings documentation;Pollicino;collective code bookmarking","","10","","17","","","","","IEEE","IEEE Conferences"
"Code Comprehension Activities in Undergraduate Software Engineering Course - A Case Study","S. K. Sripada; Y. R. Reddy","NA; NA","2015 24th Australasian Software Engineering Conference","","2015","","","68","77","In industry, inspections, reviews, and refactoring are considered as necessary software engineering activities for enhancing quality of code. In academia, such activities are rarely taught and practiced at Undergraduate level due to various reasons like limited skill set, limited knowledge of the available tools, time constraints, project setting, project client availability, flexibility with Syllabus, etc. However, we argue that such activities are an essential part of introductory software engineering courses and can result in improvement of coding skills, knowledge of coding standard and compliance to the same, and peer communication within teams. We have studied the use of such activities in a sophomore level Software Engineering course consisting of more than 200 students working in teams on projects from start-ups and present our experiences, findings and challenges. We present the results of quantitative evaluation of the impact of code comprehension activities before and after each iteration of the team projects.","1530-0803","978-1-4673-9390","10.1109/ASWEC.2015.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365795","Code Inspection;Code Reviews;Program Comprehension;Refactoring;Static Code Analysis;Textual Analysis","Measurement;Inspection;Software engineering;Software;Standards;Encoding;Writing","computer science education;educational courses;further education;software engineering","undergraduate software engineering course;code comprehension activities;introductory software engineering courses","","","","23","","","","","IEEE","IEEE Conferences"
"Source code comprehension analysis in software maintenance","N. A. Al-Saiyd","Computer Science Department, Faculty of Information Technology Applied Science Private University, Amman-Jordan","2017 2nd International Conference on Computer and Communication Systems (ICCCS)","","2017","","","1","5","Source code comprehension is considered as an essential part of the software maintenance process. It is considered as one of the most critical and time-consuming task during software maintenance process. The difficulties of source code comprehension is analyzed. A static Bottom-up code comprehension model is used. The code is partitioned into functional-based blocks and their data and control dependencies that preserve the functionality of the program are analyzed. The data-flow and control-flow graphs reflects the dependencies and assist in refactoring process. The proposed strategy helps in improving the readability of the program code, increase maintainer productivity, and reducing the time and effort of code comprehension. It helps maintainers to locate the required lines of code that constitute the functional area that the maintainers are searching for in their maintenance work.","","978-1-5386-0539-4978-1-5386-0538-7978-1-5386-0537-0978-1-5386-0540","10.1109/CCOMS.2017.8075175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075175","source code comprehension;code comprehension;models;refactoring;effort estimation;source line of code (SLOC)","Maintenance engineering;Computer languages;Software maintenance;Documentation;Software systems","data flow graphs;software maintenance;source code (software)","source code comprehension analysis;software maintenance process;functional-based blocks;control dependencies;control-flow graphs;refactoring process;program code;maintenance work;data-flow graph;static bottom-up code comprehension model;code partitioning","","","","18","","","","","IEEE","IEEE Conferences"
"CodeSurveyor: Mapping large-scale software to aid in code comprehension","N. Hawes; S. Marshall; C. Anslow","Oracle Labs, Brisbane, Australia; Victoria University of Wellington, New Zealand; Middlesex University, London, UK","2015 IEEE 3rd Working Conference on Software Visualization (VISSOFT)","","2015","","","96","105","Large codebases - in the order of millions of lines of code (MLOC) - are incredibly complex. Whether fixing a fault, or implementing a new feature, changes to such systems often have unanticipated effects, as it is impossible for a developer to maintain a complete understanding of the code in their head. This paper presents CodeSurveyor, a spatial visualization technique that aims to support code comprehension in large codebases by allowing developers to view large-scale software at all levels of abstraction. It uses a cartographic metaphor to produce an interactive map of a codebase where users can zoom from a view of a system's high-level architectural components, represented as continents, down to the individual source files and the entities they define, shown as countries and states, respectively. The layout of the produced code map incorporates system dependency data and sizes regions according to a userconfigurable metric (line count by default), to create distinctive shapes and positions that serve as strong visual landmarks and keep users oriented. We detail the CodeSurveyor algorithm, show it generates code maps of the Linux kernel (1.4 MLOC) in 1.5 minutes, and evaluate the intuitiveness of the metaphor to software developers and its utility in navigation tasks. Results show the effectiveness of the approach with developers of varying experience levels.","","978-1-4673-7526-9978-1-4673-7525","10.1109/VISSOFT.2015.7332419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332419","software visualization;code comprehension","Layout;Visualization;Shape;Vocabulary;Measurement;Software systems","data visualisation;Linux;source code (software)","CodeSurveyor;large-scale software mapping;code comprehension;codebase;millions of lines of code;MLOC;spatial visualization technique;cartographic metaphor;interactive map;system high-level architectural components;source files;code map;system dependency data;user configurable metric;visual landmark;Linux kernel;navigation task;time 1.5 min","","4","","17","","","","","IEEE","IEEE Conferences"
"Low complexity iterative decoding of product codes using a Generalized Array Code form of the Nordstrom-Robinson code","H. Kim; G. Markarian; V. C. da Rocha","VTT Technical Research Centre of Finland, P.O. Box 1100, FI-90571 Oulu, Finland; Department of Communication systems, Lancaster University, England, UK; Department of Electronics and Systems, Federal University of Pernambuco, Brazil","2010 6th International Symposium on Turbo Codes & Iterative Information Processing","","2010","","","88","92","This paper proposes encoding and decoding for nonlinear turbo product codes and investigates the performance of nonlinear turbo product codes. The proposed nonlinear turbo product codes are constructed as N-dimensional product codes where the constituent codes are nonlinear binary codes derived from the linear codes over higher order alphabets, e.g., Preparata or Kerdock codes. The performance and the complexity of the proposed construction are evaluated using the well known nonlinear Nordstrom-Robinson code which is presented in the Generalized Array Code format with a low complexity trellis. The proposed construction shows the additional coding gain, reduced error floor and lower implementation complexity. The (64,24,12) nonlinear turbo product code has about effective 2.5dB gain and 1dB gain at a BER of 10<sup>-6</sup> when compared to the (64,15,16) linear turbo product code and the (64,24,10) linear turbo product code, respectively. The (256,64,36) nonlinear turbo product code composed of two Nordstrom-Robinson codes has about 0.7dB gain at a BER of 10<sup>-5</sup> when compared to the (256,64,25) linear turbo product code composed of two (16,8,5) Quasi-Cyclic codes. These are achieved for codes with relatively short block lengths that make them particularly attractive for new emerging standards that aim at efficient mobile data transmission.","2165-4700;2165-4719","978-1-4244-6746-4978-1-4244-6744-0978-1-4244-6745","10.1109/ISTC.2010.5613809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613809","Turbo Product Code;Nonlinear Error Control Coding;etc","Bit error rate;Complexity theory;Variable speed drives;Lead","cyclic codes;encoding;error statistics;iterative decoding;nonlinear codes;product codes;turbo codes","low complexity iterative decoding;nonlinear turbo product codes;generalized array code;nonlinear Nordstrom-Robinson code;encoding;N-dimensional product code;Preparata codes;Kerdock codes;BER;quasi-cyclic codes","","1","","30","","","","","IEEE","IEEE Conferences"
"A low complexity pixel Domain Video Coding based on Turbo code and Arithmetic code","C. Lahsini; R. Pyndiah; S. Zaibi; A. Bouallegue","Signal and Communication Department, Telecom Bretagne, Brest, France; Signal and Communication Department, Telecom Bretagne, Brest, France; Syscoms Laboratory, National Engineering School of Tunis, Tunisia; Syscoms Laboratory, National Engineering School of Tunis, Tunisia","2012 International Conference on Wireless Communications in Underground and Confined Areas","","2012","","","1","6","In recent years, with emerging applications such as multimedia sensors networks, wireless low-power surveillance and mobile camera phones, the traditional video coding architecture in being challenged. In fact, these applications have different requirements than those of the broadcast video delivery systems: a low power consumption at the encoder side is essential. In this context, we propose a pixel-domain video coding scheme which fits well in these senarios. In this system, both the arithmetic and turbo codes are used to encode the video sequence's frames. Simulations results show significant gains over Pixel-domain Wyner-Ziv video coding.","","978-1-4673-1291-2978-1-4673-1290-5978-1-4673-1289","10.1109/ICWCUCA.2012.6402501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402501","Turbo code;arithmetic code;distributed video coding","Decoding;Video coding;Turbo codes;Vectors;Complexity theory;Video sequences","arithmetic codes;turbo codes;video coding;video surveillance","low complexity pixel domain video coding;turbo code;arithmetic code;multimedia sensors networks;wireless low-power surveillance;mobile camera phones;broadcast video delivery systems","","","","7","","","","","IEEE","IEEE Conferences"
"The prediction of software complexity based on complexity requirement using artificial neural network","W. M. Purawinata; F. L. Gaol; A. Nugroho; B. S. Abbas","Faculty Of Engineering and Computer Science, Indonesian Computer University, Bandung, Indonesia 40132; Computer Science, Department, BINUS Graduate, Program - Doctor of Computer Science, Bina Nusantara University Jakarta, Indonesia 11480; Computer Science, Department, BINUS Graduate, Program - Doctor of Computer Science, Bina Nusantara University Jakarta, Indonesia 11480; Industrial Engineering Department Faculty of Engineering, Bina Nusantara University, Jakarta, Indonesia 11480","2017 IEEE International Conference on Cybernetics and Computational Intelligence (CyberneticsCom)","","2017","","","73","78","In the recent years, the productivity of software has grown in size, complexity, and also cost. As that software productivity growth, several problems has been appeared in software project management especially that correlated to complexity. One of complexity factors is requirement. A unit of requirement used as an option to the design phase of product development. The requirement is also a main option in verification process. So the the requirement complexity in this research is used as parameter to predict the software complexity. Because of the data pattern to connect between the requirement and the complexity is complex. So that this paper attempt to make a connectivity model between requirement complexity and prediction complexity of software using artificial neural network method with Levenberg Marquadt and Bayesian Regulation algorithm. So it can be seen comparison of experimental results by using the two algorithms.","","978-1-5386-0784-8978-1-5386-0783-1978-1-5386-0785","10.1109/CYBERNETICSCOM.2017.8311687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8311687","prediction;complexity;requirement;software;artificial neural network","Software;Complexity theory;Artificial neural networks;Neurons;Computer science;Software reliability","Bayes methods;formal specification;formal verification;neural nets;software metrics;systems analysis","software project management;complexity factors;product development;software complexity;artificial neural network method;complexity requirement;software productivity growth;prediction complexity;Levenberg Marquadt algorithm;Bayesian regulation algorithm;data pattern;connectivity model","","1","","15","","","","","IEEE","IEEE Conferences"
"Reduced-Complexity Multiplicity Assignment Algorithm and Architecture for Low-Complexity Chase Decoder of Reed-Solomon Codes","X. Peng; W. Zhang; W. Ji; Z. Liang; Y. Liu","NA; NA; NA; NA; NA","IEEE Communications Letters","","2015","19","11","1865","1868","A reduced-complexity multiplicity assignment (RCMA) algorithm for low-complexity chase (LCC) is proposed for decoding Reed-Solomon (RS) codes. The properties of bit-level received voltages over an additive white Gaussian noise (AWGN) channel with binary modulation are explored. By using the properties, the hard-decision, secondary hard-decision, and unreliable positions can be easily determined, and hence, significantly reduce the computational complexity. Compared with the original multiplicity assignment (MA) algorithm, the RCMA eliminates n<sup>2</sup> log<sup>2</sup> n multiplication operations and reduces the number of comparison operations from 2n<sup>2</sup> to n log<sup>2</sup> n, where n is the code length of an RS code. Moreover, a hardware design for RCMA is proposed and applied in the unified syndrome computation (USC) based LCC decoder. Implementation results show the throughput of the proposed decoder can reach a Gbps order, which meets the requirements of high-speed communications.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2477495","National Nature Science Foundation of China; Program for New Century Excellent Talents in University of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7247663","Reed-Solomon (RS) codes;algebraic Softdecision;multiplicity assignment;low-complexity chase;VLSI;Reed-Solomon (RS) codes;algebraic soft-decision;multiplicity assignment;low-complexity chase;VLSI","Decoding;Hardware;Complexity theory;Probability;Algorithm design and analysis;Interpolation;Clocks","AWGN channels;computational complexity;decoding;Reed-Solomon codes","reduced complexity multiplicity assignment;low-complexity chase decoder;Reed-Solomon codes;bit-level received voltages;additive white Gaussian noise channel;AWGN channel;binary modulation;secondary hard-decision;unreliable positions;computational complexity;original multiplicity assignment;unified syndrome computation","","6","","9","","","","","IEEE","IEEE Journals & Magazines"
"Complexity Control for HEVC Inter Coding Based on Two-Level Complexity Allocation and Mode Sorting","J. Zhang; S. Kwong; T. Zhao; X. Wang; S. Wang","Department of Computer Science, City University of Hong Kong, KowLoon, Hong Kong; Department of Computer Science, City University of Hong Kong, KowLoon, Hong Kong; College of Physics and Information Engineering, Fuzhou University, Fuzhou, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science, City University of Hong Kong, KowLoon, Hong Kong","2018 25th IEEE International Conference on Image Processing (ICIP)","","2018","","","3628","3632","High coding complexity is an obstruction when promoting the HEVC standard. In this paper, we propose a complexity control scheme for HEVC, which improves our previous work by employing a finer-grained complexity allocation scheme and mode sorting. In CTU-level complexity allocation, the time budget is allocated to each CTU proportionally to its estimated complexity. Then, the time budget for a CTU is further allocated to each CTU quadtree depth according to its probability of being the dominant depth. At last, a subset of modes are selected to reach the target complexity based on mode sorting. Compared with our previous work of which the least supported complexity ratio is 40%, the proposed scheme further reduces the lower bound of the ratio range to 20%.","2381-8549","978-1-4799-7061-2978-1-4799-7062","10.1109/ICIP.2018.8451088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451088","HEVC;Complexity control;Complexity allocation;Mode sorting","Complexity theory;Encoding;Resource management;Erbium;Standards;Training;Sorting","quadtrees;video coding","HEVC inter coding;two-level complexity allocation;mode sorting;HEVC standard;complexity control scheme;finer-grained complexity allocation scheme;CTU-level complexity allocation;time budget;CTU quadtree depth;supported complexity ratio;probability","","","","16","","","","","IEEE","IEEE Conferences"
"Tolerance to complexity: Measuring capacity of development teams to handle source code complexity","M. A. Barbosa; F. B. de Lima Neto; T. Marwala","Faculty of Engineering and the Built Environment, University of Johannesburg, South Africa; Polytechnic school of Pernambuco, University of Pernambuco, Brazil; Faculty of Engineering and the Built Environment, University of Johannesburg, South Africa","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2016","","","002954","002959","A well defined testing strategy is essential for any software development project. Testing efforts need to be carefully planed and executed in order to ensure effectiveness. Programming failures can represent a high risk for business. In order to mitigate such risk, companies have been increasingly investing more resources on software testing. In despite of massive investments on software testing and extensive collection of static analysis techniques and tools, there are still few conclusive explanations for what causes human programming failures on software. The hypothesis investigated in this paper is that a metric based on development teams characteristics can be more effective to predict defective source code than metrics purely focused on information about source code, alone. Aiming to assist software engineers during testing initiatives, this article presents a new approach to systematically measure capacity of development teams to handle source code complexity. The proposed metric can be effective for raising information and comparing multiple development teams, planning training initiatives and prioritising testing efforts. Experiments were carried out with the entire source code base of device drivers for Linux Operating System. Our approach was able to predict, with 80% of accuracy rate, which development teams introduced more issues from 2010 to 2014.","","978-1-5090-1897-0978-1-5090-1819-2978-1-5090-1898","10.1109/SMC.2016.7844689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844689","","Complexity theory;Software;Measurement;Linux;Mathematical model;Testing;Conferences","Linux;program diagnostics;program testing;software development management;software metrics;source code (software);team working","complexity tolerance;development team capacity measurement;source code complexity handling;software development project;software testing strategy;programming failures;risk mitigation;static analysis tools;human programming failures;training initiative planning;device drivers;Linux Operating System","","","","15","","","","","IEEE","IEEE Conferences"
"Dataflow programming in CAL—balancing expressiveness, analyzability, and implementability","J. Eker; J. W. Janneck","Ericsson Research, Lund, Sweden; Department of Computer Science, Lund University, Sweden","2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)","","2012","","","1120","1124","In this paper we lay out a case for the use of dataflow programming and the CAL language as a way of addressing current challenges in programming parallel hardware such as multicore systems and FPGAs. We show how the design of the CAL language balances conflicting concerns of expressiveness, analyzability, and implementability, making it a promising tool for the implementation of parallel stream processing applications. The language itself as well as the design considerations are presented and illustrated with a number of different use cases from a wide range of application domains.","1058-6393;1058-6393;1058-6393","978-1-4673-5051-8978-1-4673-5050-1978-1-4673-5049","10.1109/ACSSC.2012.6489194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489194","","","data flow analysis;data flow computing;parallel programming","dataflow programming;CAL language design considerations;parallel hardware programming;multicore systems;FPGA;expressiveness;analyzability;implementability;parallel stream processing application implementation","","9","","22","","","","","IEEE","IEEE Conferences"
"Complexity measurement: A new approach to ensure equal distribution of programming problems for evaluation","G. M. M. Bashir; S. K. Dey; S. S. M. Tariq; M. S. Islam","Dept. of CCE, Patuakhali Science and Technology University, Bangladesh; Faculty of CSE, Patuakhali Science and Technology University, Bangladesh; Faculty of CSE, Patuakhali Science and Technology University, Bangladesh; Faculty of CSE, Patuakhali Science and Technology University, Bangladesh","8th International Conference on Electrical and Computer Engineering","","2014","","","780","783","Distributing equivalent programming problems among the students for evaluation is an inherent problem. To remove this inherent problem, weight measurement of computer programs is necessary. Although software is the outcome of human creative activity, cognitive informatics plays an important role in understanding its fundamental characteristics. In our research, a new software complexity measure based on cognitive weight of basic control structure has been proposed that reduce the limitations of existing measures. Primarily our aim is to build a question bank with programming problems. Our designed system will access the solution codes for finding out the weight of respective programming problem from the question bank by applying the proposed method. The method will find out the complexity according to our proposed cognitive weight model. It will facilitate the instructors distributing the programming problems among the students equivalently. Thus, the automatic complexity measurement will ensure the students obtaining equal problems for programming evaluation.","","978-1-4799-4166-7978-1-4799-4167","10.1109/ICECE.2014.7026848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7026848","Cognitive Weights;Basic Control Structures;Weight Measurement;Software Complexity","Complexity theory;Programming;Software;Weight measurement;Software measurement;Algorithm design and analysis;Computers","computer science education;educational courses;software metrics","automatic software complexity measurement;equivalent programming problem distribution;weight measurement;computer programs;human creative activity;cognitive informatics;fundamental characteristics;software complexity measure;control structure;solution codes;question bank;cognitive weight model;programming evaluation","","1","","7","","","","","IEEE","IEEE Conferences"
"Reducing the complexity of the linear programming decoding","H. Tavakoli","Department of Electrical Engineering, Faculty of Engineering, University of Guilan, Rasht, Iran","7'th International Symposium on Telecommunications (IST'2014)","","2014","","","582","584","In this paper we show how the complexity of Linear Programming (LP) decoder can decrease. We use the degree 3 check equation to model all variation check degrees. The complexity of LP decoding is directed relative to the number of constraint. Number of constraint for original LP decoder is O(n2<sup>n</sup>). Our method decrease the number of the constraint to O(n).","","978-1-4799-5359-2978-1-4799-5358","10.1109/ISTEL.2014.7000772","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000772","Linear Programming Decoder;Complexity;Check Degree Distribution","Decoding;Complexity theory;Linear programming;Equations;Mathematical model;Iterative decoding","computational complexity;decoding;linear programming","linear programming decoding;complexity reduction;variation check degrees;LP decoding","","3","","7","","","","","IEEE","IEEE Conferences"
"Minimax Design of Low-Complexity Even-Order Variable Fractional-Delay Filters Using Second-Order Cone Programming","T. Deng","Department of Information Science, Faculty of Science, Toho University, Chiba , Japan","IEEE Transactions on Circuits and Systems II: Express Briefs","","2011","58","10","692","696","This brief proposes an optimal minimax method for designing even-order finite-impulse-response variable fractional-delay (VFD) digital filters by using the second-order cone programming (SOCP), which minimizes the peak error of the variable-frequency response and yields a true minimax design. To minimize the VFD filter complexity, we also present an algorithm for jointly optimizing all the subfilter orders in the Farrow structure such that all the subfilter orders can be simultaneously optimized for exactly meeting a given upper error bound. A design example is given to demonstrate the high performance and low complexity of the SOCP-based VFD filter.","1549-7747;1558-3791","","10.1109/TCSII.2011.2164160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6015538","Finite-impulse-response (FIR) variable fractional-delay (VFD) filter;second-order cone programming (SOCP);variable digital filter","Finite impulse response filter;Programming;Delay;Complexity theory;Passband;Optimization;Joints","digital filters;minimax techniques","low-complexity even-order variable fractional-delay filters;second-order cone programming;optimal minimax method;even-order finite-impulse-response variable fractional-delay digital filters;even-order finite-impulse-response VFD digital filters;variable-frequency response;Farrow structure;upper error bound;SOCP-based VFD filter","","23","","30","","","","","IEEE","IEEE Journals & Magazines"
"Notice of Retraction: Similarity Analysis of DNA Sequences Based on LZ Complexity and Dynamic Programming Algorithm","X. Guo; Q. Dai; B. Han; L. Zhu; L. Li","NA; NA; NA; NA; NA","2011 5th International Conference on Bioinformatics and Biomedical Engineering","","2011","","","1","4","This article has been retracted by the publisher.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>","2151-7622;2151-7614;2151-7614","978-1-4244-5089-3978-1-4244-5088","10.1109/icbbe.2011.5780038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5780038","","DNA;Complexity theory;Dynamic programming;Heuristic algorithms;Phylogeny;Bioinformatics;History","bioinformatics;DNA;dynamic programming;genetics;molecular biophysics;molecular configurations","similarity analysis;DNA sequences;LZ complexity;dynamic programming algorithm;shared information;word sets;exon;b-globin gene;multiple sequence alignment","","2","","25","","","","","IEEE","IEEE Conferences"
"Cryogenic Computing Complexity Program: Phase 1 Introduction","M. A. Manheimer","NA","IEEE Transactions on Applied Superconductivity","","2015","25","3","1","4","The ultimate goal of the Intelligence Advanced Research Projects Activity (IARPA)'s Cryogenic Computing Complexity (C3) program is to demonstrate a complete superconducting computer including processing units and cryogenic memory. IARPA expects that the C3 program will be a five-year two-phase program. Phase one, which encompasses the first three years, primarily serves to develop the technologies that are required to separately demonstrate a small superconducting processor and memory units. Phase two, which is for the final two years, will integrate those new technologies into a small-scale working model of a superconducting computer. Program goals are presented, and the approaches of the phase-one teams are reviewed.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2015.2399866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029597","complexity;cryogenic memory;IARPA;superconducting digital logic;Complexity;cryogenic memory;IARPA;superconducting digital logic","Cryogenics;Computers;Superconducting logic circuits;Superconducting integrated circuits;Energy efficiency;Josephson junctions;Memory management","computational complexity;computer architecture;cryogenics","cryogenic computing complexity program;Intelligence Advanced Research Projects Activity;IARPA;superconducting computer;processing units;cryogenic memory;superconducting processor;memory units","","48","","19","","","","","IEEE","IEEE Journals & Magazines"
"Rank: A Tool to Check Program Termination and Computational Complexity","C. Alias; A. Darte; P. Feautrier; L. Gonnord","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","238","238","Summary form only given. Proving the termination of a flowchart program can be done by exhibiting a ranking function, i.e., a function from the program states to a well-founded set that strictly decreases at each program step. In a previous paper , we proposed an algorithm to compute multidimensional affine ranking functions for flowcharts of arbitrary structure. Our method, although greedy, is provably complete for the class of rankings we consider. The ranking functions we generate can also be used to get upper bounds for the computational complexity (number of transitions) of the source program. This estimate is a polynomial, which means that we can handle programs with more than linear complexity. This abstract aims at presenting RANK, the tool that implements our algorithm.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571638","","Computational complexity;Conferences;Abstracts;Electronic mail;Automata;Software testing","computational complexity;polynomials;program verification","Rank;flowchart program termination checking;computational complexity;multidimensional affine ranking functions;source program;polynomial;linear complexity","","1","","","","","","","IEEE","IEEE Conferences"
"Low-complexity minimax design of digital FIR filters utilising linear programming in BI-DAC","X. Yang; H. Wang; K. Liu; Y. Xiao","University of Electronic Science and Technology of China, People's Republic of China; University of Electronic Science and Technology of China, People's Republic of China; University of Electronic Science and Technology of China, People's Republic of China; University of Electronic Science and Technology of China, People's Republic of China","Electronics Letters","","2019","55","7","408","411","This Letter presented a new-order optimal method for lowering the computational complexity of the traditional minimax design of digital finite impulse response (FIR) filter utilising linear programming (LP) in bandwidth interleaving digital-to-analogue converter (BI-DAC). In this order optimal method, a one-by-one increasing method was presented to optimise all of digital FIR subfilters' orders in BI-DAC so that these digital FIR subfilters' orders could be optimised to simultaneously satisfy all of given upper bounds of the distortion and aliasing errors. A design example was used to verify the effectiveness and low computational complexity of the presented low-complexity minimax design utilising LP, and the simulation results were well-pleasing.","0013-5194","","10.1049/el.2018.7660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685055","","","FIR filters;computational complexity;minimax techniques;linear programming;digital-analogue conversion","BI-DAC;new-order optimal method;digital FIR subfilters;digital finite impulse response filter;computational complexity;low-complexity minimax design;linear programming;bandwidth interleaving digital-to-analogue converter","","","","3","","","","","IET","IET Journals & Magazines"
"On the complexity of sos programming: formulas for general cases and exact reductions","G. Chesi","Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong","2017 SICE International Symposium on Control Systems (SICE ISCS)","","2017","","","1","6","The minimization of a linear cost function subject to the condition that some matrix polynomials depending linearly on the decision variables are sums of squares of matrix polynomials (SOS) is known as SOS programming. This paper proposes an analysis of the complexity of SOS programming, in particular of the number of linear matrix inequality (LMI) scalar variables required for establishing whether a matrix polynomial is SOS. This number is analyzed in the general case and in the case of some exact reductions achievable for some classes of matrix polynomials. An analytical formula is proposed in each case in order to provide this number as a function of the number of polynomial variables, degree and size of the matrix polynomials. Some tables reporting this number are also provided as reference for the reader. An application in robust stability analysis of polytopic systems is presented to show the usefulness of the proposed results.","","978-4-9077-6454-8978-1-5090-5614","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889619","LMI;SDP;SOS","Programming;Complexity theory;Linear matrix inequalities;Cost function;Symmetric matrices;Testing;Control systems","linear matrix inequalities;mathematical programming;minimisation;polynomials;robust control","sums of squares programming;SOS programming;linear cost function minimization;matrix polynomial;linear matrix inequality;LMI;robust stability analysis;polytopic system","","1","","8","","","","","IEEE","IEEE Conferences"
"A novel low complexity LDPC encoder based on RU algorithm with dynamic programming","X. Sun; Z. Zeng; Z. Yang","Information Engineering School, Communication University of China, Beijing, China; Engineering Research Center of Digital Audio &amp; Video, Ministry of Education, Communication University of China, Beijing, China; Engineering Research Center of Digital Audio &amp; Video, Ministry of Education, Communication University of China, Beijing, China","2010 3rd International Congress on Image and Signal Processing","","2010","7","","3258","3262","In this paper, we present a low-complexity high-efficiency LDPC encoder, based on classic method of Richardson and Urbanke with a novel dynamic programming algorithm, which we propose to substitute greedy algorithm for approximate triangulation with sparse matrix of LDPC codes. For the LDPC code in CMMB, for example, the complexity of encoding is reduced effectively, and an implementation of LDPC encoder for two different code rate (1/2 and 3/4) on Altera Stratix II EP1S180F102014 can achieve encoding rate 34 Mbps and 69 Mbps.","","978-1-4244-6516-3978-1-4244-6513-2978-1-4244-6515","10.1109/CISP.2010.5647849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647849","LDPC;encoder;dynamic programming;CMMB","Parity check codes;Encoding;Heuristic algorithms;Hardware;Sparse matrices;Dynamic programming;Algorithm design and analysis","computational complexity;digital video broadcasting;dynamic programming;mobile radio;multimedia communication;parity check codes;sparse matrices","low-complexity high-efficiency LDPC encoder;RU algorithm;dynamic programming algorithm;Richardson-Urbanke method;sparse matrix;CMMB;Altera Stratix II EP1S180F102014;Chinese mobile multimedia broadcasting standard;low density parity check codes;greedy algorithm;bit rate 34 Mbit/s;bit rate 69 Mbit/s","","1","","9","","","","","IEEE","IEEE Conferences"
"The effectiveness of reliability programs and tools based on design maturity and complexity","J. Lucas; A. Thiraviam; A. Elshennawy; A. M. Albar","Teledyne Oil & Gas; Teledyne Oil & Gas; University of Central Florida; Jazan University","2017 Annual Reliability and Maintainability Symposium (RAMS)","","2017","","","1","5","Many modern companies view reliability as a critical consideration during design, but often fail in achieving the required level of reliability in their products. The reasons for failing to achieve a product line's required reliability are numerous, but it is clear that the lack of proper implementation of an effective reliability program is one of the main drivers for this lack of success. In working with a number of companies that produce products ranging from simple to complex and with a variety of maturities, it is clear that reliebility programs are not “one-size-fits-all”, and rather need to be tailored to a product's complexity and current life cycle maturity. This paper examines products at three different levels of complexity (Low, Medium, and High), and three different levels of maturity (Qualified, Deployed, and Field Proven). Data from product lines at a variety of combinations of these categories have been examined. Results of this analysis indicate that levels of reliability are highly correlated to complexity, with an increase in complexity resulting in a decrease in reliability. Additionally, product line reliability is also observed to increase with product line maturity. Neither of these results were unexpected, but the analysis also indicated that some reliability tools, specifically FMECAs and FRACAS implementation, were most effective in increasing reliability in all product complexity levels, whereas other tools, such as RBDA, were effective in some cases, but had a more limited effectiveness on less complex products.","","978-1-5090-5284-4978-1-5090-5285","10.1109/RAM.2017.7889658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889658","Reliability Implementation;Effectiveness;FRACAS;FMECA Product Maturity;Complexity","Complexity theory;Companies;Reliability engineering;Testing;Product development;Bibliographies","design engineering;product life cycle management;reliability","reliability programs;design maturity;design complexity;life cycle maturity;product line maturity;reliability tools;FMECA;FRACAS implementation;product complexity levels;RBDA","","","","6","","","","","IEEE","IEEE Conferences"
"Visual end-user programming in smart homes: Complexity and performance","M. Reisinger; J. Schrammel; P. Fröhlich","Center for Technology Experience, Austrian Institute of Technology, Vienna, Austria; Center for Technology Experience, Austrian Institute of Technology, Vienna, Austria; Center for Technology Experience, Austrian Institute of Technology, Vienna, Austria","2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","","2017","","","331","332","End-user programming in smart homes addresses tasks that range from very simple to very complex. In this study we investigate how task complexity impacts performance when using two different visual programing representations: form-filling and data-flow programming. We invited 16 participants to create rules to solve smart-home situations of varying complexity and analyzed their completion rates for the two visual programming representations. We identify the following areas of difficulty for programming novices in our smart home scenario: choosing and connecting triggers and their specifications, and using Boolean operators. Both visual representations enabled users to specify complex rules, with advantages in different areas. They indicate that overall task complexity might be less decisive for performance than complexity of triggers and Boolean operators.","1943-6106","978-1-5386-0443-4978-1-5386-0444","10.1109/VLHCC.2017.8103495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103495","","Visualization;Programming;Prototypes;Complexity theory;Smart homes;Atmospheric measurements","Boolean algebra;data flow computing;formal specification;home automation;user interfaces;visual programming","visual end-user programming;task complexity;form-filling;data-flow programming;smart-home situations;visual programming representations;smart home scenario;Boolean operators;visual representations;visual programing representations;complex rule specification","","","","2","","","","","IEEE","IEEE Conferences"
"Quantifying the Analyzability of Software Architectures","E. Bouwers; J. P. Correia; A. v. Deursen; J. Visser","NA; NA; NA; NA","2011 Ninth Working IEEE/IFIP Conference on Software Architecture","","2011","","","83","92","The decomposition of a software system into components is a major decision in any software architecture, having a strong influence on many of its quality aspects. A system's analyzability, in particular, is influenced by its decomposition into components. But into how many components should a system be decomposed to achieve optimal analyzability? And how should the elements of the system be distributed over those components? In this paper, we set out to find answers to these questions with the support of a large repository of industrial and open source software systems. Based on our findings, we designed a metric which we call Component Balance. In a case study we show that the metric provides pertinent results in various evaluation scenarios. In addition, we report on an empirical study that demonstrates that the metric is strongly correlated with ratings for analyzability as given by experts.","","978-1-61284-399-5978-0-7695-4351","10.1109/WICSA.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959722","","Software systems;Electric breakdown;Size measurement;Computer architecture;Software architecture","public domain software;software architecture;software metrics;software quality;systems analysis","software architecture analyzability;software system decomposition;software quality aspects;distributed system;open source software systems;software metric;component balance","","17","","26","","","","","IEEE","IEEE Conferences"
"The Impact of Service Cohesion on the Analyzability of Service-Oriented Software","M. Perepletchikov; C. Ryan; Z. Tari","RMIT University, Melbourne; RMIT University, Melbourne; RMIT University, Melbourne","IEEE Transactions on Services Computing","","2010","3","2","89","103","Service-Oriented Computing (SOC) is intended to improve software maintainability as businesses become more agile and underlying processes and rules change more frequently. However, to date, the impact of service cohesion on the analyzability subcharacteristic of maintainability has not been rigorously studied. Consequently, this paper extends existing notions of cohesion in the Procedural and OO paradigms in order to account for the unique characteristics of SOC, thereby supporting the derivation of design-level software metrics for objectively quantifying the degree of service cohesion. The metrics are theoretically validated, and an initial empirical evaluation using a small-scale controlled study suggests that the proposed metrics could help predict analyzability early in the Software Development Life Cycle. If future industrial studies confirm these findings, the practical applicability of such metrics is to support the development of service-oriented systems that can be analyzed, and thus maintained, more easily. In addition, such metrics could help identify design problems in existing systems.","1939-1374;2372-0204","","10.1109/TSC.2010.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467025","Services systems;design concepts;maintainability;product metrics.","Software;System-on-a-chip;Business;Programming;Decision support systems;Software metrics","software architecture","service cohesion;service-oriented software;service-oriented computing;software maintainability;procedural paradigm;object-oriented paradigm;software development life cycle","","39","","46","","","","","IEEE","IEEE Journals & Magazines"
"Stall estimation metric: An architectural metric for estimating software complexity","A. Pandey","Computer Science Department, Sunrise University, Alwar, Rajasthan, India","2016 5th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","","2016","","","391","396","Software metrics can be classified in to two categories of code metrics and architectural metrics [1,2]. Code metrics basically consider analysis of data structures and algorithms for determining the complexity of the program [3,4,5]. Whereas architectural metrics consider the mechanism how system is processing data within its components and any existing dependencies between the processed data for estimating the complexity [3,6]. In any pipelined RISC processor program is executed instruction after instruction and it is also possible to have program dependencies between them [7]. These dependencies are of two types, data dependencies and control dependencies [8,9,10,11]. Data dependencies can be resolved by forwarding the data between stages of the pipelined RISC processor. Stall can be induced between the instructions while resolving some of these data dependencies. Stall can also be induced between instructions during branch prediction. The proposed architectural metric considers all those cases which will affect the overall execution of the program by causing stall together with the count of statements actually executed, for estimating the overall software complexity.","","978-1-5090-1489-7978-1-5090-1490","10.1109/ICRITO.2016.7784987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784987","Software metric;Architectural metric;Software complexity;Code complexity;Program dependency","Complexity theory;Estimation;Reduced instruction set computing;Software measurement;Micromechanical devices","data structures;pipeline processing;program diagnostics;reduced instruction set computing;software metrics","code metrics;architectural metrics;data structures;program complexity;data processing;pipelined RISC processor program execution;data dependencies;control dependencies;branch prediction;software metrics;software complexity;stall estimation metric","","","","23","","","","","IEEE","IEEE Conferences"
"Petri-object simulation: Software package and complexity","I. V. Stetsenko; V. I. Dorosh; A. Dyfuchyn","Bukovyna State University of Finance and Economics, 1 M. Shtern str., Chernivtsi, Ukraine, 58000; Ternopil State Economic University, 11 Lvivska str., Ternopil, Ukraine, 46000; Nathional Technical University of Ukraine “KPI”, 37 Peremogy prospect, Kiev, Ukraine, 03056","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","","2015","1","","381","385","This paper presents the Petri-object model formalization based on description of system dynamics with the stochastic timed Petri net. The model structure consists of Petri-objects with the use of object-oriented approach. The evaluation of computational complexity of model implementation based on mathematical description of Petri-object model is obtained. The theoretical evaluation of complexity is confirmed by experimental research. A significant reduction of Petri-object simulation complexity in comparison with stochastic Petri net is proved.","","978-1-4673-8361-5978-1-4673-8359-2978-1-4673-8358-5978-1-4673-8360","10.1109/IDAACS.2015.7340762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340762","computational complexity;descrete-event system;simulation modeling;stochastic Petri net;object-oriented approach;state equation","Petri nets;Computational modeling;Mathematical model;Object oriented modeling;Stochastic processes;Computational complexity","computational complexity;formal verification;object-oriented methods;Petri nets;software packages;stochastic processes","Petri-object simulation;software package;Petri-object model formalization;system dynamics description;stochastic timed Petri net;object-oriented approach;computational complexity","","2","","8","","","","","IEEE","IEEE Conferences"
"Application of computational intelligence in measuring the elasticity between software complexity and deliverability","S. Lavania; M. Darbari; N. J. Ahuja; I. A. Siddqui","University of Petroleum & Energy Studies, Dehradun, India; Department of Computer Science, Babu Banarsi Das University, Lucknow, India; Department Of Computer Science, University of Petroleum & Energy Studies, Dehradun, India; Celerity Networks Pvt. Ltd., Noida, Uttar Pradesh, India","2014 IEEE International Advance Computing Conference (IACC)","","2014","","","1415","1418","The paper highlights various issues of complexity and deliverability and its impact on software popularity. The use of Expert Intelligence system helps us in identifying the dominant and non-dominant impediments of software. FRBS is being developed to quantify the trade-off between complexity and deliverability issues of a software system.","","978-1-4799-2572-8978-1-4799-2571","10.1109/IAdCC.2014.6779533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779533","Deliverability;Complexity;Expert System","Conferences;Handheld computers;Decision support systems","computational complexity;expert systems;software quality","computational intelligence;software complexity;software deliverability;elasticity measurement;software popularity;expert intelligence system;nondominant impediments;dominant impediments;FRGS","","3","","10","","","","","IEEE","IEEE Conferences"
"Agave: A benchmark suite for exploring the complexities of the Android software stack","M. K. Brown; Z. Yannes; M. Lustig; M. Sanati; S. A. McKee; G. S. Tyson; S. K. Reinhardt","Florida State University, United States; Florida State University, United States; Florida State University, United States; Chalmers University of Technology, Sweden; Chalmers University of Technology, Sweden; Florida State University, United States; AMD Research, United States","2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2016","","","157","158","Traditional suites used for benchmarking high-performance computing platforms or for architectural design space exploration use much simpler virtual memory layouts and multitasking/ multithreading schemes, which means that they cannot be used to study the complex interactions among the layers of the Android software stack. To demonstrate this, we present memory reference and concurrency data showing how Android applications differ from traditional C benchmarks. We propose the Agave suite of open-source applications as the basis for a standard, multipurpose Android benchmark suite. We make all sources and tools available in hopes that the community will adopt and build on this initial version of Agave.","","978-1-5090-1953-3978-1-5090-1952","10.1109/ISPASS.2016.7482089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482089","","Androids;Humanoid robots;Benchmark testing;Instruction sets;Kernel;Standards","Android (operating system);concurrency (computers);multiprogramming;multi-threading;parallel processing;public domain software","Agave;Android software stack;high-performance computing platforms;architectural design space exploration;virtual memory layouts;multitasking schemes;multithreading schemes;memory reference;concurrency data;open-source applications;multipurpose Android benchmark suite","","","","2","","","","","IEEE","IEEE Conferences"
"Impact of multiple inheritance on cohesion complexity in software design","U. S. Poornima; V. Suma","Department of Computer Science and Engineering, Raja Reddy Institute of Technology, Bangalore, India; Department of Information Science and Engineering, Dayananda Sagar Institutions, Bangalore, India","2016 International Conference on Inventive Computation Technologies (ICICT)","","2016","1","","1","4","Research in Software Engineering has its own stride in recent years. Quality measures during both pre and post software development process has become very essential to control development time and cost. In Object Oriented methodology, Cohesion measures the design complexity. Many cohesion metrics are proposed for both high level and low level design. Most of them are at class level to measure relatedness among attribute-attribute, attribute-method and method-method. Since reusability is widely accepted, many Inheritance metrics are also proposed and proved using Weyuker9 property, which states complexity increases with interaction. This paper presents all Inheritance metrics and also proposes impact of Inheritance on Cohesion. This knowledge further enables one to design quality.","","978-1-5090-1285-5978-1-5090-1283-1978-1-5090-1286","10.1109/INVENTIVE.2016.7823223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823223","Quality metrics;Inheritance metrics;Cohesion;W9 property","Complexity theory;Software;Couplings;Computer science;Software measurement;Software engineering","inheritance;object-oriented methods;software metrics;software quality;software reusability","multiple inheritance metrics;cohesion complexity;software design;software engineering;software quality measures;software development process;object oriented methodology;design complexity;software reusability;Weyuker9 property","","1","","15","","","","","IEEE","IEEE Conferences"
"Exploring software complexity metric from procedure oriented to object oriented","D. Pawade; D. J. Dave; A. Kamath","Dept. of IT, K. J. Somaiya College of Engg, Mumbai, India; B. E. (IT), K. J. Somaiya College of Engg, Mumbai, India; B. E. (IT), K. J. Somaiya College of Engg, Mumbai, India","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","","2016","","","630","634","Software metrics are developed and used by various software organizations for evaluating and assuring software code quality, operation and continuance. We differentiate software complexity metrics in accordance with the procedural and object oriented approach of programming languages. Software developers and maintainers need to read and understand source programs. The increase in size and complexity of software affects several quality attributes, especially understandability and maintability. In this paper we discuss various procedural and object oriented software metrics. We tried to calculate complexity of sample code by using different procedural metrics. The propose this simulation is to show that complexity for same code differs from metric to metric. The effectiveness of any metric is different for procedural and object oriented approach. So we proposed a hybrid approach to get accurate complexity value.","","978-1-4673-8203-8978-1-4673-8202-1978-1-4673-8204","10.1109/CONFLUENCE.2016.7508195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508195","Software Complexity Metric;Procedural Metric;Object Oriented Metric","Complexity theory;Software;Software metrics;Vocabulary;Flow graphs;Programming","object-oriented methods;programming languages;software maintenance;software metrics;software quality;source code (software)","hybrid approach;object oriented software metrics;procedural oriented software metrics;quality attributes;source programs;software maintainers;software developers;programming languages;software code quality;software organizations;software complexity metrics","","2","","15","","","","","IEEE","IEEE Conferences"
"Deployment of integrated design for the reduction of software complexity","S. Yousefi; N. Modiri","Department of Computer Science, Islamic Azad University of Zanjan, Zanjan, Iran; Department of Computer Science, Islamic Azad University of Zanjan, Zanjan, Iran","The 7th International Conference on Networked Computing and Advanced Information Management","","2011","","","172","174","Software complexity is a well known paradigm within the software engineering and one which boasts a rich supply of metrics claiming to be able to define and measure the complexity of software. Within this area, different types of complexity have been identified and defined in an attempt to measure productivity, dependence, testing and maintenance effort. Software complexity has significant impact on cost and time of software development and maintenance. During the recent decades many efforts have been made to measure and control the complexity of software. This paper attempts to demonstrate the complexity which is based on IEEE Requirement Engineering document and impact in reducing the complexity of integrated design software.","","978-89-88678-37-4978-1-4577-0185","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967539","integrated design;software complexity;enterprise systems","Complexity theory;Software;Maintenance engineering;Software measurement;Software engineering;Computer science","formal specification;program testing;program verification;software architecture;software maintenance;systems analysis","integrated design deployment;software complexity reduction;software engineering;software maintenance;software development;IEEE requirement engineering document;integrated design software","","","","9","","","","","IEEE","IEEE Conferences"
"Demonstration of software-defined multiband OFDM with low-complexity phase noise compensation","X. Chen; Jiayuan He; Di Che; W. Shieh","Dept. of Electrical and Electronic Engineering, The University of Melbourne, VIC 3010, Australia; Dept. of Electrical and Electronic Engineering, The University of Melbourne, VIC 3010, Australia; Dept. of Electrical and Electronic Engineering, The University of Melbourne, VIC 3010, Australia; Dept. of Electrical and Electronic Engineering, The University of Melbourne, VIC 3010, Australia","OFC 2014","","2014","","","1","3","We demonstrate low-complexity phase noise compensation for a software-defined multiband OFDM system. Experimental results show that laser phase noise up to 1 MHz can be compensated for 114.8-Gb/s 16-QAM signals after 480-km SSMF transmission.","","978-1-5575-2993-0978-1-5575-2994","10.1364/OFC.2014.Tu2G.3","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6886908","","OFDM;Phase noise;Laser noise;Optical noise;Bit error rate;Fiber lasers;Receivers","OFDM modulation;optical communication;phase noise;quadrature amplitude modulation;software radio","software-defined multiband OFDM;low-complexity phase noise compensation;laser phase noise;16-QAM signals","","1","","9","","","","","IEEE","IEEE Conferences"
"Investigating the Complexity of Computational Intelligence using the Levels of Inheritance in an AOP based Software","S. S. Velan","Department of Computer Science and Engineering, Amity University, Dubai, UAE","2019 Advances in Science and Engineering Technology International Conferences (ASET)","","2019","","","1","5","Aspect Oriented Programming (AOP) is a software development methodology that aims to improve the modularity by encapsulating the cross-cutting concerns into modular units called aspects. Inheritance of classes and aspects play a vital role while redefining the units of encapsulation. In this research, the impact of using multi-level inheritance in Aspect Oriented Soft-ware is quantitatively evaluated, with an extended and validated metric, namely the Weighted Average Depth of Inheritance. The metric has been applied on two equivalent OO and AO versions of a multi-level inheritance based Banking application case study which was developed using Java and AspectJ programming languages respectively. Based on the measurement it was found that the AspectJ version exhibits reduced cognitive complexity of computational intelligence compared to its equivalent Java version.","","978-1-5386-8271-5978-1-5386-8272","10.1109/ICASET.2019.8714229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714229","AOP;Software Metrics;Inheritance;Cognitive Complexity;Computational Intelligence","Software;Java;Complexity theory;Software measurement;Computational intelligence;Encapsulation","","","","","","15","","","","","IEEE","IEEE Conferences"
"Complexity Theory: A New Paradigm for Software Integration","G. F. Hurlburt","Change Index","IT Professional","","2013","15","3","26","31","Software integration testing has always been problematic, and current methods are becoming even less practical given today's networked software components. As the need for a more holistic approach grows, complexity theory presents opportunities for better understanding and testing of software.","1520-9202;1941-045X","","10.1109/MITP.2012.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6256673","network science;complexity theory;software testing;software integration testing;software engineering;information technology","Software testing;Software engineering;Information technology;Performance evaluation;Complexity theory;Network architecture","computational complexity;integrated software;program testing","complexity theory;software integration testing;new paradigm","","3","","25","","","","","IEEE","IEEE Journals & Magazines"
"Proposal of cost-efficient and low-complexity platform for software defined visible light communication","M. Che; T. Kuboki; K. Kato","Graduate School of Information Science and Electrical Engineering, Kyushu University 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan; Graduate School of Information Science and Electrical Engineering, Kyushu University 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan; Graduate School of Information Science and Electrical Engineering, Kyushu University 744 Motooka, Nishi-ku, Fukuoka 819-0395, Japan","2017 22nd Microoptics Conference (MOC)","","2017","","","330","331","With the increasing popularity of LED lighting and a growing demand for bandwidth in wireless communication, the visible light communication (VLC) is one of the promising scheme for addressing the bottleneck in the emerging broadband access networks. For research in the field of VLC focused on solutions of contradiction between massive available bandwidth and limited modulation bandwidth of LED, the purpose of this paper is to design an VLC experiment platform with characteristics of high flexibility and rapid productization. As a result, this platform not only expand the concept of software defined communication, but has the advantage of simple structure and low cost.","","978-4-8634-8609-6978-1-5090-4924","10.23919/MOC.2017.8244620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244620","","Light emitting diodes;Software;TCPIP;Visible light communication;Lighting;Bandwidth;Real-time systems","broadband networks;free-space optical communication;light emitting diodes;software radio","broadband access networks;wireless communication;LED lighting;software defined visible light communication;low-complexity platform;software defined communication","","","","3","","","","","IEEE","IEEE Conferences"
"Using Relative Complexity Measurement Which from Complex Network Method to Allocate Resources in Complex Software System's Gray-Box Testing","L. Meng; M. Lu; B. Huang; X. Xu","NA; NA; NA; NA","2011 International Symposium on Computer Science and Society","","2011","","","189","192","Software testing costs would be reduced if we could find which classes or models of software are more complex and thus more likely to have defects. This paper defines a method which uses the software complexity measurement from complex network method to allocate resources in complex software system for gray-box testing. It presents the way of mapping the software codes to complex network and some metric's effects in software network, such as degree and degree of ripple. By using these metrics, it defines the complexity measurement to complex software network. The work about C++ complex software system shows how to use this method.","","978-1-4577-0644","10.1109/ISCCS.2011.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004417","software complex network;software complexity measurement;software gray-box testing;test resource allocation","Complexity theory;Testing;Complex networks;Software systems;Software measurement","C++ language;computational complexity;program testing;resource allocation","relative complexity measurement;complex network method;resource allocation;complex software system gray box testing;software codes;software network;C++ complex software system","","","","19","","","","","IEEE","IEEE Conferences"
"Notice of Removal: Functionality versus Complexity Analysis of Project Management Software in Kingdom of Bahrain","S. Mohammad","NA","2016 6th International Conference on IT Convergence and Security (ICITCS)","","2016","","","1","6","Document is unavailable: This DOI was registered to an article that was not presented by the author(s) at this conference. As per section 8.2.1.B.13 of IEEE's ""Publication Services and Products Board Operations Manual,"" IEEE has chosen to exclude this article from distribution. We regret any inconvenience.","","978-1-5090-3765-0978-1-5090-3766","10.1109/ICITCS.2016.7740339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740339","","","","","","","","","","","","","IEEE","IEEE Conferences"
"Framework of software complexity methodology","M. Rafighi; N. Modiri","NA; NA","The 7th International Conference on Networked Computing and Advanced Information Management","","2011","","","41","44","It is broadly clear that complexity is one of the software natural features. Software natural complexity and software requirement functionality are two inseparable part and they have special range. measurement complexity have explained with using the MacCabe and Halsted models and with an example discuss about software complexity in this paper Flow metric information Henry and Kafura, complexity metric system Agresti-card-glass, design metric in item's level have compared and peruse then categorized object oriented and present a model with 4 level of software complexity.","","978-89-88678-37-4978-1-4577-0185","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5967514","McCabe model;Halstead model;measurement software complexity","Complexity theory;Software;Software measurement;Maintenance engineering;Object oriented modeling;Organizations","software maintenance","software complexity methodology;software natural complexity;software requirement functionality;measurement complexity;Halsted models;MacCabe models;Flow metric information;object oriented","","","","6","","","","","IEEE","IEEE Conferences"
"Software complexity measurement based on complex network","L. Hanyan; W. Shihai; L. Bin; X. Peng","Science &amp; Technology on Reliability &amp; Environmental Engineering Laboratory, School of Reliability and Systems Engineering, Beihang university, Beijing, 100191, PR China; Science &amp; Technology on Reliability &amp; Environmental Engineering Laboratory, School of Reliability and Systems Engineering, Beihang university, Beijing, 100191, PR China; Science &amp; Technology on Reliability &amp; Environmental Engineering Laboratory, School of Reliability and Systems Engineering, Beihang university, Beijing, 100191, PR China; Science &amp; Technology on Reliability &amp; Environmental Engineering Laboratory, School of Reliability and Systems Engineering, Beihang university, Beijing, 100191, PR China","2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2017","","","262","265","Usually the complexity metric of software focuses on the complexity of code level, function level or structure level separately. It lacks of measurement for the comprehensive complexity of software system. This paper proposes a complexity metric model of three-level cascade network that based on complex network theory. In this metric model, the complexity of code level, function level and structure level are measured and the cascaded relationship between the three levels are analyzed. At last, the three-level cascade network model is built and the comprehensive complexity of software system is measured though the three-level cascade network model. The experiment result shows that the comprehensive complexity of the software system is correlated positively to the number of software defects.","2327-0594","978-1-5386-0497-7978-1-5386-0496-0978-1-5386-0498","10.1109/ICSESS.2017.8342910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342910","software complexity;cascade network;software metric;complex network;defect prediction","Complexity theory;Software systems;Complex networks;Software measurement","cascade networks;complex networks;software metrics","three-level cascade network model;structure level;complex network theory;complexity metric model;software system;function level;code level;software complexity measurement;software defects","","","","13","","","","","IEEE","IEEE Conferences"
"Software complexity: A fuzzy logic approach","S. Sabharwal; R. Sibal; P. Kaur","Department of Computer Engineering and Information Technology, Netaji Subhas Institute of Technology, Delhi, India; Department of Computer Engineering and Information Technology, Netaji Subhas Institute of Technology, Delhi, India; Department of Computer Engineering and Information Technology, Netaji Subhas Institute of Technology, Delhi, India","2012 International Conference on Communication, Information & Computing Technology (ICCICT)","","2012","","","1","6","Software complexity is one of the important quality attribute and predicting complexity is a difficult task for software engineers. Current measures can be used to compute complexity but these methods are not sufficient. New methods or paradigms are being searched for predicting complexity because complexity prediction can help us in estimating many other quality attributes like testability and maintainability. The main goal of this paper is to explore the role of new paradigms like fuzzy logic in complexity prediction. In this paper we have proposed a fuzzy logic based approach to predict software complexity.","","978-1-4577-2078-9978-1-4577-2077-2978-1-4577-2076","10.1109/ICCICT.2012.6398233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398233","cyclomatic complexity;essential complexity;looping depth;fuzzy logic","Complexity theory;Software;Fuzzy logic;Pragmatics;Measurement;Computers;Testing","fuzzy logic;program testing;software maintenance;software metrics;software quality","fuzzy logic approach;software complexity;quality attribute;testability;maintainability","","1","","23","","","","","IEEE","IEEE Conferences"
"Low complexity and narrow transition band filter banks for software defined radio applications","W. Zhang; Y. Chen; Z. Dou; J. Hu","College of Information and Communication Engineering, Harbin Engineering University, Harbin 150001, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin 150001, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin 150001, China; 92956 Unit of the Chinese Peoples Liberation Army, Dalian 116041, China","2016 IEEE 13th International Conference on Signal Processing (ICSP)","","2016","","","1171","1175","We presented an improved efficient architecture of low complexity and narrow transition band filter banks for software defined radio applications. A drawback in filter banks is that the narrow transition band filter is designed with high computation complexity. In order to get around this problem, this paper shows how the filter banks architecture can zbe designed with low complexity by using frequency response masking technology. We derives efficient architecture of exponentially modulated filter banks based the theory of polyphase, and uses frequency response masking technology to design narrow transition band filter. With employed this method we can extract uniform bandwidths and very narrow bandwidth channels compared to polyphase filter banks. A simulation is provided to illustrate the method of the proposed filter banks architecture. It is shown that the resulting filter banks entail substantially less computational complexity compared to the polyphase filter banks.","2164-5221;2164-5221","978-1-5090-1345-6978-1-5090-1344-9978-1-5090-1343-2978-1-5090-1346","10.1109/ICSP.2016.7878012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878012","low complexity;filter banks;polyphase;frequency response masking","Filter banks;Digital filters;Filtering theory;Complexity theory;Bandwidth;Filtering algorithms;Computer architecture","channel bank filters;computational complexity;frequency response;software radio","narrow transition band filter banks;low complexity filter banks;software defined radio;computational complexity;frequency response masking technology;bandwidth channels;polyphase filter banks","","","","12","","","","","IEEE","IEEE Conferences"
"A trade-off establishment between software complexity and its usability using evolutionary multi-objective optimization (EMO)","V. Yadav; S. Lavania; A. Chaudhary; N. Dhanda","Department of Computer Science, Vishveshwarya Institute of Technology &amp; Management, G.B. Nagar, India; Department of Computer Science, Goel Institute of Technology &amp; Management, Lucknow, India; Department of Computer Science, Vishveshwarya Institute of Technology &amp; Management, G.B. Nagar, India; Department of Computer Science, Goel Institute of Technology &amp; Management, Lucknow, India","2014 International Conference on Contemporary Computing and Informatics (IC3I)","","2014","","","80","82","In this paper, by the application of evolutionary multi-objective optimization (EMO) technique, we will be establishing a trade-off balance between the two conflicting aspects i.e. complexity of a software and the usability (business value) of the software.","","978-1-4799-6629","10.1109/IC3I.2014.7019730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019730","Software Complexity;Usability;Multi-Objective Optimization","Complexity theory;Usability;Companies;Optimization;Computer science","evolutionary computation;optimisation;software engineering","trade-off establishment;software complexity;software usability;evolutionary multiobjective optimization technique;EMO;business value","","","","27","","","","","IEEE","IEEE Conferences"
"Improvement of complexity matrix of IFPUG in embedded-software projects","Hongjun He; Lei Xia; Li Luo; Huzhong Yan; Jiao Zhu; Jibao Tang","College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China","2016 2nd IEEE International Conference on Computer and Communications (ICCC)","","2016","","","911","914","IFPUG is a widely used functional size measurement method, but when applied in embedded software project, the measurement results and the actual sizes have big deviations. The paper finds that functional types with low complexity of embedded software is relatively large, so proposes to refine and adjust the low level part of complexity matrix. Measure the same software projects based on adjusted complexity matrix, the deviation of measurement results reduced from 24%~73.4% to 3.8%~38.8%, and the average deviation reduced from 45.7% to 12.8%.","","978-1-4673-9026-2978-1-4673-9025-5978-1-4673-9027","10.1109/CompComm.2016.7924836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924836","functional size measurement;IFPUG;embedded software;complexity matrix","Size measurement;Complexity theory","embedded systems;matrix algebra;project management;software metrics","complexity matrix improvement;IFPUG;embedded-software projects;functional size measurement method;average deviation","","","","15","","","","","IEEE","IEEE Conferences"
"Dynamic analysis of Object-Oriented software complexity","H. Li","Center of Modern Educational Technology, Shanghai University of Political Science and Law, Shanghai, China","2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)","","2012","","","1791","1794","How to understand, manage and control Object-Oriented software complexity is a great challenge for software engineering. Recent research achievements in software engineering introduce complex network theory to explore structural complexity. These achievements emphasize understanding Object-Oriented software system as a whole structure instead of focusing on local codes and behaviors. A common way to define software network is based on static structural properties. However, for real-time is a primary characteristic of software system, and polymorphism, dynamic binding, and the unused codes present in software system, the static structural properties are imprecise as they do not perfectly reflect the actual code taking place among classes. This paper describes the reason that software complexity can precisely measured based on dynamic analysis of systems and refers the means to analyze dynamic characteristic of software system structure as complex network.","","978-1-4577-1415-3978-1-4577-1414-6978-1-4577-1413","10.1109/CECNet.2012.6201902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6201902","software complexity;Dynamic analysis;Complex network;software network","Complexity theory;Java;Complex networks;Software systems;Unified modeling language;Instruments","complex networks;object-oriented methods;software engineering","object-oriented software complexity dynamic analysis;object-oriented software complexity management;object-oriented software complexity control;software engineering;complex network theory;structural complexity;static structural properties;polymorphism;dynamic binding;unused codes;software system structure dynamic characteristics","","2","","14","","","","","IEEE","IEEE Conferences"
"Application of complexity and brittleness on software architecture","H. Zhang; C. Hu; X. Wang","School of Computer Science &amp; Technology, Beijing Institute of Technology, Beijing, China; School of Software, Beijing Institute of Technology, Beijing, China; School of Computer Science &amp; Technology, Beijing Institute of Technology, Beijing, China","2016 8th IEEE International Conference on Communication Software and Networks (ICCSN)","","2016","","","570","573","Just like the catastrophe in power grid, software system may collapse during its operation. It reflects the complexity in software system itself, and the brittleness of software system is the main reason which results in the collapse. The notion of complex system and brittleness is introduced into the study of software system and some aspects which can induce the system to collapse are also discussed. Specifically, the notions of complex system, complex network and brittleness are introduced at first, then a detailed description of the complexity of software system is given; following that a fast-slow alternative dynamic model is built, which consists of a slow-dynamic model and a fast-dynamic model. In the end, a complex theory framework of brittleness on software architecture is presented, which covers the brittleness of software architecture from the way of language description, the analytical method, modeling to the evaluation. This can give a comprehensive research platform on the brittleness of software system. Some main research areas are also given for future study.","","978-1-5090-1781-2978-1-5090-1779-9978-1-5090-1782","10.1109/ICCSN.2016.7586587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586587","brittleness analysis;complex network;complex system;software architecture;software security","Software systems;Complex systems;Software architecture;Topology;Complexity theory;Complex networks;Power system faults","software architecture;software metrics","software architecture;software system collapse;software system complexity;software system brittleness;complex network;fast-slow alternative dynamic model;language description","","","","12","","","","","IEEE","IEEE Conferences"
"Managing Software Complexity with Business Rules","M. Zygmunt; M. Budyn","NA; NA","2010 International Conference on Complex, Intelligent and Software Intensive Systems","","2010","","","534","537","Next generation solutions for the plant maintenance must be flexible, self maintaining self configuring, robust and, in general must follow the latest trends of the internet revolution. To achieve this vision, next generation plant maintenance rules technology has been chosen and successfully applied to the continuous monitoring solution. In this paper we present a method of managing complexity of the system diagnostics using selected enabling technologies. The main enabling technologies listed in the paper are business rules and OPC Unified Architecture protocol. In the section ¿Self Maintaining Asset (SMA)¿, the authors present some monitoring agents and the framework applied to the continuous monitoring solution.","","978-1-4244-5918-6978-1-4244-5917-9978-0-7695-3967","10.1109/CISIS.2010.176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447472","business rules;diagnostics;AI","Logic programming;Software maintenance;Monitoring;Application software;Technology management;Logic devices;Logic testing;Software systems;Robustness;Collaboration","program diagnostics;software development management;software metrics","software complexity management;business rules;next generation plant maintenance rules technology;continuous monitoring solution;system diagnostics complexity;enabling technology;OPC unified architecture protocol;self maintaining asset","","1","","12","","","","","IEEE","IEEE Conferences"
"A large-scale empirical study on the cognitive complexity of software","V. Chiew; Y. Wang","Theoretical and Empirical Software Engineering Research Centre, Dept. of Electrical and Computer Engineering, Schulich Schools of Engineering, University of Calgary, 2500 University Drive, NW, Calgary, Alberta, Canada T2N 1N4; Theoretical and Empirical Software Engineering Research Centre, Dept. of Electrical and Computer Engineering, Schulich Schools of Engineering, University of Calgary, 2500 University Drive, NW, Calgary, Alberta, Canada T2N 1N4","CCECE 2010","","2010","","","1","4","There are many measures for software complexities in software engineering. An emerging developer-oriented measure is recently developed known as the cognitive complexity from the field of cognitive informatics and cognitive computing. This paper describes an empirical approach using a Software Cognitive Complexity Analysis Tool (SCCAT) to analyze a comprehensive set of real-world software system programs. Practical and quantifiable results of the cognitive complexity measure are then presented. Additional characteristics of cognitive complexity measure are described that provide insights into human cognition in software engineering.","0840-7789;0840-7789","978-1-4244-5376-4978-1-4244-5377","10.1109/CCECE.2010.5575116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5575116","Software engineering;cognitive complexity;measurement;metics;empirical studies;comparative studies","Complexity theory;Software measurement;Software;Libraries;Software engineering;Java;Humans","computational complexity;software engineering","large scale empirical study;software cognitive complexity;software engineering;cognitive complexity analysis tool;SCCAT;software system programs","","","","12","","","","","IEEE","IEEE Conferences"
"A Data Mining Model to Predict Software Bug Complexity Using Bug Estimation and Clustering","N. K. Nagwani; A. Bhansali","NA; NA","2010 International Conference on Recent Trends in Information, Telecommunication and Computing","","2010","","","13","17","Software defect(bug) repositories are great source of knowledge. Data mining can be applied on these repositories to explore useful interesting patterns. Complexity of a bug helps the development team to plan future software build and releases. In this paper a prediction model is proposed to predict the bug's complexity. The proposed technique is a three step method. In the first step, fix duration for all the bugs stored in bug repository is calculated and complexity clusters are created based on the calculated bug fix duration. In second step, bug for which complexity is required its estimated fix time is calculated using bug estimation techniques. And in the third step based on the estimated fix time of bug it is mapped to a complexity cluster, which defines the complexity of the bug. The proposed model is implemented using open source technologies and is explained with the help of illustrative example.","","978-1-4244-5957-5978-1-4244-5956-8978-0-7695-3975","10.1109/ITC.2010.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460784","Software bug repositories;Bug complexity;Complexity Prediction","Data mining;Predictive models;Computer bugs;Open source software;Programming;Software testing;Project management;Software development management;Telecommunication computing;System testing","data mining;pattern clustering;program debugging","data mining;software bug complexity;bug estimation;bug clustering;software defect repositories;prediction model;complexity cluster;open source technology","","4","","14","","","","","IEEE","IEEE Conferences"
"Static and dynamic software metrics complexity analysis in regression testing","M. K. Debbarma; N. Kar; A. Saha","Department Computer Science &amp; Engineering National Institute of Technology, Agartala, India; Department Computer Science &amp; Engineering National Institute of Technology, Agartala, India; Department Computer Science &amp; Engineering National Institute of Technology, Agartala, India","2012 International Conference on Computer Communication and Informatics","","2012","","","1","6","In maintenance, assuring code quality and operation, software metrics is widely used by the various software organizations. Software metrics quantify different types of software complexity like size metrics, control flow metrics and data flow metrics. These software complexities must be continuously calculated, followed and controlled. One of the main objectives of software metrics is that measures static and dynamic metrics analysis. It is always considered that high degree of complexity in a fragment is bad in comparison to a low degree of complexity in a fragment. Software metrics can be used in different phases of the software lifecycle. In this paper we will discuss the different metrics and comparison between both static and dynamic metrics. We try to evaluate and analyze different aspects of software static and dynamic metrics in regression testing which offers of estimating the effort needed for testing.","","978-1-4577-1583-9978-1-4577-1580-8978-1-4577-1582","10.1109/ICCCI.2012.6158825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6158825","Software metrics;Regression testing;object-oriented programming","Complexity theory;Software;Testing;Software metrics;Programming","program diagnostics;program testing;regression analysis;software metrics","static software metrics complexity analysis;dynamic software metrics complexity analysis;regression testing;size metrics;software lifecycle;data flow metrics;control flow metrics","","1","","19","","","","","IEEE","IEEE Conferences"
"Addressing the impact of road scheme complexity on the driver: Development of a software tool to analyse driver workload arising from its road features and other infrastructure","A. Mckenzie-Kerr; C. Turner; A. Peron; P. Wadsworth; C. Lansdown","Human Engineering Limited, UK; Human Engineering Limited, UK; Human Engineering Limited, UK; Capita Symonds Limited, UK; Highways Agency, UK","IET Road Transport Information and Control Conference and the ITS United Kingdom Members' Conference (RTIC 2010) - Better transport through technology","","2010","","","1","6","As road scheme design including ITS features, has a direct impact on the behaviour of drivers and consequently on road safety, understanding the demands placed on the driver by different scheme designs is critical. This paper outlines the purpose for investigating driver workload and scheme design, the applicable science and the development of a software tool created to allow scheme designers to predict driver workload. The tool can be used to isolate infrastructure arrangements such as message signs and signals and other design features contributing to high workload. The designer can trial alternative scheme designs to evenly spread driver workload and consequently the potential for human error.","","978-1-84919-242","10.1049/cp.2010.0394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5549222","DRIVER WORKLOAD;SCHEME DESIGN;ATM;DEPARTURES;DRIVABILITY;ITS","","","","","","","","","","","","IET","IET Conferences"
"2nd International Workshop on Variability and Complexity in Software Design (VACE 2017)","M. Galster; M. Goedicke; D. Weyns; U. Zdun","NA; NA; NA; NA","2017 IEEE/ACM 2nd International Workshop on Variability and Complexity in Software Design (VACE)","","2017","","","1","1","Summary form only given, as follows. VACE 2017 Workshop Summary. Many of today's software systems must accommodate a wide range of usage and deployment scenarios (e.g., product lines/families, self-adaptive systems, configurable or customizable single systems, open platforms, contextaware mobile apps, dynamic service-based systems). This variability can occur in software functionality and quality. Given the increasing size of software-intensive systems (e.g., ecosystems, systems of systems), emerging application domains (e.g., unmanned aerial vehicles, software-defined networking), dynamic and critical operating conditions (e.g., disaster monitoring and response systems), fast moving and highly competitive markets (e.g., mobile apps), and increasingly powerful and versatile hardware (e.g., Raspberry Pi), the additional complexity in the design caused by variability becomes increasingly difficult to handle. Variability has previously been targeted within software engineering sub-communities (e.g., requirements engineering, product lines, architecture), domains and venues (e.g., MobileSoft, EMSoft). VACE aims at offering one venue to jointly discuss experiences and potential synergies, forge new collaborations and discuss innovative solutions that address engineering high-quality software for dynamic, flexible and variable environments. This second edition of the VACE workshop has the theme ""Broadening Perspectives"" to acknowledge the multi- and cross-disciplinary nature of problems and solutions.","","978-1-5386-2803-4978-1-5386-2804","10.1109/VACE.2017.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968055","","Complexity theory;Vehicle dynamics;Software design;Mobile communication;Software systems","","","","","","","","","","","IEEE","IEEE Conferences"
"A Cognitive Model for Software Architecture Complexity","E. Bouwers; J. Visser; C. Lilienthal; A. van Deursen","NA; NA; NA; NA","2010 IEEE 18th International Conference on Program Comprehension","","2010","","","152","155","This paper introduces a Software Architecture Complexity Model (SACM) based on theories from cognitive science and system attributes that have proven to be indicators of maintainability in practice. SACM can serve as a formal model to reason about why certain attributes influence the complexity of an implemented architecture. Also, SACM can be used as a starting point in existing architecture evaluation methods such as the ATAM. Alternatively, SACM can be used in a stand-alone fashion to reason about a software architecture's complexity.","1092-8138;1092-8138","978-1-4244-7603-9978-1-4244-7604-6978-0-7695-4113","10.1109/ICPC.2010.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521750","Software Architecture Evaluation;Software Architecture;Complexity;Cognitive models","Software architecture;Computer architecture;Cognitive science;Software engineering;Software maintenance;Paper technology;Software systems;Investments;Performance evaluation;Counting circuits","software architecture","software architecture complexity model;cognitive science;system attributes;architecture evaluation methods","","2","","14","","","","","IEEE","IEEE Conferences"
"Software complexity analysis using halstead metrics","T. Hariprasad; G. Vidhyagaran; K. Seenu; C. Thirumalai","MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India; MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India; MS Software Engineering, School of Information Technology and Engineering, VIT University, Vellore, India; School of Information Technology and Engineering, VIT University, Vellore, India","2017 International Conference on Trends in Electronics and Informatics (ICEI)","","2017","","","1109","1113","Software Complexity influences inward connections. Higher the multifaceted nature, bigger the deformities. Programming complexity for any product or a program is hard to discover without utilizing any measurements. The unpredictability, time and exertion fluctuate starting with one program then onto the next. For this reason, Halstead measurements are presented which recognizes the product complexity of a program by utilizing source line with the assistance of operands and operators. This metric was produced by Maurice Halstead to decide a quantitative measure of complexity specifically from the operands and operators in the module. This article gives a correlation between a program that was composed in two unique dialects to distinguish unpredictability, time and exertion of both and furthermore to gauge which programming dialect was better in wording less time to execute, least exertion.","","978-1-5090-4257-9978-1-5090-4258","10.1109/ICOEI.2017.8300883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300883","Software Complexity;Halstead Metrics","Programming;Complexity theory;Volume measurement;Software;Correlation;Conferences","programming;software maintenance;software metrics","software complexity analysis;inward connections;programming complexity;product complexity;source line;operands;operators;Maurice Halstead;programming dialect;Halstead metrics","","3","","55","","","","","IEEE","IEEE Conferences"
"Information Flow Complexity Analysis for Design Flaws Detection in Object-Oriented Software","S. Mekruksavanich","Department of Computer Engineering, University of Phayao, Phayao, Thailand","2019 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering (ECTI DAMT-NCON)","","2019","","","101","104","In order to achieve desirable levels of software availability and extendibility it is first necessary to be able to detect design flaws effectively. Flaw detection must be considered a key element of the software development process and also a part of the maintenance process. When design flaws are discovered and addressed early in the development process, the cost and life cycle of the software can be improved, and to accomplish this aim there have been attempts made to develop an automated method for fixing flaws. This paper seeks to create an improved technique to offer better threshold determination which can be used in a metric-based approach to flaw detection employing information flow complexity. The metrics applied are fan-in and fan-out, along with the calling mechanism in an object-oriented system since these can emphasize the improved information flow complexity estimation method, which allows assessment of the software design static measure and enables limitation of the current work. The findings reveal that the approach established in this study permits the determination of thresholds for a metric-based technique to detect design flaws.","","978-1-5386-8072-8978-1-5386-8071-1978-1-5386-8073","10.1109/ECTI-NCON.2019.8692287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692287","software metrics;information flow;flaw detection;software maintenance","","flaw detection;object-oriented programming;software maintenance;software metrics","maintenance process;flaw detection;object-oriented system;software design static measure;information flow complexity analysis;design flaws detection;object-oriented software;software availability;software development process;information flow complexity estimation method;metric-based approach","","","","18","","","","","IEEE","IEEE Conferences"
"Predicting software complexity by means of evolutionary testing","A. F. Nogueira","University of Coimbra, Portugal","2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering","","2012","","","402","405","One characteristic that impedes software from achieving good levels of maintainability is the increasing complexity of software. Empirical observations have shown that typically, the more complex the software is, the bigger the test suite is. Thence, a relevant question, which originated the main research topic of our work, has raised: ""Is there a way to correlate the complexity of the test cases utilized to test a software product with the complexity of the software under test?"". This work presents a new approach to infer software complexity with basis on the characteristics of automatically generated test cases. From these characteristics, we expect to create a test case profile for a software product, which will then be correlated to the complexity, as well as to other characteristics, of the software under test. This research is expected to provide developers and software architects with means to support and validate their decisions, as well as to observe the evolution of a software product during its life-cycle. Our work focuses on object-oriented software, and the corresponding test suites will be automatically generated through an emergent approach for creating test data named as Evolutionary Testing.","","978-1-4503-1204-2978-1-4503-1204","10.1145/2351676.2351759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494966","Complexity Measurement;Evolutionary Testing;Object-Oriented Software","","object-oriented programming;program testing;software maintenance","software complexity prediction;evolutionary testing;software maintainability;software product testing;automatically generated test cases;software product test case profile;software architects;developers;software product lifecycle;object-oriented software","","","","16","","","","","IEEE","IEEE Conferences"
"A graph theory based algorithm for the computation of cyclomatic complexity of software requirements","C. W. Mohammad; M. Shahid; S. Z. Husain","Computer Science and Technology Research Group, Department of Applied Sciences and Humanities, Faculty of Engineering and Technology, Jamia Millia Islamia (A Central University), New Delhi-110025, India; Computer Science and Technology Research Group, Department of Applied Sciences and Humanities, Faculty of Engineering and Technology, Jamia Millia Islamia (A Central University), New Delhi-110025, India; Department of Computer Science, Faculty of Natural Sciences, Jamia Millia Islamia (A Central University), New Delhi-110025, India","2017 International Conference on Computing, Communication and Automation (ICCCA)","","2017","","","881","886","Cyclomatic complexity (CC) is used to find out the number of independent paths in a program or software. In literature, different algorithms have been proposed to compute the CC of a program or software. In this paper, we proposed an algorithm for the computation of CC based on graph theory. We apply the proposed method to compute the CC of login module of an Institute Examination System (IES).","","978-1-5090-6471-7978-1-5090-6472","10.1109/CCAA.2017.8229931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229931","Graph theory;Control flow graph;Software Testing;Cyclomatic Complexity","Flow graphs;Complexity theory;Software;Software algorithms;Graph theory;Software testing","educational administrative data processing;graph theory;software metrics","graph theory;software requirements;independent paths;program;different algorithms;cyclomatic complexity computation;CC;login module;Institute Examination System;IES","","","","12","","","","","IEEE","IEEE Conferences"
"On the Complexity of Software Systems","T. Mens","University of Mons, Belgium","Computer","","2012","45","8","79","81","Developing the tools necessary for reasoning about and understanding large, complex software systems requires interdisciplinary research that borrows from other domains where complexity similarly comes into play.","0018-9162;1558-0814","","10.1109/MC.2012.273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272264","software engineering;software technologies;complexity","Software engineering;Complexity theory","software tools","complex software systems;interdisciplinary research;software tools","","3","","","","","","","IEEE","IEEE Journals & Magazines"
"Modeling Code Analyzability at Method Level in J2EE Applications","P. Meananeatra; E. Rattanaleadnusorn; S. Rongviriyapanish; T. Kitcharoensup; T. Wisuttikul; B. Charoendouysil","NA; NA; NA; NA; NA; NA","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","","2013","2","","61","66","One of the main reasons for improving code structure is to make the cause of error code easily identified. Thus, developers need an analyzability prediction model to evaluate the analyzability of code in order to locate classes to be improved. To identify classes to be improved, developers must analyze all methods of classes for finding problem methods. Therefore, analyzability prediction model must be created for calculating analyzability level of method. Currently, J2EE applications are legacy systems and need to be continually maintained. Hence, code analyzability prediction at method level in J2EE application helps developers to know which method should be improved for increasing code understanding and reducing time for finding error causes. However, there is a lack of analyzability prediction model for J2EE application and existing research works on analyzability prediction model do not focus on method level. Therefore, this paper proposes how to create analyzability prediction model at method level for J2EE applications through ordinal logistic regression.","1530-1362;1530-1362","978-1-4799-2144-7978-1-4799-2143","10.1109/APSEC.2013.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754352","analyzability;maintainability;software maintenance;J2EE","Analytical models;Measurement;Predictive models;Data models;Logistics;Mathematical model;Software","Java;program diagnostics;regression analysis;software maintenance","method level;J2EE applications;legacy systems;code analyzability prediction model;code understanding;ordinal logistic regression","","2","","14","","","","","IEEE","IEEE Conferences"
"Low-Complexity LLR Calculation for Gray-Coded PAM Modulation","I. Kang; J. Shin; H. Kim","Department of Electronics Engineering, Pusan National University, Busan, Korea; Department of Electronics Engineering, Pusan National University, Busan, Korea; Department of Electronics Engineering, Pusan National University, Busan, Korea","IEEE Communications Letters","","2016","20","4","688","691","Higher-order modulation schemes are employed to improve the spectral efficiency in modern communication systems. However, the conventional Max-Log-MAP symbol-to-bit demapper suffers from a large computational burden. To cope with this problem, this letter proposes a novel log-likelihood ratio computation algorithm for Gray-coded higher-order pulse-amplitude modulation (PAM). The proposed method reduces the order of complexity from O(2<sup>M</sup>) to O(M) without any difference of the LLR values compared with the Max-Log-MAP demapper, where M is the number of bits in one PAM symbol. Moreover, it is also shown that the proposed algorithm achieves superior complexity reduction compared with a recently proposed low-complexity demapper.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2532880","National Research Foundation of Korea; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421967","higher-order modulation;log-likelihood ratio;Max-Log-MAP demapper;pulse-amplitude modulation;Higher-order modulation;log-likelihood ratio;Max-Log-MAP demapper;pulse-amplitude modulation","Complexity theory;Algorithm design and analysis;Indexes;Approximation algorithms;Phase shift keying;Computational modeling","communication complexity;Gray codes;pulse amplitude modulation","low-complexity LLR calculation;gray-coded PAM modulation;higher-order modulation scheme;spectral efficiency;communication system;max-log-MAP symbol-to-bit demapper;log-likelihood ratio computation algorithm;pulse-amplitude modulation;complexity reduction;low-complexity demapper","","3","","11","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity near-optimal detection of dc2-balanced codes","K. A. S. Immink","Turing Machines Inc, The Netherlands","Electronics Letters","","2016","52","9","719","720","Two constructions of a low-complexity near-optimal detection method of dc2-balanced codes are presented. The methods presented are improvements on Slepian's algorithm for optimal detection of permutation codes.","0013-5194","","10.1049/el.2015.3933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456466","","","codes;communication complexity;signal detection","low-complexity near-optimal detection;dc2-balanced codes;Slepian's algorithm;permutation codes","","1","","8","","","","","IET","IET Journals & Magazines"
"An efficient motion estimation scheme with low complexity in Video Coding","H. Yoon; M. Kim","Department of computer science, Chonnam national university, Gwang-ju, South Korea; Department of health and medical, Jeonnam Provincial college, Damyang, Jeollanamdo, South Korea","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","","2015","","","198","201","To transmit and to store digital video sequences, compression is necessary. Motion Estimation (ME) Technique is used to redundant data in video sequences. ME which limits the performance of image quality, generated bitrates and encoding time requires huge complexity. To reduce the computational complexity, a Hierarchical motion estimation scheme in Multi-view Video Coding (MVC) is proposed. The proposed method exploits the characteristics of the distribution of motion vectors to terminate the motion estimation early and to place the search points in the search area. Experiment results show that the complexity reduction of the proposed method over FS and TZ can be up to 98.9% and 42~78% respectively while maintaining image quality and bitrates. This electronic document is a “live” template and already.","","978-1-4799-8389-6978-1-4673-7317","10.1109/ICIEA.2015.7334110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334110","motion estimation;motion vector;multi-view video coding","Motion estimation;Diamonds;Image quality;Bit rate;Search methods;Image coding;Computational complexity","computational complexity;image sequences;motion estimation;vectors;video coding","digital video sequence;ME technique;image encoding;hierarchical motion estimation scheme;multiview video coding;MVC;motion vector distribution;computational complexity","","","","12","","","","","IEEE","IEEE Conferences"
"Low-complexity architecture design for modified WiMAX Low-Density Parity-Check codes","K. Lin; M. Lin; J. Tseng","Department of Electronic Engineering, National Chin-Yi University of Technology, Taichung, Taiwan (R.O.C.); Department of Electronic Engineering, National Chin-Yi University of Technology, Taichung, Taiwan (R.O.C.); Department of Electronic Engineering, National Chin-Yi University of Technology, Taichung, Taiwan (R.O.C.)","2011 8th International Conference on Information, Communications & Signal Processing","","2011","","","1","4","In this paper, a modified WiMAX Low-Density Parity-Check (LDPC) codes for realistic LDPC coding system architectures is presented. The LDPC code, which is a special class of quasi-cyclic LDPC (QC-LDPC), has an efficient encoding algorithm owing to the simple structure of their parity-check matrices. A proposed distribution of irregular parity-check matrix for the modified WiMAX LDPC is developed so that we can obtain a low-complexity architecture design, and achievable circuit implementation. The modified WiMAX LDPC code decoding employs the iterative min-sum algorithm (MSA) and its decoder architecture design uses the bit node unit (BNU) and check node unit (CNU). Different word-length of hardware design for LDPC decoder can be simulated. Therefore, the word-length with 8-bit is enough for hardware implementation.","","978-1-4577-0031-6978-1-4577-0029-3978-1-4577-0030","10.1109/ICICS.2011.6174313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174313","WiMAX;Low-Density Parity-Check (LDPC);coding;QC-LDPC;low-complexity;hardware architecture","Parity check codes;WiMAX;Decoding;Encoding;Computer architecture;Hardware;Bit error rate","cyclic codes;iterative methods;matrix algebra;parity check codes;WiMax","low-complexity architecture design;modified WiMAX low-density parity check codes;modified WiMAX LDPC code decoding;LDPC coding system architectures;quasicyclic LDPC;QC-LDPC;parity-check matrices;circuit implementation;iterative min-sum algorithm;iterative MSA;bit node unit;BNU;check node unit;CNU;hardware design;LDPC decoder","","","","10","","","","","IEEE","IEEE Conferences"
"A full-rate and low-detection complexity cooperative transmission scheme based on distributed space-time-frequency code design","Hongyu Fang; Ruzi Li; Li Wang; Xiaohui Li","Key Lab of Intelligent Computing and Signal Processing of Ministry of Education, Anhui University, Hefei 230039, China; Key Lab of Intelligent Computing and Signal Processing of Ministry of Education, Anhui University, Hefei 230039, China; Key Lab of Intelligent Computing and Signal Processing of Ministry of Education, Anhui University, Hefei 230039, China; Key Lab of Intelligent Computing and Signal Processing of Ministry of Education, Anhui University, Hefei 230039, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","977","980","The cooperative transmission schemes based on the amplify and forward (AF) mode have the defect of low transmission rate and the exiting full-rate cooperative transmission schemes based on space-time code design have the defect of high detection complexity. Therefore, this paper makes the best of linear constellation precoding (LCP) technique and cyclic delay diversity (CDD) technique, and adopts the non-orthogonal amplify and forward (NAF) mode, finally proposes a new full-rate wireless cooperative transmission scheme based on space-time-frequency code design. Compared with the cooperative transmission schemes that only pursue the transmission rate or the BER performance, this new scheme can apply to multi-relay scenario, and has the advantages of simple structure, low signal detection complexity that does not increases with the number of relay nodes. All of above makes the real-time data transmission service of high quality come to be possible.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615469","cooperative;space-time-frequency;full-rate;low-detection complexity","Frequency division multiplexing;Time-frequency analysis;Complexity theory","communication complexity;cooperative communication;data communication;diversity reception;error statistics;linear codes;precoding;signal detection;space-time codes","low-detection complexity cooperative transmission scheme;full-rate complexity cooperative transmission scheme;distributed space-time-frequency code design;transmission rate;space-time code design;detection complexity;linear constellation precoding;cyclic delay diversity technique;linear constellation precoding technique;nonorthogonal amplify and forward mode;full-rate wireless cooperative transmission scheme;BER performance;multirelay scenario;signal detection complexity;relay nodes;real-time data transmission service","","","","13","","","","","IEEE","IEEE Conferences"
"Reducing Search Complexity of Coded Caching by Shrinking Search Space","H. Cao; Q. Yan; X. Tang","Information Security and National Computing Grid Laboratory, Southwest Jiaotong University, Chengdu, China; Laboratoire Traitement et Communication de l’Information, Télécom ParisTech, Paris, France; Information Security and National Computing Grid Laboratory, Southwest Jiaotong University, Chengdu, China","IEEE Communications Letters","","2019","23","4","568","571","Coded caching is a new technique to reduce the communication load by utilizing local memories. In this letter, we propose an improved decentralized coded caching delivery algorithm to reduce the search complexity with limited performance loss. Besides, the problem of shrinking the search space is formulated as an optimization problem, and we relax this problem into an efficient computable one. Consequently, the proposed scheme can be applied to various network topologies efficiently.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2019.2900237","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643756","Coded caching;complexity;search space","Servers;Complexity theory;Search problems;Optimization;Probability density function;Multicast communication;Numerical models","cache storage;computational complexity;computer networks;encoding;optimisation;radio networks;search problems;telecommunication network topology","search complexity;search space;communication load;local memories;optimization problem;decentralized coded caching delivery algorithm;network topologies","","","","5","","","","","IEEE","IEEE Journals & Magazines"
"A Low Complexity Iterative Receiver for Multiuser Space-Time Coding System","N. Du; N. Cao; P. Gu","NA; NA; NA","2010 6th International Conference on Wireless Communications Networking and Mobile Computing (WiCOM)","","2010","","","1","4","In multiuser space-time coding system the research already made is almost focused on STTC and STBC scheme. For this reason, a multiuser space-time coding system combined with Turbo-BLAST scheme is investigated. Iterative processing has been shown to be very effective in multiuser space-time coding systems, where each user employs channel coding and iterations are performed between the soft-output channel decoders. The complexity of an iterative receiver depends heavily on how the log-likelihood ratios (LLRs) are computed for users' coded bits in order to implement the iterations. The bit-level cancellation (BLC) scheme is proposed in this paper to ease the computation of the LLRs and allow bit-level interference cancellation in the iterative processing. It is demonstrated that the proposed iterative receiver performs very close to the conventional one, but with a much less computational complexity. Also the high spectral efficiency of the system from BLAST is retained.","2161-9646;2161-9654","978-1-4244-3708-5978-1-4244-3709","10.1109/WICOM.2010.5600756","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5600756","","Receiving antennas;Complexity theory;Multiuser detection;Encoding;Demodulation;Filtering","channel coding;interference suppression;iterative methods;multiuser detection;receivers;space-time codes","low complexity iterative receiver;multiuser space-time coding system;STTC scheme;STBC scheme;Turbo-BLAST scheme;channel coding;soft-output channel decoders;log-likelihood ratios;bit-level interference cancellation","","","","10","","","","","IEEE","IEEE Conferences"
"Dimensionality reduction for the golden code with worst-case decoding complexity of O(m2)","S. Kahraman; M. E. Çelebi","National Research Institute of Electronics and Cryptology (UEKAE), TU¨ B˙ITAK, 41470 Kocaeli, Turkey; Electronics and Communication Eng., Istanbul Technical University, 34469 Istanbul, Turkey","2011 8th International Symposium on Wireless Communication Systems","","2011","","","362","366","In this paper we introduce an efficient decoding method which is based on the dimensionality reduction of the sphere decoder search tree for the golden code. A codeword of the golden code has four independent m-QAM data symbols, hence, the required complexity of the exhaustive-search decoder is m4. An efficient implementation of the maximum-likelihood decoder for the golden code with a worst-case complexity is known to be proportional to m2.5. Our motivation is for an efficient decoder with a worst-case complexity of no more than m2.5. In this purpose, we show that our proposed method has m2complexity in the worst-case with a loss of only 1 dB with respect to optimal decoding.","2154-0225;2154-0217;2154-0217","978-1-61284-402-2978-1-61284-403-9978-1-61284-401","10.1109/ISWCS.2011.6125384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125384","Golden code;fast decoding;sphere decoding","Complexity theory;Maximum likelihood decoding;Phase shift keying;Delay;Estimation","codecs;decoding;quadrature amplitude modulation","dimensionality reduction;golden code;worst-case decoding complexity;O(m2);decoding method;sphere decoder;m-QAM data symbols;exhaustive-search decoder","","3","","15","","","","","IEEE","IEEE Conferences"
"A low-complexity algorithm for decoding of M-ary LDPC codes","Baomao Pang; Haoshan Shi; Guanghua He","College of Electronic Information, Northwest Polytechnical University, Xi'an 710072, China; College of Electronic Information, Northwest Polytechnical University, Xi'an 710072, China; Telecommunication Institute, Xi'an University of Electronic Science and Technology, 710071, China","2012 IEEE Symposium on Electrical & Electronics Engineering (EEESYM)","","2012","","","345","348","M-ary LDPC codes provide higher performances than their binary counterpart but suffer from highest decoding complexity. The use of stochastic decoding algorithm can reduce the decoding complexity, but the computational complexity of probability generation is very high. A simple method to fast generate the probability especially in high-order modulation schemes is proposed in this paper. Simulation results show the proposed algorithm causes negligible performance degradation with reduction in computational complexity, which gives some advantages in hardware implementation of the decoders.","","978-1-4673-2365-9978-1-4673-2363-5978-1-4673-2364","10.1109/EEESym.2012.6258660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6258660","LDPC decoding;approximate probability generation;M-QAM;algorithm Complexity","Decoding;Message systems;Parity check codes;Complexity theory","computational complexity;decoding;modulation coding;parity check codes;probability;stochastic processes","low-complexity algorithm;M-ary LDPC codes;decoding complexity;stochastic decoding algorithm;computational complexity;probability generation;high-order modulation schemes","","","","11","","","","","IEEE","IEEE Conferences"
"Low complexity adaptation of H.264/MPEG-4 SVC for multiple description video coding","D. Radakovic; R. Ansari; Y. Yao","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, USA","2010 IFIP Wireless Days","","2010","","","1","4","The scalable video coding (SVC) extension of the H.264/MPEG-4 AVC standard provides a feature-rich partition of the video that is amenable to multiple description coding (MDC) for transmission over multiple unreliable channels. In this paper we propose a novel low-complexity MDC scheme where we enforce compliance with H.264/MPEG-4 SVC and restrict our processing to the network adaptation layer (NAL) level. In our scheme video is encoded with two spatial and multiple temporal layers. The lower resolution spatial layer is coarsely encoded using a high quantization parameter (QP) value, while the higher resolution spatial layer is encoded with finer QP. The MDC is performed on an already encoded video stream by partitioning the NAL units from the encoded video stream into the two descriptions, where the coarse lower spatial resolution is duplicated in the two descriptions while finely encoded enhancement layer information is symmetrically distributed between the two descriptions. Experimental results show that compared with single description coding (SDC) the performance of proposed scheme results in gains of up to 3 dB.","2156-972X;2156-9711;2156-9711","978-1-4244-9229-9978-1-4244-9230-5978-1-4244-9228","10.1109/WD.2010.5657732","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5657732","Scalable video coding (SVC);multiple description coding (MDC)","Streaming media;Static VAr compensators;Bit rate;PSNR;Encoding;MIMO;Transform coding","video coding","low complexity adaptation;H.264/MPEG-4 SVC;multiple description video coding;scalable video coding;high quantization parameter;single description coding","","1","","10","","","","","IEEE","IEEE Conferences"
"Complexity Controlled Side Information Creation for Distributed Scalable Video Coding","Q. H. Van; L. D. T. Hue; V. D. Du; V. N. Hong; X. HoangVan","1Hanoi University of Industry; 2VNU-University of Engineering and Technology; 1Hanoi University of Industry; 3Radio Electronics Association of Vietnam; 2VNU-University of Engineering and Technology","2019 3rd International Conference on Recent Advances in Signal Processing, Telecommunications & Computing (SigTelCom)","","2019","","","104","108","Distributed scalable video coding (DSVC) has recently been gaining many attentions due to its benefits in terms of computational complexity, error resilience and scalability, which are important for emerging video applications like wireless sensor networks and visual surveillance system (VSS). In DSVC, the side information (SI) creation plays a key role as it directly affects the DSVC compression performance and the encoder/decoder computational complexity. However, for many VSS applications, the energy of each VSS node is usually attenuating along the time, making the difficulty in transmitting surveillance video in real time. To address this problem, we propose a complexity controlled SI creation solution for the newly DSVC framework. To achieve the flexible SI creation, the complexity associated with SI creation process is modeled using a linear model in which the model parameters are estimated from a fitting process. To adjust the SI complexity, a user parameter is defined based on the availability of the VSS energy resource. Experiments conducted for a rich set of video surveillance data have revealed the benefits of the proposed complexity control solution, notably in both complexity control and compression performance.","","978-1-5386-7963-0978-1-5386-7962-3978-1-5386-7964","10.1109/SIGTELCOM.2019.8696264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8696264","Distributed scalable video coding;side information;visual sensor networks","Video coding;Surveillance;Computational modeling;Decoding;Computational complexity;Encoding","computational complexity;decoding;video coding;video surveillance","VSS node;newly DSVC framework;flexible SI creation;SI creation process;SI complexity;VSS energy resource;video surveillance data;complexity control solution;complexity controlled side information creation;distributed scalable video coding;error resilience;video applications;DSVC compression performance;encoder/decoder computational complexity;VSS applications;surveillance video;user parameter;complexity controlled SI creation solution","","","","13","","","","","IEEE","IEEE Conferences"
"Low complexity encoding algorithm of RS-based QC-LDPC codes","M. Zhang; L. Tang; Q. Huang; Z. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191","2014 Information Theory and Applications Workshop (ITA)","","2014","","","1","4","This paper presents a novel encoding algorithm for QC-LDPC codes constructed from Reed-Solomon codes. The encoding is performed in the transform domain via Galois Fourier transformation. Message bits are encoded in sections corresponding to sub-matrices of the parity-check matrix in the transform domain. Because of the structure of the parity-check matrices of these LDPC codes, the encoding can be easily implemented with some linear-feedback shift registers, thus efficiently reduces the hardware cost.","","978-1-4799-3589","10.1109/ITA.2014.6804249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804249","LDPC codes;RS codes;encoding complexity;matrix transformation;Galois Fourier transform","Parity check codes;Encoding;Transforms;Generators;Polynomials;Computational complexity","communication complexity;cyclic codes;Fourier transforms;Galois fields;matrix algebra;parity check codes;Reed-Solomon codes;shift registers","low complexity encoding algorithm;RS-based QC-LDPC codes;Reed-Solomon codes;transform domain;Galois Fourier transformation;message bits encoding;parity check matrix;linear feedback shift registers","","","","15","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Three-Error-Correcting BCH Decoder with Applications in Concatenated Codes","J. Freudenberger; M. Rajab; S. Shavgulidze","NA; NA; NA","SCC 2019; 12th International ITG Conference on Systems, Communications and Coding","","2019","","","1","5","Error correction coding (ECC) for optical communication and persistent storage systems require high rate codes that enable high data throughput and low residual errors. Recently, different concatenated coding schemes were proposed that are based on binary Bose-Chaudhuri-Hocquenghem (BCH) codes that have low error correcting capabilities. Commonly, hardware implementations for BCH decoding are based on the Berlekamp-Massey algorithm (BMA). However, for single, double, and triple error correcting BCH codes, Peterson's algorithm can be more efficient than the BMA. The known hardware architectures of Peterson's algorithm require Galois field inversion. This inversion dominates the hardware complexity and limits the decoding speed. This work proposes an inversion-less version of Peterson's algorithm. Moreover, a decoding architecture is presented that is faster than decoders that employ inversion or the fully parallel BMA at a comparable circuit size.","","978-3-8007-4862","10.30420/454862002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661299","","","","","","","","","","","","","VDE","VDE Conferences"
"A Scene Complexity Adaptive Intra-frame Rate Control for Video Coding","Z. Jiang; Q. Dai; Y. Ai","NA; NA; NA","2010 International Conference on Multimedia Communications","","2010","","","108","111","Rate distortion of Intra-Frame (I-frame) plays a significant role in controlling video quality. Aiming at the deficiency of I-frame rate control of H.264, this paper presents a novel I-frame rate estimation method for H.264 rate control. Firstly, Rate-Complexity-Qstep (RCQ) model of I-frame was gotten by curve fitting, secondly, the information of encoded frames was used to update I-frame weight relative to P-frame, and a reasonable target bits was allocated for I-frame, finally, I-frame Qstep was calculated under the target bits by RCQ model. Experimental results show that the proposed method obtains more accurate I-frame output bit rate and more steady PSNR fluctuation compared with H.264 standard algorithm.","","978-0-7695-4136","10.1109/MEDIACOM.2010.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5694156","video coding;rate control;H.264;scene complexity","Bit rate;Complexity theory;Quantization;Streaming media;PSNR;Video sequences;Adaptation model","curve fitting;rate distortion theory;video coding","scene complexity adaptive intra-frame rate control;video coding;rate distortion;video quality control;frame rate estimation method;H.264 rate control;rate-complexity-qstep model;curve fitting","","","","7","","","","","IEEE","IEEE Conferences"
"A low complexity turbo coded BICM-ID","D. Kang; Y. Lee; W. Oh","Department of Information Communication Engineering, Chungnam National University, Daejeon 305-764, Korea; Department of Information Communication Engineering, Chungnam National University, Daejeon 305-764, Korea; Department of Information Communication Engineering, Chungnam National University, Daejeon 305-764, Korea","2013 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2013","","","1","3","In this paper, we propose a low complexity turbo coded BICM-ID (Bit-Interleaved Coded Modulation with Iterative Decoding) scheme. To increase the spectral efficiency, capacity approaching turbo codes can be combined with high order modulations and form a turbo coded modulation (CM) or a turbo-BICM (Bit-Interleaved Coded Modulation). On the other hand, it is also well known that the performance of turbo-BICM can be further improved by performing iterative demodulation and decoding between demodulator and turbo decoder which known as turbo-BICM-ID. However, compared to turbo-BICM, the decoding complexity of turbo-BICM-ID is highly increased. To reduce the required complexity of turbo-BICM-ID, we propose a low complexity turbo-BICM-ID scheme which offers virtually identical performance compared to turbo-BICM-ID.","2155-5044;2155-5052","978-1-4673-6047","10.1109/BMSB.2013.6621797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621797","Turbo code;Bit interleaving;Coded Modulation;Iterative Decoding","Decoding;Complexity theory;Turbo codes;Demodulation;Iterative decoding;Bit error rate","interleaved codes;iterative decoding;turbo codes","decoding complexity;turbo decoder;iterative demodulation;bit-interleaved coded modulation with iterative decoding scheme;low complexity turbo coded BICM-ID scheme","","1","","6","","","","","IEEE","IEEE Conferences"
"Low-complexity intra coding algorithm in enhancement layer for SHVC","T. Katayama; W. Shi; T. Song; T. Shimamoto","Graduate School of Advanced Technology and Science, Tokushima University, Tokushima City, 770-8506, Japan; Graduate School of Advanced Technology and Science, Tokushima University, Tokushima City, 770-8506, Japan; Computer Systems Engineering, Institute of Technology and Science, Graduate School of Engineering, Tokushima University, Minami-Josanjima 2-1, Tokushima City, 770-8506, Japan; Computer Systems Engineering, Institute of Technology and Science, Graduate School of Engineering, Tokushima University, Minami-Josanjima 2-1, Tokushima City, 770-8506, Japan","2016 IEEE International Conference on Consumer Electronics (ICCE)","","2016","","","419","422","In this paper, we propose an improved low complexity algorithm for the scalable extension of the HEVC (SHVC) using the information of Rate-Distortion cost and the coding unit (CU) size in the base layer. The proposed algorithm is used for inter layer reference prediction (ILRP) which is defined as a new mode in SHVC. Firstly, the relationship between of the different resolutions is evaluated when using ILRP mode. Next, based on the evaluation results, an improved algorithm focused on the complexity reduction of the enhancement layer is proposed. The proposed algorithm is evaluated by the reference software of SHVC. The simulation results show that the proposed algorithm can achieve over 60% computation complexity reduction comparing to the original SHVC algorithm.","2158-4001","978-1-4673-8364-6978-1-4673-8363","10.1109/ICCE.2016.7430673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430673","HEVC;HEVC scalable extension (SHVC);Inter Layer Reference Prediction;Intra Prediction","Encoding;Complexity theory;Prediction algorithms;Signal processing algorithms;Bit rate;Consumer electronics;Software algorithms","computational complexity;image enhancement;video coding","low-complexity intracoding algorithm;enhancement layer;SHVC reference software;scalable extension;HEVC;rate-distortion cost;coding unit size;interlayer reference prediction;ILRP mode;computation complexity reduction;SHVC algorithm","","4","","7","","","","","IEEE","IEEE Conferences"
"A Low-Complexity SNR Estimation Algorithm Based on Frozen Bits of Polar Codes","Y. Li; R. Liu; R. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Communications Letters","","2016","20","12","2354","2357","Polar codes attract a lot of attention due to their provably capacity-achieving property with low encoding and decoding complexity. Frozen bits, the special structure of polar codes, are helpful for signal-to-noise ratio (SNR) estimation. A novel SNR estimation algorithm based on frozen bits is proposed in this letter. In the proposed algorithm, the mapping from SNR to frozen bit error rate is theoretically analyzed when code length is infinite and approximately calculated when code length is finite. Then, SNR can be estimated according to the mapping. Simulation results show that the proposed algorithm has better estimation performance with a relatively lower complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2605106","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7558208","Polar codes;frozen bits;FBER;SNR estimation","Signal to noise ratio;Estimation;Approximation algorithms;Mathematical model;Decoding;Complexity theory;Algorithm design and analysis","channel coding;decoding;error statistics;mobility management (mobile radio)","signal-to-noise ratio estimation;low-complexity SNR estimation algorithm;polar codes;encoding complexity;decoding complexity;frozen bit error rate;code length","","5","","9","","","","","IEEE","IEEE Journals & Magazines"
"An amended rate distortion model for video coding under the complexity constrains","W. Geng; W. Lenan","School of Information Science & Engineering, Southeast University, Nanjing, China; School of Information Science & Engineering, Southeast University, Nanjing, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","195","199","The classical rate distortion (R-D) theories which study the relationship between the coding bit rate and signal distortion have limit in studying the impact of the coding complexity. However, the advanced video coding technology lead to not only high coding efficiency but also the extremely high coding complexity. The high complexity is not suitable for the real time applications and mobile devices. Therefore, it is necessary to amend the traditional R-D model by incorporating the complexity constraints into the R-D framework. In this paper, a novel rate model and a novel distortion model were proposed respectively by incorporating the complexity factor. The amended R-D model can be obtained by joining the two models to study on the relationship among the coding bit rate, signal distortion and the coding complexity. The experimental results showed the accuracy of the proposed model.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5477546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477546","complexity constraint;rate-distortion (R-D);intra rate;quantization parameter;video coding","Rate-distortion;Video coding;Bit rate;Video compression;Rate distortion theory;Information science;Quantization;Communication systems;Digital TV;Digital video broadcasting","data compression;nonlinear distortion;video coding","amended rate distortion model;video coding;coding complexity constrains;coding bit rate;signal distortion","","","","11","","","","","IEEE","IEEE Conferences"
"Low-complexity intra coding algorithm based on convolutional neural network for HEVC","T. Katayama; K. Kuroda; W. Shi; T. Song; T. Shimamoto","Graduate School of Advanced Technology and Science, Tokushima University, Tokushima City, 770-8506, Japan; Graduate School of Advanced Technology and Science, Tokushima University, Tokushima City, 770-8506, Japan; Graduate School of Advanced Technology and Science, Tokushima University, Tokushima City, 770-8506, Japan; Computer Systems Engineering, Institute of Technology and Science, Graduate School of Engineering, Tokushima University, Tokushima City, 770-8506, Japan; Computer Systems Engineering, Institute of Technology and Science, Graduate School of Engineering, Tokushima University, Tokushima City, 770-8506, Japan","2018 International Conference on Information and Computer Technologies (ICICT)","","2018","","","115","118","In this paper, we propose a fast coding unit (CU) size decision algorithm for high efficiency video coding (HEVC) based on convolutional neural network. The proposed fast algorithm contributes to decrease no less than two CU partition modes in each coding tree unit for full rate-distortion optimization processing, thereby reducing the encoder hardware complexity. Moreover, our algorithm use only texture information and it does not depend on the correlations among CU depths or spatially nearby CUs. It is friendly to the parallel processing and it can improve the pipeline process of RDO. The proposed algorithm is implemented in the reference software of HEVC (HM16.7). The simulation results show that the proposed algorithm can achieve over 67.3% computation complexity reduction comparing to the original HEVC algorithm.","","978-1-5386-5384-5978-1-5386-5382-1978-1-5386-5383-8978-1-5386-5385","10.1109/INFOCT.2018.8356852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356852","High Efficiency Video Coding (HEVC);intra coding;convolutional newral network (CNN)","Encoding;Kernel;Computational complexity;Prediction algorithms;High efficiency video coding;Software algorithms","computational complexity;neural nets;optimisation;pipeline processing;rate distortion theory;video coding","convolutional neural network;fast coding unit size decision algorithm;high efficiency video coding;CU partition modes;coding tree unit;rate-distortion optimization processing;encoder hardware complexity;parallel processing;fast algorithm;computation complexity reduction;low-complexity intra coding algorithm;texture information;HEVC algorithm;pipeline process","","","","9","","","","","IEEE","IEEE Conferences"
"Soft-decision list decoding of Reed-Muller codes with linear complexity","I. Dumer; G. Kabatiansky; C. Tavernier","University of California at Riverside, USA; Inst. for Info. Transmission Problems, Moscow, Russia; National Knowledge Center (NKC-EAI), Abu-Dhabi, UAE","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","2303","2307","Let a binary Reed-Muller code RM(s;m) of length n be used on a memoryless channel with an input alphabet ±1 and a real-valued output ℝ. Given a received vector y in ℝn; we define its generalized distance T to any codeword c as the sum Σ|yj|taken over all positions j, in which vectors y, c have opposite signs. We then consider the list ℒTof codewords located within distance T from the received vector y and estimate the size LTof this list using the generalized Johnson bound. For any RM code RM(s,m) of fixed order s, the algorithm is proposed that performs list decoding beyond the error-correcting radius with linear complexity in length n and retrieves the code list ℒTwith complexity of order nsLTfor any decoding radius T within the generalized Johnson bound.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6033973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033973","","Complexity theory;Decoding;Vectors;Boolean functions;Hamming distance;Error correction codes","binary codes;decoding;Reed-Muller codes","soft decision list decoding;Reed Muller codes;linear complexity;memoryless channel;real valued output;Johnson bound","","1","","10","","","","","IEEE","IEEE Conferences"
"Low-Complexity and Piecewise Systematic Encoding of Non-Full-Rank QC-LDPC Codes","C. Tseng; J. Tarng","NA; NA","IEEE Communications Letters","","2015","19","6","897","900","In this letter, we present a low-complexity encoding method for quasi-cyclic (QC) low-density parity-check (LDPC) codes in the case of non-full-rank parity-check matrices. Gaussian elimination can achieve systematic encoding, but it is usually too complex to implement. For QC-LDPC codes, efficient encoding methods have been presented by using shift-registers, but the encoded codewords are not systematic when parity-check matrices are non-full-rank. However, a systematic structure is important in practice. Therefore, we propose an encoding method which allows all information bits appear piecewise in the codeword, called piecewise systematic encoding.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2411255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056546","systematic;encoding;quasi-cyclic (QC) lowdensity parity-check (LDPC) codes;Systematic;encoding;quasi-cyclic (QC) low-density parity-check (LDPC) codes","Systematics;Generators;Channel coding;Parity check codes;Arrays;Registers","cyclic codes;matrix algebra;parity check codes;shift registers","nonfull-rank QC-LDPC codes;low-complexity encoding method;piecewise systematic encoding;parity-check matrices;quasicyclic low-density parity-check codes;shift-registers;Gaussian elimination;systematic structure;codeword","","3","","7","","","","","IEEE","IEEE Journals & Magazines"
"Real-time Complexity Control for High Efficiency Video Coding","J. Fang; Y. Tu; L. Yu; P. Chang","Department of Electronic Engineering, Ming Chuan University, Taoyuan City, 333, Taiwan; Department of Communication Engineering, National Central University, Taoyuan City, 320, Taiwan; Department of Communication Engineering, National Central University, Taoyuan City, 320, Taiwan; Department of Communication Engineering, National Central University, Taoyuan City, 320, Taiwan","2018 IEEE International Conference on Information Communication and Signal Processing (ICICSP)","","2018","","","85","89","The current video coding standard, High Efficiency Video Coding (HEVC), provides quad-tree structures of the coding unit (CU) to achieve high coding efficiency. Compared with previous standards, the HEVC encoder increases much computational complexity to levels inappropriate for applications of power-constrained devices. This work thus proposes a real-time complexity control scheme to control each frame complexity if the complexity of encoded frames is counted and its accumulated value is over the threshold. To further improve the coding efficiency, a fast CU depth decision algorithm is proposed. Experimental results show that a two-level of complexity control scheme was designed. In addition, the loss of the average BD-PSNR was about 0.23 dB and 0.27 dB as the target complexity was set to 80% and 60% of the unconstrained complexity, respectively.","","978-1-5386-8003-2978-1-5386-8002-5978-1-5386-8004","10.1109/ICICSP.2018.8549738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8549738","HEVC;coding unit;complexity control","Encoding;Computational complexity;Real-time systems;High efficiency video coding;Urban areas;Standards","computational complexity;video coding","unconstrained complexity;High Efficiency Video Coding;coding unit;high coding efficiency;HEVC encoder;computational complexity;real-time complexity control scheme;frame complexity;target complexity;video coding standard;noise figure 0.23 dB;noise figure 0.27 dB","","","","14","","","","","IEEE","IEEE Conferences"
"Reduced-complexity Deep Neural Network-aided Channel Code Decoder: A Case Study for BCH Decoder","C. Deng; S. L. Bo Yuan","Department of Electrical and Computer Engineering, Rutgers University; Department of Electrical and Computer Engineering, Rutgers University","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2019","","","1468","1472","Error-correcting codes are very important in modern communication systems. In this paper, we investigate efficient reduced-complexity deep neural network (DNN)-aided channel decoders. Specifically, we leverage DNN training to obtain individual scaling parameters for normalized min-sum algorithms, thereby leading to much faster convergence for the same target bit error rate (BER). Also, we propose to compress the DNN-aided channel decoders via weight sharing. A case study on DNN-aided BCH decoders is investigated. Simulation results and hardware complexity analysis show that our method can reduce 2.59 times of memory cost than non-compressed DNN-aided BCH decoders. Meanwhile, compared to the conventional BCH decoders, our method can improve convergence rate by 6 times with similar decoding performance.","2379-190X;1520-6149","978-1-4799-8131-1978-1-4799-8132","10.1109/ICASSP.2019.8682871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682871","Neural Network;Weight Sharing;BCH Decoder","Decoding;Approximation algorithms;Random access memory;Neural networks;Complexity theory;Clustering algorithms;Training","","","","","","19","","","","","IEEE","IEEE Conferences"
"A low-complexity decoding algorithm for systematic binary deterministic rateless codes","X. Zhang; B. Li; D. Lin; S. Li","National Key laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, China","2013 International Conference on Communications, Circuits and Systems (ICCCAS)","","2013","2","","15","18","In this paper, we propose two low-complexity decoding algorithms for systematic binary deterministic rateless codes. One is based on segmentation available for large information block. Segmentation-based decoding algorithm can dramatically reduce the complexity on solving inverse of matrix, which dominates the complexity of decoding. Another is based on enumeration for small information block. Enumeration-based decoding algorithm can avoid solving the inverse of matrix, which is of advantage in term of complexity.","","978-1-4799-3051-7978-1-4799-3050","10.1109/ICCCAS.2013.6765275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6765275","","Decoding;Complexity theory;Equations;Encoding;Systematics;Vectors;Network coding","binary codes;decoding;matrix algebra","low-complexity decoding algorithm;segmentation based decoding;enumeration based decoding;systematic binary deterministic rateless codes;decoding complexity;matrix inverse","","1","","13","","","","","IEEE","IEEE Conferences"
"A novel method of adaptive GOP structure base on coding efficiency and complexity joint model","D. Qian; X. Zhou","Department of Computer Science and Technology, Dalian Neusoft University of Information, Dalian, China; Department of Computer Science and Technology, Dalian Neusoft University of Information, Dalian, China","Fifth International Conference on Intelligent Control and Information Processing","","2014","","","239","243","In this paper, we focus on the models that the coding time and efficiency affected by GOPsize, and then find that large GOPsize will usually bring the more coding time and less coding efficiency, and vice verse. We analyze the reason which resulting in the above circumstances and propose two models: coding efficiency and time. Their maximum errors are less than 5% and 2.7%, respectively. In practice, the coding system is expected to reduce coding time while improve coding efficiency as possible. Base on the two models, we further propose a joint function of choose GOPsize adaptively to achieve the optimal tradeoff between coding time and efficiency. We improve the joint function to make it adapt to different system requirements by changing its parameters.","","978-1-4799-3650-2978-1-4799-3649-6978-1-4799-3648","10.1109/ICICIP.2014.7010346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010346","","Encoding;Complexity theory;Video sequences;Joints;Scalability;Adaptation models;Streaming media","computational complexity;encoding","adaptive GOP structure;coding efficiency;complexity joint model;coding time;GOPsize;coding system","","","","7","","","","","IEEE","IEEE Conferences"
"PAPR reduction of coded OFDM system using low complexity PTS","A. Joshi; S. Bali; H. Tiwari","Department of ECE, Jaypee Institute of Information Technology, Noida (India); Department of ECE, Jaypee Institute of Information Technology, Noida (India); Department of ECE, Jaypee Institute of Information Technology, Noida (India)","2016 International Conference on Signal Processing and Communication (ICSC)","","2016","","","203","207","For achieving high throughput OFDM (orthogonal frequency division multiplexing) is a good technique for parallel transmission of information at improved rate. Two important areas of concern with OFDM especially in fading environments are less BER (bit error rate) and large value of PAPR. Channel coding combined with OFDM system such as Turbo codes increase the error correction capabilities of the system thus improving BER performance of un-coded OFDM system. High PAPR (peak to average power ratio) increase the circuit complexity and cost of high power amplifier (HPA) used in the system. Partial transmit sequence (PTS) is the good probabilistic scrambling technique which is most preferred for PAPR reduction. In this paper a Turbo coded OFDM scheme with Modified PTS using Riemann sequences and post clipping-filtering is used. The BER efficiency of the coded OFDM system with un-coded OFDM and PAPR efficiency of modified PTS with conventional PTS is compared and excellence of the proposed system in terms of BER and PAPR is justified. For implementation of COFDM system IEEE 802.11a standard is used.","","978-1-5090-2684-5978-1-5090-2685","10.1109/ICSPCom.2016.7980576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7980576","BER;PAPR;Turbo codes;PTS;CCDF","Peak to average power ratio;Turbo codes;Partial transmit sequences;Bit error rate;Complexity theory;Fading channels","channel coding;error correction codes;error statistics;fading channels;filtering theory;modulation coding;OFDM modulation;radiofrequency power amplifiers;turbo codes;wireless LAN","coded OFDM system PAPR reduction;low complexity PTS;high throughput OFDM;orthogonal frequency division multiplexing;information parallel transmission;fading environments;BER;bit error rate;channel coding;error correction capabilities;uncoded OFDM system BER performance improvement;circuit complexity;high power amplifier cost increment;HPA cost increment;Partial transmit sequence;probabilistic scrambling technique;turbo coded OFDM scheme;Riemann sequences;post clipping-filtering;IEEE 802.11a standard;COFDM system","","1","","17","","","","","IEEE","IEEE Conferences"
"HEVC Intra coding of ultra HD video with reduced complexity","N. Dhollande; O. Le Meur; C. Guillemot","b&#x003C;&#x003E;com, Cesson-Sevign&#x00E9;, France; IRISA / University of Rennes 1, Rennes, France; INRIA, Rennes, France","2014 IEEE International Conference on Image Processing (ICIP)","","2014","","","4122","4126","The HEVC (High Efficiency Video Coding) standard brings the necessary quality versus rate performance for efficient transmission of Ultra High Definition formats (UHD). However, one of the remaining barriers to its adoption for UHD content is its high encoding complexity. In this paper, we address the problem of HEVC encoding complexity reduction by proposing a strategy to infer UHD coding modes and quadtree structure from those optimized for the lower (HD) resolution version of the input video. A speed-up factor of 3 is achieved compared to directly encoding the UHD format at the expense of a limited quality loss.","1522-4880;2381-8549","978-1-4799-5751","10.1109/ICIP.2014.7025837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025837","High Efficiency Video Coding (HEVC);UHDTV;intra prediction;mode decision;complexity reduction","Encoding;High definition video;Complexity theory;Video coding;Standards;Proposals;Rate-distortion","image resolution;quadtrees;video coding","HEVC intracoding;high efficiency video coding standard;ultrahigh definition video coding;UHD video coding;complexity reduction;quadtree structure;video resolution","","3","","17","","","","","IEEE","IEEE Conferences"
"Complexity reduction scheme in encoding and decoding for polar codes","K. M. Prakash; G. S. Sunitha","ECE, Bapuji Institute of Engineering and Technology, Davangere-577004, Karnataka, India; ECE, Bapuji Institute of Engineering and Technology, Davangere-577004, Karnataka, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","","2016","","","1558","1562","Polar coding has received a lot of consideration due to its nature of capacity achieving under various condition of coding. Low complexity is introduced by using successive cancelation decoding method. By using this large-error correction capacity is achieved for finite length of codes.in this work we propose a new approach for polar codes to achieve the better performance. Reduction of complexity is performed by reducing the number of XOR operations and the performance of the system is enhanced by using systematic polar codes. XOR operations are discarded by using path searching method, in this approach only desired path is considered others are discarded with the help of a posteriori probability. The proposed approach is implemented using MATLAB tool and the performance is achieved and compared in terms of bit error rate and frame error rate.","","978-1-5090-0774-5978-1-5090-0775","10.1109/RTEICT.2016.7808094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808094","Polar code;systematic encoding;low complexity;successive cancelation decoding","Decoding;Complexity theory;Systematics;Bit error rate;Channel coding","decoding;error correction codes;error statistics;probability","polar coding;successive cancelation decoding method;large-error correction capacity;complexity reduction;XOR operations;systematic polar codes;path searching method;a posteriori probability;bit error rate;frame error rate","","","","7","","","","","IEEE","IEEE Conferences"
"Modified polynomial selection architecture for low-complexity chase decoding of Reed-Solomon codes","H. Wang; W. Zhang; B. Pan","School of Electronic Information Engineering, Tianjin University, China; School of Electronic Information Engineering, Tianjin University, China; School of Electronic Information Engineering, Tianjin University, China","2012 IEEE International Symposium on Circuits and Systems","","2012","","","1791","1794","Reed-Solomon (RS) codes are widely used in modern communication and computer systems. Compared with the hard-decision decoding algorithms, the algebraic soft-decision decoding (ASD) algorithm can achieve significant coding gain. Among ASD algorithms, the low-complexity Chase (LCC) decoding has a better performance and lower complexity. In the LCC decoding, 2ηtest vectors need to be interpolated and a polynomial selection scheme is required to choose the right interpolation output. A modified polynomial selection (MPS) algorithm is proposed in this paper. By deleting the reliability information, the MPS requires less hardware and provides the same performance as its present counterpart. For a (63, 55) RS code over GF (26), the MPS can save 20% chip area and 21.2% power consumption.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6271613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271613","Algebraic soft-decision decoding;Low-complexity Chase decoding;Polynomial selection;Reed-Solomon codes;VLSI design","Decoding;Polynomials;Interpolation;Computer architecture;Vectors;Algorithm design and analysis;Reliability","","","","3","","13","","","","","IEEE","IEEE Conferences"
"Low-Complexity Joint Channel Estimation and List Decoding of Short Codes","M. C. Coskun; G. Liva; J. Oestman; G. Durisi","NA; NA; NA; NA","SCC 2019; 12th International ITG Conference on Systems, Communications and Coding","","2019","","","1","5","A pilot-assisted transmission (PAT) scheme is proposed for short blocklengths, where the pilots are used only to derive an initial channel estimate for the list construction step. The final decision of the message is obtained by applying a non-coherent decoding metric to the codewords composing the list. This allows one to use very few pilots, thus reducing the channel estimation overhead. The method is applied to an ordered statistics decoder for communication over a Rayleigh block-fading channel. Gains of up to 1.2 dB as compared to traditional PAT schemes are demonstrated for short codes with QPSK signaling. The approach can be generalized to other list decoders, e.g., to list decoding of polar codes.","","978-3-8007-4862","10.30420/454862046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661343","","","","","","","","","","","","","VDE","VDE Conferences"
"Low complexity reference frame selection in QTBT structure for JVET future video coding","S. Park; T. Dong; E. S. Jang","Communications & Media R&D Division, Korea Electronics Technology Institute, Gyeonggi-do, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea; Department of Computer Science, Hanyang University, Seoul, South Korea","2018 International Workshop on Advanced Image Technology (IWAIT)","","2018","","","1","4","In this paper, we propose a reference frame search method for JVET future video codec (FVC) that employs the quadtree plus binary tree (QTBT) structure. Among many new technologies proposed in FVC, QTBT poses a significant challenge since it contains the structural change of coding tree unit from HEVC. To reduce the encoding complexity of FVC with QTBT structure, we investigated some redundancy in motion estimation process-particularly, the reference frame search. In this paper, we present a method that effectively restricts the reference frame search range of general motion estimation as well as of affine motion estimation, exploiting the dependence within QTBT structure. The proposed method minimizes the maximum of the reference frame search ranges per each coding unit (CU) based on the prediction information of parents node. To be specific, the prediction direction and the index of reference frame of parent node were used. In addition, the proposed method utilizes the information of binary tree depth and of temporal layer to prevent undesired coding loss. The experimental results showed that the proposed method decreased the encoding time of motion estimation by 34% on average in comparison with joint exploration test model (JEM) 3.1, maintaining a reasonable coding efficiency (less than a 0.3% BD-rate loss).","","978-1-5386-2615-3978-1-5386-2614-6978-1-5386-2616","10.1109/IWAIT.2018.8369627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8369627","FVC;JEM;video coding;video compression;motion estimation;reference frame search;encoder complexity","Encoding;Binary trees;Motion estimation;Complexity theory;Copper;Indexes;Correlation","computational complexity;motion estimation;quadtrees;search problems;video codecs;video coding","QTBT structure;reference frame search range;general motion estimation;affine motion estimation;coding unit;parent node;binary tree depth;undesired coding loss;reasonable coding efficiency;low complexity reference frame selection;JVET future video coding;reference frame search method;JVET future video codec;FVC;quadtree plus binary tree structure;structural change;tree unit;encoding complexity;motion estimation process;prediction information","","","","9","","","","","IEEE","IEEE Conferences"
"A low complexity mode decision approach for HEVC-based 3D video coding using a Bayesian method","H. R. Tohidypour; M. T. Pourazad; P. Nasiopoulos","Department of Electrical & Computer Engineering, University of British Columbia, Canada; Department of Electrical & Computer Engineering, University of British Columbia, Canada; Department of Electrical & Computer Engineering, University of British Columbia, Canada","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2014","","","895","899","The 3D extension of High Efficiency Video Coding (HEVC) standard (3D-HEVC) aims at improving coding efficiency by introducing new and unique approaches for utilizing correlations between the different views of a scene. Reported coding efficiency, however, comes at the expense of increased computational complexity. For real-time applications, reducing the computational complexity of 3D-HEVC is very important. In this paper, we propose an adaptive fast mode assigning method based on a Bayesian classifier that reduces 3D-HEVC's coding complexity by up to 51.95%, while maintaining the overall quality and bitrate.","1520-6149;2379-190X","978-1-4799-2893","10.1109/ICASSP.2014.6853726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853726","3D HEVC;video compression;low complexity compression;Bayesian classifier","Complexity theory;Encoding;Video coding;Bit rate;Three-dimensional displays;Standards;Training","Bayes methods;computational complexity;data compression;image classification;video coding","3D-HEVC coding complexity;Bayesian classifier;adaptive fast mode assigning method;computational complexity;high efficiency video coding;Bayesian method;HEVC-based 3D video coding;low complexity mode decision approach","","10","","12","","","","","IEEE","IEEE Conferences"
"A low complexity iterative soft detection for bit interleaved coded CPM","M. Malek; A. Karine; G. Frédéric","Institut Télécom; Télécom Bretagne; UMR CNRS 3192 Lab-STICC Technopôle Brest Iroise CS 83818 29238 Brest, France Université européenne de Bretagne; Institut Télécom; Télécom Bretagne; UMR CNRS 3192 Lab-STICC Technopôle Brest Iroise CS 83818 29238 Brest, France Université européenne de Bretagne; Institut Télécom; Télécom Bretagne; UMR CNRS 3192 Lab-STICC Technopôle Brest Iroise CS 83818 29238 Brest, France Université européenne de Bretagne","2014 8th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2014","","","12","16","In this paper, we propose a low-complexity iterative soft detection algorithm for a bit interleaved coded continuous phase modulation (CPM). The reduction of complexity is obtained by using a modulation index at the receiver side which is different from the one used at the transmitter side. This difference is balanced thanks to a soft-input soft-output (SISO) coherent CPM demodulator based on the per survivor processing (PSP) technique. Compared to the state of the art, the error rate performance is improved. We also show that our algorithm converges close to the coherent maximum a posteriori (MAP) algorithm, with a few additional iterations.","2165-4719;2165-4700","978-1-4799-5985","10.1109/ISTC.2014.6955076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955076","","","computational complexity;continuous phase modulation;error statistics;iterative methods;maximum likelihood estimation;signal detection","low complexity iterative soft detection algorithm;bit interleaved coded CPM;continuous phase modulation;soft-input soft-output;per survivor processing technique;PSP technique;error rate performance;maximum a posteriori algorithm;MAP algorithm;SISO demodulator","","","","15","","","","","IEEE","IEEE Conferences"
"Full-Diversity Space-Time-Frequency Coding with Very Low Complexity for the ML Decoder","M. Shahabinejad; S. Talebi","Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran, and the Advanced Communications Research Institute, Sharif University, Tehran, Iran; Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran, and the Advanced Communications Research Institute, Sharif University, Tehran, Iran","IEEE Communications Letters","","2012","16","5","658","661","Recently proposed full-diversity space-time-frequency block codes (STFBCs) generally suffer from very high computational complexity at the receiver side. In this paper, we introduce a new class of full-diversity STFBCs for quasi-static (QS) channels which features a comparatively low complexity at the receiver. We also demonstrate that our proposed algorithms could offer maximum coding advantage if the transmitter knows partial channel side information. Simulation results also verify that our coding schemes outperform other recently published STFBCs that were considered.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2012.031212.112648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6171804","Wireless communication;MIMO-OFDM;quasi-static channels;space-time-frequency coding;fading channels","Delay;Channel models;Encoding;OFDM;Receiving antennas;Complexity theory;Transmitters","computational complexity;maximum likelihood decoding;radio receivers;space-time block codes","full-diversity space-time-frequency coding;ML decoder;block codes;STFBC;computational complexity;receiver;quasi-static channels","","8","","8","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity PARCOR Coefficient Quantizer and Prediction Order Estimator for G.711.0 (Lossless Speech Coding)","Y. Kamamoto; T. Moriya; N. Harada","NA; NA; NA","2010 Data Compression Conference","","2010","","","475","483","This paper presents two low-complexity tools used for the new ITU-T recommendation G.711.0, which is the standard for lossless compression of G.711 (A-law/Mu-law logarithmic PCM) speech data. One is an algorithm for quantizing the PARCOR/reflection coefficients and the other is an estimation method for the optimal prediction order. Both tools are based on a criterion that minimizes the entropy of the prediction residual signals and can be implemented in a fixed-point low-complexity algorithm. G.711.0 with the developed practical tools will be widely used everywhere because it can losslessly reduce the data rate of G.711, the prevailing speech-coding technology.","2375-0359;1068-0314","978-1-4244-6426-5978-1-4244-6425","10.1109/DCC.2010.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5453469","Speech coding;Lossless compression;Standardization;Linear predictive coding;G.711","Speech coding;Phase change materials;Entropy;Signal processing;Reflection;Pulse modulation;Codecs;Quantization;Linear predictive coding;Decoding","data compression;quantisation (signal);speech coding","low-complexity PARCOR coefficient quantizer;prediction order estimator;ITU-T recommendation;lossless compression;G.711 lossless compression;reflection coefficients;optimal prediction order;entropy;residual signals;fixed-point low-complexity algorithm;speech-coding technology","","2","","14","","","","","IEEE","IEEE Conferences"
"Reduced-complexity decoding of low-density parity check codes based on adaptive convergence","J. Su; Z. Lu; X. Yu; Y. Liu","Department of Microelectronics, Soochow University Suzhou, 215006, China; Department of Microelectronics, Soochow University Suzhou, 215006, China; Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; University of Electronics Science and Technology of China, Chengdu, 610054, China","2010 International SoC Design Conference","","2010","","","319","322","Low-density parity-check (LDPC) codes have recently been considered as a viable candidate for forward error correction in system level hardware-redundant, fault-tolerant logics. An important factor that influences the choosing of a specific FEC technique in a nano-scale system implementation is its real-time performance, namely its computational complexity. In this paper, we propose a set of rules to decide whether a variable node in a LDPC decoder should update its value in subsequent iterations of the decoding process, or be considered as converged. We show that by carefully choosing the convergence rules for variable nodes, significant reduction of decoding complexity can be achieved with endurable performance loss.","","978-1-4244-8632-8978-1-4244-8633-5978-1-4244-8631","10.1109/SOCDC.2010.5682908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5682908","LDPC;adaptive convergenc;min-sum decoding","Convergence;Complexity theory;Decoding;Iterative decoding;Binary phase shift keying;AWGN channels","computational complexity;convergence;decoding;fault tolerance;forward error correction;parity check codes","reduced-complexity decoding;low-density parity check codes decoder;adaptive convergence;forward error correction;system level hardware-redundant logic;fault-tolerant logic;nano-scale system;computational complexity","","","","8","","","","","IEEE","IEEE Conferences"
"Real time low complexity VLSI decoder for prefix coded images","A. I. Ahangar; R. Agarwal; K. Lakhotia","Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India","2016 IEEE International Symposium on Circuits and Systems (ISCAS)","","2016","","","1694","1697","Rise in bandwidth requirement for multimedia processing is forcing designers to use complex or wider buses in SOCs. Compression schemes using bit serial codes offer a low complexity encoding solution to such problems. However, their inherent sequential nature makes it challenging to achieve real time throughput at the decoder end. Conventionally, this problem is addressed by high frequency application processors or LUT based hardware, both of which are power hungry solutions. In this paper, we present a novel hardware architecture capable of decoding image bit planes encoded using bit serial codes, while overcoming the above limitation. It achieves parallelism without the use of any markers between bit planes. The architecture is also able to decode interleaved RAW data with minimal header information. The area of proposed architecture scales linearly with output bitrate, making it suitable for use cases involving high resolution and/or high frame rates.","2379-447X","978-1-4799-5341-7978-1-4799-5340-0978-1-4799-5342","10.1109/ISCAS.2016.7538893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538893","real time;low complexity;bit serial codes;low memory requirement;scalable","Decoding;Throughput;Image coding;Complexity theory;Random access memory;Memory management;Hardware","codecs;decoding;image coding;multimedia communication;VLSI","real time low complexity VLSI decoder;prefix coded images;multimedia processing;complexity encoding solution;decoder;LUT based hardware;hardware architecture;decoding image bit planes;interleaved RAW data","","1","","8","","","","","IEEE","IEEE Conferences"
"Irregular mapping design for bit-interleaved coded modulation with low complexity iterative decoding","Q. Wang; C. Zhang; J. Dai","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 101408, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 101408, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 101408, China","2016 6th International Conference on Electronics Information and Emergency Communication (ICEIEC)","","2016","","","42","45","Bit-interleaved coded modulation with iterative decoding (BICM-ID) is an effective technology to approach the channel capacity for high-order modulations, which makes it especially fit for the circumstances where both efficient code rate and superior performance are needed, such as bandwidth limited wireless broadcasting or mobile communications. However, to achieve the target of near capacity decoding, many times of iteration is assumed, which heavily adds complexity to the receiving side. If the times of iteration lowers down, considerable degradation could be observed. The design rules of near capacity decoding cannot generally guarantee a still acceptable performance for a few times of iteration. In this paper, to lessen the decoding complexity, an irregular mapping design aiming at maximizing the extrinsic mutual information after a fixed times of iteration is studied. And the simulations show that our method obtains coding gains from 0.4 dB to 1.2 dB at BER=10<sup>-5</sup> compared to other known mappings on condition of 4 times of iteration.","2377-844X","978-1-5090-1997-7978-1-5090-1995-3978-1-5090-1998","10.1109/ICEIEC.2016.7589683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589683","BICM;Iterative Decoding;Irregular Mapping;EXIT Charts","Iterative decoding;Decoding;Modulation;Complexity theory;Encoding;Doping;Mutual information","channel capacity;computational complexity;iterative decoding;modulation coding","irregular mapping design;bit-interleaved coded modulation-iterative decoding;low-complexity iterative decoding;BICM-ID;channel capacity;high-order modulation;code rate;bandwidth-limited wireless broadcasting;mobile communication;near-capacity decoding;decoding complexity;extrinsic mutual information;coding gains","","1","","16","","","","","IEEE","IEEE Conferences"
"Reduced complexity rate-matching/de-matching architecture for the LTE turbo code","A. Spanos; F. Gioulekas; M. Birbas; A. Vgenis","The University of Edinburgh, Scotland, UK; Subdirectorate of Informatics, University General Hospital of Larissa, 41110, Greece; Analogies S.A., Patras Science Park, Patras, Greece, 26504; Analogies S.A., Patras Science Park, Patras, Greece, 26504","2014 21st IEEE International Conference on Electronics, Circuits and Systems (ICECS)","","2014","","","411","414","The task of rate matching is to extract from the blocks of code bits, delivered by the Long Term Evolution (LTE) Turbo Encoding (TE), the exact set of bits to be transmitted within a given transmission time interval, depending on the existing channel conditions. Within this context, we propose algorithmic improvements on the rate matching procedure, which are applied to the TE output streams, and to the corresponding rate de-matching at the receiver side, respectively. The proposed method facilitates an address assigner that on-the-fly controls the memory addressing operations by defining the interleaving/ puncturing of bits to be transmitted. The usage of this controller leads to a significant latency reduction and to the utilization of low memory resources when compared with other conventional approaches deduced by the LTE standard.","","978-1-4799-4242","10.1109/ICECS.2014.7050009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050009","Address assigner;rate matching;LTE;Turbo Code;low memory resources","Complexity theory;Multiplexing;Read only memory;Calculators;Long Term Evolution;Memory management","Long Term Evolution;turbo codes","reduced complexity rate-matching;rate dematching architecture;LTE turbo code;Long Term Evolution;turbo encoding;transmission time interval;channel conditions;algorithmic improvements;TE output streams;on-the-fly controls;memory addressing operations;latency reduction","","1","","7","","","","","IEEE","IEEE Conferences"
"Low-Complexity Polarization Coding for PDL-Resilience","A. Dumenil; E. Awwad; C. Méasson","Nokia Bell Labs, 1 route de Villejust, Nozay, 91620, France; Nokia Bell Labs, 1 route de Villejust, Nozay, 91620, France; Nokia Bell Labs, 1 route de Villejust, Nozay, 91620, France","2018 European Conference on Optical Communication (ECOC)","","2018","","","1","3","We propose a new and low-complexity 4D-signaling that ensures polarization-multiplexed systems to be resilient against polarization dependent loss. For 16QAM modulation above 0.85-coding rate, the required SNR for error-free transmission at 6dB-PDL is relaxed by 0.5 dB or more.","","978-1-5386-4862-9978-1-5386-4861-2978-1-5386-4863","10.1109/ECOC.2018.8535488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8535488","","Signal to noise ratio;Gain;Optical polarization;Encoding;Optical losses;Modulation;MIMO communication","encoding;multiplexing;optical fibre communication;optical fibre polarisation;optical modulation;quadrature amplitude modulation","polarization dependent loss;16QAM modulation;low-complexity polarization coding;PDL-resilience;coding rate;polarization-multiplexed systems;low-complexity 4D-signaling;SNR;error-free transmission","","1","","7","","","","","IEEE","IEEE Conferences"
"Low-Complexity Channel-Likelihood Estimation for Non-Binary Codes and QAM","G. He; G. Sarkis; S. Hemati; W. J. Gross; B. Bai","State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China 710071; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada H3A 2A7; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada H3A 2A7; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada H3A 2A7; State Key Lab. of Integrated Service Networks, Xidian University, Xi'an, China 710071","IEEE Communications Letters","","2012","16","6","801","804","An approximate channel-likelihood estimation algorithm is introduced for non-binary coded, square quadrature-amplitude-modulated systems. It is shown that the proposed approximation greatly simplifies the calculation of channel-likelihoods and yet provides results close to those based on exact calculations for non-binary low-density parity-check codes over the additive white Gaussian noise channel.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2012.042312.120382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189816","Channel-likelihoods;non-binary LDPC codes;QAM","Table lookup;Parity check codes;Complexity theory;Quadrature amplitude modulation;Approximation methods;Decoding;Signal to noise ratio","AWGN channels;binary codes;channel estimation;quadrature amplitude modulation","low-complexity channel-likelihood estimation;nonbinary code;QAM;approximate channel-likelihood estimation algorithm;square quadrature-amplitude-modulated systems;nonbinary low-density parity-check code;additive white Gaussian noise channel","","1","","10","","","","","IEEE","IEEE Journals & Magazines"
"Two quasi orthogonal space-time block codes with better performance and low complexity decoder","A. Lotfi-Rezaabad; S. Talebi; A. Chizari","Sharif University of Technology (SUT), Tehran, Iran; Advanced Communication Research Institute (ACRI), Sharif University of Technology (SUT), Tehran, Iran; Sharif University of Technology (SUT), Tehran, Iran","2016 10th International Symposium on Communication Systems, Networks and Digital Signal Processing (CSNDSP)","","2016","","","1","5","This paper presents two new space time block codes (STBCs) with quasi orthogonal structure for an open loop multi-input single-output (MISO) systems. These two codes have been designed to transmit from three or four antennas at the transmitter and be given to one antenna at the receiver. In this paper first, the proposed codes are introduced and their structures are investigated. This is followed by the demonstration of how the decoder decodes half of transmitted symbols independent of the other half. The last part of this paper discusses the simulation results, makes performance comparison against other popular approaches and concludes that the proposed solutions offer superiority.","","978-1-5090-2526-8978-1-5090-2527","10.1109/CSNDSP.2016.7573924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7573924","Multi-input single-output;5G;space time block codes;fading channels;multipath channels","Decoding;Transmitting antennas;Receiving antennas;Quality of service;Fading channels","decoding;orthogonal codes;receiving antennas;space-time block codes;transmitting antennas","quasiorthogonal space-time block codes;STBC;low complexity decoder;multiinput single-output;MISO systems","","","","12","","","","","IEEE","IEEE Conferences"
"Fast and Low-Complexity Decoding Algorithm and Architecture for Quadruple-Error-Correcting RS codes","Z. Yan; W. Li; J. Lin; Z. Wang","School of Electronic Science and Engineering, Nanjing University, China; School of Electronic Science and Engineering, Nanjing University, China; School of Electronic Science and Engineering, Nanjing University, China; School of Electronic Science and Engineering, Nanjing University, China","2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)","","2018","","","191","194","The key-equation solver (KES) and the Chien search & error evaluator (CSEE) are two stages of the decoding algorithms for Reed-Solomon (RS) codes. They determine the error locator polynomial and compute the error magnitudes, respectively. A recursive algorithm called reformulated inversionless Berlekamp Massey (riBM) algorithm is widely used for the KES block. However, for high rate RS codes, such as single-error-correcting (SEC) and double-error-correcting (DEC) RS codes, a method to find errata patterns directly has been proposed in the literature. We extend the method and derive the direct calculation algorithm for quadruple-error-correcting (QEC) RS codes in this work. An optimized architecture of the proposed algorithm is further developed. The computational complexity is reduced significantly by the proposed direct calculation method. Moreover, the data paths of the proposed architecture are all feed-forward and thus pipelining can be easily employed for high-speed applications.","","978-1-5386-8240-1978-1-5386-8239-5978-1-5386-8241","10.1109/APCCAS.2018.8605720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8605720","Error correction codes;Reed-Solomon codes;direct calculation;high-speed architecture","Computer architecture;Decoding;Computational complexity;Hardware;Adders;Reed-Solomon codes;Pipeline processing","computational complexity;decoding;error correction codes;polynomials;Reed-Solomon codes","low-complexity decoding algorithm;quadruple-error-correcting RS codes;key-equation solver;decoding algorithms;Reed-Solomon codes;error locator polynomial;recursive algorithm;Berlekamp Massey algorithm;KES block;direct calculation algorithm;optimized architecture;computational complexity;double-error-correcting RS codes;single-error-correcting RS codes;Chien search-error evaluator","","","","10","","","","","IEEE","IEEE Conferences"
"Low complexity surveillance Video Coding based on Distributed Compressive Video Sensing","S. Narayanan; A. Makur","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2016 IEEE Region 10 Conference (TENCON)","","2016","","","993","996","In video surveillance applications, the foreground moving objects in the frame are segmented from its background and are coded with fewer bits compared to frame-based coding. In such techniques, encoder becomes complex due to object segmentation and object motion estimation. Actual motivation of compressive sensing on video is to have a simple encoder, and therefore, we propose Distributed Compressive Video Sensing based Video Object Compression (DCVS-VOC) technique in which, (i) the object segmentation is done only for certain frames, and (ii) the motion estimation is performed at the decoder instead of encoder. Experimental results show that the proposed DCVS-VOC is capable of handling any CS reconstruction algorithm at its decoder.","2159-3450","978-1-5090-2597-8978-1-5090-2596-1978-1-5090-2598","10.1109/TENCON.2016.7848154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848154","Surveillance video coding;distributed compressive video sensing and video object compression","Decoding;Encoding;Motion estimation;Reconstruction algorithms;Object segmentation;Complexity theory;Sensors","compressed sensing;computational complexity;image reconstruction;image segmentation;motion estimation;video coding;video surveillance","low complexity surveillance video coding;distributed compressive video sensing;foreground moving object segmentation;object motion estimation;video object compression;DCVS-VOC technique;CS reconstruction algorithm","","","","15","","","","","IEEE","IEEE Conferences"
"Low-complexity rate-distortion optimization for robust H.264 video coding","Jing Yang; Bowen An","College of Information Engineering, Shanghai Maritime University, China; College of Information Engineering, Shanghai Maritime University, China","2011 IEEE International Conference on Multimedia and Expo","","2011","","","1","4","In this paper, the conventional H.264 Rate Distortion (RD) optimization algorithm is extended to improve error resilience for video transmission over error-prone channels. Novel RD cost functions are derived for both motion estimation and mode decision by separating the end-to-end distortion into independent items. The modified cost functions involve additional items which are very simple and can be easily computed in a recursive way, hence low computation complexity can be achieved. Moreover, the proposed scheme maintains complete decoder compatibility and high RD performance. Extensive simulation results show that significant performance improvements are gained over compared algorithms. The computation complexity issue is also discussed.","1945-788X;1945-7871;1945-7871","978-1-61284-350-6978-1-61284-348-3978-1-61284-349","10.1109/ICME.2011.6012062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012062","RD optimization;H.264;error resilient coding;motion-compensated prediction","Motion estimation;Bit rate;Cost function;Encoding;Complexity theory;PSNR","computational complexity;motion compensation;motion estimation;optimisation;video coding","low complexity rate distortion optimization;robust H.264 video coding;H.264 rate distortion optimization algorithm;error resilience;video transmission;error-prone channels;motion estimation;mode decision;end-to-end distortion;computation complexity;decoder compatibility","","","","10","","","","","IEEE","IEEE Conferences"
"Bit-inverted Gray coded bit-plane matching for low complexity motion estimation","C. Choi; J. Jeong","Department of Electronics and Computer Engineering, Hanyang University, Seoul, Korea; Department of Electronics and Computer Engineering, Hanyang University, Seoul, Korea","2015 International Conference on Pervasive and Embedded Computing and Communication Systems (PECCS)","","2015","","","230","234","In this paper, a bit-inverted Gray coded bit-plane matching algorithm is proposed for low complexity motion estimation. Unlike the typical Gray coded bit-plane matching algorithms, the proposed algorithm uses bit-inverted Gray codes for transforming image frames and a corresponding extended matching criterion to enhance the motion estimation accuracy. Experimental results show that the proposed algorithm outperforms other bit-plane matching based motion estimation algorithms while preserving the binary matching characteristic.","","978-989-758-137","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483763","Block Matching;Motion Estimation;Bit-Plane Matching","Measurement;Motion estimation;Algorithm design and analysis;Computational complexity;Reflective binary codes;Transforms","","","","","","11","","","","","IEEE","IEEE Conferences"
"Low complexity depth intra coding combining fast intra mode and fast CU size decision in 3D-HEVC","K. Peng; J. Chiang; W. Lie","Department of Electrical Engineering, Advanced Institute of Manufacturing with High-tech Innovations, National Chung Cheng University, Taiwan; Department of Electrical Engineering, Advanced Institute of Manufacturing with High-tech Innovations, National Chung Cheng University, Taiwan; Department of Electrical Engineering, Advanced Institute of Manufacturing with High-tech Innovations, National Chung Cheng University, Taiwan","2016 IEEE International Conference on Image Processing (ICIP)","","2016","","","1126","1130","3D-HEVC is the new coding standard dealing with both the texture and the associated depth video. In addition to some new coding tools designed for texture video with improved coding efficiency, some specified tools are devoted for depth video, such as depth modeling mode (DMM), segment-wise DC (SDC) mode and single depth intra mode. In this paper, we propose two techniques to speed up the encoding of depth video, including fast intra mode decision and fast CU size decision. For the fast intra mode decision, early termination is performed if the minimum rate-distortion (RD) cost of test candidate modes is smaller than the threshold computed from full mode search. For the fast CU size decision, smaller CU size will not be evaluated if the current CU presents some desired properties. The experimental results report that the proposed techniques achieve on average 37.6% time saving with 0.8% bitrate increase for the synthesized views under the all intra scenario.","2381-8549","978-1-4673-9961-6978-1-4673-9962","10.1109/ICIP.2016.7532533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532533","depth coding;intra mode;HEVC;CU","Manufacturing;Technological innovation;Image processing;IEC;ISO;Circuits and systems","computational complexity;image texture;rate distortion theory;video coding","low complexity depth intra coding;3D-HEVC;texture video;improved coding efficiency;depth video encoding;fast intra mode decision;fast CU size decision;rate-distortion cost","","4","","12","","","","","IEEE","IEEE Conferences"
"Low Complexity Mode Depended Local Adaptive Interpolation Filter for Next Generation Video Coding","J. Zhou; S. Sakaida; T. Ikenaga","NA; NA; NA","2010 6th International Conference on Wireless Communications Networking and Mobile Computing (WiCOM)","","2010","","","1","4","H.264/AVC utilizes the motion prediction with motion vectors at fractional (1/4-pixel). Interpolation filter is utilized to generate the information in the positions between integer positions. Adaptive interpolation filters that adaptively calculate the coefficients by frame have been adopted in KTA tools which were set up as software platform of new proposals for the next video standard. Among several AIFs, EAIF achieved best rate reduction with averagely 7% over all sequences, with complexity doubled. The high complexity restricts its practical applications under current digital computing ability. To reduce the total encoding complexity, mode depended AIF adoption and local macroblock utilization methods are proposed in this paper. It is shown that efficiency of AIF can be related to mode partition distributions which are generated in the first coding pass. By applying different threshold with different QP, the usage of AIF in each frame can be decided. A further complexity reduction is achieved by locally determining the filter usage on each macroblock based on the macroblock characteristics in the second coding pass. Combing two methods, encoding complexity reduction of 40% could be achieved averagely over CIF sequences by means of motion estimation time with negligible penalty to PSNR.","2161-9646;2161-9654","978-1-4244-3708-5978-1-4244-3709","10.1109/WICOM.2010.5601037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601037","","Adaptive filters;Encoding;Complexity theory;Interpolation;Wiener filter;Bit rate;Proposals","adaptive filters;image motion analysis;interpolation;video coding","low complexity mode;local adaptive interpolation filter;next generation video coding;H.264/AVC;motion prediction;motion vector;integer position;KTA tool;software platform;video standard;digital computing;encoding complexity;AIF adoption;local macroblock utilization;complexity reduction;filter usage;macroblock characteristics","","","","7","","","","","IEEE","IEEE Conferences"
"Gaussian Elimination Decoding of$t$-Error Correcting Reed-Solomon Codes in$t$Steps and$O(t^2)$Complexity","M. Fossorier","NA","IEEE Communications Letters","","2015","19","7","1101","1104","In this letter, a decoding algorithm based on Gaussian elimination is presented to decode a t-error correcting Reed-Solomon (RS) code. This algorithm requires only t steps, as opposed to the “classic” Berlekamp-Massey (BM) algorithm which requires 2t steps. Both algorithms compute 2t discrepancies which are used to iteratively update the error locator polynomial with roughly the same O(t2) complexity, but the new algorithm is twice as fast as the conventional BM algorithm as two discrepancies can be computed in parallel at each step.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2436379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111261","Reed-Solomon codes;decoding;block codes;Berlekamp-Massey algorithm;Gaussian elimination","Decoding;Complexity theory;Polynomials;Manganese;Reed-Solomon codes;Upper bound;Iterative methods","error correction codes;Gaussian processes;Reed-Solomon codes","Gaussian elimination decoding;t-error correcting Reed-Solomon codes;t steps;O(t2) complexity;error locator polynomial","","","","3","","","","","IEEE","IEEE Journals & Magazines"
"The computational complexity of some algorithms for the rank bounded distance on cyclic codes","J. Zheng; T. Kaiday","Faculty of Humanities, Kyushu Women's University, Kitakyushu, Fukuoka, Japan; Faculty of Humanity-Oriented Science and Engineering, Kinki University, Iizuka, Fukuoka, Japan","2015 Seventh International Workshop on Signal Design and its Applications in Communications (IWSDA)","","2015","","","38","41","The authors proposed an algorithm for calculation of new lower bound using the discrete Fourier transform in 2010. Afterward, we considered some algorithms to improve the original algorithm. In this paper, we discuss the computational complexity of the rank bounded distance for cyclic codes.","2150-3699","978-1-4673-8308-0978-1-4673-8307","10.1109/IWSDA.2015.7458409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458409","defining set;discrete Fourier transform;rank bounded distance;minimum distance;cyclic code","Computational complexity;Discrete Fourier transforms;Zinc;Algorithm design and analysis;Hamming weight","computational complexity;cyclic codes;discrete Fourier transforms","computational complexity;cyclic code;rank bounded distance;discrete Fourier transform","","","","16","","","","","IEEE","IEEE Conferences"
"Adaptive low complexity colour transform for video coding","R. Weerakkody; M. Mrak","BBC Research and Development London, United Kingdom; BBC Research and Development London, United Kingdom","2014 IEEE 16th International Workshop on Multimedia Signal Processing (MMSP)","","2014","","","1","6","For video compression, the RGB signals are usually converted at the source to a perceptual colour space, followed by chroma sub-sampling, for coding efficiency. This is based on the typically higher human visual system sensitivity to the luminance than chrominance of image signals. However, there are specific applications that demand carrying the full RGB signals through the transmission chain, which may also benefit from lossless colour transforms, for efficient coding. In either case, the best colour transform function is noted to be content dependent, although fixed transforms are typically adopted for convenience. This paper presents a method of dynamically adapting this colour transform function for each picture block, using a class of low complexity lifting based schemes. The performance of the proposed algorithm is compared with a number of fixed colour transform schemes and shows a significant compression gain over native RGB coding and YC<sub>o</sub>C<sub>g</sub> transform.","","978-1-4799-5896","10.1109/MMSP.2014.6958820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958820","","Transforms;Color;Image color analysis;Encoding;Complexity theory;Decoding;PSNR","data compression;image colour analysis;video coding","adaptive low complexity colour transform;video coding;video compression;RGB signals;red-green-blue signals;perceptual colour space;chroma subsampling;coding efficiency;human visual system;image luminance;image chrominance;picture block;complexity lifting based scheme;colour transform schemes;compression gain","","","","9","","","","","IEEE","IEEE Conferences"
"Distributed MIMO coding scheme with low decoding complexity for future mobile TV broadcasting","M. Liu; M. Helard; M. Crussiere; J. -. Helard","INSA, IETR, Universite - Europeenne de Bretagne (UEB), France; INSA, IETR, Universite - Europeenne de Bretagne (UEB), France; INSA, IETR, Universite - Europeenne de Bretagne (UEB), France; INSA, IETR, Universite - Europeenne de Bretagne (UEB), France","Electronics Letters","","2012","48","17","1079","1081","A novel distributed space-time block code (STBC) for the next generation mobile TV broadcasting is proposed. The new code provides efficient performance within a wide range of power imbalance showing strong adaptivity to the single frequency network broadcasting deployments. The new code outperforms existing STBCs with equivalent decoding complexity and approaches those with much higher complexities.","0013-5194","","10.1049/el.2012.1778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272463","","","decoding;MIMO communication;mobile radio;next generation networks;radio broadcasting;space-time block codes","distributed MIMO coding scheme;low decoding complexity;distributed space-time block code;distributed STBC;next generation mobile TV broadcasting;power imbalance;single frequency network broadcasting deployments","","","","","","","","","IET","IET Journals & Magazines"
"High-Rate and Low-Complexity Space-Time Block Codes for$2 \times 2$MIMO Systems","V. Vakilian; H. Mehrpouyan","Department of Electrical and Computer Engineering and Computer Science, California State University, Bakersfield, CA, USA; Department of Electrical and Computer Engineering, Boise State University, Boise, ID, USA","IEEE Communications Letters","","2016","20","6","1227","1230","The main design criteria for space-time block codes (STBCs) are the code rate, diversity order, coding gain, and low decoder complexity. In this letter, we propose a full-rate full-diversity STBC for 2 × 2 multiple-input multiple-output (MIMO) systems with a substantially lower maximum likelihood (ML) detection complexity than that of existing schemes. This makes the implementation of high-performance full-rate codes feasible for practical systems. Our numerical evaluation shows that the proposed code achieves significantly lower decoding complexity while maintaining a similar performance compared to that of existing rate-2 STBCs.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2545651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439783","Multiple-input multiple-output (MIMO);space-time coding;decoding complexity;Multiple-input multiple-output (MIMO);space-time coding;decoding complexity","Complexity theory;Decoding;MIMO;Block codes;Transmitting antennas;Constellation diagram","maximum likelihood decoding;maximum likelihood detection;MIMO communication;space-time block codes","high-rate and low-complexity space-time block code;high-rate and low-complexity STBC;MIMO system;code rate;diversity order;coding gain;low decoder complexity;full-rate full-diversity STBC;multiple-input multiple-output system;maximum likelihood detection complexity;ML detection complexity;lower decoding complexity","","6","","15","","","","","IEEE","IEEE Journals & Magazines"
"Lowered-complexity soft decoding of generalized LDPC codes over AWGN channels","S. I. Elsanadily; A. M. Mahran; O. M. Elghandour","Department of Communications and Electronics, Faculty of engineering, Helwan university, Cairo, Egypt; Department of Communications and Avionics, Military Technical College, Cairo, Egypt; Department of Communications and Electronics, Faculty of engineering, Helwan university, Cairo, Egypt","2017 12th International Conference on Computer Engineering and Systems (ICCES)","","2017","","","320","324","This paper presents a lowered complexity version of a reliability-based iterative decoding algorithm for Generalized Low Density Parity Check (GLDPC) codes with BCH constituent codes using Soft-Input Soft-Output (SISO) Chase-II algorithm. Applied to Gallager-based global LDPC code, it shows that the proposed algorithm, when employed on GLDPC with the multi-error correction BCH subcodes, reduces the complexity of the conventional Soft-Decision Decoding (SDD) with Chase algorithm at a cost of slight degradation in the error performance. The approach is built on eliminating the employment of the Chase SISO decoder at the Generalized Check Nodes (GCNs) where the expected number of errors is within the correction capability of the Hard-Decision Decoder (HDD). As a case study, the decoder is considered under AWGN channel and Simulation results are presented showing the complexity and performance of the conventional and proposed algorithms.","","978-1-5386-1191-3978-1-5386-1190-6978-1-5386-1192","10.1109/ICCES.2017.8275325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275325","Iterative decoding;APP decoding;Hamming codes;Chase algorithm;low-density parity-check codes","Decoding;Iterative decoding;Complexity theory;Reliability;AWGN channels;Sparse matrices","AWGN channels;BCH codes;channel coding;error correction codes;iterative decoding;parity check codes;telecommunication network reliability","AWGN channels;Generalized Low Density Parity Check codes;GLDPC;BCH constituent codes;Soft-Input Soft-Output Chase-II algorithm;multierror correction BCH;Chase SISO decoder;Hard-Decision Decoder;low-complexity soft decoding;Soft-Decision Decoding","","","","17","","","","","IEEE","IEEE Conferences"
"Complexity-Adjustable H.264 Video Coding in Wireless Environment","J. Zhang; Y. Zhang; T. Yu; K. Zhao","NA; NA; NA; NA","2011 7th International Conference on Wireless Communications, Networking and Mobile Computing","","2011","","","1","4","In this paper, we propose a novel complexity-adjustable video coding algorithm for H.264/AVC-based wireless video communications through portable devices. To optimize the rate-distortion performance under power constraint in packet loss environment, we present a novel model that can adapt to the different channel conditions and computation power constraints. The experimental results demonstrate that the proposed scheme can minimize the end-to-end distortion in the environments with both power constraint and packet loss.","2161-9654;2161-9646;2161-9646","978-1-4244-6252-0978-1-4244-6250-6978-1-4244-6251","10.1109/wicom.2011.6040541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040541","","Encoding;Complexity theory;Rate-distortion;Computational modeling;Motion estimation;Wireless communication;Streaming media","multimedia communication;radiocommunication;video coding;video communication","complexity-adjustable H.264 video coding algorithm;wireless video communications;portable devices;rate-distortion performance;packet loss environment;channel conditions;computation power constraints;end-to-end distortion","","1","","8","","","","","IEEE","IEEE Conferences"
"Reduced Complexity Decoding of Polar Codes with Reed-Solomon Kernel","P. Trifonov","Saint Petersburg Polytechnic University, Russia","2018 Information Theory and Applications Workshop (ITA)","","2018","","","1","9","We propose to reduce the decoding complexity of polar codes with Reed-Solomon kernels by exploiting its algebraic similarity with Arikan kernel, and employing the stack algorithm for computing the probabilities of kernel input symbols. Simulation results show that polar codes with 8 × 8 Reed-Solomon kernel under SC decoding provide performance comparable to Arikan polar codes with CRC under list SC decoding.","","978-1-7281-0124-8978-1-7281-1995","10.1109/ITA.2018.8503265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8503265","","Kernel;Decoding;Complexity theory;Reed-Solomon codes;Standards;Data structures;Heuristic algorithms","computational complexity;decoding;error correction codes;Reed-Solomon codes","Arikan polar codes;complexity decoding;Arikan kernel;kernel input symbols;Reed-Solomon kernel;stack algorithm;SC decoding","","","","14","","","","","IEEE","IEEE Conferences"
"A Low Complexity Sequential Decoding Algorithm for Rateless Spinal Codes","W. Yang; Y. Li; X. Yu; J. Li","NA; NA; NA; NA","IEEE Communications Letters","","2015","19","7","1105","1108","A forward stack decoding (FSD) algorithm for rateless spinal codes is proposed in this letter. This decoding algorithm divides the decoding tree of the spinal code into several layers, and then searches the decoding paths in each single layer. Compared with the bubble decoder, which was initially designed for spinal codes, the proposed FSD algorithm can decrease the complexity significantly without sacrificing the rate performance. We also illustrate that the complexity of the FSD algorithm decreases with the increase of the signal-to-noise ratio (SNR). Simulation shows that the complexity of the FSD algorithm is only 15.37% of that of the bubble decoder when SNR is 20 dB.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2438062","973 Program; NSFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113798","Rateless spinal codes;sequential decoding;forward stack decoding algorithm;Rateless spinal codes;sequential decoding;forward stack decoding algorithm","Complexity theory;Measurement;Maximum likelihood decoding;Algorithm design and analysis;Signal to noise ratio;Encoding","sequential decoding;trees (mathematics)","FSD algorithm;bubble decoder;decoding tree;forward stack decoding algorithm;rateless spinal codes;low complexity sequential decoding algorithm","","9","","14","","","","","IEEE","IEEE Journals & Magazines"
"An adaptive-equalizer-less low-complexity DSP using differential code shift keying","A. Matsushita; K. Horikoshi; S. Okamoto; M. Nakamura; F. Hamaoka; Y. Kisaka; A. Hirano","NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Access Network Service Systems Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan","2017 Opto-Electronics and Communications Conference (OECC) and Photonics Global Conference (PGC)","","2017","","","1","2","We propose the use of differential code shift keying (DCSK) for a symbol rate adaptive low-complexity DSP algorithm by eliminating adaptive-equalizer (AEQ) with limited influence on circuit size. As a proof of concept, a 300km transmission using DCSK format was demonstrated without using AEQ.","2166-8892","978-1-5090-6293-5978-1-5090-6294","10.1109/OECC.2017.8114865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8114865","Coherent communications;Optical communications;Modulation formatting","Signal to noise ratio;Optical noise;Power demand;Encoding;Receivers;Phase shift keying","differential phase shift keying;digital signal processing chips;optical information processing;optical receivers","adaptive-equalizer-less low-complexity DSP;digital signal processers;differential code shift keying;DCSK;symbol rate adaptive low-complexity DSP algorithm;circuit size;AEQ;distance 300 km","","","","11","","","","","IEEE","IEEE Conferences"
"Computational Thinking, Code Complexity, and Prior Experience in a Videogame-Building Assignment","P. Boechler; C. Artym; E. Dejong; M. Carbonaro; E. Stroulia","NA; NA; NA; NA; NA","2014 IEEE 14th International Conference on Advanced Learning Technologies","","2014","","","396","398","Computational-thinking skills are an essential intellectual amplifier for all scientific and professional disciplines. Embedding these skills in the K-12 and University curricula is necessary for training the next generation of thinkers. A widely adopted approach to doing so is through simple and visual programming languages like Scratch and engaging assignments like video-game construction. In this work, we report on an empirical study we conducted with senior undergraduate education students aiming to understand how prior experience enables students to better develop their computation-thinking skills through a Scratch-based video-game assignment.","2161-3761;2161-377X","978-1-4799-4038","10.1109/ICALT.2014.118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901492","computational thinking;video-games;game-based learning;scratch","Games;Complexity theory;Education;Correlation;Measurement;Software;Media","computer based training;computer games;further education;visual programming","computational thinking skill;code complexity;prior experience;videogame-building assignment;K-12;university curricula;visual programming languages;video-game construction;senior undergraduate education students;Scratch-based video-game assignment","","3","","8","","","","","IEEE","IEEE Conferences"
"Iterative Reduced-Complexity Graph-Based Detection for LDPC Coded 2D Recording Channels","Z. Qin; K. Cai; K. S. Chan","Data Storage Institute,, Agency for Science, Technology and Research (A*STAR),, Singapore; Data Storage Institute, Agency for Science, Technology and Research (A*STAR), Singapore; Data Storage Institute, Agency for Science, Technology and Research (A*STAR), Singapore","IEEE Transactions on Magnetics","","2013","49","6","2598","2602","In this paper, we consider joint detection and decoding for low-density parity-check (LDPC) coded two-dimensional (2D) intersymbol interference (ISI) channels, where message-passing is performed over an overall factor graph representing both the code structure and the channel model. We propose a low-complexity channel detector based on the Chase decoding algorithm to simplify the check node operation by restricting the a posteriori probability (APP) computation to a small subset of candidate vectors. Simulation results show that the proposed receiver provides an adjustable performance/complexity tradeoff and performs close to the full-complexity graph-based receiver.","0018-9464;1941-0069","","10.1109/TMAG.2013.2253763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522234","Chase decoding;graph detector;LDPC codes;message-passing;two-dimensional (2D) channels","Receivers;Vectors;Iterative decoding;Detectors;Decoding;Complexity theory","intersymbol interference;message passing;parity check codes","iterative reduced-complexity graph-based detection;low-density parity-check coded 2D recording channels;low-density parity-check coded 2D intersymbol interference channels;message-passing;overall factor graph;code structure;channel model;low-complexity channel detector;Chase decoding algorithm;check node operation;a posteriori probability computation;vector subset;performance-complexity tradeoff;full-complexity graph-based receiver","","1","","11","","","","","IEEE","IEEE Journals & Magazines"
"Very low-complexity coding of images using adaptive Modulo-PCM","J. Prades-Nebot","Departamento de Comunicaciones, Universitat Politècnica de València","2011 18th IEEE International Conference on Image Processing","","2011","","","305","308","Some video applications require the use of extremely simple source coding techniques. For these applications, Modulo-PCM is an interesting alternative to PCM since it has a very low complexity and performs better than PCM. In this paper, we present an adaptive Modulo-PCM algorithm for image coding. Our algorithm divides the image into blocks and performs a proper Modulo-PCM coding of each block. Since our algorithm allows different degrees of encoding complexity, it can adapt to the computational resources that are available in each video application. Experimental results show that our algorithm improves the coding efficiency of both non-adaptive MPCM and PCM. The magnitude of the improvements depends on the encoding complexity and the target rate.","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116310","Low complexity coding;video recording","Image coding;Phase change materials;Encoding;Decoding;Complexity theory;Transform coding;Silicon","adaptive codes;computational complexity;pulse code modulation;source coding;video coding","very low-complexity image coding;source coding techniques;adaptive Modulo-PCM algorithm;encoding complexity;nonadaptive MPCM;video recording","","1","","9","","","","","IEEE","IEEE Conferences"
"H.264 hierarchical P coding in the context of ultra-low delay, low complexity applications","D. Hong; M. Horowitz; A. Eleftheriadis; T. Wiegand","Vidyo, Inc; Vidyo, Inc; Vidyo, Inc; Vidyo, Inc","28th Picture Coding Symposium","","2010","","","146","149","Despite the attention that hierarchical B picture coding has received, little attention has been given to a related technique called hierarchical P picture coding. P picture only coding without reverse prediction is necessary in constrained bit rate applications that require ultra-low delay and/or low complexity, such as videoconferencing. Such systems, however, have been using the traditional IPPP picture coding structure almost exclusively. In this paper, we investigate the use of hierarchical P coding vs. traditional IPPP coding and demonstrate that it has significant advantages which have not yet been well documented or understood. From a pure coding efficiency point of view, we show that for encoders configured to use ultra-low delay and low complexity coding tools, hierarchical P coding achieves an average advantage of 7.86% BD-rate and 0.34 dB BD-SNR.","","978-1-4244-7135-5978-1-4244-7134-8978-1-4244-7133","10.1109/PCS.2010.5702445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702445","H.264;hierarchical P coding","Encoding;Delay;Bit rate;Complexity theory;Automatic voltage control;Video sequences;Video coding","image coding","H.264 hierarchical P coding;hierarchical B picture coding;IPPP picture coding structure;coding efficiency;encoder;ultra-low delay coding tool;low complexity coding tool","","13","","14","","","","","IEEE","IEEE Conferences"
"Towards Low-Complexity Scalable Coding for Ultra-High Resolution Video And Beyond","E. Thomas; A. Gabriel; O. Niamut; S. Dijkstra-Soudarissane","TNO, The Hague, The Netherlands; TNO, The Hague, The Netherlands; TNO, The Hague, The Netherlands; TNO, The Hague, The Netherlands","2018 IEEE Visual Communications and Image Processing (VCIP)","","2018","","","1","4","The current state of the art of video coding lacks specific tools to efficiently deal with ultra-high resolutions such as 8K video both at a hardware and software level. At the same time, the increase in video resolutions combined with a larger variety in display resolutions results in a need for spatial scalability at low computational complexity. The proposed solution allows for legacy 4K encoders and decoders both software and in particular hardware to be still exploited with 8K content. The proposed format also offers spatial scalability for an average BD-rate increase of 16%. This scalability is achieved by a polyphase subsampling of the input sequence and by leveraging the existing temporal scalability to induce spatial scalability.","1018-8770","978-1-5386-4458-4978-1-5386-4459","10.1109/VCIP.2018.8698680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698680","video coding;ultra-high resolution;scalable video coding;polyphase subsampling;HEVC","Encoding;Signal resolution;Streaming media;Scalability;Spatial resolution;Decoding","computational complexity;image resolution;video coding","low computational complexity;spatial scalability;low-complexity scalable coding;ultra-high resolution video;video coding;software level;average BD-rate;temporal scalability","","","","8","","","","","IEEE","IEEE Conferences"
"On the Decoding Radius Realized by Low-Complexity Decoded Non-Binary Irregular LDPC Codes","P. Rybin; A. Frolov","Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia","2018 International Symposium on Information Theory and Its Applications (ISITA)","","2018","","","384","388","In this paper we consider the low complexity majority-logic decoding algorithm for irregular non-binary low-density parity-check (LDPC) codes. The decoding algorithm is a generalization of the bit-flipping algorithm for binary LDPC codes. The lower estimate on the decoding radius realized by this algorithm is derived for the first time for irregular non-binary LDPC codes. We present the numerical results for the derived lower bound.","","978-4-88552-318-2978-4-88552-317-5978-1-5386-8223","10.23919/ISITA.2018.8664375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664375","","Decoding;Iterative decoding;Complexity theory;Sockets;Sparse matrices;Clustering algorithms","binary codes;decoding;majority logic;parity check codes","nonbinary irregular LDPC codes;low-density parity-check codes;bit-flipping algorithm;binary LDPC codes;decoding radius;low complexity majority-logic decoding","","","","17","","","","","IEEE","IEEE Conferences"
"Improving the probability of complete decoding of random code by trading-off computational complexity","Z. Chong; B. Goi; H. Ohsaki; C. Bryan Ng; H. Ewe","Universiti Tunku Abdul Rahman, Malaysia; Universiti Tunku Abdul Rahman, Malaysia; Kwansei Gakuin University, Japan; Victoria University of Wellington, New Zealand; Universiti Tunku Abdul Rahman, Malaysia","IET Communications","","2015","9","18","2281","2286","Random code is a rateless erasure code that can reconstruct the original message ofksymbols from anyk+ 10 encoded symbols with high probability of complete decoding (PCD), i.e. 99.9% successful decoding, irrespective of the message length,k. Nonetheless, random code is inefficient in reconstructing short messages. For example, a message ofk= 10 symbols requiresk+ 10 = 20 encoded symbols, i.e. two times the original message length in order to achieve high PCD. In this study, the authors propose micro-random code that encodes and decodes the original message using symbols of smaller dimensions, namely micro symbols. The authors' analysis and numerical simulations show that micro-random code achieves high PCD with onlyk+ 1 encoded symbols. As the trade-off for such a gain, the number of steps for decoding increases exponentially with each incrementing segmentation factor,α. In addition, the numerical results show that the decoding time increases by about 400% atα= 10, depending on the processing power of the system.","1751-8628;1751-8636","","10.1049/iet-com.2015.0295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343832","","","computational complexity;decoding;matrix algebra;probability;random codes","complete decoding probability;random code;segmentation factor;micro symbol;microrandom code;original message reconstruction;rateless erasure code;computational complexity","","","","7","","","","","IET","IET Journals & Magazines"
"Low-Complexity, Backward-Compatible Coding of High Dynamic Range Images and Video","E. Salvucci","NA","2016 Data Compression Conference (DCC)","","2016","","","632","632","This paper presents a low-complexity yet effective method for encoding High Dynamic Range data into two distinct Low Dynamic Range images to be subsequently lossy or losslessly compressed using any LDR image and video codec. The method and principles described herein are applicable to both still HDR images and sequences used for HDR-video creation. The peculiar features of the method allow LDR image and video filters to be applied, including de-blocking and de-ringing, thereby avoiding, in many cases, the need to adapt existing filtering algorithms to HDR data. The coding method presented in this paper is included in the JPEG XT (ISO/IEC 18477) standard as Part-7, Profile-B.","1068-0314","978-1-5090-1853-6978-1-5090-1854","10.1109/DCC.2016.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7786256","hdr;coding;jpeg;jpeg xt;high dynamic range;video;compression","Transform coding;Encoding;Dynamic range;Signal to noise ratio;Image coding;Standards","filtering theory;video codecs;video coding","high dynamic range images;LDR image;video codec;HDR-video creation;LDR image filter;filtering algorithm","","","","2","","","","","IEEE","IEEE Conferences"
"A Low Complexity Loglikelihood Ratio Scheme in Coded Multiple-Input Multiple-Output Systems","X. Qiang; Z. Xudong; H. Yanhui","NA; NA; NA","2013 Ninth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","","2013","","","627","629","The higher data rate in coded MIMO systems needs higher performance and lower computational complexity. This paper proposes a method to improve the performance and reduce the computational complexity in coded MIMO system, such as IEEE 802.11ac. The proposed scheme based on log likelihood ratio with QRM-MLD (QRM-MLD-LLR) needs to expand the estimated channel matrix H by MMSE scheme firstly, and use the partial constellations set S to replace the all constellations set C to reduce the computational complexity by the MMSE-SIC method secondly. Simulation results show that the proposed scheme with expanding H by MMSE can obtain better performance than non-expanding H, the proposed scheme can reduce more than 70% computational complexity but keep only 0.5dB performance loss.","","978-0-7695-5120","10.1109/IIH-MSP.2013.161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846717","QRM-MLD LLR MMSE-SIC MIMO","MIMO;Computational complexity;Channel estimation;Measurement;Receiving antennas;Simulation","channel coding;channel estimation;computational complexity;interference suppression;least mean squares methods;matrix algebra;maximum likelihood detection;MIMO communication;radiofrequency interference","low complexity log likelihood ratio scheme;coded multiple-input multiple-output system;coded MIMO system;computational complexity;IEEE 802.11ac standard;QRM-MLD-LLR;channel matrix estimation;partial constellations set;MMSE-SIC method;serial interference cancellation;maximum likelihood detection;loss 0.5 dB","","","","8","","","","","IEEE","IEEE Conferences"
"Reduced complexity detection for network-coded slotted ALOHA using sphere decoding","T. Ferrett; M. C. Valenti","West Virginia University, Morgantown, WV, USA; West Virginia University, Morgantown, WV, USA","2015 49th Asilomar Conference on Signals, Systems and Computers","","2015","","","1611","1615","Network-coded slotted ALOHA (NCSA) is a refinement to the classic slotted ALOHA protocol which improves throughput by enabling multiple source transmissions per ALOHA slot using physical-layer network coding (PNC). The receiver detects the network-coded combination of bits during every slot and recovers information bits by solving a system of linear equations. This work develops a receiver capable of detecting the network-coded combination of bits during a slot considering an arbitrary number of sources, orthogonal modulation, and a block fading channel. Maximum-likelihood detection of the network-coded symbol at the receiver becomes complex as the number of sources is increased. To reduce this complexity, sphere decoding is applied at the receiver to limit the number of constellation symbols the receiver must consider for detection. The system is simulated for two modulation orders and two through five sources, and error-rate performance results are provided.","1058-6393","978-1-4673-8576-3978-1-4673-8574-9978-1-4673-8575","10.1109/ACSSC.2015.7421420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421420","","Modulation;Receivers;Fading channels;Maximum likelihood decoding;Complexity theory;Throughput","access protocols;channel coding;computational complexity;decoding;fading channels;maximum likelihood detection;modulation coding;network coding;radio receivers;source coding","complexity detection reduction;sphere decoding;network-coded slotted ALOHA protocol;NCSA;physical-layer network coding;PNC;multiple source transmission;radio receiver;information bit recovery;linear equation;orthogonal modulation;block fading channel;network-coded symbol maximum-likelihood detection;constellation symbol;error-rate performance","","","","9","","","","","IEEE","IEEE Conferences"
"A FPGA Implementation of Hard Systematic Error Correcting codes based Matching of Data Encoded Architecture with Low-Complexity, Low-Latency","S. B. Shirol; R. S; P. Kumar; R. B. Shetta","Dept. of Electronics & Communication Engineering, B V Bhoomraddi College of Engineering & Technology, Hubballi, India; Dept. of Electronics & Communication Engineering, B V Bhoomraddi College of Engineering & Technology, Hubballi, India; Dept. of Electronics & Communication Engineering, B V Bhoomraddi College of Engineering & Technology, Hubballi, India; Dept. of Electronics & Communication Engineering, B V Bhoomraddi College of Engineering & Technology, Hubballi, India","2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)","","2018","","","1695","1699","Nowadays to get error free data is toughest task. To verify whether the data is error free or not and decrease overall area and latency the new architecture has been designed which matches the data saved in the system and incoming data using Error Correcting Code (ECC).ECC basically consist of two parts parity part and raw data which are generated by encoder. In this method the matching of the data is made parallel so that we can further decrease the area and one more method has been designed called Buttery weight accumulator which correctly calculates the hamming distance. To still reduce the area a new technique has introduced in which we have used the modified XOR gate. This has been implemented using Xilinx Simulator and Spartan 6 FPGA board.","","978-1-5386-0965-1978-1-5386-0966","10.1109/ICECA.2018.8474614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8474614","Error Correcting Code (ECC);Saturated Adder (SA);Butterfly Weight Accumulator (BWA)","Logic gates;Error correction codes;Computer architecture;Conferences;Hamming distance;Delays;Systematics","error correction codes;field programmable gate arrays;logic design","Buttery weight accumulator;Hamming distance;Xilinx Simulator;XOR gate;hard systematic error correcting codes;Spartan 6 FPGA board;encoder","","","","14","","","","","IEEE","IEEE Conferences"
"Linear precoding for MIMO with LDPC coding and reduced receiver complexity","T. Ketseoglou; E. Ayanoglu","ECE California State University, Pomona, California; CPCC, EECS University of California, Irvine, California","2013 Asilomar Conference on Signals, Systems and Computers","","2013","","","2067","2071","In this paper, the problem of designing a linear precoder for Multiple-Input Multiple-Output (MIMO) systems employing Low-Density Parity-Check (LDPC) codes is addressed under the constraint of minimizing the dependence between the system's receiving branches, thus reducing the relevant transmitter and receiver complexities. Our approach constitutes an interesting generalization of Bit-Interleaved Coded Modulation with Multiple Beamforming (BICMB) which has shown many benefits in MIMO systems. We start with a Pareto optimal surface modeling of the system and show the difficulty involved in the corresponding optimization problem. We then propose an alternative, practical technique, called per-Group Precoding (PGP), which groups together multiple input symbol streams and corresponding receiving branches, and thus results in independent transmitting/receiving streams between groups. We show with simulated results that PGP offers almost optimal performance, albeit with significant reduction both in the precoder optimization, and LDPC EXIT chart based decoding complexities.","1058-6393","978-1-4799-2390-8978-1-4799-2388","10.1109/ACSSC.2013.6810670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6810670","","MIMO;Complexity theory;Optimization;Receivers;Parity check codes;Mutual information;Signal to noise ratio","computational complexity;interleaved codes;MIMO communication;parity check codes;precoding","linear precoding;LDPC coding;reduced receiver complexity;MIMO system;multiple-input multiple-output system;low-density parity-check codes;bit-interleaved coded modulation with multiple beamforming;BICMB;Pareto optimal surface modeling;optimization problem;per-group precoding;multiple input symbol streams;independent transmitting-receiving streams","","1","","9","","","","","IEEE","IEEE Conferences"
"Low Complexity Wiener Filtering in CDMA Systems Using a Class of Pseudo-Noise Spreading Codes","R. Carvajal; K. Mahata; J. C. Aguero","School of Electrical Engineering and Computer Science, The University of Newcastle, Callaghan, NSW, 2308, Australia; School of Electrical Engineering and Computer Science, The University of Newcastle, Callaghan, NSW, 2308, Australia; School of Electrical Engineering and Computer Science, The University of Newcastle, Callaghan, NSW, 2308, Australia","IEEE Communications Letters","","2012","16","9","1357","1360","Code division multiple access (CDMA) has become a widely adopted technology for wireless communications, particularly in mobile third generation systems (3G) and global positional system (GPS). In CDMA, a particular class of pseudonoise (PN) spreading codes yields a code-set Grammian with only two distinct eigenvalues. For these spreading codes, we propose a computationally efficient method for signal detection using Wiener filtering. Our approach relies on a matrix inversion step, where we gain in computational efficiency by using the Sherman-Morrison-Woodbury formula. The resulting approach involves significantly less computational load compared to popular approaches like Conjugate Gradient (CG) method and the (traditional) Cholesky decomposition.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2012.070512.121051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6235080","CDMA;Wiener filter;m-sequences;Gold Codes;Kasami sequences","Multiaccess communication;Gold;Complexity theory;Mobile communication;Global Positioning System;Vectors;Eigenvalues and eigenfunctions","3G mobile communication;code division multiple access;codes;Global Positioning System;signal detection;Wiener filters","low complexity Wiener filtering;CDMA systems;pseudo-noise spreading codes;code division multiple access;wireless communications;mobile third generation systems;global positional system;code-set Grammian;computationally efficient method;signal detection;Sherman-Morrison-Woodbury formula;conjugate gradient method;Cholesky decomposition","","2","","18","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity and High Performance SISO Decoding for Block Product Turbo Code (105,44)","J. H. Lim; J. H. Lee; M. S. Shin; G. H. Cho; Y. J. Song","NA; NA; NA; NA; NA","2015 8th International Conference on Control and Automation (CA)","","2015","","","13","16","Benjamin's low complexiry sof input soft output (SISO) decoding algorithm is simple in the configuration and but shows less significant bit error rate (BER) performance in an additive white Gaussian noise (AWGN). To solve this problem, we propose a new algorithm which is based on the syndrome check with an alpha value into the decoder to enhance BER performance. We also confirm the performance of the proposed method using the block product turbo code (105,44).","","978-1-5090-0397-6978-1-4673-9857","10.1109/CA.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437285","SISO;iterative decoding;block product turbo code;alpha value","","block codes;decoding;error statistics;turbo codes","SISO decoding;block product turbo code;Benjamin's low complexiry sof input soft output decoding algorithm;bit error rate;BER;additive white Gaussian noise;AWGN;decoder","","","","4","","","","","IEEE","IEEE Conferences"
"Re-engineering ITU-T G.722: Low delay and complexity superwideband coding at 64 kbit/s with G.722 bitstream watermarking","B. Kovesi; S. Ragot; C. Lamblin; L. Miao; Z. Liu; C. Hu","France Telecom Orange, France; France Telecom Orange, France; France Telecom Orange, France; France Telecom Orange, France; Huawei Technologies, China; Huawei Technologies, China","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2011","","","5248","5251","This paper presents the lowest bitrate mode (64 kbit/s) of the new superwideband (SWB, 50-14000 Hz) coder, recently standardized as ITU-T G.722 Annex B. This mode provides a superwideband extension of G.722 at 56 kbit/s with one 8 kbit/s enhancement layer divided in two sub-layers. The resulting bitstream is compatible with ITU-T G.722 at 64 kbit/s and can be viewed as watermarking G.722 least significant bits (LSBs). The novel technologies in this mode include G.722 enhancements (noise feedback coding, scalable quantization in G.722 higher band), as well as multimode bandwidth extension (BWE). Selected ITU-T characterization test results and additional informal test results show that the 64 kbit/s mode of G.722 Annex B gives high SWB quality with low delay and complexity.","2379-190X;1520-6149;1520-6149","978-1-4577-0539-7978-1-4577-0538-0978-1-4577-0537","10.1109/ICASSP.2011.5947541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947541","Speech coding;audio coding;G.722B;superwideband;bandwidth extension;watermarking","Decision support systems","speech coding;watermarking","reengineering ITU-T G.722;complexity superwideband coding;G.722 bitstream watermarking;multimode bandwidth extension;BWE;high SWB quality;bit rate 64 kbit/s;frequency 50 Hz to 14000 Hz;bit rate 56 kbit/s;bit rate 8 kbit/s","","2","","13","","","","","IEEE","IEEE Conferences"
"Low-Complexity List Decoding of Reed-Solomon Coded Pulse Position Modulation","V. Sidorenko; R. Fischer","Institute of Communications Engineering, Ulm University, Germany; Institute of Communications Engineering, Ulm University, Germany","SCC 2013; 9th International ITG Conference on Systems, Communication and Coding","","2013","","","1","6","Reed-Solomon coded pulse position modulation - also known as the Kautz-Singleton code - found many applications in multiple access communications, in nonadaptive group testing, and in pooling designs in DNA mapping. Usually an exhaustive search was used to decode these codes which results in an exponential complexity in the code dimension. In this paper, a list decoding algorithm for the Kautz-Singleton codes based on the Guruswami-Sudan and Koetter-Vardy list decoding algorithms is presented. The analysis reveals that the algorithm has only polynomial complexity and is hence very well suited for low-complexity applications as they are desired, e.g., in ultrawideband communications.","","978-3-8007-3482","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468986","Ultra-Wideband Communications;Pulse Position Modulation;Reed-Solomon Codes;List Decoding;Disjunctive Channel","Decoding;Polynomials;Complexity theory;Vectors;Receivers;Reed-Solomon codes;Interpolation","","","","1","","","","","","","VDE","VDE Conferences"
"Temporal signal energy correction and low-complexity encoder feedback for lossy scalable video coding","M. J. H. Loomans; C. J. Koeleman; P. H. N. de With","VDG Security BV, Radonstraat 10-14, 2718 TA, Zoetermeer, NL; VDG Security BV, Radonstraat 10-14, 2718 TA, Zoetermeer, NL; Eindhoven University of Technology, Den Dolech 2, 5612 AZ, Eindhoven, NL","28th Picture Coding Symposium","","2010","","","394","397","In this paper, we address two problems found in embedded implementations of Scalable Video Codecs (SVCs): the temporal signal energy distribution and frame-to-frame quality fluctuations. The unequal energy distribution between the low- and high-pass band with integer-based wavelets leads to sub-optimal rate-distortion choices coupled with quantization-error accumulations. The second problem is the quality fluctuation between frames within a Group Of Pictures (GOP). To solve these two problems, we present two modifications to the SVC. The first modification aims at a temporal energy correction of the lifting scheme in the temporal wavelet decomposition. By moving this energy correction to the leaves of the temporal tree, we can save on required memory size, bandwidth and computations, while reducing floating/fixed-point conversion errors. The second modification feeds back the decoded first frame of the GOP (the temporal low-pass) into the temporal coding chain. The decoding of the first frame is achieved without entropy decoding while avoiding any required modifications at the decoder. Experiments show that quality fluctuations within the GOP are significantly reduced, thereby significantly increasing the subjective visual quality. On top of this, a small quality improvement is achieved on average.","","978-1-4244-7135-5978-1-4244-7134-8978-1-4244-7133","10.1109/PCS.2010.5702518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702518","Scalable Video Coding;Wavelets;Real-time Systems;Embedded Systems","Encoding;Decoding;Static VAr compensators;Codecs;Complexity theory;Discrete wavelet transforms;Cities and towns","computational complexity;video codecs;video coding;wavelet transforms","temporal signal energy correction;low-complexity encoder feedback;lossy scalable video coding;SVC;sub-optimal rate-distortion;integer-based wavelets;quantization-error accumulations;temporal wavelet decomposition;GOP;group of pictures;decoder","","","","8","","","","","IEEE","IEEE Conferences"
"Low complexity decoding of Reed–Solomon codes over magnetic recording channels","Z. Fang; Z. Ma; G. K. Karagiannidis; M. Xiao; P. Fan","Southwest Jiaotong University, People's Republic of China; Southwest Jiaotong University, People's Republic of China; Aristotle University of Thessaloniki, Greece; KTH Royal Institute of Technology, Sweden; Southwest Jiaotong University, People's Republic of China","Electronics Letters","","2019","55","3","159","161","A low complexity soft-decision decoding technique is presented for Reed-Solomon codes, over magnetic recording channels (MRC), by considering symbol and bit reliability, jointly. A symbol detection scheme for burst error scenarios has been put forward for the iterative error-and-erasure decoding algorithm (IEED), which is based on multiple trials, and is a combination of the generalised minimum distance and Chase-2 decoding schemes. In order to decrease complexity, the decoding test patterns are used in a certain order, while the stopping criteria is also discussed. Simulations show that the proposed algorithms can achieve almost the same performance as that of IEED for the additive white Gaussian noise channel and slightly better performance in MRC, but with significantly less complexity.","0013-5194","","10.1049/el.2018.6059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634615","","","AWGN channels;channel coding;decoding;error correction codes;error statistics;Reed-Solomon codes","additive white Gaussian noise channel;MRC;Reed-Solomon codes;magnetic recording channels;low complexity soft-decision decoding technique;symbol detection scheme;IEED;Chase-2 decoding schemes;iterative error-and-erasure decoding algorithm","","","","","","","","","IET","IET Journals & Magazines"
"Chinese coding type identification based on Kolmogorov complexity theory","G. He; N. Zhu; X. Wu; Q. Xu","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China","2010 2nd IEEE InternationalConference on Network Infrastructure and Digital Content","","2010","","","293","297","Identification of Chinese coding type is a major and challenging issue in Chinese web content audit and analysis. In this paper we develop a novel algorithm based on the theory of Kolmogorov complexity to identify the coding type of Chinese characters of a given text segment. An array of text compressors are used as filters to evaluate the information distance of text under examination and the training corpus coded in different coding type. The information distance can be used to decide the coding type according to the Kolmogorov theory. In this paper a particular compressing algorithm is used to minimize computing complexity by separating coding book training stage and compressing stage. Finally, we present the experimental results through which the accuracy and performance of the algorithm is confirmed. The result also proves that this algorithm is especially efficient when short text segment is under examination comparing with the n-gram algorithms.","2374-0272","978-1-4244-6853-9978-1-4244-6851-5978-1-4244-6852","10.1109/ICNIDC.2010.5657789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5657789","Chinese encoding identification;Kolmogorov complexity;information distance;text compression","Encoding;Books;Complexity theory;Training;Accuracy;Algorithm design and analysis;Grippers","data compression;encoding;text analysis","Chinese coding type identification;Kolmogorov complexity theory;Chinese web content audit;Chinese characters;text compressors;text segment;n-gram algorithms;information distance","","","","12","","","","","IEEE","IEEE Conferences"
"A decoding algorithm with reduced complexity for non-binary LDPC codes over large fields","J. Lin; Z. Yan","Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015; Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","1688","1691","Non-binary low-density parity-check (NB-LDPC) codes outperform their binary counterparts in some cases, but their high decoding complexity is a significant hurdle to their applications. In this paper, we propose a decoding algorithm with reduced computational complexities and smaller memory requirements for NB-LDPC codes over large fields. First, a simplified algorithm is proposed to reduce the computational complexity of variable node processing. To reduce memory requirements, existing NB-LDPC decoders often truncate the message vectors to a limited number n<sub>m</sub> of values. However, the memory requirements of these decoders remain high when the field size is large, since n<sub>m</sub> needs to be large enough to alleviate error performance degradation. In this paper, an improved trellised based check node processing algorithm is proposed to significantly reduce the memory requirement. The number of elements in a variable-to-check message is reduced to n<sub>v</sub> (n<sub>v</sub> <; n<sub>m</sub>). The sorted log likelihood ratio (LLR) vector of a check-to-variable message is approximated using a piece-wise linear function. Thus, only few LLRs are stored and other LLRs are computed on-the-fly when needed. For each a priori message, most LLRs are approximated with a linear function. Our numerical results demonstrate that the proposed decoding algorithm outperforms existing algorithms. Two LLR generation units (LGUs) are proposed to compute LLR vectors for check-to-variable messages, and the two LGUs require only a fraction of the area needed to store n<sub>m</sub> LLRs.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6572189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572189","","Decoding;Approximation algorithms;Vectors;Complexity theory;Approximation methods;Parity check codes;Random access memory","computational complexity;decoding;parity check codes","decoding algorithm;reduced complexity;nonbinary LDPC codes;nonbinary low-density parity-check codes;decoding complexity;computational complexities;NB-LDPC codes;NB-LDPC decoders;variable node processing;error performance degradation;variable-to-check message;log likelihood ratio;check-to-variable message;linear function;LLR generation units","","1","","8","","","","","IEEE","IEEE Conferences"
"A low complexity joint compression-error detection-cryptography based on arithmetic coding","M. Sinaie; V. TabaTaba Vakili","Department of Electrical Engineering, Iran University of science and Technology (IUST), Islamic, Republic of Iran; Department of Electrical Engineering, Iran University of science and Technology (IUST), Islamic, Republic of Iran","10th International Conference on Information Science, Signal Processing and their Applications (ISSPA 2010)","","2010","","","233","236","This paper proposes a new joint source-cryptographic-channel coding (JSCC) based on arithmetic coding which is used basically for compression. The proposed method uses arithmetic coding (AC) with forbidden symbol for channel coding as an error detection codes. In this method, we use random length forbidden symbol and placing this dummy symbol in random places to provide security. The proposed technique provides secure codes; in addition, added redundancy is half of redundancy which is added by arithmetic coding with fixed forbidden symbol length. Our proposed technique is less complicated than cascaded source-channel coding and encryption. Moreover, our coder provides a flexible switch between a standard compression model and joint model.","","978-1-4244-7167-6978-1-4244-7165-2978-1-4244-7166","10.1109/ISSPA.2010.5605483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5605483","","Cryptography;Modems","arithmetic codes;channel coding;cryptography;error detection codes;source coding","low complexity joint compression-error detection-cryptography;arithmetic coding;source-cryptographic-channel coding;random length forbidden symbol;cascaded source-channel coding;encryption","","1","","9","","","","","IEEE","IEEE Conferences"
"A low-complexity IRA-LDPC coded noncoherent unitary space-time modulation system on Rayleigh flat fading channel","Li Peng; Lingling Yang; Qiuping Peng","Wuhan National Laboratory For Optoelectronics, Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China; Wuhan National Laboratory For Optoelectronics, Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China; Wuhan National Laboratory For Optoelectronics, Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China","2011 Second International Conference on Mechanic Automation and Control Engineering","","2011","","","7115","7118","This letter constructs a practical low- complexity coded modulation MIMO system which concatenates the IRA-LDPC codes based on the permutation Q-matrix with the noncoherent unitary space-time modulation (USTM) based on sine-cosine function for Rayleigh flat fading channel. In the receiver, we adopt dual detecting scheme: first relative coarse low complexity ML detect and then relative precise MAP detect with shrinking search range of constellation points. Combining the dual detecting scheme with the Belief-Propagation (BP) iterative decoding algorithm comes into being an iterative feedback demodulate-decoding system. The IRA-LDPC coded modulation system can obtain about 15 dB coding gain at 10<sup>-5</sup> BER over the uncoded USTM system. Some results are based on simulation testing.","","978-1-4244-9439-2978-1-4244-9436-1978-1-4244-9438","10.1109/MACE.2011.5988690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5988690","Unitary space-time modulation (USTM);coded modulation system;iterative feedback demodulate-decoding low-density parity-check (LDPC) codes","Signal to noise ratio;Bit error rate;Iterative decoding;Complexity theory;Demodulation","iterative decoding;matrix algebra;MIMO communication;parity check codes;radio receivers;Rayleigh channels;space-time codes","noncoherent unitary space-time modulation system;Rayleigh flat fading channel;MIMO system;IRA-LDPC codes;permutation Q-matrix;USTM;sine-cosine function;receiver;belief-propagation;iterative decoding","","","","8","","","","","IEEE","IEEE Conferences"
"Analysis and complexity reduction of high efficiency video coding for low-delay communication","J. Kim; J. Kim; K. Yoo; K. Lee","DMC R&amp;D Center, Samsung Electronics, Suwon, Republic of Korea; DMC R&amp;D Center, Samsung Electronics, Suwon, Republic of Korea; DMC R&amp;D Center, Samsung Electronics, Suwon, Republic of Korea; DMC R&amp;D Center, Samsung Electronics, Suwon, Republic of Korea","2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin)","","2012","","","11","12","Bi-predictive motion compensation is an important coding tool in low-delay coding of HEVC. The proposed method reduces computational complexity about 30% by removing exhaustive bi-predictive motion estimation. The coding gain is about 6% in low-delay P coding.","2166-6822;2166-6814;2166-6814","978-1-4673-1547-0978-1-4673-1546-3978-1-4673-1545","10.1109/ICCE-Berlin.2012.6336484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6336484","HEVC;low-delay communication;low-complexity video coding","Encoding;Vectors;Video coding;Computational complexity;Motion compensation;Interpolation;Bandwidth","computational complexity;motion compensation;motion estimation;video coding","high efficiency video coding;low-delay communication;complexity reduction;bi-predictive motion compensation;low-delay coding;computational complexity;bi-predictive motion estimation","","2","","9","","","","","IEEE","IEEE Conferences"
"Reduce the Complexity of List Decoding of Polar Codes by Tree-Pruning","K. Chen; B. Li; H. Shen; J. Jin; D. Tse","Communications Technology Research Laboratory, Huawei Technologies, Shenzhen, China; Communications Technology Research Laboratory, Huawei Technologies, Shenzhen, China; Communications Technology Research Laboratory, Huawei Technologies, Shenzhen, China; Communications Technology Research Laboratory, Huawei Technologies, Shenzhen, China; Department of Electrical Engineering, Stanford University, CA, USA","IEEE Communications Letters","","2016","20","2","204","207","Polar codes under cyclic redundancy check-aided successive cancellation list (CA-SCL) decoding can outperform the turbo codes and the LDPC codes when code lengths are configured to be several kilobits. In order to reduce the decoding complexity, a novel tree-pruning scheme for the SCL/CA-SCL decoding algorithms is proposed in this letter. In each step of the decoding procedure, the candidate paths with metrics less than a threshold are dropped directly to avoid the unnecessary computations for the path searching on the descendant branches of them. Given a candidate path, an upper bound of the path metric of its descendants is proposed to determined how much pruning this candidate path would affect frame error rate (FER) performance. By utilizing this upper bounding technique and introducing a dynamic threshold, the proposed scheme deletes as many as possible of the redundant candidate paths while keeping the performance deterioration in a tolerant region, thus it is much more efficient than the existing pruning scheme. With only a negligible loss of FER performance, the computational complexity of the proposed pruned decoding scheme is only about 40% of the standard algorithm in the low signal-to-noise ratio (SNR) region (where the FER under CA-SCL decoding is about 0.1 ~ 0.001), and it can be very close to that of the successive cancellation (SC) decoder in the moderate- and high-SNR regions.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2506568","Science and Technology Project of Shenzhen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348682","Polar codes;successive cancellation decoding;tree-pruning;Polar codes;successive cancellation decoding;tree-pruning","Decoding;Measurement;Signal to noise ratio;Upper bound;Indexes;Complexity theory;Sorting","codes;computational complexity;decoding;error statistics;trees (mathematics)","list decoding;polar codes;cyclic redundancy check-aided successive cancellation list decoding;SCL CA-SCL decoding algorithms;novel tree-pruning scheme;path searching;frame error rate;upper bounding technique;computational complexity;FER performance","","6","","8","","","","","IEEE","IEEE Journals & Magazines"
"Complexity-aware adaptive bit-rate control with dynamic ROI pre-processing for scalable video coding","D. Grois; O. Hadar","Comm. Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Comm. Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel","2011 IEEE International Conference on Multimedia and Expo","","2011","","","1","4","We present a novel efficient complexity-aware adaptive bit rate control with dynamic Region-of-Interest pre-processing (pre-filtering) for the Scalable Video Coding (SVC), which is an extension of H.264/AVC. According to the proposed approach, we adaptively vary various parameters of the SVC pre-filters, such as standard deviations and a number of filters for the dynamic pre-processing of a transition region between the ROI and background, thereby enabling to dynamically adjust the desired SVC settings. Our adaptive bit-rate control is based on an SVC computational complexity-rate-distortion (C R-D) analysis, thereby adding a complexity dimension to the conventional Region-of-Interest SVC rate-distortion analysis. As a result, the ROI SVC visual presentation quality is significantly improved, which can be especially useful for various resource-limited devices, such as mobile devices. The performance of the presented adaptive ROI SVC pre processing scheme is evaluated and tested in detail, further comparing it to the Joint Scalable Video Model reference software (JSVM 9.19) and demonstrating significant improvements both in quality and bit-rate.","1945-788X;1945-7871;1945-7871","978-1-61284-350-6978-1-61284-348-3978-1-61284-349","10.1109/ICME.2011.6012110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012110","Regions-of-interest video coding;pre-processing/pre-filtering;bit-rate control;image/video coding;Regions-of-interest scalability;high-quality visual presentation;Scalable Video Coding;H.264/AVC","Static VAr compensators;Video coding;Computational complexity;Encoding;Video sequences;Scalability;Visualization","computational complexity;filtering theory;rate distortion theory;video coding","adaptive bit-rate control;dynamic region-of-interest pre-processing;scalable video coding;H.264/AVC;transition region;computational complexity;rate-distortion analysis","","8","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity wavelet-based image/video coding for home-use and remote surveillance","M. J. H. Loomans; C. J. Koeleman; K. M. J. Joosen; P. H. N. de With","VDG Security BV, Radonstraat 10-14, 2718 TA, Zoetermeer, NL; VDG Security BV, Radonstraat 10-14, 2718 TA, Zoetermeer, NL; VDG Security BV, Radonstraat 10-14, 2718 TA, Zoetermeer, NL; Eindhoven University of Technology, Den Dolech 2, 5612 AZ, Eindhoven, NL","2011 IEEE International Conference on Consumer Electronics (ICCE)","","2011","","","641","642","The availability of inexpensive cameras enables alternative applications beyond personal video communication. For example, surveillance of rooms and home premises is such an alternative application, which can be extended with remote viewing on hand-held battery-powered consumer devices. Scalable wavelet image/video coding is attractive for this application since the video can be scaled easily to many different formats including long-term viewing at low-power or even battery-operated devices. To this end, we present a modification to the SPECK wavelet image encoding algorithm which significantly improves efficient mapping on embedded systems. The key to our improvement is that the quadtree for bit assignment is stored in an efficient way such that the position of the first significant bit can be computed, so that later the code can be developed with minimal and direct memory access. This efficient storage is possible by structuring the data of the quadtree into nodes covering groups of pixels. Our approach gives a substantial reduction of the memory access and thus power consumption, which is essential for embedded applications and mobile devices.","2158-4001;2158-3994;2158-3994;2158-3994","978-1-4244-8712-7978-1-4244-8711-0978-1-4244-8709-7978-1-4244-8710","10.1109/ICCE.2011.5722785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722785","","Sorting;Pixel;Buffer storage;Image coding;Encoding;Image resolution;Codecs","cameras;computational complexity;image coding;quadtrees;surveillance;video coding","low-complexity wavelet-based image coding;home-use surveillance;remote surveillance;low-complexity wavelet-based video coding;personal video communication;cameras;hand-held battery-powered consumer devices;scalable wavelet video coding;scalable wavelet image coding;SPECK wavelet image encoding algorithm;embedded systems;quadtree;power consumption","","","","6","","","","","IEEE","IEEE Conferences"
"Low complexity PTS algorithm based on gray code and its FPGA implementation","J. Liu; Zhang Wei; Yuan Zhu; Ma Teng","University of Electronic Science and Technology of China, No. 2006, Xiyuan Road, West section of high-tech Zone, Chengdu 611731, China; University of Electronic Science and Technology of China, No. 2006, Xiyuan Road, West section of high-tech Zone, Chengdu 611731, China; University of Electronic Science and Technology of China, No. 2006, Xiyuan Road, West section of high-tech Zone, Chengdu 611731, China; University of Electronic Science and Technology of China, No. 2006, Xiyuan Road, West section of high-tech Zone, Chengdu 611731, China","IEEE 2011 10th International Conference on Electronic Measurement & Instruments","","2011","3","","208","211","This paper addresses the problem of peak to average power ratio (PAPR) reduction in multi-band transmitters by PTS approaches. In the conventional PTS approaches, the computational complexity is high and need to transmit side information, which is hard for hardware implementation. Aiming at the problems, a PTS algorithm which using gray code to reduce the complexity is proposed. The MATLAB simulation proved the algorithm is workable. On this basis, a PAPR suppression module is designed to be implemented on FPGA, and the results show that this module not only reduced the complexity of OFDM systems, but also worked well in PAPR suppression.","","978-1-4244-8161-3978-1-4244-8158-3978-1-4244-8160","10.1109/ICEMI.2011.6037889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6037889","orthogonal frequency division multiplexing (OFDM);peak to average power ratio(PAPR);partial transmit sequence (PTS);Gray Code","Peak to average power ratio;Partial transmit sequences;Computational complexity;Reflective binary codes;Field programmable gate arrays","computational complexity;field programmable gate arrays;Gray codes;OFDM modulation;radio transmitters","partial transmit sequence;PTS algorithm;gray code;FPGA implementation;peak-to-average-power ratio reduction;multiband transmitters;computational complexity;MATLAB simulation;OFDM systems","","2","","8","","","","","IEEE","IEEE Conferences"
"Complexity-constrained rate-distortion optimization for h.264/avc video coding","T. A. da Fonseca; R. L. de Queiroz","Universidade de Brasilia, Department of Electrical Engineering, DF, Brazil; Universidade de Brasilia, Department of Electrical Engineering, DF, Brazil","2011 IEEE International Symposium of Circuits and Systems (ISCAS)","","2011","","","2909","2912","In order to enable real-time software-based video encoding, in this work we optimized the prediction stage of an H.264 video encoder, in the complexity sense. Thus, besides looking for the coding options which lead to the best coded representation in terms of rate and distortion (RD), we constrain to a complexity (C) budget. We present a complexity optimized framework (RDC-optimized) which allows for real-time video compression and that does not make use of frame-skipping to comply to the desired encoding speed. We developed our framework around an open source software implementation of the H.264/AVC, the the x264 encoder. Results show that tight complexity control is attainable in practice, with very little loss in RD performance.","2158-1525;0271-4302;0271-4302","978-1-4244-9474-3978-1-4244-9473-6978-1-4244-9472","10.1109/ISCAS.2011.5938240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5938240","H.264;mode decision;complexity scalability;rate control;real-time video coding","Complexity theory;Encoding;Codecs;Streaming media;Bit rate;Video sequences;Real time systems","data compression;encoding;optimisation;public domain software;real-time systems;video coding","H.264/AVC video coding;complexity-constrained rate-distortion optimization;real-time software-based video encoding;H.264 video encoder;coded representation;complexity budget;complexity optimized framework;real-time video compression;open source software;x264 encoder","","3","","20","","","","","IEEE","IEEE Conferences"
"The reduction of linear receivers complexity in ovelapped Alamouti code family","M. Shahabinejad; S. S. H. Bidaki; S. Talebi; S. Abasi","Shahid Bahonar University of Kerman; Shahid Bahonar University of Kerman; Shahid Bahonar University of Kerman; Shahid Bahonar University of Kerman","2011 19th Iranian Conference on Electrical Engineering","","2011","","","1","1","Linear receivers, due to their simpler decoding in comparison with maximum likelihood (ML) receiver, are of great importance. In recent years, the codes such as Toeplitz codes, overlapped Alamouti codes (OACs), and group orthogonal Toeplitz codes (GOTCs) are designed based on the linear receivers decoding. This paper completely explains the zero-forcing (ZF) and the minimum mean square error (MMSE) receivers of the OACs for different numbers of transmit antennas and different numbers of used symbols in each space-time matrix code. It also proposes a method that decreases the complexity of the ZF and the MMSE linear receivers of the OACs without sacrificing the system performance. It is explained that the proposed simplicity method especially gets very effective by increasing the symbol transmission rate.","2164-7054","978-964-463-428-4978-1-4577-0730","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5955764","multi-input multi-output (MIMO);linear receivers;space-time coding;overlapped Alamouti code;zero-forcing (ZF);minimum mean square error (MMSE)","","communication complexity;least mean squares methods;maximum likelihood decoding;receivers;space-time codes","linear receiver complexity reduction;ovelapped alamouti code family;maximum likelihood receiver;linear receiver decoding;minimum mean square error receiver;MMSE receiver;symbol transmission rate;group orthogonal Toeplitz code;space-time matrix code","","","","","","","","","IEEE","IEEE Conferences"
"A Low-complexity Video Coding Scheme Based on Compressive Sensing","Y. Hou; F. Liu","NA; NA","2011 Fourth International Symposium on Computational Intelligence and Design","","2011","2","","326","329","Traditional video coding method is able to achieve wonderful performance in data compressing. However, it has high complexity, which is not suitable for some environments where low complexity coding is needed. A new method of video coding which is based on compressive sensing is proposed. In this system, the sparsity of residual of successive frames is exploited, which is a crucial requirement in CS theory. This method encodes video frames in a fast and low-complexity way, while the compressive rate of data remains low. The reconstruction of original video frame, as well as the PSNR-rate curve under this framework, is given.","","978-1-4577-1085","10.1109/ISCID.2011.184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079803","compressive sensing;video coding;sparsity","Image reconstruction;Image coding;Video coding;Decoding;Complexity theory;Compressed sensing;Encoding","computational complexity;data compression;video coding","low complexity video coding scheme;compressive sensing;data compression;complexity coding;CS theory;compressive rate;video frame;PSNR rate curve","","2","","9","","","","","IEEE","IEEE Conferences"
"A low complexity encoder construction for systematic quasi-cyclic LDPC codes","Y. Genga; O. Ogundile; O. Oyerinde; J. Versfeld","School of Electrical &amp; Information Engineering, University of the Witwatersrand, Private Bag 3, 2050, Johannesburg, South Africa; School of Electrical &amp; Information Engineering, University of the Witwatersrand, Private Bag 3, 2050, Johannesburg, South Africa; School of Electrical &amp; Information Engineering, University of the Witwatersrand, Private Bag 3, 2050, Johannesburg, South Africa; Department of Electrical and Electronic Engineering, Stellenbosch University, Private Bag X1, 7602, Matieland, South Africa","2017 IEEE AFRICON","","2017","","","167","170","For practical applications in forward error correction, the importance of a systematic codeword cannot be overemphasized. Thus, this paper proposes the construction of a systematic quasi-cyclic (QC) LDPC code. This systematic structure is achieved by a row reduction technique different from the conventional Gaussian elimination method. This row reduction technique has the advantage of being easier to implement when compared to Gaussian row reduction techniques. The proposed row reduction technique maintains the quasi cyclic structure and the sparsity of the QC-LDPC parity check matrix while providing a low complexity approach to the construction of the generator matrix. In addition, the proposed construction exhibits a good BER performance for high rate codes.","2153-0033","978-1-5386-2775-4978-1-5386-2774-7978-1-5386-2776","10.1109/AFRCON.2017.8095475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8095475","Encoding Complexity;LDPC codes;quasi-cyclic;systematic structure;row reduction","Parity check codes;Systematics;Sparse matrices;Encoding;Complexity theory;Generators;Decoding","cyclic codes;error statistics;forward error correction;Gaussian processes;matrix algebra;parity check codes","forward error correction;systematic codeword;quasicyclic LDPC code;systematic structure;row reduction technique;systematic quasicyclic LDPC codes;BER performance;low complexity encoder construction;low complexity approach;QC-LDPC parity check matrix;quasicyclic structure;Gaussian row reduction techniques;conventional Gaussian elimination method","","","","16","","","","","IEEE","IEEE Conferences"
"Adaptive search window for low complexity surveillance video coding","Mohd Azraf Mohd Razif; M. Mokji; M. M. A. Zabidi","Electronics and Computer Engineering Department, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Johor, Malaysia; Electronics and Computer Engineering Department, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Johor, Malaysia; Electronics and Computer Engineering Department, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Johor, Malaysia","2015 International Symposium on Technology Management and Emerging Technologies (ISTMET)","","2015","","","360","363","Static video surveillance is an effective tool for maintaining security. Videos from stationary cameras contain mainly background, with movement appearing rarely. Standard video coders record everything in the video frames at the same quality, including the background which contains no information of interest. Knowledge of the application could improve the video compression efficiency. Standard video encoders takes a lot of computation power, with Motion Estimation (ME) taking around 60-80% of the encoder compute budget. Encoder computation cost can be reduced by encoding the background region at lower quality. This paper proposes the use of adaptive search window to reduce the search range to achieve the objective of lowering computation complexity.","","978-1-4799-1723-5978-1-4799-1722","10.1109/ISTMET.2015.7359059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359059","static surveillance video;ROI;video compression;H.264;background subtraction;early termination;search window","Surveillance;Standards;Motion estimation;Cameras;Image coding;Complexity theory;Encoding","computational complexity;search problems;video coding;video surveillance","search range reduction;adaptive search window;background region encoding;encoder computation cost reduction;ME;motion estimation;standard video encoders;video compression efficiency improvement;video frames;stationary cameras;static video surveillance;low-complexity surveillance video coding","","","","27","","","","","IEEE","IEEE Conferences"
"Reduced-complexity window decoding of spatially coupled LDPC codes for magnetic recording systems.","S. Khittiwitchayakul; W. Phakphisut; P. Supnithi","Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Ladkrabang, Thailand; Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Ladkrabang, Thailand; Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Ladkrabang, Thailand","2018 IEEE International Magnetics Conference (INTERMAG)","","2018","","","1","1","In channel coding theory, the performance of error correcting codes (ECCs) approaching the Shannon limit can be achieved through increasing code lengths. Unfortunately, the complexity of ECCs will be increased as the code length increases. Nowadays, the magnetic recording (MR) system takes advantage of powerful ECCs by using 4 Kbytes sector. Among the advanced ECCs, the spatially coupled LDPC (SC-LDPC) codes (also known as a LDPC convolutional code) [1]are shown to have the decoding latency and complexity lower than those of the underlying LDPC block codes (LDPC-BC). Moreover, the SC-LDPC codes with threshold decoding outperform the LDPC-BC codes [2]. Hence, the SC-LDPC codes are the strong candidate for the future MR systems, when the sector size is increased beyond 4 Kbytes. An SC-LDPC decoder can use sliding window decoding [3]whereby the received signals are decoded by sliding window along the bit sequence. The window decoder is called “uniform window decoding (U-WD),” when all variable nodes (VNs) within a window are updated. In order to reduce the complexity of window decoding, some researchers proposed the non-uniform window decoding (N-WD) [4], which do not update the VNs with no improvement in the bit error rate (BER). This approach provides about 35-50% reduction in complexity compared to U-WD. In this work, we consider the application of SC-LDPC codes in MR systems, whereby SC-LDPC decoder cooperates with BCJR detector to encounter inter-symbol interference (ISI). We propose the dynamic shifting of window decoding (DS-WD) to reduce the complexity of SC-LDPC codes. Herein, the number of shifted bits is defined according to their soft BERs which are estimated at each decoding position. In addition, we modify the N-WD [4]to reinforce our proposed algorithm called “dynamic-shifting non-uniform window decoding (DS-N-WD).” The DS-WD and DS-N-WD achieve the complexity reduction of 7% and 25% without any loss in performance compared to the N-WD algorithms.","2150-4601;2150-4598","978-1-5386-6425-4978-1-5386-6426","10.1109/INTMAG.2018.8508405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8508405","","Magnetics;Conferences","block codes;channel coding;convolutional codes;decoding;error correction codes;intersymbol interference;magnetic recording;parity check codes","SC-LDPC codes;nonuniform window decoding;reduced-complexity window decoding;spatially coupled LDPC codes;channel coding theory;error correcting codes;code length;LDPC convolutional code;threshold decoding;LDPC-BC codes;sliding window;window decoder;uniform window decoding;SC-LDPC decoder;LDPC block codes;variable nodes;MR systems;bit error rate;dynamic-shifting nonuniform window decoding;magnetic recording systems;memory size 4.0 KByte","","","","6","","","","","IEEE","IEEE Conferences"
"Adaptive H.264 video coding for low-complexity decoding in packet-loss environment","Kun Zhao; Yuan Zhang; Yuanhang Guo","Information Engineering School, Communication University of China, Beijing, China; Information Engineering School, Communication University of China, Beijing, China; Information Engineering School, Communication University of China, Beijing, China","2010 Second Pacific-Asia Conference on Circuits, Communications and System","","2010","1","","203","206","The state-of-the-art H.264 video coding standard enables significant improvement of compression performance compared with previous standards; however, the computational complexity at both encoder and decoder are inevitably increased. In this paper, we propose an adaptive H.264 video encoding scheme to reduce the decoding complexity based on an end-to-end distortion model, especially considering its transmission over wireless networks. In order to yield the good rate-distortion performance under power constraint in packet loss environment, we generate the lightweight bit-stream at encoder side, which can be decoded with less interpolation operations. Moreover, channel distortion caused by the instability of network is considered in the proposed model. The experiment results show that the proposed method performs well in both error free and packet loss environments, while the complexity in video decoding is significantly reduced.","","978-1-4244-7970-2978-1-4244-7969-6978-1-4244-7967","10.1109/PACCS.2010.5627070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5627070","motion estimation;mode decision;interpolation complexity;end to end distortion","Decoding;Interpolation;Switches;Manganese;Automatic voltage control","computational complexity;data compression;interpolation;radio networks;video coding","adaptive H.264 video encoding scheme;low-complexity decoding;packet-loss environment;computational complexity;end-to-end distortion model;wireless networks;channel distortion;motion estimation;interpolation complexity","","","","6","","","","","IEEE","IEEE Conferences"
"Code Complexity versus Performance for GPU-accelerated Scientific Applications","A. W. U. Munipala; S. V. Moore","University of Texas at El Paso, TX, USA; NA","2016 Fourth International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering (SE-HPCCSE)","","2016","","","50","50","Summary form only given. Graphics Processing Units (GPUs) are becoming widely used as parallel accelerators in high-performance computing. GPU programming until recently, has been done by using low-level programming models such as CUDA and OpenCL. The directive-based OpenACC programming model has been growing in popularity due to its higher level of abstraction. This technique, which uses “directive” or “pragma” statements to annotate source code written in traditional high-level languages such as Fortran, C, and C++, is intended to allow a single code base to work across multiple computational platforms. We attempt to compare code complexity and performance of CUDA, OpenCL, and OpenACC implementations for three benchmark codes - the Game of Life (GOL) example code, the LULESH hydrodynamics proxy application, and the CloverLeaf mini-app from the Mantevo suite For the GOL C, CUDA C, and OpenCL codes and the LULESH C++, CUDA, and OpenCL codes, we measured source lines of code (SLOC) and cyclomatic complexity using the Oxbow toolkit static analysis tools. We ran the commercial McCabe IQ tool on the CloverLeaf Fortran90, Fortran90 + OpenACC, and Fortran portion of CloverLeaf_CUDA to measure cyclomatic complexity, design complexity, and essential complexity. We found that the CUDA and OpenCL implementations have significantly more lines of code than the corresponding OpenACC implementations but that the measured cyclomatic complexity is not always higher. The CUDA and OpenCL implementations generally have better performance, but there is not a drastic difference if the OpenACC code is optimized. We conclude that the available metrics and tools for measuring complexity of GPU programs are inadequate, since they do not quantify the portability and maintainability of the codes, and that specialization and extensions are needed.","","978-1-5090-5224-0978-1-5090-5225","10.1109/SE-HPCCSE.2016.012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839472","","Complexity theory;Graphics processing units;Programming;Measurement;Software engineering;Computational modeling;C++ languages","C++ language;graphics processing units;parallel architectures;software metrics;software performance evaluation;source code (software)","code complexity;GPU-accelerated scientific applications;graphics processing units;game of life;GOL;LULESH hydrodynamics proxy application;CloverLeaf mini-app;Mantevo suite;GOL C;CUDA C;OpenCL codes;LULESH C++;source lines of code;SLOC;cyclomatic complexity;Oxbow toolkit static analysis tools;McCabe IQ tool;CloverLeaf Fortran90;Fortran90 + OpenACC;design complexity;essential complexity","","2","","","","","","","IEEE","IEEE Conferences"
"Low-complexity dictionary based lossless screen content coding","M. Xu; Z. Ma; W. Wang; X. Wang; H. Yu","Huawei USA R&amp;D Center, Santa Clara, CA 95050; Huawei USA R&amp;D Center, Santa Clara, CA 95050; Huawei USA R&amp;D Center, Santa Clara, CA 95050; Huawei USA R&amp;D Center, Santa Clara, CA 95050; Huawei USA R&amp;D Center, Santa Clara, CA 95050","2014 IEEE International Conference on Image Processing (ICIP)","","2014","","","3200","3203","This paper presents a low-complexity lossless screen content coding method on top of a well structured dictionary based solution DictSCC. Both methods are implemented on the emerging High-Efficiency Video Coding (HEVC) Range Extensions (RExt) to demonstrate the coding efficiency. Compared with the DictSCC our scheme further explores the 2D correlation between neighboring image blocks for performance improvement, and reduces the complexity significantly by constraining the dictionary buffer which will be more favored by the hardware implementation. Results demonstrate averaged 17%, 10% and 8% bit rate reduction over HEVC for screen content compression with respect to All Intra, Random Access and Low-Delay encoder configurations.","1522-4880;2381-8549","978-1-4799-5751","10.1109/ICIP.2014.7025647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025647","HEVC;screen content coding;dictionary coding","Encoding;Dictionaries;Video coding;Complexity theory;Bit rate;Image color analysis;Artificial intelligence","video coding","low-complexity dictionary;lossless screen content coding;DictSCC;high-efficiency video coding range extensions;HEVC RExt","","1","","14","","","","","IEEE","IEEE Conferences"
"Improved low complexity hybrid turbo codes and their performance analysis","A. Bhise; P. D. Vyavahare","K. J. S. Institute of Engineering and Information Technology; S. G. S. Institute of Technology and Science","IEEE Transactions on Communications","","2010","58","6","1620","1622","This paper proposes a new class of Turbo type codes referred to as Improved Low Complexity Hybrid Turbo Codes (ILCHTC). For low Bit Energy to Noise Ratio (E<sub>b</sub>/N<sub>o</sub>)ILCHTC shows better convergence of errors than that of Low Complexity Hybrid Turbo Code (LCHTC). Simulation results show that ILCHTC achieves Bit Error Rate (BER) of 10<sup>-5</sup> at E<sub>b</sub>/N<sub>o</sub> of 1.9 dB, which is 0.4 dB more than that for Turbo Convolutional Codes (TCC). Analysis of ILCHTC and LCHTC decoders show that the number of computations reduce by a factor of two as compared to the computations of TCC decoder.","0090-6778;1558-0857","","10.1109/TCOMM.2010.06.080507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5474625","A posteriori probability, bit error rate, concatenated codes, decoder complexity, hybrid turbo codes.","Turbo codes;Performance analysis;Bit error rate;Concatenated codes;Convolutional codes;Iterative decoding;AWGN;Additive white noise;Convergence;Block codes","convolutional codes;error statistics;turbo codes","performance analysis;improved low complexity hybrid turbo codes;energy to noise ratio;error convergence;bit error rate;BER;turbo convolutional codes;ILCHTC decoders;TCC decoder","","5","","8","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-Complexity Trellis Min-Max Decoder for Non-Binary Ldpc Codes","H. P. Thi; H. Lee","Dept. of Information and Communication Engineering, Inha University, Incheon, 22212, Korea; Dept. of Information and Communication Engineering, Inha University, Incheon, 22212, Korea","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2018","","","1179","1183","In this paper, a novel algorithm and corresponding reduced-complexity decoder architecture are proposed for decoding the trellis min-max NB-LDPC code. This proposal reduces the number of messages exchanged between check node and variable node as well as the hardware complexity. Thus, the memory requirement and the wiring congestion is decreased, which increases the throughput of the decoder with a negligible error-correcting performance loss. A layered decoder architecture is implemented for the (2304, 2048) NB-LDPC code over GF(16) based on the proposed algorithm with a 90-nm CMOS technology. The results show an area reduction of 19.4% for the check node unit, 26.56% for the whole decoder and a throughput of 1396 Mbps with almost similar error-correcting performance, compared to previous works.","2379-190X","978-1-5386-4658-8978-1-5386-4657-1978-1-5386-4659","10.1109/ICASSP.2018.8462192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462192","Nonbinary LDPC codes;check node processing;trellis min-max;layered decoding;VLSI design","Decoding;Complexity theory;Throughput;Parity check codes;Logic gates;Computer architecture;Reliability","CMOS integrated circuits;decoding;parity check codes;trellis codes","CMOS technology;error-correcting performance;check node unit;layered decoder architecture;negligible error-correcting performance loss;hardware complexity;variable node;trellis min-max NB-LDPC code;reduced-complexity decoder architecture;nonbinary LDPC codes;reduced-complexity trellis min-max decoder;size 90 nm","","","","11","","","","","IEEE","IEEE Conferences"
"A low-complexity min-sum decoding algorithm for LDPC codes","H. Li; J. Guo; C. Guo; D. Wang","Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing 100190, P. R. China; Institute of Automation, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing 100190, P. R. China; Institute of Automation, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing 100190, P. R. China; Institute of Automation, Chinese Academy of Sciences, Chinese Academy of Sciences, Beijing 100190, P. R. China","2017 IEEE 17th International Conference on Communication Technology (ICCT)","","2017","","","102","105","This paper proposes a low-complexity LDPC decoding algorithm with simplified check nodes updating. The proposed algorithm simplifies the result of the second-minimum in check nodes based on the first-minimum computation instead of computing it directly. In order to obtain the approximate result, effective corrected coefficients are utilized, which can reduce the complexity by eliminating the complex computations. Complexity analysis is provided and the results indicate that the complexity of the proposed algorithm is much lower than the general NMS algorithm and MS-based simplified algorithms. Simulation results show that the performance of the proposed algorithm can closely match the NMS algorithm with the same number of iterations.","2576-7828","978-1-5090-3944-9978-1-5090-3942-5978-1-5090-3945","10.1109/ICCT.2017.8359612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359612","LDPC decoding;low-complexity;Min-Sum;Simplified check nodes","Manganese;Parity check codes;Approximation algorithms;Complexity theory;Decoding;Simulation;Standards","computational complexity;decoding;parity check codes","low-complexity LDPC decoding algorithm;first-minimum computation;complexity analysis;general NMS algorithm;low-complexity minsum decoding algorithm;simplified check nodes;second-minimum computation;MS-based simplified algorithms","","","","13","","","","","IEEE","IEEE Conferences"
"Complexity of Analog Modulo Block Codes","T. Schmitz; F. Schaefer; P. Jax; P. Vary","NA; NA; NA; NA","SCC 2017; 11th International ITG Conference on Systems, Communications and Coding","","2017","","","1","6","Analog Modulo Block codes (AMB codes) provide a low-complexity channel coding strategy for discrete-time, continuous-amplitude signals like audio, video, or other sensor data. In contrast to digital coding, the transmission does not suffer from saturation of the quality of the decoded signal with increasing channel quality even if no feedback channel is available. In this paper, the complexity of AMB encoding and decoding is analyzed for the Discrete Maximum Likelihood decoder (DML), the Zero Forcing decoder with Lattice Reduction (ZFLR), and the Lattice Maximum Likelihood decoder (LML). A low-complexity algorithm for generating all valid lattice points, which are needed for the DML decoder, is introduced and analyzed. For the complexity reduction of the LML decoder by radius pre-check, further studies of the channel noise probabilities are conducted. Run time measurements qualitatively illustrate the validity of the results.","","978-3-8007-4362","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938009","","","","","","","","","","","","","VDE","VDE Conferences"
"Low Complexity ML-Detection of Arbitrary Spherical Codes","C. Rachinger; R. R. Mueller; J. B. Huber","NA; NA; NA","SCC 2017; 11th International ITG Conference on Systems, Communications and Coding","","2017","","","1","5","We present two algorithms that quantize arbitrary points to a given constellation efficiently, even though there may not be an analytic description of this constellation. We choose spherical codes as constellations, because the best spherical codes can often only be computed numerically. The quantization is equivalent to a maximum likelihood-detection, but the average number of operations can be greatly reduced. The comparison between our proposed algorithms and a brute force approach is done by numerical simulations.","","978-3-8007-4362","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938013","","","","","","","","","","","","","VDE","VDE Conferences"
"Low-complexity PARCOR coefficient quantizer and prediction order estimator for lossless speech coding","Y. Kamamoto; T. Moriya; N. Harada","NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, JAPAN; NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, JAPAN; NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, JAPAN","2010 IEEE International Conference on Acoustics, Speech and Signal Processing","","2010","","","4678","4681","This paper describes two low-complexity tools used for the new ITU-T recommendation G.711.0, the lossless coding of G.711 (A-law/μ-law logarithmic PCM) speech data. One is an algorithm for quantizing the PARCOR/reflection coefficients and the other is an estimation method for the optimal prediction order. Both tools are based on a criterion that minimizes the entropy of the prediction residual signals and can be implemented in a fixed-point low-complexity algorithm. G.711.0 with the developed practical tools will be widely used everywhere because it can losslessly reduce the data rate of G.711, the prevailing speech-coding technology.","2379-190X;1520-6149;1520-6149","978-1-4244-4295-9978-1-4244-4296","10.1109/ICASSP.2010.5495188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5495188","Speech coding;Lossless compression;Standardization;Linear predictive coding;G.711","Speech coding;Phase change materials;Linear predictive coding;Reflection;Entropy;Pulse modulation;Codecs;Decoding;Propagation losses;Laboratories","pulse code modulation;speech coding","low-complexity PARCOR coefficient quantizer;prediction order estimator;lossless speech coding;ITU-T recommendation G.711.0;pulse code modulation;PCM;speech data;speech-coding technology","","1","","13","","","","","IEEE","IEEE Conferences"
"Polybinary Coding for Low Complexity High Speed Error-Free VCSEL-MMF Links","S. Varughese; J. Lavrencik; J. S. Gustavsson; E. Haglund; A. Larsson; S. E. Ralph","Georgia Institute of Technology, Atlanta, GA 30332, USA; Georgia Institute of Technology, Atlanta, GA 30332, USA; Chalmers University of Technology, SE-41296, G&#x00F6;teborg, Sweden; Chalmers University of Technology, SE-41296, G&#x00F6;teborg, Sweden; Chalmers University of Technology, SE-41296, G&#x00F6;teborg, Sweden; Georgia Institute of Technology, Atlanta, GA 30332, USA","2017 European Conference on Optical Communication (ECOC)","","2017","","","1","3","We demonstrate and analyze polybinary signaling as a low complexity alternative to PAM-4 for achieving higher bitrates in VCSEL-MMF links. Experimental results demonstrate that duobinary and polybinary-3 achieves similar performance as PAM-4 through 105m of wideband fiber at 10<sup>-12</sup> BER.","","978-1-5386-5624-2978-1-5386-4993","10.1109/ECOC.2017.8346161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8346161","","Bit rate;Vertical cavity surface emitting lasers;Bandwidth;Bit error rate;Modulation;Extinction ratio;Optical filters","encoding;error statistics;optical fibre communication;optical modulation;pulse amplitude modulation","polybinary coding;low complexity high speed error-free VCSEL-MMF links;polybinary signaling;low complexity alternative;PAM-4;duobinary-3;polybinary-3","","3","","9","","","","","IEEE","IEEE Conferences"
"Joint low-complexity detection and reliability-based BP decoding for non-binary LDPC coded TDMR channels","G. Han; M. Wang; Y. Fang; L. Kong","School of Information Engineering, Guangdong University of Technology, Guangzhou; School of Information Engineering, Guangdong University of Technology, Guangzhou; School of Information Engineering, Guangdong University of Technology, Guangzhou; School of Computer Science and Technology, Nanjing University of Posts and Communications, Nanjing","2015 IEEE International Magnetics Conference (INTERMAG)","","2015","","","1","1","Bit Patterned media recording (BPMR), heat assisted magnetic recording (HAMR), and shingled writing (SMR) with 2-dimensional (2-D) readback have been explored to further scale magnetic recording areal density over the next decade. However, from a signal processing perspective, all these magnetic recording technologies face a common challenge that is how to efficiently recover recorded data from the readback signals corrupted by inter-symbol interference and inter-track interference. The extension of Viterbi algorithm and BCJR algorithm to the 2D ISI channel has prohibitive complexity for the typical 2-D data size. In [1], the authors proposed an IRCSDFA-GA-BCJR detection algorithm which has an enormous complexity reduction compared with the conventional IRCSDFA and the optimal BCJR. In this paper, an efficient iterative detection and decoding scheme is proposed for Non-binary LDPC (NB-LDPC) codes coded two dimensional magnetic recording (TDMR) channels, in which both the low-complexity IRCSDFA-GA-BCJR detection and the fast convergence BP decoding are employed to improve the performance of 2-D channel detection.","2150-4598;2150-4601","978-1-4799-7322-4978-1-4799-7321","10.1109/INTMAG.2015.7157693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7157693","","Iterative decoding;Decoding;Reliability;Magnetic recording;Detectors;Joints","intersymbol interference;iterative decoding;magnetic recording;parity check codes;reliability","joint low-complexity detection;reliability-based BP decoding;nonbinary LDPC coded TDMR channels;magnetic recording areal density;readback signals;intersymbol interference;intertrack interference;Viterbi algorithm;iterative detection;iterative decoding;two-dimensional magnetic recording channels;low-complexity IRCSDFA-GA-BCJR detection;2D channel detection","","1","","3","","","","","IEEE","IEEE Conferences"
"Iterative reduced-complexity detection for LDPC coded 2-D recording channels","Z. Qin; K. Cai; K. S. Chan","Data Storage Institute, 5 Engineering Drive 1, Singapore, 117608; Data Storage Institute, 5 Engineering Drive 1, Singapore, 117608; Data Storage Institute, 5 Engineering Drive 1, Singapore, 117608","2012 Digest APMRC","","2012","","","1","2","In this paper, we consider iterative graph-based detection for low-density parity-check (LDPC) coded two-dimensional (2-d) intersymbol interference (ISI) channels and propose a novel approach to reduce the complexity of the check node operation in the channel Tanner Graph by restricting the computation to a small subset of binary vectors. Simulation results show that the proposed receiver approaches closely the performance of the full-complexity receiver with a much lower complexity.","","978-9-8107-2056-8978-1-4673-4734","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407560","Iterative decoding;LDPC codes;message-passing;two-dimensional channels","Interference;Complexity theory;Simulation","graph theory;intersymbol interference;iterative methods;parity check codes;radio receivers","LDPC coded 2D recording channels;iterative reduced-complexity detection;iterative graph-based detection;low-density parity-check code;2D intersymbol interference channels;check node operation;channel tanner graph;binary vectors;full-complexity receiver","","","","4","","","","","IEEE","IEEE Conferences"
"A Low-complexity Implementation of Full-rate Polarization-Time Codes for PDL Mitigation in Single-Carrier Optical Transmissions using the Constant","E. Awwad; P. Tran; G. Charlet","NA; NA; NA","ECOC 2016; 42nd European Conference on Optical Communication","","2016","","","1","3","We demonstrate through simulations and experimental measurements a low-complexity implementation of the 2x2 Silver code to mitigate PDL in a single-carrier PDM system using CMA equalization. The obtained gains are compared to the optimal ones achieved by an ML decoder.","","978-3-8007-4274","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7767531","","","","","","","","","","","","","VDE","VDE Conferences"
"A Novel Joint PAPR Reduction Algorithm With Low Complexity Using LT Codes","D. Bi; P. Ren; Z. Xiang","School of Telecommunications Engineering, Xidian University, Xi&#x2019;an, China; School of Telecommunications Engineering, Xidian University, Xi&#x2019;an, China; School of Telecommunications Engineering, Xidian University, Xi&#x2019;an, China","IEEE Wireless Communications Letters","","2018","7","2","166","169","In this letter, a low complexity joint peak to average power ratio (PAPR) reduction scheme is proposed for orthogonal frequency division multiplexing systems based on Luby transform (LT) codes. In the encoding process of LT codes, a predetermined threshold is introduced to control PAPR and reduce complexity. Moreover, we use the number of IFFT operations to model and formulate the theoretical algorithm complexity. Simulation results show that the proposed scheme can effectively reduce PAPR with a huge complexity reduction. Compared with the existing scheme, a maximum complexity reduction of 81% can be obtained concerning the total number of IFFT operations. Regarding the IFFT operations per degree, experimental curves are also consistent with the mathematical analysis.","2162-2337;2162-2345","","10.1109/LWC.2017.2762312","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066380","OFDM;peak to average power ratio;LT codes;complexity reduction","Peak to average power ratio;Complexity theory;Decoding;Encoding;Simulation;Algorithm design and analysis","forward error correction;mathematical analysis;OFDM modulation","novel joint PAPR reduction algorithm;LT codes;IFFT operations;theoretical algorithm complexity;maximum complexity reduction;orthogonal frequency division multiplexing systems;Luby transform codes;low complexity joint peak-to-average power ratio reduction scheme;encoding process;predetermined threshold;mathematical analysis","","2","","15","","","","","IEEE","IEEE Journals & Magazines"
"Rate-Distortion-Complexity Optimized Coding Scheme for Kvazaar HEVC Intra Encoder","A. Lemmetti; E. Kallio; M. Viitanen; J. Vanne; T. D. Hämäläinen","NA; NA; NA; NA; NA","2018 Data Compression Conference","","2018","","","419","419","This paper summarizes a low-complexity rate-distortion optimization (RDO) scheme for Kvazaar HEVC intra encoder (github.com/ultravideo/kvazaar). Our work particularly addresses RDO quantization (RDOQ) since it is the most complex intra coding tool taking almost 60% of the Kvazaar complexity.","2375-0359","978-1-5386-4883-4978-1-5386-4884","10.1109/DCC.2018.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416636","HEVC;RDO;RDOQ;Intra coding;Intra encoder","Encoding;Optimization;Complexity theory;Quantization (signal);Acceleration;Proposals;Bit rate","quantisation (signal);rate distortion theory;video coding","Kvazaar HEVC intra encoder;Kvazaar complexity;rate-distortion-complexity optimized coding scheme;low-complexity rate-distortion optimization scheme;RDO quantization","","","","0","","","","","IEEE","IEEE Conferences"
"Reduced-complexity iterative decoder for wavelet-coded systems in flat fading channels","L. G. de Queiroz Silveira","Department of Communications Engineering, Federal University of Rio Grande do Norte, Natal, RN, Brazil","2014 21st International Conference on Telecommunications (ICT)","","2014","","","374","378","This paper presents a quantization scheme for log-likelihood ratios in order to reduce the computational complexity of an iterative decoder recently proposed for wavelet-coded communication systems. In order to reduce the computational effort, the real-valued LLRs of encoded bits are quantized into three levels of reliability, with a threshold that is determined by minimizing the mean squared error between the original LLRs and the quantized ones. As result, a reduced-complexity iterative decoder is obtained. Performance evaluations of this iterative decoder were carried out by computer simulation and show that it is an effective means for reducing the computational load of wavelet-coded systems. Therefore, this new approach for iterative decoding may lead to new alternatives for exploiting the potential of wavelet coding for digital communications over wireless channels.","","978-1-4799-5141-3978-1-4799-5139","10.1109/ICT.2014.6845142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845142","wavelet coding;iterative decoding;Rayleigh fading channels;flat fading;time diversity","Iterative decoding;Decoding;Fading;Encoding;Telecommunications;Probability distribution;Wireless communication","fading channels;iterative decoding;mean square error methods;quantisation (signal)","reduced-complexity iterative decoder;flat fading channels;quantization scheme;log-likelihood ratios;wavelet-coded communication systems;real-valued LLR;mean squared error;wavelet-coded systems;digital communications;wireless channels","","","","17","","","","","IEEE","IEEE Conferences"
"Low complexity sparse code multiple access decoder based on tree pruned method","J. Chen; K. Han; J. Hu; Z. Zhang","National key Lab. of Communications, University of Electronic Science and Technology of China, UESTC, Chengdu, China; National key Lab. of Communications, University of Electronic Science and Technology of China, UESTC, Chengdu, China; National key Lab. of Communications, University of Electronic Science and Technology of China, UESTC, Chengdu, China; National key Lab. of Communications, University of Electronic Science and Technology of China, UESTC, Chengdu, China","2016 IEEE International Conference on Digital Signal Processing (DSP)","","2016","","","341","345","With the requirement of rapid traffic growth and tremendous access, non-orthogonal multiple access becoming a prominent technology in the current 5G research area. The sparse code multiple access (SCMA) scheme is one of the most promising technology among the non-orthogonal technologies. In this work, we propose a tree pruned method based on MAX log-message decoding algorithm (MPA) to reduce the decoding complexity of SCMA significantly. We first formulate the decoding problem to a tree updating process. By using the tree pruned method, the sub nodes with lower weight can be updated by low complexity arithmetic operations. According to the simulation result, the proposed tree pruned method reduces 30% computation complexity compared with existing methods. The results provide in this paper indicates the potential practical application of SCMA decoder.","2165-3577","978-1-5090-4165-7978-1-5090-4166","10.1109/ICDSP.2016.7868575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868575","Sparse code multiple access (SCMA);MAX log-message decoding algorithm (MPA);Tree Pruned method","Decision support systems;Niobium;Decoding;Performance analysis;Complexity theory","5G mobile communication;communication complexity;decoding;telecommunication traffic;trees (mathematics)","traffic growth;nonorthogonal multiple access;5G research area;sparse code multiple access;nonorthogonal technologies;tree pruned method;MAX log-message decoding algorithm;decoding complexity;SCMA;tree updating process;low complexity arithmetic operations;computation complexity;SCMA decoder","","","","","","","","","IEEE","IEEE Conferences"
"Low Complexity Decoding and Capacity of Index Coding Problems with Symmetric Side-Information","M. B. Vaddi; B. S. Rajan","Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, KA, 560012, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, KA, 560012, India","2018 IEEE Information Theory Workshop (ITW)","","2018","","","1","5","A single unicast index coding problem (SUICP) with symmetric side-information has K messages and K receivers, the kth receiver Rk wants xk, Rk has some subset of messages as side-information and the side-information is symmetric to its wanted message xk. Maleki, Cadambe and Jafar studied various symmetric index coding problems because of their importance in topological interference management problems. In our previous work, we constructed binary matrices of size mxn for any given arbitrary positive integers m and n such that any n adjacent rows of this matrix are linearly independent. We refer these matrices as Adjacent Independent Row (AIR) matrices. We designed optimal and near-optimal vector linear index codes for various symmetric SUICPs by using AIR matrices. To design the optimal and nearoptimal vector linear index codes, we convert the respective symmetric SUICP into an SUICP with symmetric neighboring and consecutive (SNC) side-information. Then, we use AIR matrix to encode the SUICP with SNC side-information. Hence, low-complexity decoding of SUICP with SNC side-information is important for efficient decoding of optimal and near-optimal index codes for various symmetric SUICPs. We analyse some of the combinatorial properties of AIR matrices in this work. By using these properties, we provide a low-complexity decoding for SUICP with SNC side-information. The low-complexity decoding explicitly identifies the set of broadcast symbols required at every receiver to decode its wanted message. By using lowcomplexity decoding, we find the capacity of SUICP with symmetric side-information Kk = {xk+g, xk+2g, . . . , xk+tg}, where g = gcd(K, D) and t = D/g for any positive integer D <; K.","","978-1-5386-3599-5978-1-5386-3598-8978-1-5386-3600","10.1109/ITW.2018.8613417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613417","","Indexes;Receivers;Encoding;Symmetric matrices;Decoding;Matrix converters;Complexity theory","binary codes;decoding;linear codes;matrix algebra;vectors","AIR matrix;SNC side-information;low-complexity decoding;near-optimal index codes;K messages;symmetric index coding problems;topological interference management problems;binary matrices;near-optimal vector linear index codes;single unicast index coding problem;arbitrary positive integers;symmetric SUICP;adjacent independent row matrices;symmetric side-information;K receivers;symmetric neighboring and consecutive side-information;combinatorial properties","","","","10","","","","","IEEE","IEEE Conferences"
"Code Complexity on Before and After Applying Design Pattern through SW Visualization","S. Y. Moon; B. K. Park; R. Y. C. Kim","NA; NA; NA","2016 International Conference on Platform Technology and Service (PlatCon)","","2016","","","1","5","Software is actually depended on his/her coding maturity of each developer. Software is inevitably used in all fields due to ICT convergence, which is increasing on the issue of code quality. His/her developer avoids to show source codes to other persons, and also repeats to modify them, which finally makes spaghetti codes. Therefore, it will be possible to increase bug occurrence, and also fall below legibility and understanding of the code. Its code quality is depended on the maturity of a developer. To protect this problem, we apply SW visualization with GOF design pattern to make good design structure. Design pattern is a verified solution to provide with object oriented design. Code complexity is essentially referred to software quality, which has low complexity to easily understand, and has low possible to occur errors. In this paper, the SW Visualization method is applied to compare the relationship between each code complexity Before and After applying design pattern to enhance the quality of codes.","","978-1-4673-8685-2978-1-4673-8684","10.1109/PlatCon.2016.7456787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456787","","Complexity theory;Couplings;Visualization;Software quality;Maintenance engineering;Java","data visualisation;object-oriented methods;object-oriented programming;software quality;source code (software)","coding maturity;ICT convergence;code quality;source codes;spaghetti codes;SW visualization;GOF design pattern;design structure;object oriented design;software quality;code complexity","","1","","6","","","","","IEEE","IEEE Conferences"
"Universal and low-complexity quantizer design for compressive sensing image coding","X. Li; X. Lan; M. Yang; J. Xue; N. Zheng","Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China","2013 Visual Communications and Image Processing (VCIP)","","2013","","","1","5","Compressive sensing imaging (CSI) is a new framework for image coding, which enables acquiring and compressing a scene simultaneously. The CS encoder shifts the bulk of the system complexity to the decoder efficiently. Ideally, implementation of CSI provides lossless compression in image coding. In this paper, we consider the lossy compression of the CS measurements in CSI system. We design a universal quantizer for the CS measurements of any input image. The proposed method firstly establishes a universal probability model for the CS measurements in advance, without knowing any information of the input image. Then a fast quantizer is designed based on this established model. Simulation result demonstrates that the proposed method has nearly optimal rate-distortion (R~D) performance, meanwhile, maintains a very low computational complexity at the CS encoder.","","978-1-4799-0290-3978-1-4799-0288","10.1109/VCIP.2013.6706403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706403","Compressive sensing imaging;quantization;Gaussian distribution;image coding","Quantization (signal);Image coding;Computational modeling;Computational complexity;Compressed sensing;Decoding","codecs;compressed sensing;computational complexity;data compression;image coding","universal quantizer design;low-complexity quantizer design;compressive sensing image coding;compressive sensing imaging;CSI system;image coding;CS encoder shifts;system complexity;decoder;lossless compression;lossy compression;universal probability model;optimal rate-distortion;computational complexity;CS encoder","","1","","11","","","","","IEEE","IEEE Conferences"
"Low-complexity multiview video coding","S. Khattak; R. Hamzaoui; S. Ahmad; P. Frossard","Faculty of Technology, De Montfort University, Leicester, UK; Faculty of Technology, De Montfort University, Leicester, UK; Faculty of Technology, De Montfort University, Leicester, UK; Signal Processing Laboratory (LTS4), Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne 1015 - Switzerland","2012 Picture Coding Symposium","","2012","","","97","100","We consider the problem of complexity reduction in Multiview Video Coding (MVC). We provide a unique comprehensive study that integrates and compares the different low complexity encoding techniques that have been proposed at different levels of the MVC system. In addition, we propose a novel complexity reduction method that takes advantage of the relationship between disparity vectors along time. The relationship is exploited with respect to the motion activity in the frame, as well as with the position of the frame in the Group of Pictures. We integrate this technique into our unique comprehensive framework and evaluate the performance of the resulting system in different setups. We show that the effective combination of complexity reduction techniques results in saving up to 93% in encoding time at the cost of only 0.08 dB in peak signal-to-noise ratio (PSNR) and 1.64% increase in bitrate compared to the standard MVC implementation (JMVM 6.0).","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213295","","Strontium;Encoding;Vectors;Complexity theory;Estimation;Video coding;PSNR","video coding","low complexity multiview video coding;MVC;complexity reduction;low complexity encoding techniques;disparity vectors;group of pictures;peak signal-to-noise ratio;JMVM 6.0","","3","","10","","","","","IEEE","IEEE Conferences"
"A* Based Algorithm for Reduced Complexity ML Decoding of Tailbiting Codes","J. Ortin; P. Garcia; F. Gutierrez; A. Valdovinos","Aragon Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza E-50018, Spain; Aragon Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza E-50018, Spain; Aragon Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza E-50018, Spain; Aragon Institute for Engineering Research (I3A), University of Zaragoza, Zaragoza E-50018, Spain","IEEE Communications Letters","","2010","14","9","854","856","The A* algorithm is a graph search algorithm which has shown good results in terms of computational complexity for Maximum Likelihood (ML) decoding of tailbiting convolutional codes. The decoding of tailbiting codes with this algorithm is performed in two phases. In the first phase, a typical Viterbi decoding is employed to collect information regarding the trellis. The A* algorithm is then applied in the second phase, using the information obtained in the first one to calculate the heuristic function. The improvements proposed in this work decrease the computational complexity of the A* algorithm using further information from the first phase of the algorithm. This information is used for obtaining a more accurate heuristic function and finding early terminating conditions for the A* algorithm. Simulation results show that the proposed modifications decrease the complexity of ML decoding with the A* algorithm in terms of the performed number of operations.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2010.072310.100295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5534603","Convolutional codes;decoding;tailbiting;A* algorithm;IA* algorithm","Maximum likelihood decoding;Computational complexity;Viterbi algorithm;Convolutional codes;Computational modeling;Tail;Land mobile radio cellular systems;WiMAX;Termination of employment;Government","computational complexity;convolutional codes;graph theory;maximum likelihood decoding;trellis codes;Viterbi decoding","graph search algorithm;computational complexity;maximum likelihood decoding;tailbiting convolutional codes;Viterbi decoding;trellis;heuristic function;A* algorithm","","7","","6","","","","","IEEE","IEEE Journals & Magazines"
"Random Linear Network Coding Schemes for Reduced Zero-Padding Overhead: Complexity and Overhead Analysis","M. Taghouti; D. Lucani; F. H. P. Fitzek; A. Bouallegue","NA; NA; NA; NA","European Wireless 2017; 23th European Wireless Conference","","2017","","","1","7","The zero-padding overhead created when performing Random Linear Network Coding (RLNC) on unequal-sized packets can curb its promising benefits since it can be as high as the data to convey. The concept of macro-symbol coding was introduced recently in order to reduce the zero-padding overhead that RLNC has brought. Macro-symbols are subsets of the packets, i.e. concatenated bytes. They allow performing coding mainly on the payload and they proved to be efficient against the naive padding. This paper studies the properties of macro-symbols and provides a characterization of their impact on the computational complexity as well as the overall overhead. Furthermore, we provide a theoretical framework for the encoding and decoding complexity for the state-of-the-art schemes for unequal-sized packets. Our simulations results performed on a series of benchmark video traces show that small macro-symbols guarantee a dramatic reduction of padding overhead, whilst it results on a higher decoding complexity on the other hand compared with RLNC in the worst case.","","978-3-8007-4426","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8011335","","","","","","","","","","","","","VDE","VDE Conferences"
"Low complexity algorithms for network coding based on singular value decomposition","Jungmin Kwon; H. Park","Multimedia Communications and Networking Laboratory, Ewha Womans University, Seoul, Republic of Korea; Multimedia Communications and Networking Laboratory, Ewha Womans University, Seoul, Republic of Korea","2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN)","","2016","","","641","643","In this paper, we propose a low complexity algorithm for decoding where network coding is deployed in client-server networks. We consider battery powered clients, so that minimizing their power consumptions is essential. Our focus is thus on developing a decoding algorithm that can reduce the computational complexity. Unlike general decoding algorithms that are based on Gaussian elimination, we propose a decoding algorithm based on the singular value decomposition, as it enables to easily compute an inverse matrix, leading to lower decoding complexity. Our simulation results confirm that proposed algorithm can reduce not only the decoding complexity but also the overall network complexity. While the network efficiency of the proposed strategy is degraded as the network dimension increases, we show that the efficiency converges into a lower bound as the network dimension increases. These are confirmed by the simulation results.","2165-8536","978-1-4673-9991-3978-1-4673-9990-6978-1-4673-9992","10.1109/ICUFN.2016.7537111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7537111","Network coding;singular value decomposition (SVD);computational complexity","Network coding;Decoding;Encoding;Servers;Computational complexity;Simulation","client-server systems;computational complexity;decoding;network coding;singular value decomposition","low-complexity algorithm;network coding;singular value decomposition;client-server networks;battery-powered clients;power consumption minimization;computational complexity reduction;general decoding algorithm;Gaussian elimination;inverse matrix;decoding complexity;network efficiency;network dimension","","","","8","","","","","IEEE","IEEE Conferences"
"Corrections to “low-complexity concatenated ldpc-staircase codes” [jun 18 2443-2449]","M. Barakatain; F. R. Kschischang","Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada","Journal of Lightwave Technology","","2019","37","3","1070","1070","We correct a minor error in the original paper.","0733-8724;1558-2213","","10.1109/JLT.2018.2880323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528352","","Forward error correction;Bit error rate;Parity check codes;Complexity theory;Error probability","","","","","","2","","","","","IEEE","IEEE Journals & Magazines"
"A reduced complexity chip-level SOR-SIC multiuser detector for long-code CDMA systems","A. Bentrcia; A. Zerguine; M. Benyoucef","Prince Sultan Advanced Technologies Research Institute/STC chair, King Saud University, P.O.Box 800, Riyadh, 11421, KSA, Saudi Arabia; Department of Electrical Engineering, King Fahd University of Petroleum and Minerals, Dhahran 31261, KSA, Saudi Arabia; Faculty of Engineering, Department of Electronics, University of Batna, 05000, Algeria","2010 4th International Conference on Signal Processing and Communication Systems","","2010","","","1","4","In this work a reduced complexity chip-level linear SIC multiuser structure that is asymptotically equivalent to successive over-relaxation (SOR) iteration, which is known to outperform the conventional Gauss-Seidel iteration by an order of magnitude in terms of convergence speed, is proposed. The main advantage of the proposed scheme is its low computational complexity compared to other chip-level and symbol-level SIC schemes equivalent to successive over-relaxation (SOR) iteration. We study the convergence behaviour of the proposed scheme and prove that it converges if the relaxation factor is in the interval. Simulation results are in excellent agreement with theory.","","978-1-4244-7907-8978-1-4244-7908-5978-1-4244-7906","10.1109/ICSPCS.2010.5709699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709699","SOR;SIC;multiuser detection","Detectors;Convergence;Multiaccess communication;Computational complexity;Silicon carbide;Decorrelation","code division multiple access;computational complexity;iterative methods;multiuser detection","long-code CDMA system;successive over-relaxation iteration;Gauss-Seidel iteration;convergence speed;computational complexity;reduced complexity chip-level SOR-SIC multiuser detector","","","","9","","","","","IEEE","IEEE Conferences"
"Low complexity image correction using color and focus matching for stereo video coding","W. Kim; J. Kim; M. Choi; I. Chang; J. Kim","Department of Electronics and Radio Engineering, Kyung Hee University, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Rep. of Korea","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","2912","2915","In three-dimensional video (3DV), two cameras capture the same scene from different viewpoints. Color and focus variations between the camera views may deteriorate the 3DV quality and performance of 3DV coding. Therefore, we need to correct the color and focus discrepancy between the camera views. In this paper, we propose algorithms that color and focus correction are combined in a preprocessing step of the stereo video coding. For both color and focus matching, we calculate only disparity vector (DV) once during disparity estimation (DE), since the proposed color and focus correction algorithm can share the disparity vector. The experimental results show that the proposed color and focus correction algorithm provides better image quality and coding performance.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6572488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572488","","Image color analysis;Complexity theory;Matched filters;Filtering algorithms;Cameras;Algorithm design and analysis;Video coding","cameras;image matching;stereo image processing;three-dimensional displays;video coding","stereo video coding;image correction;color matching;focus matching;three-dimensional video;3DV quality;cameras;color variations;focus variations;3DV coding;disparity vector;disparity estimation;image quality","","","","6","","","","","IEEE","IEEE Conferences"
"Low-complexity feedback-channel-free distributed video coding with enhanced classifier","Y. Wang; S. Hsu; T. Cheng; C. Lee; S. Chien","Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","257","260","Distributed video coding (DVC) is an emerging video coding paradigm due to its flexibility to introduce much lower encoding complexity than conventional predictive codecs, which is beneficial for some applications such as wireless video surveillance, wireless sensor networks, and disposable video cameras. Although DVC systems without feedback channel address a wider range of applications, it is not commonly discussed in literatures due to its lower coding performance. In this paper, on the basis of PRISM DVC architecture, a low-complexity feedback-channel-free DVC system is proposed with a new classifier to improve the coding performance. Experimental results show that the proposed system can provide about 0.8 dB gain in PSNR to the state-of-the-art and 2 dB gain to IST-PRISM. It is also competitive regarding other feedback-channel-free DVC systems and can even achieve similar performance to DVC systems with feedback channel for low-motion sequences.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6571831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571831","Distributed video coding;Wyner-Ziv (WZ) Video Coding;PRISM;feedback-channel-free","Decoding;Encoding;Video coding;Complexity theory;Codecs;Transforms;Support vector machine classification","video coding","low-complexity feedback-channel-free distributed video coding;classifier enhancement;encoding complexity;PRISM DVC architecture;PSNR;IST-PRISM;low-motion sequence","","","","9","","","","","IEEE","IEEE Conferences"
"Low Complexity CTU Partition Structure Decision and Fast Intra Mode Decision for Versatile Video Coding","H. Yang; L. Shen; X. Dong; Q. Ding; P. An; G. Jiang","NA; NA; NA; NA; NA; NA","IEEE Transactions on Circuits and Systems for Video Technology","","2019","PP","99","1","1","Quatree with nested multi-type tree (QTMT) partition structure is an efficient improvement in versatile video coding (VVC) over the quadtree structure in the advanced high efficiency video coding (HEVC) standard. With the exception of the recursive quadtree partition structure, recursive multi-type tree partition is applied to each leaf node, which generates more flexible block sizes. Besides, intra prediction modes are extended from 35 to 67 so as to satisfy various texture patterns. These newly developed techniques achieve high coding efficiency, but also result in very high computational complexity. To tackle this problem, we propose a fast intra coding algorithm consisting of low complexity coding tree units (CTU) structure decision and fast intra mode decision in this paper. The contributions of the proposed algorithm lie in the following aspects: 1) the new block size and coding mode distribution features are firstly explored for a reasonable fast coding scheme; 2) a novel fast QTMT partition decision framework is developed, which can determine the partition decision on both quadtree and multi-type tree with a novel cascade decision structure; 3) fast intra mode decision with gradient descent search is introduced, while the best initial search point and search step are also investigated in this paper. Simulation results show that the complexity reduction of the proposed algorithm is up to 70% compared to VVC reference software (VTM), and averagely 63% encoding time saving is achieved with 1.93% BDBR increasing. Such results demonstrate that our method yields a superior performance in terms of computational complexity and compression quality compared to the state-of-the-art methods.","1051-8215;1558-2205","","10.1109/TCSVT.2019.2904198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8664144","Versatile video coding;intra coding;fast coding algorithm;joint video exploration team;quadtree with multi-type tree","Encoding;Complexity theory;Video coding;Partitioning algorithms;Prediction algorithms;Transforms;Standards","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Message Passing-Based Decoding of Convolutional Codes: Performance and Complexity Analysis","H. Mani; H. Saeedi","Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Communications Letters","","2016","20","2","216","219","In this letter, we propose to apply message passing algorithms to decode standard convolutional codes and assess the resulting performance and the required complexity compared to conventional decoding algorithms for convolutional codes by concentrating on the Viterbi algorithm (VA). We show that, in contrast to the VA for which the decoding complexity increases exponentially with m, the number of memory blocks for the proposed framework, such an increase, is only linear in m. This suggests that applying message passing algorithms can provide considerable savings in the required computational power if it can also exhibit a comparable bit-error-rate performance to that of the VA. In this letter, we show via simulations that this is in fact the case for convolutional codes.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2508459","Iran National Elites Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7355317","Convolutional codes;message passing algorithm;low-density parity-check codes","Complexity theory;Iterative decoding;Maximum likelihood decoding;Generators;Convolutional codes","convolutional codes;message passing;Viterbi decoding","convolutional codes;complexity analysis;message passing algorithms;conventional decoding algorithms;Viterbi algorithm;VA;decoding complexity;memory blocks;bit-error-rate performance","","","","21","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity LDPC-coded USTM system based on dual demodulator","Li Peng; Lingling Yang","Wuhan National Laboratory for Optoelectronics, Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China; Wuhan National Laboratory for Optoelectronics, Department of Electronics and Information Engineering, Huazhong University of Science and Technology, China","10th International Conference on Wireless Communications, Networking and Mobile Computing (WiCOM 2014)","","2014","","","322","326","As compared to uncoded unitary space-time modulation (USTM), channel coded USTM has received increased attention for its large coding gain. However, low-density parity-check (LDPC) coded USTM has only received limited attention. This paper proposes a scheme of combining LDPC code with USTM for noncoherent MIMO communication system over Rayleigh flat fading and additive white Gaussian noise (AWGN) channel. At the receiver, considering the soft information required by the belief-propagation (BP) iterative decoder of the LDPC code, we deduce a low complexity maximum a posteriori probability (MAP) demodulating algorithm for a special USTM based on the sine-cosine function, SC-USTM for short. We combine the MAP with the maximum likelihood (ML) demodulating algorithm based on the SC-USTM in order to generate a novel dual-demodulator for further decreasing the computational complexity of the demodulating/decoding system. For improving the performance of the receiver, the iterative feedback scheme between the MAP demodulator and the BP decoder is designed. Comparing with the uncoded USTM, our LDPC-coded USTM can obtain about 15 - 17 dB coding gain at 10<sup>-5</sup> BER.","","978-1-84919-845","10.1049/ic.2014.0121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7129649","Unitary Space-Time Modulation (USTM);Maximum a Posteriori Probability (MAP);Demodulating Algorithm;Iterative Feedback;Low-Density Parity-Check (LDPC) Code","","AWGN channels;channel coding;computational complexity;demodulators;iterative decoding;maximum likelihood decoding;maximum likelihood estimation;MIMO communication;parity check codes;probability;Rayleigh channels;space-time codes","low-complexity LDPC-coded USTM system;dual demodulator;uncoded unitary space-time modulation;channel coded USTM;low-density parity-check codes;noncoherent MIMO communication system;Rayleigh flat fading channel;additive white Gaussian noise channel;AWGN channel;receiver;soft information;belief-propagation iterative decoder;low complexity maximum a posteriori probability demodulating algorithm;sine-cosine function;maximum likelihood demodulating algorithm;ML;SC-USTM;computational complexity;demodulating-decoding system;iterative feedback scheme;MAP demodulator;BP decoder","","","","","","","","","IET","IET Conferences"
"A class of QC-LDPC codes with low encoding complexity and good error performance","W. M. Tam; F. C. M. Lau; C. K. Tse","Department of Electronic and Information Engineering, The Hong Kong Polytechnic University; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University","IEEE Communications Letters","","2010","14","2","169","171","We propose a novel approximate lower triangular structure for the parity part of the parity-check matrix of QC-LDPC codes. About half of the non-zero elements in the parity part are set to locate on the upper diagonal while the remaining non-zero elements can be located almost anywhere within the lower triangular area, provided certain rules are observed. Compared with the typical dual-diagonal structure, the proposed structure requires very similar encoding complexity and produces lower error rates over an AWGN channel.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2010.02.091930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5403623","Approximate lower triangular structure, encoding complexity, QC low-density-parity-check (LDPC) codes","Encoding;Parity check codes;Error analysis;Senior members;AWGN channels;Wireless communication;Decoding;Shift registers","AWGN channels;parity check codes","QC LDPC codes;QC low density parity check codes;encoding complexity;AWGN channel;approximate lower triangular structure","","29","","10","","","","","IEEE","IEEE Journals & Magazines"
"An abstract to calculate big o factors of time and space complexity of machine code","S. Gayathri Devi; K. Selvam; S. P. Rajagopalan","Department of Mathematics, Dr.M.G.R.University, Chennai, India; Department of Computer Applications, Dr.M.G.R.University, Chennai, India; Department of Computer Applications, Dr.M.G.R.University, Chennai, India","International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011)","","2011","","","844","847","Algorithms are generally written for solving some problems or mechanism through machines, the algorithms may be several in numbers, further to these the efficiency of the produced algorithms for the said issue need to be quantified: the factors which are to be quantified are time complexity, space complexity, administrative cost and faster implementation etc.,..One of the effective methods for studying the efficiency of algorithms is Big-O notations, though the Big-O notation is containing mathematical functions and their comparison step by step, it illustrates the methodology of measuring the complexity facto. The output is always expected as a smooth line or curve with a smaller and static slope.","","978-9-38043-000","10.1049/cp.2011.0483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143432","Time complexity;Space complexity;BigO;f(n);cg(n);O(G(n))","","computational complexity;mathematical analysis","time complexity;space complexity;machine code;mechanism through machines;administrative cost;algorithms efficiency;Big-O notations;mathematical functions;complexity facto;static slope","","9","","","","","","","IET","IET Conferences"
"A low-complexity decoding algorithm for concatenated tree codes","D. Kim; J. Ha","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 305 - 701, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 305 - 701, Korea","2015 International Conference on Information and Communication Technology Convergence (ICTC)","","2015","","","488","490","This work considers a modified belief-propagation algorithm with two way scheduling for concatenated tree (CT) codes in which tree codes are employed as constituent codes. It will be shown that constituent tree codes can be efficiently decoded with the modified algorithm. Thus, by repeatedly using the modified BP algorithm for the constituent tree codes, CT codes can be decoded in an efficient way. Moreover, we show that the modified algorithm is especially suited for low complexity hardware implementation. Finally, a simple analysis technique for error-correcting performances of tree codes will be presented.","","978-1-4673-7116-2978-1-4673-7115","10.1109/ICTC.2015.7354592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354592","","Iterative decoding;Maximum likelihood decoding;Complexity theory;Algorithm design and analysis;Scheduling","concatenated codes;decoding;tree codes","modified belief-propagation algorithm;simple analysis technique;error-correcting performances;modified BP algorithm;CT codes;concatenated tree codes;low-complexity decoding algorithm","","1","","9","","","","","IEEE","IEEE Conferences"
"Low-Complexity Intra Prediction Refinements for Video Coding","X. Zhao; V. Seregin; A. Said; K. Zhang; H. E. Egilmez; M. Karczewicz","Qualcomm Technologies Inc., San Diego, California, USA; Qualcomm Technologies Inc., San Diego, California, USA; Qualcomm Technologies Inc., San Diego, California, USA; Qualcomm Technologies Inc., San Diego, California, USA; Qualcomm Technologies Inc., San Diego, California, USA; Qualcomm Technologies Inc., San Diego, California, USA","2018 Picture Coding Symposium (PCS)","","2018","","","139","143","In existing video coding standards such as H.264/AVC and HEVC, the intra prediction is typically derived using fixed, symmetric prediction filters along the prediction direction, e.g., in planar mode, top-right and bottom-left samples are predicted using symmetric prediction filters. However, in case of asymmetric availability of neighboring reference samples, the performance of intra prediction filters designed in HEVC may not be optimal. To further refine the intra prediction and achieve higher accuracy of prediction samples, this paper proposes low-complexity refinements over HEVC intra prediction, which are applied on frequently used planar, DC, horizontal and vertical modes. The proposed method only requires simple addition and bit-shift operations on top of HEVC's intra prediction implementation. Experimental results show that, an average of 0.7% coding gain is achieved for intra coding with no increase in run-time complexity.","2472-7822","978-1-5386-4160-6978-1-5386-4159-0978-1-5386-4161","10.1109/PCS.2018.8456305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456305","","Encoding;Standards;High efficiency video coding;Table lookup;Transform coding;Complexity theory","computational complexity;filtering theory;prediction theory;video coding","symmetric prediction filters;video coding standards;fixed prediction filters;prediction direction;complexity intra prediction refinements;HEVCs intra prediction implementation;planar mode;optimal;DC;vertical modes;horizontal modes;bit shift operations;coding gain;run time complexity","","1","","9","","","","","IEEE","IEEE Conferences"
"A hybrid low complexity decoding of LDPC codes","H. Wang; G. Fan; J. Kuang","School of Information and Electronics, Beijing Institute of Technology, China; School of Information and Electronics, Beijing Institute of Technology, China; School of Information and Electronics, Beijing Institute of Technology, China","IET 3rd International Conference on Wireless, Mobile and Multimedia Networks (ICWMNN 2010)","","2010","","","108","112","An efficient early stopping criterion based on the evolution of the number of reliable variable nodes (NRVN) is proposed for low density parity check (LDPC) codes in this paper. This criterion can significantly reduce the average number of iterations (ANI) at low to middle signal to noise ratios (SNRs) with minimal performance degradation. Furthermore, a hybrid decoding method is presented to reduce the decoding complexity of LDPC codes by combining the proposed stopping criterion and forced convergence (FC) decoding algorithm. Compared with the classic belief propagation (BP) decoding, significant computation complexity reduction can be achieved with small computation overhead.","","978-1-84919-240","10.1049/cp.2010.0630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702968","ldpc codes;iterative decoding;reduced complexity decoding;early stopping criterion","","communication complexity;decoding;parity check codes","hybrid low complexity decoding;LDPC codes;number of reliable variable nodes;low density parity check codes;average number of iterations;forced convergence decoding algorithm;computation complexity reduction","","","","","","","","","IET","IET Conferences"
"Low-Complexity Detection for Space-Time Block Coded Spatial Modulation Systems","M. -. El Astal; A. H. Al Habbash; A. M. Abu-Hudrouss","NA; NA; NA","2018 International Conference on Promising Electronic Technologies (ICPET)","","2018","","","49","51","Space-time block coded spatial modulation (STBC-SM) systems offer higher capacity by increasing the number of transmitter antennas. This is at the cost of complexity for optimally decoding the received symbol increasing exponentially. In this paper, a low-complexity detector is proposed for STBC-SM systems. It offers high capacity with a small loss in diversity gain when compared to the optimal detection.","","978-1-5386-5697-6978-1-5386-5698","10.1109/ICPET.2018.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531228","Spatial-Modulation,;Space-Time-Coding,;low-complexity,;optimal-decoding.","Detectors;Modulation;Complexity theory;Transmitting antennas;Decoding;Diversity methods","diversity reception;modulation coding;space-time block codes;transmitting antennas","low-complexity detection;space-time block coded spatial modulation systems;STBC-SM systems;optimal detection;transmitter antennas;diversity gain","","","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity frame scheduler using shared frame memory for multi-view video coding","Minsu Choi; Jinsang Kim; Ik Joon Chang; Won-Kyung Cho","Kyung Hee University, Rep. of Korea; Kyung Hee University, Rep. of Korea; Kyung Hee University, Rep. of Korea; Kyung Hee University, Rep. of Korea","2012 International SoC Design Conference (ISOCC)","","2012","","","498","502","Multi-view video coding (MVC) produces more realistic three-dimensional scenes using disparity (or depth) information derived from cameras placed in parallel than mono and stereo video coding. However, the performances such as chip area, power consumption and processing time are dependent on how to schedule the multi-view frames. This paper proposes a low-complexity frame waiting time aware scheduler (WTaS) for faster MVC. In order to reduce the frame memory size for MVC, we design a shared frame memory scheme so that the frame memories of many views can be shared using a smaller frame memory for high frame memory utilization. Experimental results show that the proposed low-complexity WTaS scheduler can reduce the size by 30%~50% without degradation of the processing time.","","978-1-4673-2990-3978-1-4673-2989-7978-1-4673-2988","10.1109/ISOCC.2012.6406905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406905","Multi-view video coding;frame scheduling;VLSI architecture;H.264/AVC","Video coding;Memory management;Encoding;Hardware;Cameras;Bandwidth;Scheduling","computational complexity;resource allocation;scheduling;shared memory systems;video coding","low-complexity frame WTaS;shared frame memory;multiview video coding;realistic 3D scenes;disparity information;depth information;mono video coding;stereo video coding;waiting time aware scheduler;frame memory utilization","","1","","7","","","","","IEEE","IEEE Conferences"
"Low complexity LLR metrics for polar coded QAM","A. A. Hasan; I. D. Marsland","Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON, K1S 5B6, Canada; Department of Systems and Computer Engineering, Carleton University, 1125 Colonel By Drive, Ottawa, ON, K1S 5B6, Canada","2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2017","","","1","4","This paper presents a low-complexity approach for calculating soft decision metrics of polar coded quadrature amplitude modulation (QAM) system. A simple and general bit log likelihood ratio (LLR) expression is provided for Gray coded QAM signals. The characteristics of Gray code mapping such as symmetries and repeated formats of the bit assignment in a symbol among bit groups, are exploited effectively for the simplification of the LLR expression. To reduce the complexity of the max-log-MAP algorithm for LLR calculation, these LLRs can be simplified even further by replacing the mathematical min function of the conventional LLR expression with simple arithmetic functions. We introduce supplemental approximation to the max-log-MAP algorithm by averaging the LLR values in different regions to get a single approximate expression. Over AWGN and Rayleigh fading channels, simulation results and complexity analysis demonstrate that the proposed demapper approaches traditional approximations in terms of BER performance, but with much lower low complexity.","","978-1-5090-5538-8978-1-5090-5539","10.1109/CCECE.2017.7946778","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946778","Polar codes;QAM;transmit diversity;log likelyhood ratios","Measurement;Complexity theory;Quadrature amplitude modulation;Computers;Fading channels;Conferences;Decoding","approximation theory;Gray codes;quadrature amplitude modulation;Rayleigh channels","low complexity LLR metrics;polar coded QAM;soft decision metrics;quadrature amplitude modulation;general bit log likelihood ratio;Gray coded QAM signals;Gray code mapping;bit assignment;bit groups;LLR expression;max-log-MAP algorithm;mathematical min function;arithmetic functions;supplemental approximation;AWGN;Rayleigh fading channels;complexity analysis","","1","","7","","","","","IEEE","IEEE Conferences"
"Low Complexity Decoding Algorithm of QC-LDPC Code","F. Yi; P. Wang","NA; NA","2010 IEEE Asia-Pacific Services Computing Conference","","2010","","","531","534","Through the research on the decoding algorithm of low-density parity-check (LDPC) code, a low complexity decoding algorithm suitable for quasi-cyclic LDPC (QC-LDPC) code is proposed in this paper. The algorithm adopts layered decoding (LD), and the sub-minimum is replaced by the sum of an optimal correction factor and the minimum during the process of the check nodes update. Therefore, it is not required to find the sub-minimum and the computational complexity reduces significantly. Matlab simulation results show that the decoding performance of this algorithm is very near to min-sum (MS) algorithm, and more superior than single min-sum (SMS) algorithm.","","978-1-4244-9396","10.1109/APSCC.2010.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708616","LDPC;decoding algorithm;layered decoding;optimal correction factor","Decoding;Computational complexity;Iterative decoding;Signal to noise ratio;Algorithm design and analysis","computational complexity;cyclic codes;decoding;parity check codes","low-complexity decoding algorithm;QC-LDPC code;quasicyclic low density parity check code;layered decoding;computational complexity;Matlab simulation;single-min-sum algorithm;SMS algorithm;optimal correction factor","","1","","11","","","","","IEEE","IEEE Conferences"
"Low complexity decoding for higher order punctured trellis-coded modulation over intersymbol interference channels","F. Schuh; J. B. Huber","Institute for Information Transmission, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany; Institute for Information Transmission, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany","2014 6th International Symposium on Communications, Control and Signal Processing (ISCCSP)","","2014","","","475","478","Trellis-coded modulation (TCM) is a power and bandwidth efficient digital transmission scheme which offers very low structural delay of the data stream. Classical TCM uses a signal constellation of twice the cardinality compared to an uncoded transmission with one bit of redundancy per PAM symbol, i.e., application of codes with rates n-1/n when 2n denotes the cardinality of the signal constellation. Recently published work allows rate adjustment for TCM by means of puncturing the convolutional code (CC) on which a TCM scheme is based on. In this paper it is shown how punctured TCM-signals transmitted over intersymbol interference (ISI) channels can favorably be decoded. Significant complexity reductions at only minor performance loss can be achieved by means of reduced state sequence estimation.","","978-1-4799-2890","10.1109/ISCCSP.2014.6877916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877916","trellis-coded modulation (TCM);punctured convolutional codes;Viterbi-Algorithm (VA);reduced state sequence estimation (RSSE);intersymbol interference (ISI)","Convolutional codes;Complexity theory;Modulation;Estimation;Measurement;Generators;Maximum likelihood decoding","convolutional codes;decoding;intersymbol interference;pulse amplitude modulation;trellis coded modulation","low complexity decoding;higher order punctured trellis-coded modulation;intersymbol interference channels;bandwidth efficient digital transmission scheme;data stream;signal constellation;PAM symbol;rate adjustment;convolutional code;CC;TCM scheme;ISI channels;state sequence estimation reduction","","3","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity Viterbi decoder for convolutional codes in Class-A noise","T. S. Saleh; I. Marsland; M. El-Tanany","Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada","2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","","2012","","","1","4","The design of a simplified Viterbi decoder for signals in Middleton Class-A noise is considered. The conventional Viterbi decoder, with a branch metric optimized for Gaussian noise, performs poorly in the Class A noise. The optimal maximum likelihood (ML) branch metric is difficult to simplify due to the complexity of the probability density function of the noise. There are different alternatives to design low complexity Viterbi decoders which are based on simplified models of the Class-A noise. Furthermore, a nonlinear preprocessor has been proposed to improve the performance of the Gaussian Viterbi decoder in Class-A noise by using a simplified expression of the probability density function of the noise. In this paper, we propose different approach to design the Viterbi decoder with simple linear branch metrics by using a simplified linear approximation of the log likelihood ratio. The proposed approach results in near-optimal performance with low complexity.","0840-7789;0840-7789","978-1-4673-1433-6978-1-4673-1431-2978-1-4673-1432","10.1109/CCECE.2012.6335020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335020","","Viterbi algorithm;Noise;Decoding;Measurement;Convolutional codes;Probability density function;Linear approximation","codecs;convolutional codes;Gaussian noise;maximum likelihood decoding;Viterbi decoding","low-complexity Viterbi decoder;convolutional codes;design;Middleton class-A noise;Gaussian noise;maximum likelihood branch metric;probability density function;nonlinear preprocessor","","","","5","","","","","IEEE","IEEE Conferences"
"Double path coding method for mode decision based on low complexity in H.264","Tong Zhou; Yong Liu","School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, China","The 2nd International Conference on Information Science and Engineering","","2010","","","4514","4517","The traditional method of mode decision of low complexity is to compare the SAD cost of all the modes and choose the best mode with the smallest one. A new method for mode decision that is based on the result of quantization result is proposed for the DSP project design. This method is very useful in hardware designing project. In the analysis of the test result, the bitrate will reduce about 5%–25% with the same QP.","2160-1283;2160-1291","978-1-4244-7618-3978-1-4244-7616-9978-1-4244-7617","10.1109/ICISE.2010.5689324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5689324","video coding;intra-prediction;inter-prediction;block mode","PSNR;Encoding;Automatic voltage control;Discrete cosine transforms;Bit rate;Digital signal processing;Complexity theory","","","","","","","","","","","IEEE","IEEE Conferences"
"Low-complexity layered iterative hard-reliability-based majority-logic decoder for non-binary quasi-cyclic LDPC codes","C. Xiong; Z. Yan","Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015 USA; Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015 USA","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","1348","1351","Non-binary low-density parity-check (NB-LDPC) codes have some advantages as opposed to their binary counterparts, but unfortunately their decoding complexity is a significant challenge. Hence, iterative hard-reliability-based majority-logic decoding (IHRB-MLGD) algorithms are attractive for NB-LDPC codes due to their low complexities. In this paper, we propose a layered improved iterative hard-reliability-based majority-logic decoding algorithm and design a partly parallel architecture for the proposed algorithm. Our improved algorithm achieves better error performance and faster convergence than existing IHRB-MLGD algorithms, while maintaining low complexities. The proposed partly parallel architecture achieves a throughput of 779 Mbps with SMIC 0.13um CMOS technology.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6572104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572104","","Decoding;Iterative decoding;Complexity theory;Reliability;Clocks;Computer architecture","CMOS logic circuits;computational complexity;cyclic codes;iterative decoding;parallel architectures;parity check codes;reliability","low-complexity layered iterative hard-reliability-based majority-logic decoder;nonbinary quasicyclic LDPC codes;decoding complexity;layered improved IHRB-MLGD algorithm;NB-LDPC code;parallel architecture;error performance;SMIC CMOS technology;size 0.13 mum","","1","","11","","","","","IEEE","IEEE Conferences"
"Low-complexity high-speed soft-hard decoding for turbo-product codes","Y. Krainyk; V. Perov; M. Musiyenko","Department of Computer Engineering, Petro Mohyla Black Sea National University, Mykolaiv, Ukraine; Department of Computer Engineering, Petro Mohyla Black Sea National University, Mykolaiv, Ukraine; Department of Computer Engineering, Petro Mohyla Black Sea National University, Mykolaiv, Ukraine","2017 IEEE 37th International Conference on Electronics and Nanotechnology (ELNANO)","","2017","","","471","474","Combined (soft-hard) method for decoding block turbo-product codes is proposed in the paper. The method allows leveraging advantages of soft input data usage with the speed of hard-decoding procedure. The main peculiarity of the method is rule-based decoding stage. The proposed approach simplifies calculation procedure and reaches better correction ability than hard-decision decoder. Mathematical model of investigated method has been tested with the set of codes based on product of extended Hamming codes. Testing has confirmed correctness of the combined decoding method.","","978-1-5386-1701-4978-1-5386-1702","10.1109/ELNANO.2017.7939798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7939798","combined method;TPC;decoding;rule-based method;Chase algorithm","Decoding;Iterative decoding;Measurement;Throughput;Hardware;Testing;Error correction codes","decoding;Hamming codes;mathematical analysis;turbo codes","low complexity high speed soft-hard decoding;turbo product codes;decoding block turbo-product codes;soft input data usage;hard-decoding procedure;calculation procedure;hard-decision decoder;mathematical model;extended Hamming codes","","","","11","","","","","IEEE","IEEE Conferences"
"A low-complexity soft-decision decoding architecture for the binary extended Golay code","P. Adde; R. Le Bidan","Institut Mines-T&#x00E9;l&#x00E9;com/T&#x00E9;l&#x00E9;com-Bretagne; CNRS Lab-STICC UMR 3192, Technop&#x00F4;le Brest Iroise, CS 83818 29238 Brest Cedex 3, Universit&#x00E9; Europ&#x00E9;enne de Bretagne, France; Institut Mines-T&#x00E9;l&#x00E9;com/T&#x00E9;l&#x00E9;com-Bretagne; CNRS Lab-STICC UMR 3192, Technop&#x00F4;le Brest Iroise, CS 83818 29238 Brest Cedex 3, Universit&#x00E9; Europ&#x00E9;enne de Bretagne, France","2012 19th IEEE International Conference on Electronics, Circuits, and Systems (ICECS 2012)","","2012","","","705","708","The (24, 12, 8) extended binary Golay code is a well-known rate-1/2 short block-length linear error-correcting code with remarkable properties. This paper investigates the design of an efficient low-complexity soft-decision decoding architecture for this code. A dedicated algorithm is introduced that takes advantage of the code's properties to simplify the decoding process. Simulation results show that the proposed algorithm achieves close to maximum-likelihood performance with low computational cost. The decoder architecture is described, and VLSI synthesis results are presented.","","978-1-4673-1260-8978-1-4673-1261-5978-1-4673-1259","10.1109/ICECS.2012.6463628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463628","","Maximum likelihood decoding;Computer architecture;Algorithm design and analysis;Encoding;Bit error rate;Complexity theory","decoding;error correction codes;Golay codes;linear codes;maximum likelihood decoding","low-complexity soft-decision decoding architecture;binary extended Golay code;extended binary Golay code;rate-1/2 short block-length linear error-correcting code;maximum-likelihood performance;computational cost;decoder architecture;VLSI synthesis","","3","","14","","","","","IEEE","IEEE Conferences"
"Low complexity list successive cancellation decoding of polar codes","C. Cao; Z. Fei; J. Yuan; J. Kuang","Beijing Institute of Technology, People's Republic of China; Beijing Institute of Technology, People's Republic of China; University of New South Wales, Australia; Beijing Institute of Technology, People's Republic of China","IET Communications","","2014","8","17","3145","3149","The authors propose a low complexity list successive cancellation (LCLSC) decoding algorithm, where the advantages of the successive cancellation (SC) decoding and the list successive cancellation (LSC) decoding are both considered. In the proposed decoding, SC decoding instead of LSC decoding is implemented when all information bits from bad subchannels are received reliably. While the reliability of each information bit is estimated by its likelihood ratio (LR), the bit channel quality is measured via its Bhattacharyya parameter. To achieve this goal, the authors introduce two thresholds: LR threshold and Bhattacharyya parameter threshold. Also, the methods to determine them are both elaborated. The numerical results suggest that the complexity of LCLSC decoding is much lower than LSC decoding and can be close to that of SC decoding, while the error performance is almost equal to that of LSC decoding. Especially, when the code rate is in low region, the advantage of our decoding is more obvious.","1751-8628;1751-8636","","10.1049/iet-com.2014.0227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6962922","","","block codes;decoding;error correction codes;error statistics","polar codes;low complexity list successive cancellation;LCLSC decoding algorithm;list successive cancellation decoding;likelihood ratio;bit channel quality;LR threshold;Bhattacharyya parameter threshold;code rate;LSC decoding;SC decoding","","8","","15","","","","","IET","IET Journals & Magazines"
"An improved distributed video coding with low-complexity motion estimation at encoder","H. Yang; H. Hsieh; S. Chang; S. Chen","Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan, ROC; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan, ROC; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan, ROC; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan, ROC","2015 28th IEEE International System-on-Chip Conference (SOCC)","","2015","","","111","114","Distributed Video Coding (DVC) is a novel scheme which is different from the state-of-the-art video compression standards. A specific characteristic of DVC is its use of a low complexity encoder. However, there exists an issue on the DVC, which requires redundant decoding time. Therefore, low-complexity motion estimation algorithm and skip mode have been applied to the encoder of a hybrid DVC to reduce half of the decoding time. The computational complexity of the encoder only increases slightly with respect to the hybrid DVC, which is still far less than H.264/AVC no motion. In some cases, the encoding time is even reduced instead of being increased. Besides, the rate-distortion performance is improved in high motion videos. However, the rate-distortion performance does not decrease in the static video, because of the added motion estimation.","2164-1706","978-1-4673-9094-1978-1-4673-9093","10.1109/SOCC.2015.7406923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406923","distributed video coding;Wyner-Ziv codec;low-complexity motion estimation","Decoding;Motion estimation;Complexity theory;Video coding;Entropy coding;Parity check codes","computational complexity;data compression;decoding;motion estimation;redundancy;video coding","improved distributed video coding;low-complexity motion estimation;DVC;state-of-the-art video compression standard;low complexity encoder;redundant decoding time;skip mode;decoding time reduction;computational complexity;rate-distortion performance","","2","","8","","","","","IEEE","IEEE Conferences"
"Capacity-Achieving Codes With Bounded Graphical Complexity and Maximum Likelihood Decoding","C. Hsu; A. Anastasopoulos","Qualcomm Inc., Santa Clara; Electrical Engineering and Computer Science Department, University of Michigan, Ann Arbor","IEEE Transactions on Information Theory","","2010","56","3","992","1006","In this paper, the existence of capacity-achieving codes for memoryless binary-input output-symmetric (MBIOS) channels under maximum-likelihood (ML) decoding with bounded graphical complexity is investigated. Graphical complexity of a code is defined as the number of edges in the graphical representation of the code per information bit and is proportional to the decoding complexity per information bit per iteration under iterative decoding. Irregular repeat-accumulate (IRA) codes are studied first. Utilizing the asymptotic average weight distribution (AAWD) of these codes and invoking Divsalar's bound on the binary-input additive white Gaussian noise (BIAWGN) channel, it is shown that simple nonsystematic IRA ensembles outperform systematic IRA and regular low-density parity-check (LDPC) ensembles with the same graphical complexity, and are at most 0.124 dB away from the Shannon limit. However, a conclusive result as to whether these nonsystematic IRA codes can really achieve capacity cannot be reached. Motivated by this inconclusive result, a new family of codes is proposed, called low-density parity-check and generator matrix (LDPC-GM) codes, which are serially concatenated codes with an outer LDPC code and an inner low-density generator matrix (LDGM) code. It is shown that these codes can achieve capacity on any MBIOS channel using ML decoding and also achieve capacity on any BEC using belief propagation (BP) decoding, both with bounded graphical complexity. Moreover, it is shown that, under certain conditions, these capacity-achieving codes have linearly increasing minimum distances and achieve the asymptotic Gilbert-Varshamov bound for all rates.","0018-9448;1557-9654","","10.1109/TIT.2009.2039084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429143","Asymptotic growth rate;average weight distribution;capacity-achieving codes;density evolution;Gilbert–Varshamov bound;irregular repeat-accumulate codes;low-density generator matrix codes;low-density parity-check (LDPC) codes;maximum likelihood decoding","Maximum likelihood decoding;Parity check codes;Iterative decoding;Belief propagation;Additive white noise;Concatenated codes;Communication system control;Bipartite graph;Message passing;Iterative algorithms","AWGN channels;channel coding;concatenated codes;maximum likelihood decoding;memoryless systems;parity check codes","capacity-achieving codes;bounded graphical complexity;maximum likelihood decoding;memoryless binary-input output-symmetric channels;decoding complexity;irregular repeat-accumulate codes;asymptotic average weight distribution;additive white Gaussian noise channel;low-density parity-check codes;low-density generator matrix codes;Shannon limit;concatenated codes;belief propagation decoding;asymptotic Gilbert-Varshamov bound","","20","","45","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive coding tree for complexity control of high efficiency video encoders","G. Corrêa; P. Assuncao; L. A. da Silva Cruz; L. Agostini","Instituto de Telecomunicações, DEEC - FCTUC - University of Coimbra, Portugal; Instituto de Telecomunicações, DEE - ESTG - Polytechnic Institute of Leiria, Portugal; Instituto de Telecomunicações, DEEC - FCTUC - University of Coimbra, Portugal; GACI - CDTEC - Federal University of Pelotas Pelotas, Brazil","2012 Picture Coding Symposium","","2012","","","425","428","The emerging HEVC standard introduces several techniques which increase compression efficiency in comparison to its predecessors. However, such advances are accompanied by increases in computational complexity, limiting the encoder use in computational or power-constrained devices. This paper proposes a novel complexity control method for the future HEVC encoders based on a dynamic adjustment of the newly proposed coding tree structures. The relationship between coding tree depths and the encoding complexity is explored to selectively constrain encoding possibilities in order to not exceed a predefined complexity target. Experimental results show that the encoder computational complexity can be downscaled to 60% with a bit rate increase under 3.5% and a PSNR decrease under 0.1 dB.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213378","","Encoding;Computational complexity;Bit rate;Video coding;Video sequences;Algorithm design and analysis","adaptive codes;computational complexity;data compression;quadtrees;video coding","adaptive coding tree;complexity control;high efficiency video encoders;HEVC standard;compression efficiency;encoder computational complexity;dynamic adjustment;coding tree structures;coding tree depths;encoding complexity","","11","","8","","","","","IEEE","IEEE Conferences"
"Low-Complexity Near-Optimal Decoding for Analog Joint Source Channel Coding Using Space-Filling Curves","O. Fresnedo; F. J. Vazquez-Araujo; L. Castedo; J. Garcia-Frias","Department of Electronics and Systems, University of A Coruna, Spain; Department of Electronics and Systems, University of A Coruna, Spain; Department of Electronics and Systems, University of A Coruna, Spain; Department of ECE, University of Delaware, USA","IEEE Communications Letters","","2013","17","4","745","748","Analog Joint Source-Channel Coding (JSCC) is a communication strategy that does not follow the separation principle of conventional digital systems but approaches the optimal distortion-cost tradeoff over AWGN channels. Conventional Maximum Likelihood (ML) analog JSCC decoding schemes suffer performance degradation at low Channel Signal to Noise Ratio (CSNR) values, while Minimum Mean Square Error (MMSE) decoding presents high complexity. In this letter we propose an alternative two step decoding approach which achieves the near-optimal performance of MMSE decoding at all CSNR values while maintaining a low complexity comparable to that of ML decoding. An additional advantage of the proposed analog JSCC decoding approach is that it can also be used in Multiple Input Multiple Output (MIMO) fading channels.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2013.021913.122782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511526","Analog joint source channel coding;optimum performance theoretically attainable;non-linear mappings;MMSE estimation and decoding","Maximum likelihood decoding;Fading;MIMO;AWGN channels;Complexity theory;Channel coding","combined source-channel coding;fading channels;least mean squares methods;maximum likelihood decoding;MIMO communication","low-complexity near-optimal decoding;analog joint source channel coding;space-filling curves;communication strategy;digital systems;optimal distortion-cost tradeoff;AWGN channels;maximum likelihood analog JSCC decoding schemes;ML-JSCC decoding schemes;low channel signal to noise ratio;CSNR;minimum mean square error decoding;MMSE decoding;two step decoding approach;multiple input multiple output fading channels;MIMO fading channels","","29","","9","","","","","IEEE","IEEE Journals & Magazines"
"A low-complexity frame/block motion activity classification algorithm for Wyner-Ziv video coding","Feng Ye; Aidong Men; Jiatian Tian; Bo Yang; Ziyi Quan","College of Mathematics and Computer Science, Fujian Normal University, Fuzhou, 350001, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, 100876, China","2010 3rd IEEE International Conference on Broadband Network and Multimedia Technology (IC-BNMT)","","2010","","","833","837","In Wyner-Ziv video coding (WZVC), the majority of the computational complexity has been shifted from encoder to the decoder in comparison to its conventional video coding technologies. This paper presents a low-complexity frame/block motion activity classification algorithm for WZVC. Improved rate-distortion performance is achieved by adjusting the coding mode of key frame according to the motion activity along the sequence. Simulation results of proposed technique show a consistent improvement in coding efficiency in comparison to the state-of-the-art TDWZ codec.","","978-1-4244-6772-3978-1-4244-6769-3978-1-4244-6771","10.1109/ICBNMT.2010.5705207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705207","Wyner-Ziv video coding (WZVC);motion activity;3DRS motion estimation","Complexity theory;Decoding","codecs;computational complexity;video coding","low complexity frame/block motion activity;classification algorithm;Wyner-Ziv video coding;computational complexity;rate distortion performance;TDWZ codec","","","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity embedded BICM-ID structure for multi-dimensional coded modulation","Z. Xiao; M. Li; F. Yu; N. Stojanovic; C. Xie; L. Li","Huawei Technologies Co., Ltd., Network Research Department, Shenzhen, 518129, China; Huawei Technologies Co., Ltd., Network Research Department, Shenzhen, 518129, China; Huawei Technologies Co., Ltd., Network Research Department, Shenzhen, 518129, China; Huawei Technologies Duesseldorf GmbH, Germany Research Center, Riesstrasse 25, D-80992 Munich, Germany; Huawei Technologies Duesseldorf GmbH, Germany Research Center, Riesstrasse 25, D-80992 Munich, Germany; Huawei Technologies Co., Ltd., Network Research Department, Shenzhen, 518129, China","2017 Optical Fiber Communications Conference and Exhibition (OFC)","","2017","","","1","3","A low-complexity BICM-ID structure which embeds demappers inside the iterative FEC decoder is proposed and demonstrated. 0.47-dB SNR gain over BICM can be obtained for the 8-dimentional modulation format with 8.5-miUion extra ASIC gates at 100-Gb/s throughput.","","978-1-9435-8023-1978-1-5090-6229","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937405","","Signal to noise ratio","application specific integrated circuits;decoding;iterative methods;optical modulation","low-complexity embedded BICM-ID structure;multidimensional coded modulation;iterative FEC decoder;SNR gain;8-dimentional modulation format;ASIC gates","","","","7","","","","","IEEE","IEEE Conferences"
"On the error-correcting capabilities of low-complexity decoded irregular LDPC codes","P. Rybin","Inst. for Information Transmission Problems, Russian Academy of Science, Moscow, Russia","2014 IEEE International Symposium on Information Theory","","2014","","","3165","3169","This paper deals with the irregular binary low-density parity-check (LDPC) codes with the constituent single parity check (SPC) codes and the error-correcting iterative low-complex decoding algorithm. The lower bound on the error fraction, guaranteed corrected by the considered iterative algorithm, was obtained for the irregular LDPC code for the first time in this paper. This lower bound was obtained as a result of analysis of Tanner graph representation of irregular LDPC code. The number of decoding iterations, required to correct the errors, is a logarithmic function of the code length. The numerical results, obtained at the end of the paper for proposed lower bound achieved similar results for the previously known best lower-bounds for regular LDPC codes and were represented for the first time for the irregular LDPC codes.","2157-8117;2157-8095","978-1-4799-5186","10.1109/ISIT.2014.6875418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875418","","Iterative decoding;Sockets;Decoding;Estimation;Polynomials","binary codes;error correction codes;iterative decoding;parity check codes","error-correcting capability;irregular LDPC codes;irregular binary low-density parity-check codes;single parity check codes;SPC codes;error-correcting iterative low-complex decoding algorithm;lower bound;error fraction;Tanner graph representation analysis;logarithmic function;code length;regular LDPC codes","","7","","15","","","","","IEEE","IEEE Conferences"
"Rate-Distortion Optimization for Video Coding under Given Computational Complexity","J. Feng; S. Zhang; F. Yang; S. Wan","NA; NA; NA; NA","2017 Data Compression Conference (DCC)","","2017","","","440","440","Rate-distortion optimization (RDO) is widely applied in video coding, which aims at minimizing the coding distortion under a target coding rate. Conventionally, RDO in video coding does not take into account the coding complexity. However, because of the diversity of video applications, the video encoders in different applications may have different requirements of or limitation on the computational complexity. Therefore, it is desirable for video encoders to perform RDO in flexible computational complexity. In this paper, we propose a novel RDO scheme under the given computational complexity for the latest H.265/HEVC standard. A model for prediction of the rate-distortion cost (RD cost) is first established based on a pre-searching process. Then according to the predicted RD cost, the rate-distortion-complexity (R-D-C) characteristics of different coding tree units (CTUs) are analyzed. Finally, the total complexity budget is properly allocated to different CTUs according to their R-D-C characteristics. Experimental results demonstrate that, compared with x265, the proposed algorithm can reduce, on average, the BD-rate by 18.8% under the same requirements of encoding speed.","2375-0359","978-1-5090-6721-3978-1-5090-6722","10.1109/DCC.2017.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7923723","","Encoding;Computational complexity;Rate-distortion;Optimization;Roads;Velocity control","computational complexity;optimisation;trees (mathematics);video coding","rate-distortion optimization;video encoders;computational complexity;RDO;frame-level complexity budget;coding tree units;optimal coding performance;H.265/HEVC standard;predicted rate-distortion costs;RD costs;probability;optimal CTU partitioning mode;encoding time consumption;candidate list;video coding","","","","","","","","","IEEE","IEEE Conferences"
"Reduced Complexity MIMO Concatenated Code in Fading Channels","W. N. An; W. Hamouda","Department of Electrical and Computer Engineering, Concordia University, Montreal, Quebec, H3G 1M8, Canada; Department of Electrical and Computer Engineering, Concordia University, Montreal, Quebec, H3G 1M8, Canada","IEEE Communications Letters","","2011","15","7","746","748","In conventional concatenations of Convolutional Codes (CC) and Space-Time Block Codes (STBC), the CC and STBC are utilized to provide coding gain and spatial diversity, respectively. We propose a concatenated code that achieves the full system diversity by appropriately selecting the outer CC with an inner reduced-rank STBC. The advantage of the lower rank STBC is that the number of RF chains can be reduced. For any number of RF chains, R, we show that a desirable diversity order Y can be easily achieved i.e., 1 ≤ R ≤ Y. Using trellis diagram, we formalize the method to determine the maximum diversity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2011.060111.102186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871410","Convolutional codes;diversity;multiple-input multiple-output","Radio frequency;Encoding;Transmitting antennas;Convolutional codes;Switches;Slot antennas;Bit error rate","concatenated codes;convolutional codes;diversity reception;fading channels;MIMO communication;space-time block codes;trellis codes","MIMO;fading channel;multiple-input multiple-output system;convolutional code;space-time block code;coding gain;spatial diversity;concatenated code;system diversity;reduced-rank STBC;RF chain;trellis diagram","","3","","4","","","","","IEEE","IEEE Journals & Magazines"
"On Decoding Complexity of Reed-Solomon Codes on the Packet Erasure Channel","G. Garrammone","Institute of Communications and Navigation of the German Aerospace Center (DLR), 82234 Wessling, Germany","IEEE Communications Letters","","2013","17","4","773","776","We study the finite-length complexity of the Berlekamp-Massey algorithm (BM) and of the Gaussian elimination algorithm (GE) for decoding a Reed-Solomon (RS) code of length n on the packet erasure channel (PEC). In particular, we are interested in the dependency of the complexity on the packet size g. We show that, for large packet sizes, the complexity of both algorithms is O(g) and the constants hidden by the O-notation are comparable. As an example, we consider a RS code from the Digital Video Broadcasting - Handhelds (DVB-H) standard and we show that, although the asymptotic complexity of the GE is O(n<sup>3</sup>) and the one of the BM is O(n<sup>2</sup>), the finite-length complexity of both algorithms is comparable already for small/moderate packet sizes, in particular for all packet sizes considered within the standard, making the GE not inferior to the BM, in this context.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2013.021913.122427","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511528","Reed-Solomon codes;Gaussian elimination algorithm;Berlekamp-Massey algorithm;packet erasure channel","Vectors;Complexity theory;Digital video broadcasting;Standards;Maximum likelihood decoding;Indexes","computational complexity;decoding;Gaussian processes;Reed-Solomon codes","decoding complexity;Reed-Solomon codes;packet erasure channel;RS codes;PEC;finite-length complexity;Berlekamp-Massey algorithm;BM;Gaussian elimination algorithm;GE;O-notation;digital video broadcasting-handhelds standard;DVB-H standard;asymptotic complexity;packet sizes","","10","","11","","","","","IEEE","IEEE Journals & Magazines"
"A low-complexity stereo video coding scheme for surveillance","Jiang Hao; Yao Qifu; Yu Mei; Ren Yibo","Zhejiang Business Technology Institute, Ningbo, China; Zhejiang Business Technology Institute, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, China; Zhejiang Business Technology Institute, Ningbo, China","2010 International Conference on Computer Application and System Modeling (ICCASM 2010)","","2010","12","","V12-231","V12-234","Low-complexity stereo video compression is important for surveillance. The existing H.264-based stereo video coding compression schemes have high coding performance, but the complexity of motion and disparity estimation is a significant burden for surveillance, especially in the 3D case. In this paper, a low-complexity stereo video coding scheme is proposed for 3D surveillance. This scheme is based on motion vector (MV) extrapolation and region-of-interest (ROI) image restoration technique. In one hand, MV extrapolation refinement is used to reduce the complexity on motion estimation. In addition, through the object segmentation in low-resolution video and information transmission in ROI enhancement layer, the quality scalability for reconstructed frame is implemented. The experimental results show that our proposed scheme can reduce the coding complexity of stereo video while maintaining a high compression performance.","2161-9069;2161-9077","978-1-4244-7237-6978-1-4244-7235","10.1109/ICCASM.2010.5622250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5622250","low-complexity;motion vector extrapolation;ROI image restoration;stereo video coding","Extrapolation;Complexity theory;Video coding;Encoding;Surveillance;Image restoration;Three dimensional displays","code standards;image restoration;image segmentation;motion estimation;stereo image processing;video coding;video surveillance","stereo video coding;stereo video compression;H.264;motion estimation;disparity estimation;3D video surveillance;motion vector extrapolation;region of interest;image restoration;object segmentation;ROI;quality scalability","","","","11","","","","","IEEE","IEEE Conferences"
"A Low Complexity Decoding Algorithm for Majority-Logic Decodable Nonbinary LDPC Codes","D. Zhao; X. Ma; C. Chen; B. Bai","Department of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510275, China; Department of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510275, China; State Key Lab of ISN, Xidian University, Xi'an, China; State Key Lab of ISN, Xidian University, Xi'an, China","IEEE Communications Letters","","2010","14","11","1062","1064","In this letter, we propose a low complexity decoding algorithm for majority-logic decodable nonbinary low-density parity-check (LDPC) codes. The proposed algorithm is initialized with the quantized squared Euclidean distances between the constellation points and the received signals. Like most of the existing reliability-based decoding algorithms, the proposed algorithm requires only integer operations and finite field operations and (hence) can be implemented with simple combinational logic circuits in practical systems. Simulation results show that the proposed algorithm suffers from a little performance degradation compared with FFT-QSPA. The algorithm provides a candidate for trade-offs between performance and complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2010.100810.101403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5601969","Low complexity decoding;majority-logic decodable;nonbinary LDPC codes;FFT-QSPA","Complexity theory;Decoding;Iterative decoding;Algorithm design and analysis;Degradation;Simulation","combinational circuits;computational complexity;decoding;parity check codes","low complexity decoding;majority-logic decodable nonbinary LDPC codes;low-density parity-check codes;quantized squared Euclidean distances;integer operations;finite field operations;combinational logic circuits","","29","","14","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-complexity decoding algorithms of Raptor codes","C. Albayrak; K. Turk","Department of Electrical Electronics Engineering, Karadeniz Technical University, Trabzon 61080, TR; Department of Electrical Electronics Engineering, Karadeniz Technical University, Trabzon 61080, TR","2016 39th International Conference on Telecommunications and Signal Processing (TSP)","","2016","","","149","152","In this paper, the belief propagation (BP) based approximation methods which are introduced for low density parity check (LDPC) codes in literature are adapted to the Raptor decoder structure in order to reduce its computational complexity. The bit error rate (BER) performances of the algorithms over the additive white Gaussian noise (AWGN) channel are obtained by both theoretical works and simulations. The Monte-Carlo based density evolution (MC-DE) method is used for theoretical analysis. In addition to this, computational complexity analyses of the considered methods are presented. Results show that the computational complexity can be significantly decreased with a limited performance loss cost.","","978-1-5090-1288-6978-1-5090-1287-9978-1-5090-1289","10.1109/TSP.2016.7760847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760847","Belief propagation;BP-based approximations;Rateless codes;Raptor codes;Reduced-complexity decoding","Decoding;Iterative decoding;Approximation algorithms;Algorithm design and analysis;Computational complexity;Mathematical model","approximation theory;AWGN channels;channel coding;computational complexity;decoding;error statistics;Monte Carlo methods;parity check codes","MC-DE method;Monte Carlo based density evolution method;AWGN channel;additive white Gaussian noise channel;BER;bit error rate;computational complexity reduction;Raptor decoder structure;LDPC code;low density parity check code;BP based approximation method;belief propagation based approximation method;Raptor code reduced-complexity decoding algorithm","","","","13","","","","","IEEE","IEEE Conferences"
"Irregular Polar Coding for Multi-Level Modulation in Complexity-Constrained Lightwave Systems","T. Koike-Akino; C. Cao; Y. Wang; S. C. Draper; D. S. Millar; K. Parsons; K. Kojima; M. Pajovic; L. Galdino; D. J. Elson; D. Lavery; P. Bayvel","Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Electrical & Computer Engineering, University of Toronto, Toronto, ON M5S 3G4, Canada; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA 02139, USA; Optical Networks Group, University College London (UCL), Torrington Place, London, WC1E 7JE, UK; Optical Networks Group, University College London (UCL), Torrington Place, London, WC1E 7JE, UK; Optical Networks Group, University College London (UCL), Torrington Place, London, WC1E 7JE, UK; Optical Networks Group, University College London (UCL), Torrington Place, London, WC1E 7JE, UK","2017 European Conference on Optical Communication (ECOC)","","2017","","","1","3","We introduce novel irregular polar codes, which extend the regular polar coding by flexible polarization pruning. We experimentally demonstrate that the proposed polar codes outperform state-of-the-art LDPC codes, while the computational complexity in encoding and decoding can be significantly reduced by at least 30% over the regular polar codes with a marginal performance improvement.","","978-1-5386-5624-2978-1-5386-4993","10.1109/ECOC.2017.8345842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345842","","Parity check codes;Encoding;Upper bound;Decoding;Forward error correction;Modulation;Complexity theory","computational complexity;decoding;optical fibre networks;parity check codes","irregular polar coding;multilevel modulation;irregular polar codes;regular polar coding;regular polar codes;LDPC codes","","2","","12","","","","","IEEE","IEEE Conferences"
"Low complexity watermarking scheme for scalable video coding","A. M. Buhari; H. Ling; V. M. Baskaran; K. Wong","Faculty of Engineering, Multimedia University, 63100 Cyberjaya, Selangor, Malaysia; Faculty of Engineering & Science, Curtin University, 98000 Miri, Sarawak, Malaysia; Faculty of Engineering, Multimedia University, 63100 Cyberjaya, Selangor, Malaysia; Faculty of Computer Science & Information Technology, University of Malaya, 50603 Kuala Lumpur, Malaysia","2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW)","","2016","","","1","2","This paper presents a low complexity human visual system based watermarking algorithm for H.264 spatial scalable coding. The proposed algorithm extracts textural feature from a set of 7 high energy quantized coefficients in 4 × 4 luma INTRA-predicted blocks of all-slices and embeds watermark into the highly textured block which has at least one non-zero coefficient in 6 selected locations. Experiments were conducted by embedding up to 8192 watermark bits into a four-layer spatial scalable coded video. Results suggest that the proposed scheme produces watermarked video with an average visual quality degradation of 0.36 dB at the expense of 2.18% bitrate overhead. In addition, the proposed watermarking scheme achieves the detection rates of 0.96, 0.94 and 0.66 against re-encoding, recompression and Gaussian filtering attacks, respectively.","","978-1-5090-2073-7978-1-5090-2074","10.1109/ICCE-TW.2016.7520710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7520710","","Watermarking;Bit rate;Algorithm design and analysis;Streaming media;Visualization;Degradation;Feature extraction","computational complexity;feature extraction;image texture;video coding;video watermarking","low complexity human visual system based watermarking algorithm;H.264 spatial scalable video coding;textural feature extraction;high energy quantized coefficients;luma INTRA-predicted block;highly textured block;four-layer spatial scalable coded video;average visual quality degradation","","1","","7","","","","","IEEE","IEEE Conferences"
"Low complexity MAP decoding of tailbiting convolutional codes","P. Wijesinghe; U. Gunawardana; R. Liyanapathirana","School of Engineering, University of Western Sydney, Australia; School of Engineering, University of Western Sydney, Australia; School of Engineering, University of Western Sydney, Australia","2010 International Conference on Signal Processing and Communications (SPCOM)","","2010","","","1","4","Tailbiting is an effective technique to terminate convolutional codes without a rate loss. The latest application of tailbiting codes can be seen in the broadcast channel of the long term evolution (LTE) cellular systems. Due to the high computational complexity of maximum likelihood (ML) decoding of tailbiting codes, suboptimum decoding techniques are often used in practice. In this paper, we propose a low-complexity MAP decoder based on the observation that the MAP decoder can forget the erroneous initial conditions faster in forward calculations than in backward calculations. The proposed decoder uses a head length for forward calculations and a tail length for backward calculations in the well-known BCJR algorithm. This allows the decoder to eliminate the starting and ending transients in the bit error rate that occur due to the uniform initial conditions thereby improving the performance. Using the tailbiting convolutional code proposed for LTE over different channel conditions, it is shown that circular MAP decoding performs close to ML decoding.","2165-0608","978-1-4244-7138-6978-1-4244-7137-9978-1-4244-7136","10.1109/SPCOM.2010.5560562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560562","","Maximum likelihood decoding;Bit error rate;Viterbi algorithm;Convolutional codes;Complexity theory;OFDM","broadcast channels;cellular radio;computational complexity;convolutional codes;maximum likelihood decoding","low complexity MAP decoding;tailbiting convolutional codes;rate loss;broadcast channel;long term evolution cellular systems;computational complexity;maximum likelihood decoding;suboptimum decoding techniques;low-complexity MAP decoder;backward calculations;head length;tail length;BCJR algorithm;bit error rate;channel conditions","","1","","11","","","","","IEEE","IEEE Conferences"
"A New Construction of EVENODD Codes With Lower Computational Complexity","H. Hou; P. P. C. Lee","School of Electrical Engineering and Intelligentization, Dongguan University of Technology, Dongguan, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Communications Letters","","2018","22","6","1120","1123","EVENODD codes are binary array codes for correcting double disk failures in RAID-6 with asymptotically optimal encoding and decoding complexities. However, the update complexity of EVENODD is sub-optimal. We propose a new construction of binary maximum distance separable array codes, namely EVENODD+, such that the encoding, decoding, and update complexities of EVENODD+ are less than those of EVENODD in general. Moreover, EVENODD+ achieves asymptotically optimal update complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2820007","National Natural Science Foundation of China; Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327621","EVENODD codes;update complexity","Encoding;Decoding;Arrays;Measurement;Computational complexity;Electrical engineering","computational complexity;decoding;error correction codes;storage management","binary array codes;double disk failures;asymptotically optimal encoding;decoding complexities;binary maximum distance separable array codes;asymptotically optimal update complexity;EVENODD codes;computational complexity","","","","11","","","","","IEEE","IEEE Journals & Magazines"
"A qualitative analysis of code clones and object oriented runtime complexity based on method access points","A. F. Desouky; M. D. Beard; L. H. Etzkorn","Computer Science Department, University of Alabama in Huntsville Huntsville, AL USA 35899; Computer Science Department, University of Alabama in Huntsville Huntsville, AL USA 35899; Computer Science Department, University of Alabama in Huntsville Huntsville, AL USA 35899","International Conference for Convergence for Technology-2014","","2014","","","1","5","In this paper, we present a new object oriented complexity metric based on runtime method access points. Software engineering metrics have traditionally indicated the level of quality present in a software system. However, the analysis and measurement of quality has long been captured at compile time, rendering useful results, although potentially incomplete, since all source code is considered in metric computation, versus the subset of code that actually executes. In this study, we examine the runtime behavior of our proposed metric on an open source software package, Rhino 1.7R4. We compute and validate our metric by correlating it with code clones and bug data. Code clones are considered to make software more complex and harder to maintain. When cloned, a code fragment with an error quickly transforms into two (or more) errors, both of which can affect the software system in unique ways. Thus a larger number of code clones is generally considered to indicate poorer software quality. For this reason, we consider that clones function as an external quality factor, in addition to bugs, for metric validation.","","978-1-4799-3759-2978-1-4799-3758","10.1109/I2CT.2014.7092292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092292","Software Engineering;Object Oriented Runtime Metrics;Code Clones;Complexity;Object Behavior","Measurement;Runtime;Cloning;Complexity theory;Correlation;Software;Computer bugs","object-oriented programming;program verification;public domain software;security of data;software metrics;software quality;source code (software)","qualitative analysis;code clones;object oriented runtime complexity;runtime method access points;software engineering metrics;metric computation;source code;open source software package;Rhino 1.7R4;bug data;software quality;metric validation","","","","23","","","","","IEEE","IEEE Conferences"
"Low-Complexity Analog Linear Coding Scheme","X. Insausti; P. M. Crespo; J. Gutiérrez-Gutiérrez; M. Zárraga-Rodríguez","Biomedical Engineering and Sciences Department, Tecnun (University of Navarra), San Sebástin, Spain; Biomedical Engineering and Sciences Department, Tecnun (University of Navarra), San Sebástin, Spain; Biomedical Engineering and Sciences Department, Tecnun (University of Navarra), San Sebástin, Spain; Biomedical Engineering and Sciences Department, Tecnun (University of Navarra), San Sebástin, Spain","IEEE Communications Letters","","2018","22","9","1754","1757","In this letter, we design a low-latency, low-complexity, and low-transmission power analog linear coding scheme to estimate the finite-length data blocks under a mean-square-error (distortion) criterion. The proposed scheme is based on the analog linear coding and the discrete Fourier transform. A remarkable feature of our scheme is that it does not require knowing the correlation matrix of the data blocks, and therefore, it can be used even if the correlation matrix changes slowly with time.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2848941","Ministerio de Economía y Competitividad; COMONSENS network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388273","Low complexity;analog linear coding;finite-length data block;DFT","Correlation;Complexity theory;Channel coding;Distortion;Discrete Fourier transforms;AWGN channels","correlation methods;discrete Fourier transforms;linear codes;matrix algebra;mean square error methods","low-complexity analog linear coding scheme;finite-length data blocks;mean-square-error criterion;low-transmission power analog linear coding scheme;discrete Fourier transform;correlation matrix","","1","","7","","","","","IEEE","IEEE Journals & Magazines"
"Complexity and performance of QC-MDPC code-based McEliece cryptosystems","O. Al Rasheed; P. Ivaniš","School of Electrical Engineering, University of Belgrade, Bul. kralja Aleksandra 73, 11120, Serbia; School of Electrical Engineering, University of Belgrade, Bul. kralja Aleksandra 73, 11120, Serbia","2015 12th International Conference on Telecommunication in Modern Satellite, Cable and Broadcasting Services (TELSIKS)","","2015","","","31","34","In this paper, we analyze security of McEliece cryptosystem based on Low Density Parity Check (LDPC) and Moderate (MDPC) codes, as well as the complexity of the corresponding cryptosystem. Several approaches are proposed to improve the cryptosystem security for the case when complexity of cryptosystem has to be below the prescribed level. A certain modifications of Gradient Descent Bit Flipping (GDBF) decoding algorithm are identified as the crucial part of the cryptosystem that could provide a good trade-off between the complexity cost, decryption latency and security level.","","978-1-4673-7516-0978-1-4673-7515-3978-1-4673-7514","10.1109/TELSKS.2015.7357731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357731","Moderate Density Parity Check;Quasi-Cyclic;McEliece cryptosystem;GDBF","Decoding;Cryptography;Complexity theory;Iterative decoding;Sparse matrices","cryptography;decoding;parity check codes","security level;decryption latency;GDBF decoding;gradient descent bit flipping;MDPC codes;LDPC;low density parity check;QC-MDPC code-based McEliece cryptosystems","","","","15","","","","","IEEE","IEEE Conferences"
"Reduced complexity of decoding algorithm for irregular LDPC codes using Split Row method","R. E. Alami; C. B. Gueye; M. Mrabti; M. Boussetta; M. Zouak","LESSI, Facult&#x00E9; des Sciences Dhar El Mehraz, Fez, Morocco; LESSI, Facult&#x00E9; des Sciences Dhar El Mehraz, Fez, Morocco; Ecole Nationale des Sciences Appliqu&#x00E9;es - F&#x00E8;s, Fez, Morocco; SSC, Facult&#x00E9; des Sciences et Techniques, Fez, Morocco; SSC, Facult&#x00E9; des Sciences et Techniques, Fez, Morocco","2011 International Conference on Multimedia Computing and Systems","","2011","","","1","5","A reduced complexity LDPC decoding method for regular LDPC code is extended to irregular LDPC codes; we present in this paper a full description of this method and its benefits for various row weight and length code word. The Split-Row method makes column processing parallelism easier to exploit and significantly simplifies row processors. Simulation results over an additive white Gaussian channel show that the error performance of high row-weight codes with Split-Row decoding is within 0.3-0.5 dB of the Min-Sum algorithm.","","978-1-61284-732-0978-1-61284-730-6978-1-61284-731","10.1109/ICMCS.2011.5945639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945639","Low-density-parity-check codes;split row;min sum;bit error rate","Decoding;Iterative decoding;Complexity theory;Program processors;Simulation;Binary phase shift keying","AWGN channels;parity check codes","decoding algorithm;split row method;LDPC code;length code word;column processing parallelism;additive white Gaussian channel;complexity reduction","","","","13","","","","","IEEE","IEEE Conferences"
"A further study on the encoding complexity of quantum stabilizer codes","K. Kuo; C. Lu","National Tsing Hua University, Hsinchu 30013, Taiwan; National Tsing Hua University, Hsinchu 30013, Taiwan","2010 International Symposium On Information Theory & Its Applications","","2010","","","1041","1044","In this paper, we investigate the encoding complexity of binary quantum stabilizer codes. When doing the encoding through a “standard generator matrix”, a tight upper bound of the encoding complexity is derived in this paper to indicate that the encoding complexity decreases quadratically as the number r1of primary generators of the stabilizer group decreases. A class of equivalent transformations on stabilizer codes is explored to reduce the number r1of primary generators. The minimum possible r1is determined for several classes of optimal stabilizer codes of distance two or three and for some codes of length n ≤ 12. It appears that a code with large minimum distance will have large r1, reflecting high encoding complexity.","","978-1-4244-6017-5978-1-4244-6016","10.1109/ISITA.2010.5649496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5649496","Quantum error-correction codes;stabilizer codes;encoding complexity","Generators;Complexity theory;Logic gates;Linear code;Error correction codes;Upper bound","binary codes;computational complexity;quantum communication","encoding complexity;binary quantum stabilizer codes;standard generator matrix;upper bound;optimal stabilizer codes","","2","","12","","","","","IEEE","IEEE Conferences"
"Effects of the Generation Size and Overlap on Throughput and Complexity in Randomized Linear Network Coding","Y. Li; E. Soljanin; P. Spasojevic","WINLAB, the Department of Electrical and Computer Engineering, Rutgers University, North Brunswick, NJ, USA; Mathematics of Networking and Communication Department, Enabling Computing Technologies, Bell Laboratories, Alcatel-Lucent, Murray Hill; WINLAB, the Department of Electrical and Computer Engineering, Rutgers University, North Brunswick, NJ, USA","IEEE Transactions on Information Theory","","2011","57","2","1111","1123","To reduce computational complexity and delay in randomized network coded content distribution, and for some other practical reasons, coding is not performed simultaneously over all content blocks, but over much smaller, possibly overlapping subsets of these blocks, known as generations. A penalty of this strategy is throughput reduction. To analyze the throughput loss, we model coding over generations with random generation scheduling as a coupon collector's brotherhood problem. This model enables us to derive the expected number of coded packets needed for successful decoding of the entire content as well as the probability of decoding failure (the latter only when generations do not overlap) and further, to quantify the tradeoff between computational complexity and throughput. Interestingly, with a moderate increase in the generation size, throughput quickly approaches link capacity. Overlaps between generations can further improve throughput substantially for relatively small generation sizes.","0018-9448;1557-9654","","10.1109/TIT.2010.2095111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5695118","Coupon collector's problem;Network coding;rateless codes","Encoding;Decoding;Throughput;Peer to peer computing;Equations;Computational complexity","decoding;network coding","linear network coding;network coded content distribution;random generation scheduling;decoding;coupon collector problem","","46","","16","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity truncated Gray-coded bit plane matching based multiple candidate motion estimation","C. Choi; J. Jeong","Department of Electronics and Communication Engineering, Hanyang University, Seoul, Korea; Department of Electronics and Communication Engineering, Hanyang University, Seoul, Korea","3rd European Workshop on Visual Information Processing","","2011","","","73","76","In this paper, we propose a low complexity truncated Gray-coded (TGC) bit plane matching (BPM) based multiple candidate motion estimation algorithm. We can efficiently determine two best motion vectors according to the respective matching criteria and can enhance the overall motion estimation accuracy by exploiting almost the identical operations in two different matching error criteria. Experimental results show that the proposed algorithm achieves peak-to-peak signal-to-noise ratio (PSNR) gains about 0.63dB on average compared with the conventional 2BT-based motion estimation with negligible complexity increase.","","978-1-4577-0071-2978-1-4577-0072-9978-1-4577-0070","10.1109/EuVIP.2011.6045530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6045530","motion estimation;bit-wise matching;bit plane matching","Motion estimation;PSNR;Complexity theory;Accuracy;Algorithm design and analysis;Transforms;Containers","computational complexity;data compression;image matching;motion estimation;video coding","low complexity truncated gray-coded bit plane matching;multiple candidate motion estimation algorithm;motion vectors;peak-to-peak signal-to-noise ratio gains;2BT-based motion estimation;video compression;matching error criteria","","","","9","","","","","IEEE","IEEE Conferences"
"Partially-regular LDPC codes with linear encoding complexity and improved thresholds","D. K. Zigangirov; K. S. Zigangirov; D. J. Costello","Institute for Problems of Information Transmission, Moscow, B.Karetny per. 19, Russia; Institute for Problems of Information Transmission, Moscow, B.Karetny per. 19, Russia; Department of Electrical Engineering, University of Notre Dame, IN 46556, U.S.A.","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","528","532","We consider an ensemble of systematic low-density parity-check (LDPC) codes of length N with linear encoding complexity, i.e., with complexity O(N). We call these codes partially-regular, since they can be considered as modifications of regular LDPC codes. Further, their iterative decoding thresholds on the binary erasure channel (BEC) are found to be significantly better than the thresholds of the corresponding regular LDPC codes.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6034184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034184","","Equations;Encoding;Mathematical model;Complexity theory;Iterative decoding;Systematics","binary codes;channel coding;iterative decoding;linear codes;parity check codes","partially-regular LDPC codes;linear encoding complexity;systematic low-density parity-check codes;iterative decoding thresholds;binary erasure channel","","","","7","","","","","IEEE","IEEE Conferences"
"Modified greedy permutation algorithm for low complexity encoding in LDPC codes","B. Rajasekar; E. Logashanmugam","Faculty of Electrical and Electronics, Sathyabama University, Chennai, India; Faculty of Electrical and Electronics, Sathyabama University, Chennai, India","2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","","2014","","","336","339","The low-density parity-check (LDPC) codes are used to achieve excellent performance with low encoding and decoding complexity. One major criticism concerning LDPC codes has been their apparent high encoding complexity and memory inefficient nature due to large parity check matrix. More generally, we consider the encoding problem for codes specified by sparse parity-check matrices. We show how to exploit the sparseness of the parity-check matrix to obtain efficient encoders. A new technique for efficient encoding of LDP Codes based on the known concept of approximate lower triangulation (ALT) is introduced. The algorithm computes parity check symbols by solving a set of sparse equations, and the triangular factorization is employed to solve the equations efficiently. The key of the encoding method is to get the systematic approximate lower triangular (SALT) form of the Parity Check Matrix with minimum gap g, because the smaller the gap is, the more efficient encoding will be obtained. The functions are to be coded in MATLAB.","","978-1-4799-4190-2978-1-4799-4191-9978-1-4799-4193-3978-1-4799-4192","10.1109/ICCICCT.2014.6992981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992981","Encoding;ALT;SALT;LDPC;MATLAB","Parity check codes;Channel coding;Matrix converters;Complexity theory;Phase change materials;Sparse matrices","greedy algorithms;matrix decomposition;parity check codes;sparse matrices","modified greedy permutation algorithm;low complexity encoding;low density parity check codes;LDPC codes;encoding complexity;parity check matrix;approximate lower triangulation;parity check symbols;sparse equations;triangular factorization;systematic approximate lower triangular form;SALT form;MATLAB","","","","11","","","","","IEEE","IEEE Conferences"
"A reduced-complexity successive-cancellation decoder of polar codes","X. Yi; A. Liu; Q. Zhang; X. Liang","College of Communication Engineering, PLA University of Science and Technology, Nanjing, China; College of Communication Engineering, PLA University of Science and Technology, Nanjing, China; College of Communication Engineering, PLA University of Science and Technology, Nanjing, China; College of Communication Engineering, PLA University of Science and Technology, Nanjing, China","2017 3rd IEEE International Conference on Computer and Communications (ICCC)","","2017","","","1450","1454","Polar codes are the first provably capacity-achieving family of codes which have recently attracted more and more attention, especially in 5G network. The complexity of successive cancellation (SC) based decoders is O (N log N), where N is the blocklength of the code. Because the frozen bit is known to the transmitter and the receiver, many redundant computation done in the conventional SC algorithm. In this paper, we prune the computation of frozen bits, and as to non-frozen bit, we further utilize the property of butterfly architecture to update the likelihood ratio (LR) messages. Compared with the conventional SC decoder, the proposed method reduces the complexity without any loss of performance and the complexity of memory.","","978-1-5090-6352-9978-1-5090-6353","10.1109/CompComm.2017.8322782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322782","polar codes;successive-cancellation algorithm;reduced-complexity decoder;butterfly architecture","Decoding;Computational complexity;Computer architecture;Encoding;Iterative decoding;Transmitters","5G mobile communication;maximum likelihood decoding","polar codes;successive cancellation based decoders;redundant computation;conventional SC decoder;reduced-complexity successive-cancellation decoder;likelihood ratio messages;5G network","","","","10","","","","","IEEE","IEEE Conferences"
"Optimal bit allocation and efficient rate control for H64/AVCsed on general rate-istortion model and enhanced coding complexity measure","Z. Xie; Z. Bao; C. Xu; G. Zhang","Nantong University; Nantong University; Nantong University; Nantong University","IET Image Processing","","2010","4","3","172","183","A general rate-distortion (RD) model for block-based video coding is proposed in this study. Based on this model, an innovative coding characteristic called texture-complexity (TC), which is defined as a function of discrete cosine transform (DCT) coefficient distribution and distortion-quantisation (DQ) relationship as well as variance of residual block, is presented for rate control. Moreover, an adaptive linear TC forward-prediction model is established to estimate the characteristic of incoming image block, and a head bits prediction model as well as an optimal bits allocation method are also proposed to best allocate available bits. Experimental results show that the H.264 encoder (JM15.1), using the proposed rate control algorithm, achieves an objective quality improvement up to 0.434 dB, produces flatter bit-rate/PSNR curve and meets closer target bits budget than outcomes of JM15.1 rate control method (JVT-W042).","1751-9659;1751-9667","","10.1049/iet-ipr.2009.0014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5471812","","","computational complexity;discrete cosine transforms;rate distortion theory;video coding","optimal bit allocation;rate control;H.264-AVC;enhanced coding complexity measure;rate-distortion model;block-based video coding;texture-complexity;discrete cosine transform;coefficient distribution;distortion-quantisation;adaptive linear TC forward-prediction model","","","","","","","","","IET","IET Journals & Magazines"
"Low-Complexity Sphere Decoding of Polar Codes Based on Optimum Path Metric","K. Niu; K. Chen; J. Lin","Key Laboratory of Universal Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China","IEEE Communications Letters","","2014","18","2","332","335","Sphere decoding (SD) of polar codes is an efficient method to achieve the error performance of maximum likelihood (ML) decoding. But the complexity of the conventional sphere decoder is still high, where the candidates in a target sphere are enumerated and the radius is decreased gradually until no available candidate is in the sphere. In order to reduce the complexity of SD, a stack SD (SSD) algorithm with an efficient enumeration is proposed in this paper. Based on a novel path metric, SSD can effectively narrow the search range when enumerating the candidates within a sphere. The proposed metric follows an exact ML rule and takes the full usage of the whole received sequence. Furthermore, another very simple metric is provided as an approximation of the ML metric in the high signal-to-noise ratio regime. For short polar codes, simulation results over the additive white Gaussian noise channels show that the complexity of SSD based on the proposed metrics is up to 100 times lower than that of the conventional SD.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2014.010214.131826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6708139","Polar codes;successive cancellation decoding;sphere decoding;maximum likelihood rule","Measurement;Complexity theory;Vectors;Signal to noise ratio;Maximum likelihood decoding;Approximation methods","AWGN channels;maximum likelihood decoding","low-complexity sphere decoding;short polar codes;optimum path metric;maximum likelihood decoding;sphere decoder;stack SD algorithm;signal-to-noise ratio;additive white Gaussian noise channels","","22","","9","","","","","IEEE","IEEE Journals & Magazines"
"Systematic Physical-Layer Raptor Coding to Attain Low Decoding Complexity","G. Li; H. Wu; H. Lee; H. Wang; M. Lin","Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, Chang Gung University, Taoyuan, Taiwan; Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","IEEE Communications Letters","","2018","22","6","1124","1127","We study physical-layer raptor (PLR) coding for message blocks of moderate lengths to achieve low decoding complexity with only a slight degradation in throughput. Approaches include systematic PLR code construction, convergence analysis, and an early termination strategy that is only appropriate for the systematic PLR code.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2828857","Ministry of Science and Technology of Taiwan; MediaTek Inc., Hsin-chu, Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344108","Raptor code;LDPC code;LT code;complexity","Iterative decoding;Decoding;Systematics;Throughput;Convergence;Complexity theory","decoding;encoding","systematic physical-layer raptor coding;systematic PLR code construction;low decoding complexity;convergence analysis","","","","9","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Polar Code Decoder for HARQ Application","H. Lee; G. Liao","Dept. of Electrical Engineering, Chang Gung University, Taoyuan City, 33302, Taiwan; Dept. of Electrical Engineering, Chang Gung University, Taoyuan City, 33302, Taiwan","2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN)","","2018","","","512","514","In this work, an incremental decoding algorithm is proposed for the punctured polar codes. The incremental decoding is based on the successive cancellation (SC) decoding algorithm. This paper shows that when the punctured polar code is adopted in the HARQ (Hybrid Automatic Repeat Request) protocol, the incremental decoding algorithm can significantly reduce the complexity of the additional decoding attempt for the re-transmission. In addition, the reduction of complexity is not effected by the punctured patterns, and the proposed incremental decoding is suitable for all punctured polar codes.","2165-8536","978-1-5386-4646-5978-1-5386-4645-8978-1-5386-4647","10.1109/ICUFN.2018.8436599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8436599","","Decoding;Complexity theory;Encoding;Protocols;Error analysis;Urban areas;Information technology","automatic repeat request;channel coding;decoding","punctured polar code;low complexity polar code decoder;incremental decoding algorithm;successive cancellation decoding algorithm;HARQ protocol;Hybrid Automatic Repeat Request;SC decoding algorithm;SC","","","","11","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Integration-Based MAP SISO Detector for Channel Coded MIMO-OFDM Systems","H. Wang; D. Huang","NA; NA","2010 IEEE 71st Vehicular Technology Conference","","2010","","","1","5","In this paper, a low-complexity integration-based (LCIB) maximum a prior probability (MAP) soft-in-soft-out (SISO) detector for channel coded MIMO-OFDM systems is developed. Our developed LCIB MAP SISO detector uses novel low-complexity integration algorithms to efficiently compute the extrinsic information of the conventional optimal MAP SISO detector. As the simulation results, the iterative detection and decoding (IDD) receiver with our develop SISO detector shows almost the same BER performance and convergence behavior as compared to the IDD receiver with the conventional optimal MAP SISO detector. However, in practical applications the complexity of our developed SISO detector can be ten times less than that of the most widely used minimum mean square error-soft interference cancellation (MMSE-SIC) SISO detector.","1550-2252;1550-2252","978-1-4244-2518-1978-1-4244-2519","10.1109/VETECS.2010.5493715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493715","","Detectors;Maximum likelihood decoding;Iterative decoding;Bit error rate;Receiving antennas;Transmitting antennas;Iterative algorithms;Computational modeling;Convergence;Computer errors","","","","","","6","","","","","IEEE","IEEE Conferences"
"Low complexity multi-view distributed video coding based on JPEG","I. Mecimore; C. D. Creusere","New Mexico State University, Klipsch School of Electrical and Computer Engineering, Las Cruces, NM; New Mexico State University, Klipsch School of Electrical and Computer Engineering, Las Cruces, NM","2010 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI)","","2010","","","165","168","We consider here a low-complexity approach to the problem of multi-view distributed video coding. Specifically, we assume that motion-JPEG is used as the base coding algorithm and analyze the effects of factors such as additive noise and region misalignment on the conditional coding problem. Our goal is to characterize the achievable compression gain for a practical coding scenario where block-wise frame correlations must be estimated on-the-fly at the encoder in a highly efficient manner. We show that additive noise has a greater impact on the performance of a distributed video coder than pixel domain shifts.","","978-1-4244-7802-6978-1-4244-7801-9978-1-4244-7800","10.1109/SSIAI.2010.5483893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483893","Multi-view distributed video coding;Wyner-Ziv coding;low-complexity video coding;JPEG;wireless video sensor networks","Video coding;Decoding;Image coding;Discrete cosine transforms;Video compression;Transform coding;Additive noise;Wireless sensor networks;Computational complexity;Base stations","video coding","multiview distributed video coding;motion JPEG;base coding algorithm;block wise frame correlations","","","","15","","","","","IEEE","IEEE Conferences"
"Low complexity image rectification for multi-view video coding","M. Choi; J. Kim; W. Cho; Y. Chung","Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Rep. of Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Rep. of Korea","2012 IEEE International Symposium on Circuits and Systems","","2012","","","381","384","Image rectification is one of the most complex pre-processing techniques for multi-view video coding (MVC). It corrects image distortion and simplifies MVC matching operations by transforming images into a standard coordinate system. Therefore, the rectification can improve both image quality and coding efficiency for MVC. In this paper, we propose two low complexity image rectification schemes for MVC: view-selective preprocessing rectification and view-selective pre- and post-processing rectification. The proposed algorithm does not perform frames of all views but few selected views which do not affect MVC performances. The experimental results show that the proposed rectification schemes feature much lower complexity without degrading image quality than the existing approach.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6272042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6272042","","Cameras;Complexity theory;Estimation;Encoding;Matrix decomposition;PSNR;Optical imaging","image matching;video coding","low complexity image rectification;multiview video coding;complex pre-processing techniques;image distortion;MVC matching operations;standard coordinate system;image quality;coding efficiency;view-selective preprocessing rectification;view-selective post-processing rectification","","3","","8","","","","","IEEE","IEEE Conferences"
"A low-complexity mapping optimization for bit-interleaved coded modulation with iterative decoding and 8PSK","Can Zhang; Chen Cheng; Jing Dai","School of Electronic, Electrical and Communication Engineering, University of Chinese, Academy of Sciences, Beijing 101408, China; School of Electronic, Electrical and Communication Engineering, University of Chinese, Academy of Sciences, Beijing 101408, China; School of Electronic, Electrical and Communication Engineering, University of Chinese, Academy of Sciences, Beijing 101408, China","2013 15th IEEE International Conference on Communication Technology","","2013","","","93","97","Bit-interleaved coded modulation with iterative decoding (BICM-ID) is investigated for bandwidth efficient transmissions, where the error performance can be improved by employing a suitable symbol mapping. This paper first introduces a low-complexity regular mapping optimization for BICM-ID and 8PSK, for the purpose of finding the optimal mapping. Furthermore, the irregular mapping optimization is proposed for a closer approach to the channel capacity. The extrinsic information transfer (EXIT) chart analysis is aided for the proposed optimization algorithms to provide design guidelines of mappings. The bit error rate (BER) results demonstrate that the BICM-ID system with the proposed optimal irregular mapping outperforms the other mappings used for comparison, and yields about 0.5dB away from the channel capacity over the additive white Gaussian noise (AWGN) channel.","","978-1-4799-0077","10.1109/ICCT.2013.6820353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820353","Bit-interleaved coded modulation with iterative decoding (BICM-ID);Mapping optimization;Irregular mapping;Extrinsic information transfer (EXIT) chart","Optimization;Modulation;Iterative decoding;Decoding;Doping;Bit error rate;AWGN channels","AWGN channels;channel capacity;channel coding;error statistics;interleaved codes;iterative decoding;modulation coding;optimisation;phase shift keying","bit-interleaved coded modulation;iterative decoding;8PSK;bandwidth efficient transmissions;error performance;symbol mapping;low-complexity regular mapping optimization;channel capacity;extrinsic information transfer chart analysis;EXIT chart analysis;bit error rate;BER;BICM-ID system;optimal irregular mapping;additive white Gaussian noise channel;AWGN channel","","","","13","","","","","IEEE","IEEE Conferences"
"A Novel Algorithm to Decrease the Computational Complexity of HEVC Intra Coding","M. Zhang; H. Zhang; Z. Liu","NA; NA; NA","2016 Data Compression Conference (DCC)","","2016","","","639","639","To satisfy the rapidly increasing demands for high resolution video, the Joint Collaborative Team on Video Coding (JCT-VC) developed a new video coding standard, i.e. High-efficiency Video Coding (HEVC). With the same perceptual video quality as H.264, HEVC can achieve a bit rate reduction of approximately 50%. However, the quad tree structured coding units (CUs) cause high computational complexity. In this paper, a novel algorithm is proposed to decrease the computational complexity of HEVC intra coding. We mainly use the total coding bits of CUs as threshold to terminate CUs' partition. Furthermore, we also consider the spatial correlation between the current LCU and its neighbor LCUs to predict the current largest CU minimum depth so that we can skip some certain CUs' rate distortion cost checking.","1068-0314","978-1-5090-1853-6978-1-5090-1854","10.1109/DCC.2016.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7786263","","Copper;Encoding;Computational complexity;Data compression","computational complexity;correlation methods;quadtrees;video coding","computational complexity;HEVC intra coding;joint collaborative team-on-video coding;JCT-VC;high-efficiency video coding;bit rate reduction;H.264;quad tree structured coding units;spatial correlation;largest CU minimum depth prediction;CU rate distortion cost checking","","","","","","","","","IEEE","IEEE Conferences"
"120-GBaud coded 8 dimensional 16QAM WDM transmission using low-complexity iterative decoding based on bit-wise log likelihood ratio","M. Nakamura; F. Hamaoka; A. Matsushita; H. Yamazaki; M. Nagatani; A. Sano; A. Hirano; Y. Miyamoto","NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan; NTT Network Innovation Labs., NTT Corporation, 1-1 Hikari-no-oka, Yokosuka, Kanagawa, Japan","2017 Optical Fiber Communications Conference and Exhibition (OFC)","","2017","","","1","3","We propose a low-complexity decoding scheme that is 8.68e-4 times that of conventional optimal decoding for 8D-16QAM. Experiments confirm that the proposed scheme allows 9-WDM 600-Gbps/ch transmission over 3,500km without penalty.","","978-1-9435-8023-1978-1-5090-6229","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937454","","Decoding;Modulation;Iterative decoding;Complexity theory;Q-factor;Wavelength division multiplexing","communication complexity;iterative decoding;modulation coding;optical modulation;quadrature amplitude modulation;wavelength division multiplexing","coded 8 dimensional 16QAM WDM transmission;low-complexity iterative decoding;Bit-Wise Log Likelihood Ratio;bit rate 600 Gbit/s","","","","23","","","","","IEEE","IEEE Conferences"
"The reduction of linear receivers complexity in ovelapped Alamouti code family","M. Shahabinejad; S. S. H. Bidaki; S. Talebi; S. Abasi","Department of Electrical Engineering Shahid Bahonar University of Kerman, Kerman, Iran; Department of Electrical Engineering Shahid Bahonar University of Kerman, Kerman, Iran; Department of Electrical Engineering Shahid Bahonar University of Kerman, Kerman, Iran; Department of Electrical Engineering Shahid Bahonar University of Kerman, Kerman, Iran","2011 19th Iranian Conference on Electrical Engineering","","2011","","","1","5","Linear receivers, due to their simpler decoding in comparison with maximum likelihood (ML) receiver, are of great importance. In recent years, the codes such as Toeplitz codes, overlapped Alamouti codes (OACs), and group orthogonal Toeplitz codes (GOTCs) are designed based on the linear receivers decoding. This paper completely explains the zero-forcing (ZF) and the minimum mean square error (MMSE) receivers of the OACs for different numbers of transmit antennas and different numbers of used symbols in each space-time matrix code. It also proposes a method that decreases the complexity of the ZF and the MMSE linear receivers of the OACs without sacrificing the system performance. It is explained that the proposed simplicity method especially gets very effective by increasing the symbol transmission rate.","2164-7054","978-964-463-428-4978-1-4577-0730","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5955437","multi-input multi-output (MIMO);linear receivers;space-time coding;overlapped Alamouti code;zero-forcing (ZF);minimum mean square error (MMSE)","Complexity theory;Simulation;Maximum likelihood decoding;Receiving antennas;Equalizers","","","","","","12","","","","","IEEE","IEEE Conferences"
"Complexity reduction of depth intra coding for 3D video extension of HEVC","T. L. da Silva; L. V. Agostini; L. A. da Silva Cruz","Instituto de Telecomunicações - Department of Electrical and Computer Engineering, University of Coimbra, Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal; Center of Technological Development, Federal University of Pelotas, Campus Porto, Federal University of Pelotas, Pelotas, Brasil; Instituto de Telecomunicações - Department of Electrical and Computer Engineering, University of Coimbra, Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal","2014 IEEE Visual Communications and Image Processing Conference","","2014","","","229","232","Three dimensional (3D) video technology, systems and applications such as 3D television and freeviewpoint television (FTV) broadcasts require efficient encoding of video information. To fill that need a 3D video extension of High Efficiency Video Coding standard, called 3D-HEVC, is being developed. This extension is based on Multiview Video plus Depth (MVD) format, which associates a depth information to each texture frame of each view. This paper presents a method to accelerate the intra coding of these depth maps to reduce the 3D-HEVC computational complexity. The proposed algorithm exploits the edge orientation of the depth blocks to reduce the number of modes to be evaluated in the intra mode decision. In addition, the correlation between the Planar mode choice and the most probable modes (MPMs) selected is also exploited, to accelerate the depth intra coding. The experimental results show that the proposed algorithm achieves an average complexity reduction of 15% on depth information encoding, with a small degradation in encoding efficiency (BD-rate increase of 0.16% on average).","","978-1-4799-6139","10.1109/VCIP.2014.7051546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051546","Intra coding;depth maps;3D-HEVC;complexity reduction;edge orientation","Encoding;Three-dimensional displays;Complexity theory;Video coding;Standards;Transform coding;Image coding","computational complexity;decision theory;encoding;image texture;stereo image processing;television broadcasting;three-dimensional television;video coding","complexity reduction;depth intracoding;3D video extension;3D video technology;3D television;freeviewpoint television broadcasts;FTV broadcasts;video information;high efficiency video coding standard;3D-HEVC computational complexity;multiview video plus depth format;MVD format;texture frame;depth maps;edge orientation;intramode decision;most probable modes;MPM;depth information encoding","","5","","11","","","","","IEEE","IEEE Conferences"
"A Full-Rate Full-Diversity 2x2 Space-Time Block Code with Linear Complexity for the Maximum Likelihood Receiver","S. S. H. Bidaki; S. Talebi; M. Shahabinejad","Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran, and the Advanced Communications Research Institute, Sharif University, Tehran, Iran; Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran, and the Advanced Communications Research Institute, Sharif University, Tehran, Iran; Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran, and the Advanced Communications Research Institute, Sharif University, Tehran, Iran","IEEE Communications Letters","","2011","15","8","842","844","From engineering point of view, the simplicity of the decoder is a very important factor in designing different space-time block codes (STBCs) in wireless communications systems. On the other hand, full-diversity is an important factor which causes STBCs to have valid performance in terms of bit error rate (BER). High symbol transmission rate is also a significant factor in designing STBCs. Regarding these three characters, this paper proposes a full-rate STBC which benefits from a linear complexity for the optimum receiver and full-diversity properties. This code is designed for two transmit antennas and the complexity of its optimum receiver is linear for any arbitrary number of receive antennas. Simulation results also show that the performance of the proposed code is close to the performance of the Golden code when 4-QAM constellation is considered, and it effectively outperforms the Golden code when BPSK constellation is employed.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2011.061011.110887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5910653","Full-diversity;full-rate;multi input-multi output (MIMO);space-time block code (STBC);linear complexity","Complexity theory;Decoding;Receiving antennas;Binary phase shift keying;Block codes;Fading","decoding;diversity reception;error statistics;Gold codes;maximum likelihood estimation;phase shift keying;quadrature amplitude modulation;radio receivers;receiving antennas;space-time block codes;transmitting antennas","full-rate full-diversity space-time block code;maximum likelihood receiver;wireless communications systems;decoder;bit error rate;BER;high symbol transmission rate;transmit antennas;optimum receiver;receive antennas;Golden code;4-QAM constellation;BPSK constellation","","9","","13","","","","","IEEE","IEEE Journals & Magazines"
"Performance and Complexity of 32 k-bit Binary LDPC Codes for Magnetic Recording Channels","S. Jeon; B. V. K. Vijaya Kumar","Data Storage Systems Center (DSSC),, Carnegie Mellon University,, Pittsburgh,, PA, USA; Data Storage Systems Center (DSSC), Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Magnetics","","2010","46","6","2244","2247","Recently, 32 k-bit sector size for hard disk drives is being investigated to take advantage of the superior performance of long error correcting codes. Meanwhile, low-density parity-check (LDPC) codes have been actively investigated for obtaining coding gains over conventional Reed-Solomon (RS) codes mainly for 4 k-bit sectors. In this paper, the coding gain of a 32 k-bit LDPC code over a 4 k-bit LDPC code, a 32 k-bit RS code, and a 4 k-bit RS code in magnetic recording channels is investigated. The decoding complexity of 32 k-bit LDPC codes and 4 k-bit LDPC codes is also discussed. It is important to evaluate whether the coding gains are enough to justify the increased complexity. Using the 32 k-bit LDPC code, 0.8-dB gain over the 32 k-bit RS code or the 4 k-bit LDPC code (the two schemes coincidentally showed similar performance) at 32 k-bit block error rate (BLER) 10<sup>-3</sup>, and 1.6-dB gain over the 4 k-bit RS code were obtained. It is shown that 32 k-bit LDPC codes require a larger number of iterations than the 4 k-bit LDPC codes. It is also shown that there is much room to improve the design of 32 k-bit LDPC codes than the code used in the simulation. To illustrate the potential, quasicyclic LDPC codes with column weights up to 13 with girth 6 are investigated.","0018-9464;1941-0069","","10.1109/TMAG.2010.2043067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5467496","Low-density parity-check (LDPC) code;magnetic recording channel;4 k-byte sector","Parity check codes;Magnetic recording;Error correction codes;Decoding;Reed-Solomon codes;Performance gain;Concatenated codes;Data storage systems;Decision support systems;USA Councils","binary codes;error correction codes;hard discs;magnetic recording;parity check codes","binary LDPC codes;magnetic recording channels;hard disk drives;long error correcting codes;low-density parity-check codes;conventional Reed-Solomon codes;coding gains;decoding complexity;block error rate;quasicyclic LDPC codes;column weights","","16","","9","","","","","IEEE","IEEE Journals & Magazines"
"Shannon information entropy as complexity metric of source code","M. Cholewa","Institute of Computer Science, University of Silesia, Sosnowiec, Poland","2017 MIXDES - 24th International Conference "Mixed Design of Integrated Circuits and Systems","","2017","","","468","471","This paper describes method, which allows comparing complexity of two or more source codes written in any programming language. The method is suitable to get the knowledge which programming language more compactly describes a given algorithm. In experiments carried out popular quick sort algorithm was analyzed. This algorithm was written in 10 most popular program languages and then complexity of source codes can be compared. The complexity is calculated using the Shannon entropy formula with statistics of syntactic units (e.g. keywords, literals, operators, etc.). All complexities was compared and discovered the source code (and hence language that generated this code) with lower entropy as optimal. The optimal source code will have smaller number of keywords, declarations, etc. compared to not optimized code. In practice, proposed approach allows to use another syntactic units (e.g. use more operators and less identifiers) to build an optimal source code according to minimization entropy rule. Proposed method is universal and can be applied to analysis of any source codes of computer programs. It should be noted that proposed measure does not take into consideration semantics of a programming language.","","978-83-63578-12-1978-83-63578-11-4978-1-5090-6486","10.23919/MIXDES.2017.8005255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8005255","Shannon entropy;software metric;complexity","Integrated circuits","information theory;programming languages;source code (software)","Shannon information entropy;complexity metric;programming language;quick sort algorithm;Shannon entropy formula;optimal source code;optimized code;minimization entropy rule","","","","13","","","","","IEEE","IEEE Conferences"
"Split Table Extension: A Low Complexity LVQ Extension Scheme in Low Bitrate Audio Coding","J. Wang; P. Liu; J. Kong; R. Ying","NA; NA; NA; NA","IEEE Signal Processing Letters","","2010","17","1","59","62","Embedded algebraic vector quantization (EAVQ) is a fast and efficient lattice vector quantization (LVQ) scheme used in low-bitrate audio coding. However, a defect of EAVQ is the overload distortion which causes unpleasant noises in audio coding. To solve this problem, specific base codebook extension schemes should be carefully considered. In this letter, we present a novel EAVQ codebook extension scheme-split table extension (STE), which splits a vector into two smaller vectors: one in the base codebook and the other in the split table. The base codebook and the split table are designed according to the appearance probability of quantized vectors in audio segments. Experiments on encoding multiple audio and speech sequences show that, compared with the existed Voronoi extension scheme, STE greatly reduces computation complexity and storage requirement while achieving similar coding quality.","1070-9908;1558-2361","","10.1109/LSP.2009.2032982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5256281","Embedded algebraic vector quantization;lattice vector quantization;split table extension;Voronoi extension","Bit rate;Audio coding;Lattices;Vector quantization;Encoding;Speech coding;Video coding;Algorithm design and analysis;Codecs;Code standards","algebraic codes;audio coding;computational complexity;lattice theory;vector quantisation","split table extension;audio coding;embedded algebraic vector quantization;lattice vector quantization;codebook extension;Voronoi extension;computation complexity;low complexity LVQ extension scheme;speech sequences;audio sequences","","1","","9","","","","","IEEE","IEEE Journals & Magazines"
"Complexity reduced turbo-bit-interleaved coded modulation with iterative decoding","D. Kang; W. Oh","Chungnam National University, Republic of Korea; Chungnam National University, Republic of Korea","IET Communications","","2015","9","8","1076","1080","Spectral efficiency of turbo code can be improved by use of coded modulation methods such as bit-interleaved coded modulation (BICM) with iterative decoding (BICM-ID). On the other hand, the performance of the turbo-BICM can be further improved by iteratively exchanging information between a decoder and a demodulator within a receiver and this scheme is known as the turbo-BICM-ID. Unfortunately, the required complexity of the turbo-BICM-ID is much higher than that of the turbo bit-interleaved coded modulation with iterative decoding turbo-BICM-ID. In this study, the authors propose a complexity reduced turbo-BICM-ID by optimising the mapping of coded bits to modulation symbols and the amount of information fed back from the decoder to the demodulator.","1751-8628;1751-8636","","10.1049/iet-com.2014.0633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102932","","","demodulators;interleaved codes;iterative decoding;modulation coding;turbo codes","turbo-bit-interleaved coded modulation with iterative decoding;spectral efficiency;complexity reduced turbo-BICM-ID;decoder;demodulator;coded bit mapping;modulation symbol","","1","","12","","","","","IET","IET Journals & Magazines"
"A novel high rate transmission scheme for space time coding with low decoding complexity","Y. Yan; X. Jiang; L. Jun; D. Wei; T. C. Shin; M. H. Lee","School of Mechanical and Electric Engineering, Guangzhou University, 51006, China; School of Information Science and Technology, Donghua University, Shanghai, 201620, China; Dept. Electric and Computer Engineering, Chonbuk National University, Korea; Dept. Electric and Computer Engineering, Chonbuk National University, Korea; Dept. Electric and Computer Engineering, Chonbuk National University, Korea; Dept. Electric and Computer Engineering, Chonbuk National University, Korea","2012 IEEE International Symposium on Circuits and Systems","","2012","","","1748","1751","In this paper, we propose a new method to transmit one more information bit by using orthogonal Space-Time Block Codes (STBC) for 4 antennas. Using two different STBCs matrices transmit one additional bit to achieve high rate-9/8. To maintain full rank and full diversity for the coding gain matrix, a new STBC code with full rate and full diversity is proposed in this letter. In order to implement a fast Maximum-likelihood (ML) decoding, a property of Frobenius norms of received signals is considered to reduce its complexity by checking the non-null values of the Frobenius at the transmitter side. Simulation results show that this method achieves better bit error rate (BER) performance and throughputs in the high SNR region without losing diversity gain.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6271601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271601","Antennas;STBC;ML;Frobenius;High Rate","Encoding;Decoding;Complexity theory;Transmitting antennas;Bit error rate;Signal to noise ratio;Throughput","","","","1","","10","","","","","IEEE","IEEE Conferences"
"Low-Complexity, Low-PAPR Polarization-Time Code for PDL Mitigation","T. Oyama; G. Huang; H. Nakashima; Y. Nomura; T. Takahara; T. Hoshida","Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan; Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan; Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan; Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan; Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan; Fujitsu Laboratories Ltd., 1-1 Kamikodanaka 4-chome, Nakahara-ku, Kawasaki, 211-8588, Japan","2019 Optical Fiber Communications Conference and Exhibition (OFC)","","2019","","","1","3","We apply a polarization-time (PT) code based on number theory for PDL mitigation. The PT code has comparable PDL mitigation performance to the best PT codes reported so far, and higher tolerance to fiber nonlinearities.","","978-1-943580-53-8978-1-7281-3620","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8696686","","Peak to average power ratio;Silver;Demodulation;Phase shift keying;Q-factor;Optical fiber polarization;Optical noise","optical fibre communication;optical fibre losses;optical fibre polarisation","PT code;low-PAPR polarization-time code;number theory;low-complexity polarization-time code;PDL mitigation performance;fiber nonlinearity;polarization dependent loss;peak-average power ratio","","","","8","","","","","IEEE","IEEE Conferences"
"A low-complexity decoding algorithm for RA code with Chebyshev Polynomial Fitting","Lei Li; Qin Wang; Li Xu","School of Information Engineering, University of Science and Technology Beijing, China; School of Information Engineering, University of Science and Technology Beijing, China; School of Information Engineering, University of Science and Technology Beijing, China","2010 3rd International Conference on Computer Science and Information Technology","","2010","5","","421","425","RA (Repeat Accumulate) code has been widely used in wireless communication systems. But the BP (Belief Propagation) decoding algorithm which is commonly used in RA Code has high complexity. In order to reduce the complexity of BP algorithm while maintaining coding performance, we proposed Chebyshev-Polynomial-Fitting based BP (CPF-BP) decoding algorithm. In CPF-BP decoding algorithm, we used polynomial which is generated by chebyshev fitting algorithm instead of hyperbolic tangent operation in BP decoding algorithm to reduce the implementation complexity. Experiment and simulation results show that this low-complexity algorithm removes hyperbolic tangent operation in conventional BP algorithm with same amount of multiplier under the parameters setting in the paper. The performance degradation of CPF-BP algorithm is negligible which is less than 0.6dB at BER of 10-4compared with conventional BP algorithm.","","978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539","10.1109/ICCSIT.2010.5564075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564075","Repeat Accumulate Code;Polynomial Fitting;Chebyshev;BP algorithm","AWGN;Bit error rate;Decoding;Polynomials;Chebyshev approximation;Approximation algorithms","Chebyshev approximation;codes;curve fitting;decoding;polynomial approximation;radiocommunication","RA code;Chebyshev polynomial fitting;repeat accumulate code;wireless communication system;belief propagation decoding algorithm;hyperbolic tangent operation","","","","8","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Analog Linear Coding Scheme for Transmitting Asymptotically WSS AR Sources","J. Gutiérrez-Gutiérrez; F. M. Villar-Rosety; M. Zárraga-Rodríguez; X. Insausti","Biomedical Engineering and Sciences Department, TECNUN (University of Navarra), San Sebastián, Spain; Biomedical Engineering and Sciences Department, TECNUN (University of Navarra), San Sebastián, Spain; Biomedical Engineering and Sciences Department, TECNUN (University of Navarra), San Sebastián, Spain; Biomedical Engineering and Sciences Department, TECNUN (University of Navarra), San Sebastián, Spain","IEEE Communications Letters","","2019","23","5","773","776","In this letter, we give a low-complexity analog linear coding scheme for transmitting finite-length data blocks of asymptotically wide sense stationary (AWSS) autoregressive (AR) sources through additive white Gaussian noise (AWGN) channels.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2019.2903799","Spanish Ministry of Economy and Competitiveness through the CARMEN project (TEC2016-75067-C4-3-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663332","Low-complexity;analog linear coding;finite-length data block;AR sources","Encoding;Decoding;AWGN channels;Computational complexity;Distortion;Manganese","autoregressive processes;AWGN channels;channel coding;linear codes","transmitting asymptotically WSS AR sources;finite-length data blocks;asymptotically wide sense stationary autoregressive sources;analog linear coding scheme;additive white Gaussian noise;AWGN channels","","","","14","","","","","IEEE","IEEE Journals & Magazines"
"A low complexity physical-layer network coding scheme for cellular two-way relaying systems","Z. Fang; Z. Wang; S. Zhang; J. Shi","Faculty of Electronic and Information Engineering, Zhejiang Wanli University, Ningbo, China; Faculty of Electronic and Information Engineering, Zhejiang Wanli University, Ningbo, China; Faculty of Electronic and Information Engineering, Zhejiang Wanli University, Ningbo, China; Faculty of Electronic and Information Engineering, Zhejiang Wanli University, Ningbo, China","2014 23rd Wireless and Optical Communication Conference (WOCC)","","2014","","","1","5","In this paper, we consider transceiver design for multiuser cellular two-way relay network (cTWRC), where a multi-antenna base station (BS) exchanges information with multiple single-antenna mobile stations (MSs) with the help of a multi-antenna relay station (RS). We propose a low complexity two-way relaying scheme for cTWRC via signal space alignment. In our scheme, the precoders at the BS and MSs are jointly designed that the two data streams delivered to and from the same MS fall in the same spatial direction at the RS. With such signal space alignment, interference-free physical-layer network coding can be performed at the relay. We analyze the end-to-end sum bit error rate (BER) performance of the proposed scheme and investigate the optimal power allocation at the BS and RS to improve the BER performance. Numerical results demonstrate that our scheme is superior to the existing amplify-and-forward based signal alignment schemes.","2379-1268;2379-1276","978-1-4799-5249","10.1109/WOCC.2014.6839962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839962","Cellular two-way relaying;signal space alignment;physical-layer network coding;power allocation","Relays;Bit error rate;Resource management;Network coding;Signal to noise ratio;Upper bound;Vectors","amplify and forward communication;antenna arrays;cellular radio;codecs;error statistics;mobile antennas;network coding;precoding;radio transceivers","low complexity physical-layer network coding scheme;cellular two-way relaying systems;transceiver design;multiuser cellular two-way relay network;cTWRC;multiantenna base station exchanges information;multiple single-antenna mobile stations;multiantenna relay station;signal space alignment;two-way relaying scheme;precoders;data streams;signal space alignment;interference-free physical-layer network coding;end-to-end sum bit error rate;optimal power allocation;BER performance;amplify-and-forward based signal alignment schemes","","3","","16","","","","","IEEE","IEEE Conferences"
"Fast CU size decision based on texture complexity for HEVC intra coding","Jiangpeng Hou; Dongmei Li; Zhaohui Li; Xiuhua Jiang","Communication University of China, Beijing, China; Communication University of China, Beijing, China; Communication University of China, Beijing, China; Communication University of China, Beijing, China","Proceedings 2013 International Conference on Mechatronic Sciences, Electric Engineering and Computer (MEC)","","2013","","","1096","1099","The quad-tree based CU partition scheme is one of the new methods proposed in HEVC to achieve higher encoding efficiency at a cost of high computational complexity. In this paper, we proposed a fast coding unit (CU) size decision algorithm for HEVC intra coding. In our method, the complexity of the current CU and its neighbors are firstly analyzed. Since the neighboring CU usually has similar textures, it is possible to employ the complexity of the neighboring CU as a threshold to skip some useless CU size for the current CU partitioning. Experimental results show that our proposed method can speed up original HEVC intra coding by averagely 23.667% with a negligible drop in encoding performance for different video sequences.","","978-1-4799-2565-0978-1-4799-2564-3978-1-4799-2563","10.1109/MEC.2013.6885226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885226","HEVC;intra coding;quad-tree coding;CU splitting;texture complexity","Encoding;Partitioning algorithms;Video coding;Video sequences;Prediction algorithms;Computational complexity","computational complexity;quadtrees;video coding","fast CU size decision;texture complexity;HEVC intra coding;quad-tree based CU partition scheme;computational complexity;fast coding unit size decision algorithm;video sequences;high efficiency video coding","","","","7","","","","","IEEE","IEEE Conferences"
"An efficient methodology for parallel concatenation of LDPC codes with reduced complexity and decoding delay","R. P. Kumar; R. S. Kshetrimayum","Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, 781039, India; Department of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, 781039, India","2013 National Conference on Communications (NCC)","","2013","","","1","5","Parallel concatenated Gallager codes (PCGC) is a class of concatenated codes based on LDPC component codes. Conventional attempts for concatenating LDPC in parallel using the well known ‘Turbo code structure’ without interleavers have not been widely successful, because of its performance limitation and decoding delay. In this paper we present an efficient methodology for parallel concatenation of LDPC codes. We show that proposed methodology for PCGC outperforms existing PCGC in terms of BER performance in both AWGN and flat fading Rayleigh channels. We also present that proposed PCGC outperforms existing PCGC in terms of complexity as well as decoding delay.","","978-1-4673-5952-8978-1-4673-5950-4978-1-4673-5951","10.1109/NCC.2013.6487947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487947","LDPC;PCGC;Turbo-decoder;SPA;Min-Sum;LLR","Decoding;Parity check codes;Bit error rate;Complexity theory;Delays;Signal to noise ratio;Fading","","","","2","","6","","","","","IEEE","IEEE Conferences"
"Low-Complexity Compression Method for Hyperspectral Images Based on Distributed Source Coding","X. Pan; R. Liu; X. Lv","School of Electronic and Information Engineering, Beihang University, Beijing , China; School of Electronic and Information Engineering, Beihang University, Beijing , China; School of Electronic and Information Engineering, Beihang University, Beijing , China","IEEE Geoscience and Remote Sensing Letters","","2012","9","2","224","227","In this letter, we propose a low-complexity discrete cosine transform (DCT)-based distributed source coding (DSC) scheme for hyperspectral images. First, the DCT was applied to the hyperspectral images. Then, set-partitioning-based approach was utilized to reorganize DCT coefficients into waveletlike tree structure and extract the sign, refinement, and significance bitplanes. Third, low-density parity-check-based Slepian-Wolf (SW) coder was adopted to implement the DSC strategy. Finally, an auxiliary reconstruction method was employed to improve the reconstruction quality. Experimental results on Airborne Visible/Infrared Imaging Spectrometer data set show that the proposed paradigm significantly outperforms the DSC-based coder in wavelet transform domain (set partitioning in hierarchical tree with SW coding), and its performance is comparable to that of the DSC scheme based on informed quantization at low bit rate.","1545-598X;1558-0571","","10.1109/LGRS.2011.2165271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029964","Auxiliary reconstruction;discrete cosine transform (DCT);distributed source coding (DSC);hyperspectral images","Image coding;Image reconstruction;Discrete cosine transforms;Hyperspectral imaging;Quantization;Decoding","data compression;discrete cosine transforms;feature extraction;image coding;image reconstruction;parity check codes;quantisation (signal);trees (mathematics);wavelet transforms","low-complexity compression method;hyperspectral image;distributed source coding;discrete cosine transform;set-partitioning-based approach;wavelet-like tree structure;sign extraction;refinement extraction;significance bitplane extraction;parity-check-based Slepian-Wolf coder;auxiliary reconstruction method;reconstruction quality;airborne visible-infrared imaging spectrometer data;quantization;wavelet transform domain","","25","","12","","","","","IEEE","IEEE Journals & Magazines"
"Low Decoding Complexity of LDPC Codes Over the Binary Erasure Channel","S. M. Ivari; M. R. Soleymani; Y. R. Shayan","Electrical & Computer Engineering, Concordia University, Montreal, Quebec, Canada; Electrical & Computer Engineering, Concordia University, Montreal, Quebec, Canada; Electrical & Computer Engineering, Concordia University, Montreal, Quebec, Canada","Electrical Engineering (ICEE), Iranian Conference on","","2018","","","505","509","In this paper, we present a new low complexity decoding algorithm outperforming existing schemes. When the Belief Propagation fails and cannot solve all the erased bits, the guessing algorithm makes assumptions on a set of erased bits. However, in our new algorithm instead of guessing the values of a set of message bits, the decoder selects a set of check nodes and makes assumption on bits connected to them. The number of possibilities is reduced by half because the field is binary resulting in lower decoding complexity. The proposed decoding algorithm is applied to two regular half rate LDPC codes with lengths of 1000 and 2000. The theoretical belief propagation (BP) threshold for these two codes is 0.429. It is shown that the actual BP threshold is improved using the new algorithm. Setting the number of possibilities to two, we achieve a threshold of 0.43 which is higher than the BP theoretical.","","978-1-5386-4916-9978-1-5386-4914-5978-1-5386-4915-2978-1-5386-4917","10.1109/ICEE.2018.8472553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472553","component;LDPC codes;BEC;Belief Propagation;Guessing","Complexity theory;Maximum likelihood decoding;Iterative decoding;Belief propagation;Approximation algorithms","channel coding;decoding;parity check codes","low complexity decoding algorithm;guessing algorithm;message bits;regular half rate LDPC codes;belief propagation;BP threshold;binary erasure channel coding","","","","15","","","","","IEEE","IEEE Conferences"
"Low Complexity Sequential Estimation Scheme for Pseudo Noise Code Acquisition","J. Baek; D. Chong; J. Kim; Y. Lee; S. G. Kang; S. Yoon","NA; NA; NA; NA; NA; NA","2011 IEEE Vehicular Technology Conference (VTC Fall)","","2011","","","1","4","In this paper, a low complexity sequential estimation scheme for pseudo noise (PN) code acquisition is proposed, which can not only achieve code acquisition for both inverted and non-inverted PN codes, but also demodulate the spreading data. Numerical results show that the proposed scheme has approximately half the complexity of the conventional scheme while exhibiting a similar mean acquisition time performance to that of the conventional scheme.","1090-3038;1090-3038;1090-3038","978-1-4244-8327-3978-1-4244-8328-0978-1-4244-8325-9978-1-4244-8326","10.1109/VETECF.2011.6093241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093241","","Estimation;Complexity theory;Correlation;Hardware;Polynomials;Loading;Receivers","communication complexity;pseudonoise codes;spread spectrum communication","low complexity sequential estimation scheme;pseudo noise code acquisition;inverted PN code;noninverted PN code;mean acquisition time","","","","11","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Decoding Algorithm for RaptorQ Code","R. Zhang; Y. Kou","NA; NA","2014 Ninth International Conference on Broadband and Wireless Computing, Communication and Applications","","2014","","","145","150","RaptorQ code, as an advanced Raptor code with great flexibility and fine reception performance, has been widely adopted in international standards. Extensive research has been focused on the design and implementation of efficient decoding algorithms for Raptor codes. In this paper, a new low-complexity decoding algorithm for RaptorQ code is proposed, where the on-the-fly decoding and the incremental decoding techniques are exploited to achieve early decoding and to reduce overall computational complexity. Computer simulations show that for the proposed algorithm the decoding process may start when thirty percents of the minimum required encoded symbols are received and the overall computational complexity can be reduced by fifty percents with regards to the ML decoding algorithm in [6].","","978-1-4799-4173","10.1109/BWCCA.2014.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016060","Fountain code;RaptorQ code;on-the-fly decoding;incremental decoding","Maximum likelihood decoding;Vectors;Algorithm design and analysis;Encoding;Standards;Computational complexity","codes;computational complexity;maximum likelihood decoding","low-complexity decoding algorithm;raptorQ code;incremental decoding technique;on-the-fly decoding technique;computational complexity;ML decoding algorithm","","","","9","","","","","IEEE","IEEE Conferences"
"Expediting Determination of Coding Unit Partition Based on Quantization Parameter and Frame Complexity","C. Su; C. Lin","NA; NA","2017 International Conference on Vision, Image and Signal Processing (ICVISP)","","2017","","","18","21","Choosing current major video coding standard, called H.264, to compress ultra-high resolution videos will cause low performance of compression in bitrate because of fixed macroblocks in H.264. For solving the large bitrate problem, a new video coding standard called High Efficiency Video Coding (HEVC) includes a quadtree-based Coding Unit (CU) block partitioning structure is established. However, HEVC causes a significant increase in computational complexity, compared to H.264 due to the necessity of searching the optimal Largest Coding Unit (LCU) partition. In this paper, we propose the fast luma-based CU partition (FLCUP) for HEVC intra prediction coding, which reduces significant computational complexity with negligible distortion in rate-distortion performance. FLCUP performs two steps: 1) analyzing the complexity of an original frame and 2) predicting LCU partition. Current the complexity of an original frame of LCU is analyzed by Discrete Wavelet Transform and several thresholds are obtained from experimental results. After analyzing the complexity of an original frame, FLCUP could predict the LCU partition for encoder without the unnecessary depth searching. Experimental results show that compared to the standard HEVC, FLCUP reduces the computational complexity by 38.63% at the cost of increasing only 1.62% in BD rate.","","978-1-5386-0612-4978-1-5386-0613","10.1109/ICVISP.2017.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123582","HEVC;Discrete Wavelet Transform;intra prediction coding","Signal processing","computational complexity;discrete wavelet transforms;quadtrees;rate distortion theory;video coding","CU partition;FLCUP;HEVC intra prediction coding;significant computational complexity;rate-distortion performance;standard HEVC;frame complexity;video coding standard;ultra-high resolution videos;bitrate problem;High Efficiency Video Coding;Unit block partitioning structure;optimal Largest Coding Unit partition;discrete wavelet transform;quadtree-based coding unit block partitioning structure","","","","10","","","","","IEEE","IEEE Conferences"
"Joint Modulation and Coding Scheme Toward Low-Complexity WBAN System","M. Oh; J. Jeong; H. Lee","Electronics and Telecommunication Research Institute, Korea; Electronics and Telecommunication Research Institute, Korea; Electronics and Telecommunication Research Institute, Korea","IEEE Communications Letters","","2011","15","2","145","147","This letter proposes a new pulse position modulation (PPM) combined with coding scheme for wireless body area network (WBAN) system. By utilizing redundant time slots in grouped PPM symbols, we incorporate a low-complexity block code to achieve coding gain without bit-rate reduction. Simulations and FPGA elaboration verify the proposed joint modulation and coding scheme.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2011.121310.101279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669625","WBAN;PPM;UWB;modulation;coding","Modulation;Decoding;Encoding;Complexity theory;Joints;Receivers;Measurement","block codes;body area networks;communication complexity;modulation coding","joint modulation;coding scheme;low-complexity WBAN system;pulse position modulation;wireless body area network system;redundant time slots;low-complexity block code;FPGA elaboration","","1","","4","","","","","IEEE","IEEE Journals & Magazines"
"Software complexity level determination using software effort estimation use case points metrics","Y. Yavari; M. Afsharchi; M. Karami","Department of Computer Engineering, Hamedan Branch Islamic Azad University, Hamedan, Iran; Department of Computer Engineering, Zanjan University, Zanjan, Iran; Department of Computer Engineering, Hamedan Branch Islamic Azad University, Hamedan, Iran","2011 Malaysian Conference in Software Engineering","","2011","","","257","262","Software estimations are regarding based on prediction properties of system, with attention to development methodology. In object-oriented analysis, Use Case models describe the functional requirements of a software system, so they can be basis for software measurement and sizing. Use Case points method that suggested by karner, is based on size and complexity of Use Cases. This method earns the great successes in industry and university. Although this method doesn't have enough precision and accuracy, because of inaccuracy in determining Use Case complexity metrics. In this research, we are going to assessment weak points of Use Case complexity metrics specially transaction metric in Use Case points method and then we will introduce other metrics for determining Use Case complexity that it can be led to simplify calculating UUC and it can be also tried to cover the most of aspects of Use Cases.","","978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529","10.1109/MySEC.2011.6140680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140680","software estimation;software requirements;software complexity;Use Case points;Use Case complexity","Measurement;Complexity theory;Software;Estimation;Business;Environmental factors;Accuracy","object-oriented programming;software metrics","software complexity level determination;software effort estimation;use case points metrics;development methodology;object-oriented analysis;use case models;functional requirements;software measurement;use case complexity metrics","","4","","18","","","","","IEEE","IEEE Conferences"
"Enterprise Architecture Cybernetics for Complex Global Software Development: Reducing the Complexity of Global Software Development Using Extended Axiomatic Design Theory","H. Kandjani; P. Bernus; L. Wen","NA; NA; NA","2012 IEEE Seventh International Conference on Global Software Engineering","","2012","","","169","173","Global Software Development projects could be best understood as intrinsically complex adaptive living systems: they can not purely be considered as 'designed systems', as deliberate design/ control episodes and processes (using 'software engineering' models) are intermixed with emergent change episodes and processes (that may perhaps be explained by models). Therefore the evolution of GSD projects includes the emergent as well as the deliberate aspects of system change. So to study GSD projects as complex systems we need to focus on both the state of the art of GSD research, as addressed in the software engineering discipline, as well as other disciplines that studied complexity such as Enterprise Architecture, Complexity and Information Theory, Axiomatic Design theory, for example. In this paper we study the complexity of GSD projects and propose the application of Extended Axiomatic Design (EAD) theory to reduce the complexity of GSD projects and to increase their probability of success. We also demonstrate that by satisfying all design axioms this 'structural' complexity could be minimised. By satisfying all three axioms of EAD, GSD management could make the life cycle activities of GSD planning and development projects as independent, controlled and uncoupled as possible so that the designer can predict the next relevant states of the life history and avoid a chaotic change in such projects.","2329-6305;2329-6313","978-1-4673-2357-4978-0-7695-4787","10.1109/ICGSE.2012.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337356","Global Software Development;Complexity;Enterprise Architecture;Extended Axiomatic Design Theory;Self-designing","Complexity theory;Planning;Software;Computer architecture;Companies;Cybernetics;Capability maturity model","computational complexity;large-scale systems;life cycle costing;probability;project management;software architecture;software development management;software metrics","enterprise architecture cybernetics;global software development complexity reduction;extended axiomatic design theory;global software development projects;complex adaptive living systems;software engineering model;change episodes;GSD project evolution;EAD theory;success probability;structural complexity minimisation;GSD management;life cycle activities;GSD planning","","3","","19","","","","","IEEE","IEEE Conferences"
"Studying Software Evolution for Taming Software Complexity","S. D. Suh; I. Neamtiu","NA; NA","2010 21st Australian Software Engineering Conference","","2010","","","3","12","Reducing software complexity is key to reducing software maintenance costs. To discover complexity-reducing practices, in this paper we study the evolution of seven sizable open source programs over a long period of time. We first measure how software complexity changes as programs evolve, and identify complexity-reducing releases. We then study the changes introduced in these releases and extract evolution patterns (we call them complexity-reducing steps) that lead to reduced program complexity. Finally, we categorize these steps and discuss their effectiveness. We believe that bringing these complexity-reducing measures to light, and encouraging developers to adopt them, has the potential to improve the state of practice in software maintenance.","2377-5408;1530-0803","978-1-4244-6476-0978-1-4244-6475-3978-0-7695-4006","10.1109/ASWEC.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5475059","software complexity;software evolution;software metrics;open source;refactoring","Software maintenance;Open source software;Costs;Software measurement;Computer science;History;Statistical analysis;Software engineering;Lead;Software metrics","public domain software;software maintenance;software metrics","software evolution;software complexity;software maintenance;open source programs;evolution patterns;complexity-reducing steps","","4","","22","","","","","IEEE","IEEE Conferences"
"Low complexity Linear Programming decoding of nonbinary linear codes","M. Punekar; M. F. Flanagan","Claude Shannon Institute, University College Dublin, Belfield, 4, Ireland; Claude Shannon Institute, University College Dublin, Belfield, 4, Ireland","2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2010","","","6","13","Linear Programming (LP) decoding of Low-Density Parity-Check (LDPC) codes has attracted much attention in the research community in the past few years. The aim of LP decoding is to develop an algorithm which has error-correcting performance similar to that of the Sum-Product (SP) decoding algorithm, while at the same time it should be amenable to mathematical analysis. The LP decoding algorithm has been derived for both binary and nonbinary decoding frameworks. However, the most important problem with LP decoding for both binary and nonbinary linear codes is that the complexity of standard LP solvers such as the simplex algorithm remain prohibitively large for codes of moderate to large block length. To address this problem, Vontobel et al. proposed a low complexity LP decoding algorithm for binary linear codes which has complexity linear in the block length. In this paper, we extend the latter work and propose a low-complexity LP decoding algorithm for nonbinary linear codes. We use the LP formulation for the nonbinary codes as a basis and derive a pair of primal-dual LP formulations. The dual LP is then used to develop the low-complexity LP decoding algorithm for nonbinary linear codes. The complexity of the proposed algorithm is linear in the block length and is limited mainly by the maximum check node degree. As a proof of concept, we also present a simulation result for a [80, 48] LDPC code defined over ℤ4using quaternary phase-shift keying over the AWGN channel, and we show that the error-correcting performance of the proposed LP decoding algorithm is similar to that of the standard LP decoding using the simplex solver.","","978-1-4244-8216-0978-1-4244-8215-3978-1-4244-8214","10.1109/ALLERTON.2010.5706881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706881","","Decoding;Parity check codes;Linear code;Algorithm design and analysis;Complexity theory;Equations;Cost function","AWGN channels;binary codes;decoding;error correction codes;linear codes;linear programming;mathematical analysis;parity check codes;phase shift keying","low complexity linear programming decoding;nonbinary linear codes;low-density parity-check codes;error-correcting code;sum-product decoding algorithm;SP decoding algorithm;mathematical analysis;nonbinary decoding frameworks;binary decoding frameworks;low complexity LP decoding algorithm;LDPC code;quaternary phase-shift keying;AWGN channel","","6","","14","","","","","IEEE","IEEE Conferences"
"Iterative Linear Programming Decoding of Nonbinary LDPC Codes With Linear Complexity","D. Goldin; D. Burshtein","school of Electrical Engineering, Tel-Aviv University, Tel-Aviv, Israel; school of Electrical Engineering, Tel-Aviv University, Tel-Aviv, Israel","IEEE Transactions on Information Theory","","2013","59","1","282","300","The problem of low-complexity linear programming (LP) decoding of nonbinary low-density parity-check (LDPC) codes is considered, and an iterative LP decoding algorithm is presented. Results that were previously derived for binary LDPC codes are extended to the nonbinary case. Both simple and generalized nonbinary LDPC codes are considered. It is shown how the algorithm can be implemented efficiently using a finite-field fast Fourier transform. Then, the convergence rate of the algorithm is analyzed. The complexity of the algorithm scales linearly in the block length, and it can approximate, up to an arbitrarily small relative error, the objective function of the exact LP solution. When applied to a typical code from an appropriate nonbinary LDPC code ensemble, the algorithm can correct a constant fraction of errors in linear (in the block length) computational complexity. Computer experiments with the new iterative LP decoding algorithm show that, in the error floor region, it can have better performance compared to belief propagation decoding, with similar computational requirements.","0018-9448;1557-9654","","10.1109/TIT.2012.2211859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261547","Iterative decoding;linear programming (LP) decoding;low-density parity-check (LDPC) codes","Vectors;Iterative decoding;Silicon;Complexity theory;Maximum likelihood decoding","computational complexity;fast Fourier transforms;iterative decoding;linear programming;parity check codes","iterative linear programming decoding;nonbinary LDPC codes;linear complexity;low-complexity linear programming decoding;nonbinary low-density parity-check codes;iterative LP decoding algorithm;nonbinary case;generalized nonbinary LDPC codes;finite-field fast Fourier transform;block length;relative error;computational complexity;error floor region;belief propagation decoding;computational requirements","","10","","27","","","","","IEEE","IEEE Journals & Magazines"
"Should I Bug You? Identifying Domain Experts in Software Projects Using Code Complexity Metrics","R. Teusner; C. Matthies; P. Giese","NA; NA; NA","2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)","","2017","","","418","425","In any sufficiently complex software system there are experts, having a deeper understanding of parts of the system than others. However, it is not always clear who these experts are and which particular parts of the system they can provide help with. We propose a framework to elicit the expertise of developers and recommend experts by analyzing complexity measures over time. Furthermore, teams can detect those parts of the software for which currently no, or only few experts exist and take preventive actions to keep the collective code knowledge and ownership high. We employed the developed approach at a medium-sized company. The results were evaluated with a survey, comparing the perceived and the computed expertise of developers. We show that aggregated code metrics can be used to identify experts for different software components. The identified experts were rated as acceptable candidates by developers in over 90% of all cases.","","978-1-5386-0592-9978-1-5386-0591-2978-1-5386-0593","10.1109/QRS.2017.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009945","domain experts;expert identification;software metrics;software quality","Measurement;Complexity theory;Software;Couplings;Computer bugs;Companies;Data mining","object-oriented programming;software metrics","code complexity metrics;complex software system;collective code knowledge;ownership high;software components;software projects","","","","44","","","","","IEEE","IEEE Conferences"
"An Empirical Validation of the Complexity of Code Changes and Bugs in Predicting the Release Time of Open Source Software","K. K. Chaturvedi; P. Bedi; S. Misra; V. B. Singh","NA; NA; NA; NA","2013 IEEE 16th International Conference on Computational Science and Engineering","","2013","","","1201","1206","With the increasing popularity of open source software, the changes in source code are inevitable. These changes in code are due to feature enhancement, new feature introduction and bug repair or fixed. It is important to note that these changes can be quantified by using entropy based measures. The pattern of bug fixing scenario with complexity of code change is responsible for the next release as these changes will cover the number of requirements and fixes. In this paper, we are proposing a method to predict the next release problem based on the complexity of code change and bugs fixed. We applied multiple linear regression to predict the time of the next release of the product and measured the performance using different residual statistics, goodness of fit curve and R2. We observed from the results of multiple linear regression that the predicted value of release time is fitting well with the observed value of number of months for the next release.","","978-0-7695-5096","10.1109/CSE.2013.201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755360","open source software;complexity of code change;next release time;bug repositories;software configuration management repositories","Computer bugs;Complexity theory;Linear regression;Open source software;History;Maintenance engineering","program debugging;public domain software;regression analysis","open source software;code changes complexity;bugs complexity;feature enhancement;feature introduction;bug repair;bug fixed;multiple linear regression","","2","","35","","","","","IEEE","IEEE Conferences"
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Y. Shin; A. Meneely; L. Williams; J. A. Osborne","DePaul University, Chicago; North Carolina State University, Raleigh; North Carolina State University, Raleigh; North Carolina State University, Raleigh","IEEE Transactions on Software Engineering","","2011","37","6","772","787","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680","Fault prediction;software metrics;software security;vulnerability prediction.","Fault diagnosis;Software security;Complexity theory;Predictive models;Charge coupled devices","Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics","code churn;software vulnerabilities;developer activity metrics;security inspection;software metrics;source code;vulnerable code locations;open-source projects;Mozilla Firefox Web browser;Red Hat enterprise Linux kernel","","128","","43","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity DCT-based distributed source coding with Gray code for hyperspectral images","R. Liu; J. Wang; X. Pan","School of Electronic and Information Engineering, Beihang University, Beijing 100191, P. R. China; School of Electronic and Information Engineering, Beihang University, Beijing 100191, P. R. China; School of Electronic and Information Engineering, Beihang University, Beijing 100191, P. R. China","Journal of Systems Engineering and Electronics","","2010","21","6","927","933","To compress hyperspectral images, a low complexity discrete cosine transform (OCT)-based distributed source coding (OSC) scheme with Gray code is proposed. Unlike most of the existing OSC schemes, which utilize transform in spatial domain, the proposed algorithm applies transform in spectral domain. Set-partitioning-based approach is applied to reorganize OCT coefficients into waveletlike tree structure and extract the sign, refinement, and significance bitplanes. The extracted refinement bits are Gray encoded. Because of the dependency along the line dimension of hyperspectral images, low density paritycheck-(LOPC)-based Slepian-Wolf coder is adopted to implement the OSC strategy. Experimental results on airborne visible/infrared imaging spectrometer (AVIRIS) dataset show that the proposed paradigm achieves up to 6 dB improvement over OSC-based coders which apply transform in spatial domain, with significantly reduced computational complexity and memory storage.","1004-4132","","10.3969/j.issn.1004-4132.2010.06.001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077539","image compression;hyperspectral images;distributed source coding (OSC);discrete cosine transform (OCT);Gray code;band-interleaved-by-pixel (BIP)","Discrete cosine transforms;Hyperspectral imaging;Correlation;Image coding;Reflective binary codes;Encoding","","","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"Exponential Error Bounds and Decoding Complexity for Block Codes Constructed by Unit Memory Trellis Codes of Branch Length Two","S. Hirasawa; H. Yagi; M. Kobayashi; M. Kasahara","Research Institute for Science and Engineering, Waseda University, Tokyo, Japan; Graduate School of Informatics and Engineering, The University of Electro-Communications, Tokyo, Japan; Center for Data Science, Waseda University, Tokyo, Japan; Research Institute for Science and Engineering, Waseda University, Tokyo, Japan","2019 53rd Annual Conference on Information Sciences and Systems (CISS)","","2019","","","1","6","Performance of block codes constructed by unit memory (UM) trellis codes is discussed from random coding arguments. There are three methods to obtain block codes from trellis codes, i.e., those of (a) Tail Termination (TT), (b) Direct Truncation (DT), and (c) Tail Biting (TB). In this paper, we derive exponential error bounds and decoding complexity for block codes constructed by the UM trellis codes of branch length two based on the above three methods to uniformly discuss their performance. For the UM trellis codes of branch length two, the error exponent of the tail biting unit memory (TB-UM) trellis codes is shown to be larger than or equal to those of the ordinary block codes, the tail termination unit memory (TT-UM) and the direct truncation unit memory (DT-UM) trellis codes for all rates less than the capacity. Decoding complexity for the TB-UM trellis codes of branch length two exhibits interesting property since their trellis diagrams become simple. Taking into account of the asymptotic decoding complexity, the TB-UM trellis codes are also shown to have a smaller upper bound on the probability of decoding error compared to the ordinary block codes for the same rate with the same decoding complexity.","","978-1-7281-1151-3978-1-7281-1152","10.1109/CISS.2019.8692938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692938","","","block codes;channel coding;decoding;error statistics;random codes;trellis codes","random coding arguments;exponential error bounds;tail biting unit memory trellis codes;ordinary block codes;TT-UM;direct truncation unit memory trellis codes;TB-UM trellis codes;trellis diagrams;error exponent;tail termination unit memory codes;DT-UM trellis codes;branch length two;asymptotic decoding complexity;decoding error probability;upper bound","","","","18","","","","","IEEE","IEEE Conferences"
"Low-complexity ad-hoc non-linearities for blind multiuser detection of long-code code-division multiple access signals and asymptotic performance evaluation","S. M. Saberali; H. Amindavar","Department of Electrical Engineering, Faculty of Engineering, University of Isfahan, P.O. Box 81746-73441, Isfahan, Iran; Department of Electrical Engineering, Amirkabir University of Technology, P.O. Box 15914, Tehran, Iran","IET Communications","","2014","8","9","1527","1533","In this study, monomials with odd integer powers are proposed for blind multiuser detection of code-division multiple access signals with long spreading codes, in the near-far situation. The performance of the new non-linear detectors are substantiated asymptotically and confirmed via simulations. It is demonstrated that the proposed non-linear detectors significantly outperform the matched filter detector in the block fading and Ricean fading channels, at the cost of a small increase in computational complexity. The optimum integer power value for the block fading channel with a limited interferer level, the Ricean channel with a given <i>K</i> factor and asynchronous channel is determined. The proposed multiuser detectors are blind in the sense that they require neither training nor the spreading code of the interferers. The detectors also do not require long convergence time for decision making in contrast to conventional blind multiuser detectors.","1751-8628;1751-8636","","10.1049/iet-com.2013.0560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830051","","","blind source separation;channel estimation;code division multiple access;computational complexity;multiuser detection;radiofrequency interference;Rician channels","low-complexity blind channel estimation;CDMA;decision making;asynchronous channel;K factor;limited interferer level;optimum integer power value;computational complexity;Ricean fading channels;block fading channel;nonlinear detectors;long spreading codes;odd integer powers;asymptotic performance evaluation;long-code code-division multiple access signals;blind multiuser detection;low-complexity ad-hoc nonlinearities","","","","32","","","","","IET","IET Journals & Magazines"
"Reduced-complexity near-capacity downlink iteratively decoded generalized multi-layer space-time coding using irregular convolutional codes","L. Kong; S. X. Ng; R. Y. S. Tee; R. G. Maunder; L. Hanzo","University of Southampton, United Kingdom; University of Southampton, United Kingdom; University of Southampton, United Kingdom; University of Southampton, United Kingdom; University of Southampton, United Kingdom","IEEE Transactions on Wireless Communications","","2010","9","2","684","695","This paper presents a low complexity iteratively detected space-time transmission architecture based on Generalized Multi-Layer Space-Time (GMLST) codes and Irregular Convolutional Codes (IRCCs). The GMLST combines the benefits of the Vertical Bell-Labs LAyered Space-Time (VBLAST) scheme and Space-Time Coding (STC). The GMLST is serially concatenated with a Unity-Rate Code (URC) and an IRCC which are used to facilitate near-capacity operation with the aid of an EXtrinsic Information Transfer (EXIT) chart based design. Reduced-complexity iterative multistage Successive Interference Cancellation (SIC) is employed in the GMLST decoder, instead of the significantly more complex Maximum Likelihood (ML) detection. For the sake of approaching the maximum attainable rate, iterative decoding is invoked to achieve decoding convergence by exchanging extrinsic information across the three serial component decoders. Finally, it is shown that the SIC-based iteratively detected IRCC-URC-GMLST system is capable of providing a feasible trade-off between the affordable computational complexity and the achievable system throughput.","1536-1276;1558-2248","","10.1109/TWC.2010.02.081117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5403549","Generalized Multi-Layer Space-Time Code, iterative detection, irregular convolutional code, EXIT charts.","Downlink;Iterative decoding;Convolutional codes;Maximum likelihood decoding;Maximum likelihood detection;Concatenated codes;Interference cancellation;Silicon carbide;Convergence;Computational complexity","communication complexity;convolutional codes;interference suppression;iterative decoding;maximum likelihood detection;space-time codes","generalized multilayer space-time coding;irregular convolutional codes;low complexity iteratively detected space-time transmission architecture;vertical bell-labs layered space-time scheme;unity-rate code;extrinsic information transfer chart;EXIT chart;reduced-complexity iterative multistage successive interference cancellation;GMLST decoder;maximum likelihood detection;serial component decoders;computational complexity;system throughput;iterative decoding","","15","","28","","","","","IEEE","IEEE Journals & Magazines"
"The error-correcting capabilities of low-complexity decoded H-LDPC code as irregular LDPC code","P. Rybin","Institute for Information Transmission Problems, Russian Academy of Sciences, Moscow, Russia","2014 XIV International Symposium on Problems of Redundancy in Information and Control Systems","","2014","","","83","88","This paper deals with the Low-Density Parity-Check codes with the constituent Hamming codes (H-LDPC codes) and two different iterative hard-decision low-complexity decoding algorithms. Both algorithms are based on the same main idea: the decreasing of the syndrome weight on each step of the decoding algorithm. The first decoding algorithm uses the properties of the constituent Hamming code. The best known lower-bound on the guaranteed corrected errors fraction for the H-LDPC codes under the first decoding algorithm was obtained in 2011. The second decoding algorithm considers H-LDPC as the irregular LDPC code and uses the well-known majority decoding algorithm. The lower-bound on the guaranteed corrected errors fraction for H-LDPC code under the second decoding algorithm is introduced for the first time in this paper. Numerical results for the lower-bound, obtained in this paper for H-LDPC code under the second decoding algorithm, significantly exceed the numerical results for the best known lower-bounds, obtained previously for H-LDPC code under the first decoding algorithm.","","978-1-4799-4050-9978-1-4799-4896-3978-1-4799-4900","10.1109/RED.2014.7016711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016711","","","decoding;error correction codes;Hamming codes;parity check codes","error-correcting capabilities;low-complexity decoded H-LDPC code;irregular LDPC code;low-density parity-check codes;constituent Hamming codes;hard-decision low-complexity decoding algorithms","","","","16","","","","","IEEE","IEEE Conferences"
"Band Codes: Controlled Complexity Network Coding for Peer-to-Peer Video Streaming","A. Fiandrotti; V. Bioglio; E. Magli; M. Grangetto; R. Gaeta","NA; NA; NA; NA; NA","2012 IEEE International Conference on Multimedia and Expo","","2012","","","194","199","We present Band Codes (BC), a novel class of rate less codes that makes possible to control the computational complexity of Network Coding (NC). NC increases throughput of the networks via packet recombinations at the network nodes. In a NC scenario based on rate less codes, the recombinations at the nodes alter the packet degree distribution selected at the source and increase the computational complexity of the packet decoding process. Unlike other classes of rate less codes, BC preserve the degree distribution of the encoded packets through the recombinations at the nodes. Furthermore, BC enable to control the decoding complexity of each network node independently from the rest of the network. We evaluate BC in a P2P scenario using a purposely designed random-push protocol for live video streaming. The experiments show that BC achieve high encoding efficiency, enable nodes with different computational capabilities to coexist within the same network and reduce the processor load on a real mobile device by nearly 50%.","1945-788X;1945-7871;1945-7871","978-1-4673-1659-0978-0-7695-4711","10.1109/ICME.2012.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298397","Network Coding;Computational Complexity;P2P","Encoding;Peer to peer computing;Complexity theory;Decoding;Streaming media;Network coding;Vectors","computational complexity;decoding;network coding;peer-to-peer computing;video coding;video streaming","band codes;controlled complexity network coding;peer-to-peer video streaming;rateless codes;computational complexity;network nodes;packet degree distribution;packet decoding process;decoding complexity control;P2P video streaming;random-push protocol;live video streaming;mobile device","","3","","9","","","","","IEEE","IEEE Conferences"
"Low-complexity detection of golden codes in LDPC-coded OFDM systems","I. Sobrón; M. Barrenechea; P. Ochandiano; L. Martínez; M. Mendicute; J. Altuna","Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain; Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain; Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain; Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain; Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain; Signal Theory and Communications Area, Department of Electronics and Computer Science, University of Mondragón, Loramendi, 4, 20500, Spain","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2011","","","3160","3163","Recent and next-generation wireless broadcasting standards, such as DVB-T2 or DVB-NGH, are considering distributed multi-antenna transmission in order to increase bandwidth efficiency and signal quality. Full-rate full-diversity (FRFD) space-time codes (STC), such as the Golden code, have been reported to be excellent candidates, being their main drawback their detection complexity, which is enhanced when soft output is required when combined with a bit interleaved coded modulation (BICM) scheme based on low-density parity check (LDPC) codes. We present a novel low-complexity soft detection algorithm for the reception of Golden codes in LDPC based orthogonal frequency-division multiplexing (OFDM) systems. Complexity and simulation-based performance results are provided which show that the proposed detector performs close to the optimal detector in a variety of DVB-T2 broadcasting scenarios.","2379-190X;1520-6149;1520-6149","978-1-4577-0539-7978-1-4577-0538-0978-1-4577-0537","10.1109/ICASSP.2011.5946692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946692","MAP detection;MIMO systems;Space-Frequency Block Coding (SFBC);Low-density parity check (LDPC) codes;Golden codes","Digital video broadcasting;Complexity theory;Parity check codes;Detectors;MIMO;Decoding;Measurement","antenna arrays;communication complexity;digital video broadcasting;diversity reception;interleaved codes;next generation networks;OFDM modulation;parity check codes;space-time codes","low-complexity golden code detection;LDPC-coded OFDM system;next generation wireless broadcasting standards;distributed multiantenna transmission;bandwidth efficiency;signal quality;FRFD space-time codes;full-rate full-diversity STC;detection complexity;bit-interleaved coded modulation;BICM scheme;low-density parity check codes;low-complexity soft detection algorithm;orthogonal frequency division multiplexing system;DVB-T2 broadcasting","","","","8","","","","","IEEE","IEEE Conferences"
"Complexity Control Based on a Fast Coding Unit Decision Method in the HEVC Video Coding Standard","A. Jiménez-Moreno; E. Martínez-Enríquez; F. Díaz-de-María","Department of Signal Theory and Communications, Carlos III University, Madrid, Spain; Instituto de Óptica, Consejo Superior de Investigaciones Científicas, Madrid, Spain; Department of Signal Theory and Communications, Carlos III University, Madrid, Spain","IEEE Transactions on Multimedia","","2016","18","4","563","575","The emerging high-efficiency video coding standard achieves higher coding efficiency than previous standards by virtue of a set of new coding tools such as the quadtree coding structure. In this novel structure, the pixels are organized into coding units (CU), prediction units, and transform units, the sizes of which can be optimized at every level following a tree configuration. These tools allow highly flexible data representation; however, they incur a very high computational complexity. In this paper, we propose an effective complexity control (CC) algorithm based on a hierarchical approach. An early termination condition is defined at every CU size to determine whether subsequent CU sizes should be explored. The actual encoding times are also considered to satisfy the target complexity in real time. Moreover, all parameters of the algorithm are estimated on the fly to adapt its behavior to the video content, the encoding configuration, and the target complexity over time. The experimental results prove that our proposal is able to achieve a target complexity reduction of up to 60% with respect to full exploration, with notable accuracy and limited losses in coding performance. It was compared with a state-of-the-art CC method and shown to achieve a significantly better trade-off between coding complexity and efficiency as well as higher accuracy in reaching the target complexity. Furthermore, a comparison with a state-of-the-art complexity reduction method highlights the advantages of our CC framework. Finally, we show that the proposed method performs well when the target complexity varies over time.","1520-9210;1941-0077","","10.1109/TMM.2016.2524995","Spanish Ministry of Economy and Competitiveness; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398085","Complexity control;fast coding unit decision;HEVC;on the fly estimation;Complexity control (CC);fast coding unit decision;high efficiency video coding (HEVC);on the fly estimation","Encoding;Complexity theory;Standards;Proposals;Streaming media","computational complexity;data structures;video coding","fast coding unit decision method;HEVC video coding standard;coding tool;prediction unit;transform unit;flexible data representation;computational complexity control algorithm;CC algorithm;hierarchical approach;encoding time;video content;encoding configuration;target complexity reduction method;coding complexity","","17","","24","","","","","IEEE","IEEE Journals & Magazines"
"BASIC Codes: Low-Complexity Regenerating Codes for Distributed Storage Systems","H. Hou; K. W. Shum; M. Chen; H. Li","Shenzhen Key Lab of Information theory and Future Internet architecture, School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; Institute of Network Coding, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Shenzhen Key Lab of Information theory and Future Internet architecture, Shenzhen Engineering Laboratory of Converged Networks, School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China","IEEE Transactions on Information Theory","","2016","62","6","3053","3069","In distributed storage systems, regenerating codes can achieve the optimal tradeoff between storage capacity and repair bandwidth. However, a critical drawback of existing regenerating codes, in general, is the high coding and repair complexity, since the coding and repair processes involve expensive multiplication operations in finite field. In this paper, we present a design framework of regenerating codes, which employ binary addition and bitwise cyclic shift as the elemental operations, named BASIC regenerating codes. The proposed BASIC regenerating codes can be regarded as a concatenated code with the outer code being a binary parity-check code, and the inner code being a regenerating code utilizing the binary parity-check code as the alphabet. We show that the proposed functional-repair BASIC regenerating codes can achieve the fundamental tradeoff curve between the storage and repair bandwidth asymptotically of functional-repair regenerating codes with less computational complexity. Furthermore, we demonstrate that the existing exact-repair product-matrix construction of regenerating codes can be modified to exact-repair BASIC product-matrix regenerating codes with much less encoding, repair, and decoding complexity from the theoretical analysis, and with less encoding time, repair time, and decoding time from the implementation results.","0018-9448;1557-9654","","10.1109/TIT.2016.2553670","National Basic Research Program of China; Natural Fund of Guangdong Province; Shenzhen Basic Research Project; University Grants Committee, Hong Kong, through the Area of Excellence Grant Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452376","Regenerating codes;distributed storage systems;low complexity;binary parity-check code;Regenerating codes;distributed storage systems;low complexity;binary parity-check code","Maintenance engineering;Encoding;Bandwidth;Computational complexity;Parity check codes;Decoding","codes;decoding","low-complexity regenerating codes;distributed storage systems;distributed storage systems;optimal tradeoff;storage capacity;repair bandwidth;regenerating codes;coding;repair processes;binary parity-check code;computational complexity;repair product-matrix construction;BASIC product-matrix regenerating codes;decoding complexity;encoding;repair time;decoding time;theoretical analysis","","20","","35","","","","","IEEE","IEEE Journals & Magazines"
"Space–Frequency Codes Based on the Space–Time Codes With Very Low Complexity for the Decoder","M. Shahabinejad; F. G. Hosseini; S. Talebi","Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran; Department of Electrical Engineering, Shahid Bahonar University of Kerman , Kerman, Iran; Department of Electrical Engineering, Shahid Bahonar University of Kerman , Kerman, Iran","IEEE Transactions on Vehicular Technology","","2013","62","9","4678","4684","In this paper, we introduce a family of full-diversity space-frequency codes (SFCs) for the multi-input-multi-output orthogonal frequency-division multiplexing (MIMO-OFDM) systems. The most important feature of the proposed SFCs is that they can be swiftly decoded. More precisely, the decoding complexity on the order of O (M) or O(M2) can be attained for the proposed full-diversity SFCs, where M is the constellation size. Simulation results have also verified that the proposed SFCs display outstanding performance in comparison with the latest SFC model published in the literature.","0018-9545;1939-9359","","10.1109/TVT.2013.2263289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516562","Fading channels;frequency-selective channels;multi-input–multi-output orthogonal frequency-division multiplexing (MIMO-OFDM) systems;space–frequency coding;wireless communication","Complexity theory;Decoding;Delays;Receivers;OFDM;Encoding;Bit error rate","computational complexity;diversity reception;MIMO communication;OFDM modulation","space-time codes;decoder;full-diversity space-frequency codes;MIMO-OFDM systems;multi-input-multi-output;orthogonal frequency-division multiplexing;decoding complexity;full-diversity SFC;constellation size;SFC model","","1","","12","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Depth Coding Assisted by Coding Information From Color Video","L. Shen; P. An; Z. Liu; Z. Zhang","Key Laboratory of Advanced Display and System Application, Shanghai University, Ministry of Education, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China","IEEE Transactions on Broadcasting","","2014","60","1","128","133","Recently, color plus depth map format 3-D video has been introduced as a new 3-D scene representation format, which gains the attention for its relatively low storage and transmission bandwidth requirement in comparison to the multiview video system. Usually, encoding of the color video and the depth data uses two traditional H.264 encoders. The complexity and hardware requirements to code one 2-D view and its associated depth data are nearly two times higher than coding one 2-D view. In this paper, we address the reduction of complexity of depth coding. Since both color videos and depth maps represent the same scene at the same time instant, it is likely for each macroblock to have similar coding information including the prediction mode and motion vectors. Taking advantage of correlations between coding information from depth maps and color videos, we propose a low complexity depth coding algorithm incorporating a fast motion search method and a fast mode decision method. Results show that the proposed algorithm reduces 80% computational complexity with a negligible loss of coding efficiency in comparison with the original Joint Multiview Video Coding encoder.","0018-9316;1557-9611","","10.1109/TBC.2013.2289635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6677558","3-D video;depth coding;mode decision","Encoding;Image color analysis;Complexity theory;PSNR;Bit rate;Color;Vectors","computational complexity;image colour analysis;image motion analysis;image representation;video coding","video coding encoder;computational complexity;motion search method;motion vectors;H.264 encoders;multiview video system;storage bandwidth requirement;transmission bandwidth requirement;3D scene representation format;color video;coding information;low complexity depth coding","","18","","19","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Multi-Stream Space-Time Codes—Part I: Direct-Sum Codes and Design Criteria","E. Stauffer; B. Hochwald","Broadcom Corporation, Sunnyvale, Ca 94086; Dept. of Electrical Engineering, University of Notre Dame","IEEE Transactions on Communications","","2012","60","1","81","88","Multi-stream codes are used in wireless broadcast services where two or more priority classes of data are transmitted simultaneously, and a receiving terminal may decode one or more of the streams as a function of its receive signal to noise ratio. Present-day commercial cellular-based broadcast services generally utilize hierarchical modulation, a clever technique for embedding high and low-priority streams in a single modulated quadrature symbol. With many cellular standards moving to multiple-transmit antenna configurations, there are opportunities for designing new multi-stream encoding techniques that exploit these antennas. We examine the design of space-time codes that allow simple encoding and decoding of high and low-priority streams of data. A desirable multi-stream space-time code has a combination of good performance and low complexity. Performance is generally measured as coded bit-error rate, assuming max-log maximum aposteriori decoding. Complexity is measured as the effort needed to compute the aposteriori probabilities for either stream. Hierarchical modulation, for single-antenna transmissions, allows each stream to be decoded simply and independently. The paper comprises two parts. In this first part, we establish a general performance criterion for two-stream space-time codes and derive a formula for the complexity of max-log maximum aposteriori decoding of either stream. We also show how existing space-time codes may be combined with hierarchical modulation in a ""direct sum"". The direct-sum codes have low complexity, but we show in the second part of this paper that these codes can be significantly outperformed by non-direct-sum codes. One of our proposed two-stream codes performs 4 dB better than the Alamouti direct-sum code; it also has the benefit of decoding complexity in one stream that is a bounded function of the rate of the other.","0090-6778;1558-0857","","10.1109/TCOMM.2011.121511.100221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111192","Space-time coding;broadcast;hierarchical coding;multi-stream","Space-time codes;Vectors;Signal to noise ratio;Complexity theory;Interference;Phase shift keying","broadcast communication;cellular radio;communication complexity;encoding;error statistics;maximum likelihood estimation;modulation;space-time codes;transmitting antennas","low complexity multistream space-time code;wireless broadcast service;receive signal to noise ratio;commercial cellular-based broadcast service;hierarchical modulation;single modulated quadrature symbol;cellular standard;multiple transmit antenna configuration;multistream encoding technique;coded bit error rate;max-log maximum a posteriori decoding complexity;a posteriori probability;single antenna transmission;two stream space-time code;max-log maximum aposteriori decoding complexity;nondirect-sum code;Alamouti direct-sum code","","8","","17","","","","","IEEE","IEEE Journals & Magazines"
"A low-complexity coding scheme for non-binary LDPC code based on IDRB-MLGD algorithm","Xin Xiao; Yichao Lu; S. Goto","Graduate School of Information, Production and System, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and System, Waseda University, Fukuoka, Japan; Graduate School of Information, Production and System, Waseda University, Fukuoka, Japan","2013 9th International Conference on Information, Communications & Signal Processing","","2013","","","1","5","Non-binary LDPC codes are flourishing in many areas for their excellent error correction performance in last decade. However, the bottleneck for NB-LDPC codes' implementation is the high decoding computational complexity. To seek for a low-complexity decoding algorithm, iterative double-reliability-based majority-logic decoding (IDRB-MLGD) algorithm has been proposed, which still suffers from relatively high error floor at BER level of 10-6. In this work, we propose a low-complexity coding scheme for non-binary LDPC code that bases on iterative IDRB-MLGD algorithm. The proposed work concatenates a Reed Solomon code with non-binary LDPC code which is decoded by IDRB-MLGD algorithm. By such concatenation, errors left by IDRB-MLGD algorithm are set into RS code blocks, which can be further reduced by these RS code blocks. Moreover, a low-complexity decoding algorithm is used to decode the RS code. The comparison results on decoding computational complexity indicate that concatenation only brings about trivial complexity increase. Simulation results show that this proposed concatenated coding scheme can trade off the performance and decoding computational complexity efficiently.","","978-1-4799-0434-1978-1-4799-0433","10.1109/ICICS.2013.6782971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782971","non-binary LDPC code;RS code;concatenation;reliability-based;majority-logic decoding","Iterative decoding;Encoding;Algorithm design and analysis;Decoding;Computational complexity;Reliability","computational complexity;error correction codes;error statistics;iterative decoding;parity check codes;Reed-Solomon codes","low complexity coding scheme;nonbinary LDPC code;IDRB-MLGD algorithm;error correction performance;computational complexity;low complexity decoding algorithm;iterative double reliability based majoritylogic decoding;BER;Reed Solomon code;RS code blocks","","1","","13","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Decoder for Turbo Product Codes Based on Extended Hamming Codes","Y. Wang; J. Lin; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, P.R. China; School of Electronic Science and Engineering, Nanjing University, Nanjing, P.R. China; School of Electronic Science and Engineering, Nanjing University, Nanjing, P.R. China","2018 IEEE 18th International Conference on Communication Technology (ICCT)","","2018","","","99","103","In this paper, a modified low-complexity Chase-II decoding algorithm for Turbo product codes(TPCs) and the corresponding hardware architecture are proposed. The proposed decoding algorithm is based on a low-complexity Fast Chase algorithm proposed in [1]. To further reduce the computational complexity, the number of the candidate codewords used to generate the extrinsic information is reduced to 2, and a new low-complexity method of calculating the extrinsic information is proposed. The proposed algorithm saves 44% real additions. The numerical simulation results show that the proposed algorithm has negligible performance loss if the component codes of TPCs are hamming codes. Moreover, an efficient hardware architecture for a (128,120)2 extended Hamming turbo product code is also proposed. The proposed decoder is synthesised under the TSMC 90-nm CMOS technology. The synthesised results show that the proposed decoder reaches a 10.5 Gbps throughput with an area of 1.7 Mgates.","2576-7828;2576-7844","978-1-5386-7635-6978-1-5386-7633-2978-1-5386-7636","10.1109/ICCT.2018.8599928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599928","Turbo product codes;block turbo codes;hardware architecture;very large scale integration","Decoding;Complexity theory;Hardware;Product codes;Computer architecture;Block codes;Reliability","CMOS integrated circuits;computational complexity;decoding;Hamming codes;numerical analysis;product codes;turbo codes","low-complexity decoder;extended Hamming codes;low-complexity Chase-II decoding algorithm;computational complexity;extrinsic information;component codes;turbo product code;TSMC CMOS technology;numerical simulation;low-complexity fast chase algorithm;hardware architecture;size 90 nm;bit rate 10.5 Gbit/s","","","","10","","","","","IEEE","IEEE Conferences"
"Polynomial complexity of polar codes for non-binary alphabets, key agreement and Slepian-Wolf coding","J. Liu; E. Abbe","Dept. of Electrical Eng., Princeton University, NJ 08544, USA; Dept. of Electrical Eng., Princeton University, NJ 08544, USA","2014 48th Annual Conference on Information Sciences and Systems (CISS)","","2014","","","1","6","We consider polar codes for memoryless sources with side information and show that the blocklength, construction, encoding and decoding complexities are bounded by a polynomial of the reciprocal of the gap between the compression rate and the conditional entropy. This extends the recent results of Guruswami and Xia to a slightly more general setting, which in turn can be applied to (1) sources with non-binary alphabets, (2) key generation for discrete and Gaussian sources, and (3) Slepian-Wolf coding and multiple accessing. In each of these cases, the complexity scaling with respect to the number of users is also controlled. In particular, we construct coding schemes for these multi-user information theory problems which achieve optimal rates with an overall polynomial complexity.","","978-1-4799-3001","10.1109/CISS.2014.6814146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6814146","","Encoding;Decoding;Joints;Polynomials;Degradation;Time complexity","encoding;Gaussian processes;polynomials","polynomial complexity;polar codes;nonbinary alphabets;key agreement;Slepian-Wolf coding;memoryless sources;conditional entropy;compression rate;Gaussian sources;discrete sources;information theory problems","","","","18","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Bit-Plane Entropy Coding and Rate Control for 3-D DWT Based Video Coding","E. Belyaev; K. Egiazarian; M. Gabbouj","Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland; Department of Signal Processing, Tampere University of Technology, Finland","IEEE Transactions on Multimedia","","2013","15","8","1786","1799","This paper is dedicated to fast video coding based on three-dimensional discrete wavelet transform. First, we propose a novel low-complexity bit-plane entropy coding of wavelet subbands based on Levenstein zero-run coder for low entropy contexts and adaptive binary range coder for other contexts. Second, we propose a rate-distortion efficient criterion for skipping 2-D wavelet transforms and entropy encoding based on parent-child subband tree. Finally, we propose one pass rate control which uses virtual buffer concept for adaptive Lagrange multiplier selection. Simulations results show that the proposed video codec has a much lower computational complexity (from 2 to 6 times) for the same quality level compared to the H.264/AVC standard in the low complexity mode.","1520-9210;1941-0077","","10.1109/TMM.2013.2269315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542759","Entropy coding;rate control;3-D DWT","","computational complexity;discrete wavelet transforms;entropy;video codecs;video coding","computational complexity;video codec;adaptive Lagrange multiplier selection;virtual buffer concept;one pass rate control;rate-distortion efficient criterion;adaptive binary range coder;low entropy context;Levenstein zero-run coder;wavelet subbands;three-dimensional discrete wavelet transform;3-D DWT based video coding;low-complexity bit-plane entropy coding","","15","","35","","","","","IEEE","IEEE Journals & Magazines"
"A Fast Coding Unit Size Decision Algorithm for HEVC Intra Coding Based on Image Complexity","D. Liu; X. Liu; Y. Li","NA; NA; NA","2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)","","2016","","","874","877","High Efficiency Video Coding (HEVC) achieves the most significant coding efficiency compared with all the existing video coding standards. However, the Intra encoding complexity is increased dramatically since the complex recursive search algorithm for the coding unit (CU) size decisions. In this paper, a fast CU size decision algorithm based on Support Vector Machines (SVM) is proposed to further alleviate the encoder computation load. Firstly, some effective image features are extracted to judge the CU complexity. Then, a three-classifier structure of CU size decisions is established based on SVM. The experimental results show that the proposed algorithm can reduce 53% intra coding time on average with 1.2% BDBR loss on average.","","978-1-5090-5880-8978-1-5090-5881","10.1109/iThings-GreenCom-CPSCom-SmartData.2016.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917208","HEVC;Intra prediction;CU size decision;SVM;image features","Encoding;Complexity theory;Support vector machines;Algorithm design and analysis;Classification algorithms;Prediction algorithms;Video coding","feature extraction;image coding;support vector machines","fast coding unit size decision algorithm;HEVC intra coding;image complexity;high efficiency video coding;video coding standards;intra encoding complexity;complex recursive search algorithm;CU size decisions;support vector machines;SVM;encoder computation load;image features;intra coding time","","","","16","","","","","IEEE","IEEE Conferences"
"Gamma Codes: A low-overhead linear-complexity network coding solution","K. Mahdaviani; M. Ardakani; H. Bagheri; C. Tellambura","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2V4; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2V4; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2V4; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada, T6G 2V4","2012 International Symposium on Network Coding (NetCod)","","2012","","","125","130","We introduce a family of sparse random linear network codes with outer-code. Due to the bold role of the incomplete gamma function in their design, we call these codes “Gamma codes”. We show that Gamma codes outperform all the existing linear-complexity network coding solutions in terms of reception overhead, while keeping the encoding and decoding complexity linear in the block length.","2374-9660","978-1-4673-1892-1978-1-4673-1890-7978-1-4673-1891","10.1109/NETCOD.2012.6261896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6261896","","Decoding;Complexity theory;Network coding;Equations;Encoding;Parity check codes;Mathematical model","computational complexity;linear codes;network coding;random codes","gamma codes;low-overhead linear-complexity network coding solution;sparse random linear network codes;outer-code;gamma function;decoding complexity;encoding complexity;block length","","18","","21","","","","","IEEE","IEEE Conferences"
"A high coding-gain reduced-complexity serial concatenated error-control coding solution for wireless sensor networks","D. P. Nguyen; T. H. Tran; Y. Nakashima","Graduated School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma-shi, Nara-ken, Japan; Graduated School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma-shi, Nara-ken, Japan; Graduated School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama-cho, Ikoma-shi, Nara-ken, Japan","2016 IEEE International Conference on Signal and Image Processing (ICSIP)","","2016","","","694","698","Error-Control Coding (ECC) now plays an important role in wireless transceivers because it helps to increase link reliability and lower required transmit power. One popular problem of ECC implementation is the trade-off between high coding-gain and decoder's complexity. In this paper, we propose a serial concatenated ECC solution which is a combination of truncated-iteration layered-decoding LDPC block code with low-constraint Viterbi decoder at low code rate. Simulation results show that very high coding-gain and reduced-complexity are achieved. This paper also gives a supposition about applying low code rate ECC for less-active wireless nodes in Wireless Sensor Networks (WSN) to reduce transmit power and expand sensor network topology.","","978-1-5090-2377-6978-1-5090-2375-2978-1-5090-2378","10.1109/SIPROCESS.2016.7888352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888352","component;error-control coding;ECC;coding-gain;concatenated;wireless sensor networks (WSN)","Wireless sensor networks;Convolutional codes;Error correction codes;Parity check codes;Decoding;Encoding;Complexity theory","block codes;communication complexity;error correction codes;iterative decoding;parity check codes;radio links;radio transceivers;telecommunication network reliability;telecommunication network topology;telecommunication power management;Viterbi decoding;wireless sensor networks","high coding-gain reduced-complexity serial concatenated error-control coding;wireless sensor networks;ECC implementation;wireless transceivers;link reliability;decoder complexity;truncated-iteration layered-decoding LDPC block code;low-constraint Viterbi decoder;less-active wireless nodes;WSN;transmit power reduction;sensor network topology","","","","10","","","","","IEEE","IEEE Conferences"
"Machine Learning-Based Coding Unit Depth Decisions for Flexible Complexity Allocation in High Efficiency Video Coding","Y. Zhang; S. Kwong; X. Wang; H. Yuan; Z. Pan; L. Xu","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Computer Science, The City University of Hong Kong, Hong Kong; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Information Science and Engineering, Shandong University, Jinan, China; Jiangsu Engineering Center of Network Monitoring, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2015","24","7","2225","2238","In this paper, we propose a machine learning-based fast coding unit (CU) depth decision method for High Efficiency Video Coding (HEVC), which optimizes the complexity allocation at CU level with given rate-distortion (RD) cost constraints. First, we analyze quad-tree CU depth decision process in HEVC and model it as a three-level of hierarchical binary decision problem. Second, a flexible CU depth decision structure is presented, which allows the performances of each CU depth decision be smoothly transferred between the coding complexity and RD performance. Then, a three-output joint classifier consists of multiple binary classifiers with different parameters is designed to control the risk of false prediction. Finally, a sophisticated RD-complexity model is derived to determine the optimal parameters for the joint classifier, which is capable of minimizing the complexity in each CU depth at given RD degradation constraints. Comparative experiments over various sequences show that the proposed CU depth decision algorithm can reduce the computational complexity from 28.82% to 70.93%, and 51.45% on average when compared with the original HEVC test model. The Bjøntegaard delta peak signal-to-noise ratio and Bjøntegaard delta bit rate are -0.061 dB and 1.98% on average, which is negligible. The overall performance of the proposed algorithm outperforms those of the state-of-the-art schemes.","1057-7149;1941-0042","","10.1109/TIP.2015.2417498","National Natural Science Foundation of China; Shenzhen Overseas High-Caliber Personnel Innovation and Entrepreneurship Project; Shenzhen Emerging Industries of the Strategic Basic Research Project; 100-Talents Program of Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070704","High Efficiency Video Coding;Coding Unit;Machine Learning, Support Vector Machine;High efficiency video coding;coding unit;machine learning;support vector machine","Complexity theory;Video coding;Support vector machines;Joints;Image coding;Classification algorithms;Prediction algorithms","binary decision diagrams;communication complexity;decision theory;decision trees;image classification;learning (artificial intelligence);quadtrees;rate distortion theory;video coding","machine learning;coding unit depth decision method;flexible complexity allocation;high efficiency video coding;HEVC;rate-distortion cost constraint;RD cost constraint;quadtree CU depth decision process;hierarchical binary decision problem;three-output joint classifier;multiple binary classifier;RD-complexity model;computational complexity;Bjøntegaard delta peak signal-to-noise ratio;Bjøntegaard delta bit rate","","67","","26","","","","","IEEE","IEEE Journals & Magazines"
"A Comparison of JEM and AV1 with HEVC: Coding Tools, Coding Efficiency and Complexity","T. Laude; Y. G. Adhisantoso; J. Voges; M. Munderloh; J. Ostermann","Institut für Informationsverarbeitung, Leibniz Universität Hannover, Germany; Institut für Informationsverarbeitung, Leibniz Universität Hannover, Germany; Institut für Informationsverarbeitung, Leibniz Universität Hannover, Germany; Institut für Informationsverarbeitung, Leibniz Universität Hannover, Germany; Institut für Informationsverarbeitung, Leibniz Universität Hannover, Germany","2018 Picture Coding Symposium (PCS)","","2018","","","36","40","The current state-of-the-art for standardized video codecs is High Efficiency Video Coding (HEVC) which was developed jointly by ISO/IEC and ITU-T. Recently, the development of two contenders for the next generation of standardized video codecs began: ISO/IEC and ITU-T advance the development of the Joint Exploration Model (JEM), a possible successor of HEVC, while the Alliance for Open Media pushes forward the video codec AV1. It is asserted by both groups that their codecs achieve superior coding efficiency over the state-of-the-art. In this paper, we discuss the distinguishing features of JEM and AV1 and evaluate their coding efficiency and computational complexity under well-defined and balanced test conditions. Our main findings are that JEM considerably outperforms HM and AV1 in terms of coding efficiency while AV1 cannot transform increased complexity into competitiveness in terms of coding efficiency with neither of the competitors except for the all-intra configuration.","2472-7822","978-1-5386-4160-6978-1-5386-4159-0978-1-5386-4161","10.1109/PCS.2018.8456291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456291","","Transforms;Encoding;Video codecs;Transform coding;Tools;Complexity theory","","","","1","","12","","","","","IEEE","IEEE Conferences"
"Low-Complexity Multi-Stream Space-Time Codes—Part II: Unitary-Transform Codes","B. Hochwald; E. Stauffer","Dept. of Electrical Engineering, University of Notre Dame; Broadcom Corporation, Sunnyvale, Ca 94086","IEEE Transactions on Communications","","2012","60","2","375","383","We examine the design of space-time codes that allow simple encoding and decoding of high and low-priority streams of data. This paper comprises two parts. In the first part we introduce the system model, establish performance and complexity criteria, and introduce ""direct-sum"" codes that combine existing space-time codes with hierarchical modulation. In this second part, we show that the direct-sum codes of the first part can be greatly improved upon by non-direct-sum designs. We demonstrate unitary-transform\/ codes for two and four antennas. In particular, one such code performs 4 dB better than the direct-sum Alamouti code, with per-bit decoding complexity on one stream that is a bounded function of the rate of the other stream.","0090-6778;1558-0857","","10.1109/TCOMM.2011.121511.100222A","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6111187","Space-time coding;broadcast;hierarchical coding;multi-stream","Complexity theory;Decoding;Phase shift keying;Turbo codes;Space-time codes;Signal to noise ratio","codes;decoding","multi stream space time codes;unitary transform codes;encoding;decoding;complexity criteria;hierarchical modulation;direct sum codes;non direct sum designs","","3","","12","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity product codes with LDPC codes achieving ultra low BER","W. Chen; Tongxin Dong","School of Electronic Information Engineering, Tianjin University, 300072, China; School of Electronic Information Engineering, Tianjin University, 300072, China","2012 IEEE 14th International Conference on Communication Technology","","2012","","","1312","1316","Ultra low bit error rate (BER) is required in high speed communications such as the 100 Gbit/s long haul optic communications. In this paper, a type of simple product codes is proposed using low density parity check (LDPC) codes with very high rate algebraic block codes by exploiting the sparsity of LDPC decoding failure and sparse error distribution in an LDPC frame with errors. Specifically, different with traditional product codes, the information bits are first encoded with very high rate algebraic block codes of low encoding and decoding complexity. Even the block codes correcting a single error using algebraic decoding method can be used. Then, LDPC codes are used as the inner code to obtain a low decoding threshold using iterative decoding. This design scheme can achieve the ultra low bit error rate at relatively low signal-to-noise ratios. The cost of the proposed scheme is the relatively large latency, which is not so critical for the ultra high speed communications.","","978-1-4673-2101-3978-1-4673-2100-6978-1-4673-2099","10.1109/ICCT.2012.6511401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511401","Product codes;LDPC codes;ultra low bit error rate;error floor","","algebraic codes;error statistics;failure analysis;iterative decoding;parity check codes;statistical distributions","low complexity product codes;ultralow BER;ultralow bit error rate;high speed communications;low density parity check codes;algebraic block codes;LDPC decoding failure;sparse error distribution;LDPC frame;algebraic decoding method;iterative decoding;signal-to-noise ratios","","2","","25","","","","","IEEE","IEEE Conferences"
"Multi-strata codes: space-time block codes with low detection complexity","M. Samuel; M. Fitz","UnWiReD Lab, UCLA Electrical Engineering Department, Los Angeles, CA 90095 USA; Northrop Grumman Space Technology, Redondo Beach, CA 90278 USA","IEEE Transactions on Communications","","2010","58","4","1080","1089","The multistrata space-time codes proposed by Wachsmann et al. were intended then to fix the drawback of orthogonal space-time block codes which is the relatively low spectral efficiency. However, these codes lack two things: 1) their performance is worse than codes designed using number theory like the Golden code. 2) The advantages of their structure were never exploited in reducing the detection complexity. In this paper, it is shown how the multistrata structure can potentially reduce the detection complexity of the sphere decoder in the uncoded case. The reduction in complexity is achieved in both the preprocessing and the search steps of the sphere decoder. Spacetime block codes having such a multistrata structure were found via optimization. Simulation results show that the obtained code has sufficiently high minimum determinant with a very small performance penalty compared to the best known 2 x 2 spacetime block code in literature, the Golden code. Also a simple means of generating bit log-likelihood ratios in a bit-interleaved coded modulation scenario is presented.","0090-6778;1558-0857","","10.1109/TCOMM.2010.04.080413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439311","MIMO channel;STBC;sphere decoder;bit LLR","Block codes;Decoding;Receiving antennas;Space time codes;Fading;Transmitting antennas;MIMO;Maximum likelihood detection;Interleaved codes;Modulation coding","block codes;decoding;interleaved codes;modulation coding;space-time codes","multistrata space-time codes;space-time block codes;spectral efficiency;detection complexity;sphere decoder;performance penalty;bit-interleaved coded modulation;Golden code;bit log-likelihood ratio","","9","","49","","","","","IEEE","IEEE Journals & Magazines"
"Visual Analytics Improving Data Understandability in IoT Projects: An Overview of the U. S. DOE ARM Program Data Science Tools","A. F. M. Batista; P. L. P. Correa; G. Palanisamy","NA; NA; NA","2016 IEEE 13th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)","","2016","","","349","354","The IoT advancement generates a massive volume and a variety of data at unprecedented velocity. There is a need to interpret and to convert such data into valuable information, enabling the discovery of knowledge as well as the development of new technologies and data products. From the Data Science research field, Visual Analytics (VA) techniques seek to support in the analysis and understanding of large datasets using statistical techniques and data mining, aided by visualization techniques. This paper demonstrates how VA techniques can be applied in Data Science projects enabling users to reason over a dataset generated by IoT devices, what we call as Data Understandability. A practical case is presented in order to provide an overview of the data management procedures and how a set of Visual Analytics tools developed in the context of The U. S. Department of Energy Atmospheric Radiation Measurement (ARM) Program are helping users to visualize and analyze the collected data from a climate sensor network spread throughout the world.","2155-6814","978-1-5090-2833-7978-1-5090-2832-0978-1-5090-2834","10.1109/MASS.2016.052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815046","Data Science;Visual Analytics;Data Understandability;IoT;ARM Program","","data mining;data visualisation;Internet of Things;statistical analysis","visual analytics;data understandability;IoT projects;U. S. DOE ARM program data science tools;statistical techniques;data mining;data visualization","","2","","31","","","","","IEEE","IEEE Conferences"
"Improving understandability in teaching of software engineering and connectivity with the industry","I. Alsmadi; B. Abul-Huda","Department of Computer Science and Information Technology, Yarmouk University, Irbid, Jordan; Department of Computer Science and Information Technology, Yarmouk University, Irbid, Jordan","2011 IEEE Global Engineering Education Conference (EDUCON)","","2011","","","20","25","As a new field of study, software engineering teaching and subjects vary from one textbook to another. Despite the fact that most of the books cover similar subjects, however, students' view of the subject is mixed. Some students have problems understanding the entire picture. Other students have problems connecting concepts with each other. In this research, an overall view of software engineering knowledge is presented. The knowledge is presented from four perspectives: Process, Project, People and Product. Those four are usually referred to as the 4Ps in literature. The goal is to make a distinction between the progresses in each area and explore the opportunities in finding windows for more research in any of those four views. Researches in this field in many published articles appear to ignore the state of the studied field in the industry and focuses on its state in the academic arenas. Related work in research papers focus on those research papers published and do not look in company websites, web logs, discussion boards, etc. Teaching software engineering should combine interactive methods of teaching besides the traditional class room teaching. Without such methods, the majority of the sought benefits and expected skills to learn may not be practical. Academic curriculums and research papers should give more attention to the industry and its current technologies. This can help students in their future jobs. It also helps the industry utilizing such researches once they become more realistic or relevant.","2165-9559;2165-9567","978-1-61284-643-9978-1-61284-642-2978-1-61284-641","10.1109/EDUCON.2011.5773106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773106","Software engineering;ontology;separation of concerns;project;product;process and people;CASE tools;software development methodology","Software engineering;Software;Education;Biological system modeling;Industries;Companies;Unified modeling language","computer aided instruction;computer science education;software engineering;teaching","software engineering teaching;software engineering knowledge;process perspective;project perspective;people perspective;product perspective;interactive methods","","1","","34","","","","","IEEE","IEEE Conferences"
"Visualizing Software Structure Understandability","P. Dugerdil; M. Niculescu","NA; NA","2014 23rd Australian Software Engineering Conference","","2014","","","110","119","Software architecture design is known to be driven by the quality attributes we may want to satisfy. Among them, modifiability plays an important role since software maintenance takes the lion's share in the software development costs. However, to successfully maintain a legacy system, the latter must be sufficiently understood so that the maintenance team will not introduce new bugs when correcting others. Then we present a software metric that we called the Autonomy Ratio (AR). We show this dynamic metric to be a good indicator of the system's structure understandability. Since we end up with hundreds of values for a single system, we represent these values as a hierarchical map: the ""Autonomy Ratio Map"". The contribution of the paper is to link the AR metric with theories of software comprehension, to show how the AR Map helps in assessing software structure understand-debility, and to present an empirical validation of it.","1530-0803;2377-5408","978-1-4799-3149","10.1109/ASWEC.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824115","program comprehension;software metrics;software visualization;dynamic analysis;software architecture","Software;Measurement;Collaboration;Couplings;Maintenance engineering;Color;Business","data visualisation;software architecture;software maintenance;software metrics;software quality","software structure understandability visualization;software architecture design;quality attributes;modifiability;software maintenance;software development costs;legacy system;software metric;autonomy ratio map;dynamic metric;system structure understandability;hierarchical map;AR metric;software comprehension;AR map;software structure understand-debility assessment","","2","","39","","","","","IEEE","IEEE Conferences"
"The Impact of Hierarchies on the Architecture-Level Software Understandability - A Controlled Experiment","S. Stevanetic; M. A. Javed; U. Zdun","NA; NA; NA","2015 24th Australasian Software Engineering Conference","","2015","","","98","107","Architectural component models represent high level designs and are frequently used as a central view of architectural descriptions of software systems. They play a crucial role in the whole development process and in achieving the desired software qualities. This paper presents an empirical study that examines the impact of hierarchies on the architecture-level software understand ability. In particular we have studied three different architectural representations of a large-size software system, one with a hierarchical representation where architectural components at all abstraction levels in the hierarchy are shown, and two that do not contain hierarchical abstractions but concentrate only on the lowest level or on the highest level components in the hierarchy. We conducted a controlled experiment in which participants of three groups received one of the three architecture documentations plus the source code of the system and had to answer understand ability related questions. Our results show that using the hierarchical architecture leads to: 1) higher quantity of correctly retrieved elements, 2) lower quantity of incorrectly retrieved elements, and 3) higher overall quality of retrieved elements. The obtained results provide empirical evidence that hierarchies play an important role in the context of architectural component models from the viewpoint of the architecture-level software understandability.","1530-0803","978-1-4673-9390","10.1109/ASWEC.2015.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365798","software architecture;hierarchies;understandability;controlled experiment","Unified modeling language;Software architecture;Software systems;Computer architecture;Context;Business","software architecture;software quality;source code (software)","architecture-level software understandability;software quality;large-size software system;source code","","","","33","","","","","IEEE","IEEE Conferences"
"On the Understandability of Semantic Constraints for Behavioral Software Architecture Compliance: A Controlled Experiment","C. Czepa; H. Tran; U. Zdun; T. T. T. Kim; E. Weiss; C. Ruhsam","NA; NA; NA; NA; NA; NA","2017 IEEE International Conference on Software Architecture (ICSA)","","2017","","","155","164","Software architecture compliance is concerned with the alignment of implementation with its desired architecture and detecting potential inconsistencies. The work presented in this paper is specifically concerned with behavioral architecture compliance. That is, the focus is on semantic alignment of implementation and architecture. In particular, this paper evaluates three representative approaches for describing semantic constraints in terms of their understandability, namely natural language descriptions as used in many architecture documentations today, a structured language based on specification patterns that abstract underlying temporal logic formulas, and a structured cause-effect language that is based on Complex Event Processing. We conducted a controlled experiment with 190 participants using a simple randomized design with one alternative per experimental unit. Overall all approaches support a high level of correct understanding, and the statistical inference suggests that all tested approaches are equally well suited for describing semantic constraints for behavioral architecture compliance in terms of understandability. In consequence this indicates that it is possible to benefit from the tested structured languages with underlying formal representations for automated verification without having to suffer from decreased understandability. Vice versa, the results suggest that the use of natural language can be a suitable way to document architecture semantics when reliable automated support for formal verification is of minor importance.","","978-1-5090-5729-0978-1-5090-5730","10.1109/ICSA.2017.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930212","Behavioral Software Architecture Compliance;Semantic Constraints;Controlled Experiment;Understandability","Computer architecture;Semantics;Runtime;Natural languages;Software architecture;Software;Model checking","formal verification;inference mechanisms;natural language processing;software architecture","behavioral software architecture compliance;semantic constraints;natural language descriptions;architecture documentations;structured language;temporal logic formulas;complex event processing;statistical inference;formal representations;automated verification;formal verification","","2","","59","","","","","IEEE","IEEE Conferences"
"Automatically assessing code understandability: How far are we?","S. Scalabrino; G. Bavota; C. Vendome; M. Linares-Vásquez; D. Poshyvanyk; R. Oliveto","University of Molise, Italy; Università della Svizzera italiana (USI), Switzerland; The College of William and Mary, USA; Universidad de los Andes, Colombia; The College of William and Mary, USA; University of Molise, Italy","2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2017","","","417","427","Program understanding plays a pivotal role in software maintenance and evolution: a deep understanding of code is the stepping stone for most software-related activities, such as bug fixing or testing. Being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity, in comparing the quality of alternative implementations, or even in predicting bugs. Unfortunately, there are no existing metrics specifically designed to assess the understandability of a given code snippet. In this paper, we perform a first step in this direction, by studying the extent to which several types of metrics computed on code, documentation, and developers correlate with code understandability. To perform such an investigation we ran a study with 46 participants who were asked to understand eight code snippets each. We collected a total of 324 evaluations aiming at assessing the perceived understandability, the actual level of understanding, and the time needed to understand a code snippet. Our results demonstrate that none of the (existing and new) metrics we considered is able to capture code understandability, not even the ones assumed to assess quality attributes strongly related with it, such as code readability and complexity.","","978-1-5386-2684-9978-1-5386-3976","10.1109/ASE.2017.8115654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115654","Software metrics;Code understandability;Empirical study;Negative result","Measurement;Complexity theory;Software;Computer bugs;Correlation;Maintenance engineering;Documentation","program debugging;public domain software;software maintenance;software metrics;software quality","program understanding;software maintenance;software-related activities;perceived understandability;code readability;code snippet;code complexity;automatic code understandability assessibility;quality attributes","","3","","37","","","","","IEEE","IEEE Conferences"
"""Automatically Assessing Code Understandability"" Reanalyzed: Combined Metrics Matter","A. Trockman; K. Cates; M. Mozina; T. Nguyen; C. Kästner; B. Vasilescu","NA; NA; NA; NA; NA; NA","2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)","","2018","","","314","318","Previous research shows that developers spend most of their time understanding code. Despite the importance of code understandability for maintenance-related activities, an objective measure of it remains an elusive goal. Recently, Scalabrino et al. reported on an experiment with 46 Java developers designed to evaluate metrics for code understandability. The authors collected and analyzed data on more than a hundred features describing the code snippets, the developers' experience, and the developers' performance on a quiz designed to assess understanding. They concluded that none of the metrics considered can individually capture understandability. Expecting that understandability is better captured by a combination of multiple features, we present a reanalysis of the data from the Scalabrino et al. study, in which we use different statistical modeling techniques. Our models suggest that some computed features of code, such as those arising from syntactic structure and documentation, have a small but significant correlation with understandability. Further, we construct a binary classifier of understandability based on various interpretable code features, which has a small amount of discriminating power. Our encouraging results, based on a small data set, suggest that a useful metric of understandability could feasibly be created, but more data is needed.","2574-3864;2574-3848","978-1-4503-5716-6978-1-5386-6171","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595214","code understandability;metric;reanalysis;statistical modeling;readability;classification;replication;program comprehension","Measurement;Java;Documentation;Principal component analysis;Data models;Computational modeling;Predictive models","Java;software maintenance;software metrics;statistical analysis","code snippets;interpretable code features;Java developers;metrics matter combination;assessing code understandability;statistical modeling techniques;syntactic structure;syntactic documentation;binary classifier","","","","22","","","","","IEEE","IEEE Conferences"
"EEG-based fuzzy cognitive load classification during logical analysis of program segments","D. Chatterjee; A. Sinharay; A. Konar","Innovation Lab, Kolkata, Tata consultancy Services Ltd., India; Innovation Lab, Kolkata, Tata consultancy Services Ltd., India; Dept. of Electronics and Tele-communication Engg, Jadavpur University, Kolkata, India","2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","","2013","","","1","6","The paper aims at designing a novel scheme for cognitive load classification of subjects engaged in program analysis. The logic of propositions has been employed here to construct program segments to be used for cognitive load analysis and classification. Electroencephalogram signals acquired from the subjects during program analysis are first fuzzified and the resultant fuzzy membership functions are then submitted to the input of a fuzzy rule-based classifier to determine the class of the cognitive load of the subjects. Experimental results envisage that the proposed classifier has a good classification accuracy of 86.2%. Performance analysis of the fuzzy classifier further reveals that it outperforms two most widely used classifiers: Support Vector Machine and Naive Bayes classifier.","1098-7584","978-1-4799-0022-0978-1-4799-0020","10.1109/FUZZ-IEEE.2013.6622508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622508","Cognitive load;EEG;fuzzy;propositional logic;BCI","Electroencephalography;Complexity theory;Cognition;Brain;Upper bound;Support vector machines;Performance analysis","electroencephalography;fuzzy logic;pattern classification;program diagnostics","EEG;fuzzy cognitive load classification;logical program segments analysis;electroencephalogram signals;resultant fuzzy membership functions;fuzzy rule-based classifier;performance analysis","","9","","28","","","","","IEEE","IEEE Conferences"
"Application domain and programming language readability yardsticks","M. Akour; B. Falah","Computer Information Systems Department, Yarmouk University, Irbid-Jordan; School of Science and Engineering, Al Akhawayn University, Ifrane, Morocco","2016 7th International Conference on Computer Science and Information Technology (CSIT)","","2016","","","1","6","Software maintainability is one of the most factors that used to ensure software quality. It targets the structure of the software rather than its functionality. Hence, code readability is highlighted whenever maintainability is discussed. There is a huge difference between an organized code and a messy code, or between easy to read code and a difficult to read code. This difference can be very subjective but many efforts have been put together in order to formalize it. Therefore, the result was a set of readability factors that have a direct or indirect impact on software readability. These factors measure to which extent readers can understand the text of the software code. Many metrics were developed to generalize the readability score across a set of pieces of code. Unfortunately, not all proposed metrics take into account the possibility of having variable readability factors. This paper tries to investigate to which extent the readability factors considering different application domains. Since readability is very subjective, it will be interesting to see whether it is useful to have specialized readability models that measure this attribute. This also implies looking into the impact of these readability models on measuring software quality.","","978-1-4673-8914-3978-1-4673-8915","10.1109/CSIT.2016.7549476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549476","software readability;application domain;readability models;programming paradigms","Syntactics;Measurement;Java;Software quality;Readability metrics","programming languages;software maintenance;software quality","application domain;programming language readability yardsticks;software maintainability;software quality;code readability;organized code;messy code;software readability models;software code;readability score;variable readability factors","","","","14","","","","","IEEE","IEEE Conferences"
"A Stride-Away Programming Scheme to Resolve Crash Recoverability and Data Readability Issues of Multi-Level-Cell Flash Memory","C. Ho; Y. Li; P. Lin; W. Wang; Y. Chang","NA; NA; NA; NA; NA","2018 IEEE 7th Non-Volatile Memory Systems and Applications Symposium (NVMSA)","","2018","","","67","72","The multi-level-cell (MLC) technique is widely adopted by flash memory vendors to increase the chip capacity and to lower the cost, but also results in serious reliability problems. To improve the reliability of MLC flash memory, conventional MLC programming approaches tend to adopt the incremental step pulse program (ISPP) procedure and N-shape programming sequence to program the MLC flash cells. These approaches can improve the reliability of MLC flash memory by reducing the effects of programming disturbance; however, it could further result in the crash recoverability and data readability issues. To ensure the crash recoverability, the backup procedure is necessary for supporting the sudden-power-off-recovery function, and it is typically adopted to avoid data corruption before programming TLC flash pages. Such a backup procedure would further result in the bad programming performance. This motivates this work to explore the innovative programming design for resolving crash recoverability and data readability issues of multi-level-cell flash memory. Thus, to eliminate the negative effects caused by the conventional programming methods, this paper presents and realizes a stride-away MLC programming scheme. The proposed stride-away programming scheme could resolve both the crash recoverability and the data readability issues without the adoption of backup procedures. As a result, the programming performance can also be improved with the proposed stride-away programming scheme. A series of experiments were conducted to evaluate the capability of the proposed design, for which we prove that the proposed scheme can boost the write performance up to 1.8 times.","2575-257X;2575-2561","978-1-5386-7403-1978-1-5386-7404","10.1109/NVMSA.2018.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537697","Stride away programming;crash recoverbility;data readability","Programming;Threshold voltage;Computer crashes;Reliability;Flash memories;Random access memory;Nonvolatile memory","flash memories;integrated circuit reliability;NAND circuits","stride-away programming scheme;data readability issues;multilevel-cell flash memory;multilevel-cell technique;flash memory vendors;MLC flash memory;incremental step pulse program procedure;N-shape programming sequence;MLC flash cells;programming disturbance;backup procedure;TLC flash pages;stride-away MLC programming scheme","","","","17","","","","","IEEE","IEEE Conferences"
"A framework for enhancing readability and opportunistic reuse of enterprise software","S. A. Amjad; S. A. Khan","Department of Computer Engineering, National University of Sciences and Technology, Islamabad, Pakistan; Department of Computer Engineering, National University of Sciences and Technology, Islamabad, Pakistan","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","48","53","During in-house development of custom enterprise web based applications, a lot of reusable code is written by software developers. This code from smaller units to sub systems and sometimes even fully functional autonomous components is, at times, required in other projects due to related needs across the enterprise. In the absence of a component model based development, it becomes nearly impossible to organize, maintain and reuse such code. The challenge of meeting enterprise automation needs vis-à-vis an ever shrinking project timeline tends to provide for lesser code documentation time. This results into poorly documented source code that though provides required functionality on time and meets user requirements accurately, yet allows for minimal reuse, mostly relying on developer's memory and is difficult to maintain. This problem is typically faced by newly built IT infrastructures in public sector organizations when business processes are being automated and new business requirements for automation keep accumulating. The job of a solution designer becomes complex when source code and component information is not organized in a way that allows for it to be reused in an upcoming project. This paper proposes a framework for documenting source code of custom developed enterprise application software that may not conform to a component model in the form of XML descriptors. Resultantly, it becomes possible to enhance the readability and opportunistic reuse of otherwise non reusable source code.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339004","XML;Software Reuse;Custom Enterprise Applications;Software Documentation","Documentation;XML;Software reusability;Business;Automation","business data processing;software reusability;source code (software)","readability;opportunistic reuse;custom enterprise Web based applications development;reusable code;enterprise automation;IT infrastructures;public sector organizations;business processes;business requirements;source code documentation;custom developed enterprise application software","","","","12","","","","","IEEE","IEEE Conferences"
"Software readability practices and the importance of their teaching","I. B. Sampaio; L. Barbosa","Institute of Engineering of Porto, Polytechnic Institute of Porto, 4200-072 Porto, Portugal; School of Science and Technology, University of Trás-os-Montes and Alto Douro, Apartado 1013, 5001-801 Vila Real, Portugal","2016 7th International Conference on Information and Communication Systems (ICICS)","","2016","","","304","309","It is known that code readability has an impact on software quality. This paper introduces a preliminary list of 33 good practices for code readability that could be taught in object oriented programming courses of informatics programs. In this set are included practices with both positive and negative impact, the latter mainly from bad smells. A survey was made with OOP teachers in order to assess the significance of teaching a set of good practices. The results showed that teachers consider it important to teach the defined set. Such result was statistically significant. Still, the same level of significance was not awarded to all practices. Some additional results are presented.","","978-1-4673-8614-2978-1-4673-8613","10.1109/IACS.2016.7476069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476069","","Education;Informatics;Software;Bibliographies;Instruments;Electronic mail;Object oriented programming","computer science education;educational courses;object-oriented programming;software quality;teaching","software readability practices;teaching;code readability;software quality;object oriented programming courses;informatics programs","","","","22","","","","","IEEE","IEEE Conferences"
"Learning a Metric for Code Readability","R. P. L. Buse; W. R. Weimer","University of Virginia, Charlottesville; University of Virginia, Charlottesville","IEEE Transactions on Software Engineering","","2010","36","4","546","558","In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from 120 human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80 percent effective and better than a human, on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with three measures of software quality: code changes, automated defect reports, and defect log messages. We measure these correlations on over 2.2 million lines of code, as well as longitudinally, over many releases of selected projects. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggest that comments, in and of themselves, are less important than simple blank lines to local judgments of readability.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2009.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332232","Software readability;program understanding;machine learning;software maintenance;code metrics;FindBugs.","Software quality;Humans;Software maintenance;Readability metrics;Documentation;Software measurement;Computer languages;Design engineering;Machine learning;Costs","human factors;software quality","code readability;software quality;local code features;human notions;code changes;automated defect reports;defect log messages;programming language design","","66","","41","","","","","IEEE","IEEE Journals & Magazines"
"Improving code readability models with textual features","S. Scalabrino; M. Linares-Vásquez; D. Poshyvanyk; R. Oliveto","University of Molise, Pesche (IS), Italy; The College of William and Mary, Williamsburg, Virginia, USA; The College of William and Mary, Williamsburg, Virginia, USA; University of Molise, Pesche (IS), Italy","2016 IEEE 24th International Conference on Program Comprehension (ICPC)","","2016","","","1","10","Code reading is one of the most frequent activities in software maintenance; before implementing changes, it is necessary to fully understand source code often written by other developers. Thus, readability is a crucial aspect of source code that may significantly influence program comprehension effort. In general, models used to estimate software readability take into account only structural aspects of source code, e.g., line length and a number of comments. However, source code is a particular form of text; therefore, a code readability model should not ignore the textual aspects of source code encapsulated in identifiers and comments. In this paper, we propose a set of textual features aimed at measuring code readability. We evaluated the proposed textual features on 600 code snippets manually evaluated (in terms of readability) by 5K+ people. The results demonstrate that the proposed features complement classic structural features when predicting code readability judgments. Consequently, a code readability model based on a richer set of features, including the ones proposed in this paper, achieves a significantly higher accuracy as compared to all of the state-of-the-art readability models.","","978-1-5090-1428","10.1109/ICPC.2016.7503707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503707","","Feature extraction;Syntactics;Visualization;Computational modeling;Semantics;Software quality","software maintenance;source code (software);text analysis","code readability models;textual features;software maintenance;source code;program comprehension effort;software readability;line length;code snippets","","9","","46","","","","","IEEE","IEEE Conferences"
"Automatic Segmentation of Method Code into Meaningful Blocks to Improve Readability","X. Wang; L. Pollock; K. Vijay-Shanker","NA; NA; NA","2011 18th Working Conference on Reverse Engineering","","2011","","","35","44","With the goal of increasing program readability for easier understanding, coding guidelines often include formatting standards such as indenting loop and conditional branch body statements. Similarly, good programming practice suggests that programmers use blank lines to visibly delineate between code segments that represent different algorithmic steps or high level actions. Unfortunately, programmers do not always follow these guidelines. While editors and IDEs can easily indent code based on syntax, they do not currently support automatic blank line insertion, which presents more significant challenges involving the semantics. This paper presents a heuristic solution to the automatic blank line insertion problem, by leveraging both program structure and naming information to identify ""meaningful blocks"", consecutive statements that logically implement a high level action. Our tool, SEGMENT, takes as input a Java method, and outputs a segmented version that separates meaningful blocks by vertical spacing. We report on an evaluation of the effectiveness of SEGMENT based on developers' opinions. SEGMENT assists in making users obtain an overall picture of a method's actions and comprehend it quicker r as well as provides hints for internal documentation placement.","2375-5369;1095-1350","978-1-4577-1948","10.1109/WCRE.2011.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079773","program understanding;readability;software tool;automatic formatting","Syntactics;Java;Semantics;Humans;Reverse engineering;Encoding;Compounds","document handling;Java;program compilers;software maintenance;source coding","automatic method code segmentation;program readability;formatting standards;indenting loop;conditional branch body statements;blank lines;IDE;automatic blank line insertion problem;SEGMENT;Java method;internal documentation placement","","11","","31","","","","","IEEE","IEEE Conferences"
"Analysis of coding tools and improvement of text readability for screen content","H. Meuel; J. Schmidt; M. Munderloh; J. Ostermann","Institut für Informationsverarbeitung, Gottfried Wilhelm Leibniz Universität Hannover, Hannover, Germany; Institut für Informationsverarbeitung, Gottfried Wilhelm Leibniz Universität Hannover, Hannover, Germany; Institut für Informationsverarbeitung, Gottfried Wilhelm Leibniz Universität Hannover, Hannover, Germany; Institut für Informationsverarbeitung, Gottfried Wilhelm Leibniz Universität Hannover, Hannover, Germany","2012 Picture Coding Symposium","","2012","","","469","472","Current video coding standards perform well for video sequences captured by a real camera. The aperture of the camera's optical system smooths the content and attenuates higher frequencies. New application scenarios, enabled by the growing number of high bit rate internet gateways, however, make it necessary to take a closer look at the efficiency of such standards in handling artificial content. Remote desktop applications for example often include text parts. As a consequence, these content types contain sharp edges or high frequencies, which are considered less important in natural video and are therefore treated less carefully. The frequent result is an increased occurrence of artefacts or the loss of information that is actually important to the user. This paper gives an analysis of such artificially created video sequences, evaluates the performance of current coding tools for this type of content and proposes a simple, yet effective way to maintain readability of text within video material using only well considered encoder control and without the need of large additional modules.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213256","","Encoding;Image coding;Bit rate;PSNR;Video sequences;Image edge detection;Video coding","image sequences;standards;video coding","coding tool analysis;text readability;screen content;video coding standards;video sequences;camera optical system;high bit rate Internet gateways;artificial content handling;remote desktop applications;natural video;video material;encoder control","","1","","13","","","","","IEEE","IEEE Conferences"
"A Refined Decompiler to Generate C Code with High Readability","G. Chen; Z. Wang; R. Zhang; K. Zhou; S. Huang; K. Ni; Z. Qi; K. Chen; H. Guan","NA; NA; NA; NA; NA; NA; NA; NA; NA","2010 17th Working Conference on Reverse Engineering","","2010","","","150","154","As a key part of reverse engineering, decompilation plays a very important role in software security and maintenance. Unfortunately, most existing decompilation tools suffer from the low accuracy in identifying variables, functions and composite structures, which results in poor readability. To address these limitations, we present a practical decompiler called C-Decompiler for Windows C programs that (1) uses a shadow stack to perform refined data flow analysis, and (2) adopts inter-basic-block register propagation to reduce redundant variables. Our experimental results illustrate that on average C-Decompiler has the highest total percentage reduction of 55.91%, lowest variable expansion rate of 55.79% in the three tools, and the same Cyclomatic Complexity as the original source code for each test application. Furthermore, in our experiment, C-Decompiler is able to recognize functions with lower false positive and false negative rate. In the studies, we show that C-Decompiler is a practical tool to produce highly readable C code.","2375-5369;1095-1350","978-1-4244-8911","10.1109/WCRE.2010.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645445","Reverse Engineering;Decompilation","Registers;Algorithm design and analysis;Benchmark testing;Accuracy;Educational institutions;Software;Binary codes","C language;computational complexity;data flow analysis;program compilers;software maintenance;software tools;source coding","C code generation;high readability;reverse engineering;software security;software maintenance;software tool;C-decompiler;Windows C program;data flow analysis;interbasic-block register propagation;source code;cyclomatic complexity","","4","","9","","","","","IEEE","IEEE Conferences"
"Measurement of Source Code Readability Using Word Concreteness and Memory Retention of Variable Names","W. Xu; D. Xu; L. Deng","NA; NA; NA","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","","2017","1","","33","38","Source code readability is critical to software quality assurance and maintenance. In this paper, we present a novel approach to the automated measurement of source code readability based on Word Concreteness and Memory Retention (WCMR) of variable names. The approach considers programming and maintenance as processes of organizing variables and their operations to describe solutions to specific problems. The overall readability of given source code is calculated from the readability of all variables contained in the source code. The readability of each variable is determined by how easily its meaning is memorized (i.e., word concreteness) and how quickly they are forgotten over time (i.e., memory retention). Our empirical study has used 14 open source applications with over a half-million lines of code and 10,000 warning defects. The result shows that the WCMR-based source code readability negatively correlates strongly with overall warning defect rates, and particularly with such warning as bad programming practices, code vulnerability, and correctness bug warning.","0730-3157","978-1-5386-0367-3978-1-5386-0368","10.1109/COMPSAC.2017.166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029587","Code Readability;Variable Definitions and References;Word Concreteness;Memory Retention","Programming;Dictionaries;Maintenance engineering;Semantics;Software quality","program debugging;public domain software;software fault tolerance;software maintenance;software quality;source code (software)","variable names;code vulnerability;software quality assurance;open source applications;software maintenance;source code readability measurement;word concreteness and memory retention;WCMR;warning defects;correctness bug warning","","","","22","","","","","IEEE","IEEE Conferences"
"Automatic FFT code generation for FPGAs with high flexibility and human readability","J. O'Sullivan; S. Weiss; G. Rice","Institute for System Level Integration, Heriot-Watt University, Edinburgh, Scotland; Centre for Signal and Image Processing, Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, Scotland, UK; Steepest Ascent Ltd, 94 Duke Street, Glasgow, G4 0UW, Scotland, UK","2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)","","2011","","","2197","2201","This paper describes a Fast Fourier Transform (FFT) core which uses code generation to create optimised Hardware Description Language (HDL) code for a radix-2, decimation in time FFT. The generated code is designed to be human readable, vendor non-specific and is available in both Verilog and VHDL languages. A choice of In-place or Multipath Delay Commutator (MDC) architectures is provided. Selectable architectures and generic, readable HDL code make the core highly flexible for use in different applications and with different hardware platforms. The implementation of the available architectures and their relative merits are discussed. Maximum clock speed and resource requirements are examined and compared.","1058-6393;1058-6393;1058-6393","978-1-4673-0323-1978-1-4673-0321-7978-1-4673-0322","10.1109/ACSSC.2011.6190421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190421","","Hardware design languages;Throughput;Clocks;Hardware;Memory management;Field programmable gate arrays","digital arithmetic;fast Fourier transforms;field programmable gate arrays;hardware description languages;logic CAD;optimising compilers","automatic FFT code generation;FPGA;human readability;fast Fourier transform core;optimised hardware description language code;radix-2;decimation;Verilog;VHDL language;In-place architecture;Multipath Delay Commutator;MDC architecture;selectable architecture;readable HDL code;hardware platform;clock speed;resource requirement","","","","9","","","","","IEEE","IEEE Conferences"
"Code Readability Testing, an Empirical Study","T. Sedano","NA","2016 IEEE 29th International Conference on Software Engineering Education and Training (CSEET)","","2016","","","111","117","Context: One of the factors that leads to improved code maintainability is its readability. When code is difficult to read, it is difficult for subsequent developers to understand its flow and its side effects. They are likely to introduce new bugs while trying to fix old bugs or adding new features. But how do software developers know they have written readable code? Objective: This paper presents a new technique, Code Readability Testing, to determine whether code is readable and evaluates whether the technique increases programmers' ability to write readable code. Method: The researcher conducted a field study using 21 software engineering master students and followed the Code Readability Testing with each student in four separate sessions evaluating different ""production ready"" software. After the observations, a questionnaire evaluated the programmer's perspective. Results: By following Code Readability Testing, half of the programmers writing ""unreadable"" code started writing ""readable"" code after four sessions. Programmers writing ""readable"" code also improved their ability to write readable code. The study reveals that the most frequent suggestions for increasing code readability are improving variable names, improving method names, creating new methods in order to reduce code duplication, simplifying if conditions and structures, and simplifying loop conditions. The programmers report that readability testing is worth their time. They observe increases in their ability to write readable code. When programmers experience a reader struggling to understand their code, they become motivated to write readable code. Conclusion: This paper defines code readability, demonstrates that Code Readability Testing improves programmers' ability to write readable code, and identifies frequent fixes needed to improve code readability.","2377-570X","978-1-5090-0765-3978-1-5090-0764","10.1109/CSEET.2016.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474473","","Testing;Software;Programming profession;Writing;Software engineering;Computers","computer science education;program control structures;software maintenance","code readability testing;code maintainability;readable code;code duplication reduction;loop conditions;software engineering master students","","1","","30","","","","","IEEE","IEEE Conferences"
"IDE-independent program comprehension tools via source file overwriting","M. Sulír; J. Porubän; O. Zoricák","Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice Letná 9, 042 00 Košice, Slovakia; Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice Letná 9, 042 00 Košice, Slovakia; Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice Letná 9, 042 00 Košice, Slovakia","2017 IEEE 14th International Scientific Conference on Informatics","","2017","","","372","376","Traditionally, we have two possibilities to design tools for program comprehension and analysis. The first option is to create a standalone program, independent of any source code editor. This way, the act of source code editing is separated from the act of viewing the code analysis results. The second option is to create a plugin for a specific IDE (integrated development environment) - in this case, a separate version must be created for each IDE. We propose an approach where information about source code elements is written directly into source files as annotations or special comments. Before committing to a version control system, the annotations are removed from the source code to avoid code pollution. We briefly evaluate the approach and delineate its limitations.","","978-1-5386-0889-0978-1-5386-0888-3978-1-5386-0890","10.1109/INFORMATICS.2017.8327277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327277","","Tools;Metadata;Java;Control systems;Measurement;Informatics;Computers","configuration management;programming environments;software tools;source code (software)","IDE-independent program comprehension tools;design tools;standalone program;source code editor;source code editing;code analysis results;integrated development environment;source file overwriting;version control system","","","","18","","","","","IEEE","IEEE Conferences"
"Enabling program comprehension through a visual object-focused development environment","F. Olivero; M. Lanza; M. D'Ambros; R. Robbes","REVEAL @ Faculty of Informatics - University of Lugano, Switzerland; REVEAL @ Faculty of Informatics - University of Lugano, Switzerland; REVEAL @ Faculty of Informatics - University of Lugano, Switzerland; PLEIAD@DCC - University of Chile, Chile","2011 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","","2011","","","127","134","Integrated development environments (IDEs) include many tools that provide the means to construct programs. Coincidentally, the very same IDEs are a primary vehicle for program comprehension. We claim that IDEs may be an impediment for program comprehension because they treat software elements as text, which may be counterproductive in the context of program understanding-where abstracting from the source text to the level of structural entities and relationships is the key. We are currently building Gaucho, a visual object-focused environment that allows developers to write programs by creating and manipulating lightweight and intuitive depictions of object-oriented constructs. The research question we investigate here is how such an environment compares with traditional IDEs when it comes to performing program comprehension tasks. To answer our question, we conducted a preliminary controlled experiment with eight subjects, comparing Gaucho against a traditional IDE. We found that Gaucho outperforms the IDE regarding the correctness of the tasks, while it is slower with respect to the completion time. Our preliminary results suggest that alternative-visual-IDEs may be superior to traditional IDEs as program comprehension aids.","1943-6106;1943-6092;1943-6092","978-1-4577-1247-0978-1-4577-1246-3978-1-4577-1244","10.1109/VLHCC.2011.6070389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6070389","","Shape;Navigation;Visualization;Software;Programming;Object oriented modeling;Layout","object-oriented methods;software maintenance","program comprehension;visual object-focused development environment;integrated development environment;program understanding context;Gaucho environment;object-oriented constructs","","3","","25","","","","","IEEE","IEEE Conferences"
"A Multiple View Interactive Environment to Support MATLAB and GNU/Octave Program Comprehension","I. d. M. Lessa; G. d. F. Carneiro; M. J. T. P. Monteiro; F. B. e. Abreu","NA; NA; NA; NA","2015 12th International Conference on Information Technology - New Generations","","2015","","","552","557","Program comprehension plays an important role in Software Engineering. In fact, many of the software lifecycle activities depend on program comprehension. Despite the importance of MATLAB and Octave programing languages in the Engineering and Statistical communities, little attention has been paid to the conception, implementation and characterization of tools and techniques for the comprehension of programs written in these languages. Considering this scenario, this paper presents a Multiple View Interactive Environment (MVIE) called Oct Miner that supports the comprehension of programs developed in the aforementioned languages. Oct Miner provides a set of coordinated visual metaphors that can be adjusted in accordance with the comprehension goals. An example is presented to illustrate the main functionalities of Oct Miner in a real scenario of program comprehension.","","978-1-4799-8828-0978-1-4799-8827","10.1109/ITNG.2015.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113531","Program Comprehension;Software Visualization;MATLAB;Octave;Crosscutting Concerns","MATLAB;Visualization;Mathematical model;Image color analysis;Computer languages;Shape","interactive programming;mathematics computing;program visualisation;programming languages;software maintenance;Unix","software visualization;OctMiner;coordinated visual metaphors;MVIE;statistical communities;engineering communities;Octave programing languages;software lifecycle activities;software engineering;Octave program comprehension;GNU program comprehension;MATLAB;multiple view interactive environment","","2","","15","","","","","IEEE","IEEE Conferences"
"A Neuro-Cognitive Perspective of Program Comprehension","N. Peitek","NA","2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)","","2018","","","496","499","Program comprehension is the cognitive process of understanding code. Researchers have proposed several models to describe program comprehension. However, because program comprehension is an internal process and difficult to measure, the accuracy of the existing models are limited. Neuro-imaging methods, such as functional magnetic resonance imaging (fMRI), provide a novel neuro-cognitive perspective to program-comprehension research. With my thesis work, we aim at establishing fMRI as a new tool for program-comprehension and software-engineering studies. Furthermore, we seek to refine our existing framework for conducting fMRI studies by extending it with eye tracking and improved control conditions. We describe how we will apply our upgraded framework to extend our understanding of program comprehension. In the long-run, we would like to contribute insights from our fMRI studies into software-engineering practices by providing code-styling guidelines and programming tools, which reduce the required cognitive effort to comprehend code.","2574-1934","978-1-4503-5663-3978-1-5386-6479","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449636","program comprehension;top down comprehension;functional magnetic resonance imaging;eye tracking","Functional magnetic resonance imaging;Brain;Gaze tracking;Cognition;Task analysis;Tools;Electroencephalography","biomedical MRI;cognition;neurophysiology;software engineering","neuro-cognitive perspective;functional magnetic resonance imaging;fMRI;eye tracking;software-engineering practices;programming tools;code-styling guidelines;program-comprehension research","","","","","","","","","IEEE","IEEE Conferences"
"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore; Australian National University, Canberra, ACT, Australia; Queen’s University, Kingston, ON, Canada; Zhejiang University, Hangzhou, China","IEEE Transactions on Software Engineering","","2018","44","10","951","976","During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ~58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2734091","NSFC; National Key Technology R&D Program; Ministry of Science and Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7997917","Program comprehension;field study;inference model","Navigation;Software;Time measurement;Browsers;Maintenance engineering;Programming;Debugging","human computer interaction;Internet;program compilers;reverse engineering;search engines;software maintenance","program comprehension activities;program comprehension time;developers time;software development;software maintenance;IDE interactions;ActivitySpace framework;human computer interaction;Web browsers;programming language;project phase;software-engineering;search engines","","1","","63","","","","","IEEE","IEEE Journals & Magazines"
"Detecting and comparing brain activity in short program comprehension using EEG","M. K. -. Yeh; D. Gopstein; Y. Yan; Y. Zhuang","College of Information Sciences and Technology, Penn State University, Brandywine; Department of Computer Science and Engineering, New York University; College of Education, Penn State University, University Park; Department of Computer Science, University of Colorado, Colorado Sprints","2017 IEEE Frontiers in Education Conference (FIE)","","2017","","","1","5","Program comprehension is a common task in software development. Programmers perform program comprehension at different stages of the software development life cycle. Detecting when a programmer experiences problems or confusion can be difficult. Self-reported data may be useful, but not reliable. More importantly, it is hard to use the self-reported feedback in real time. In this study, we use an inexpensive, non-invasive EEG device to record 8 subjects' brain activity in short program comprehension. Subjects were presented either confusing or non-confusing C/C++ code snippets. Paired sample t-tests are used to compare the average magnitude in alpha and theta frequency bands. The results show that the differences in the average magnitude in both bands are significant comparing confusing and non-confusing questions. We then use ANOVA to detect whether such difference also presented in the same type of questions. We found that there is no significant difference across questions of the same difficulty level. Our outcome, however, shows alpha and theta band powers both increased when subjects are under the heavy cognitive workload. Other research studies reported a negative correlation between (upper) alpha and theta band powers.","","978-1-5090-5920-1978-1-5090-5919-5978-1-5090-4920","10.1109/FIE.2017.8190486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8190486","computer programming;electroencephalograph;EEG","Electroencephalography;Brain;Software;Electrodes;Real-time systems;Performance evaluation","electroencephalography;medical signal processing","EEG device;program comprehension;C/C++ code snippets;ANOVA;brain activity;software development life cycle","","","","20","","","","","IEEE","IEEE Conferences"
"Quantifying Program Comprehension with Interaction Data","R. Minelli; A. Mocci; M. Lanza; T. Kobayashi","NA; NA; NA; NA","2014 14th International Conference on Quality Software","","2014","","","276","285","It is common knowledge that program comprehension takes up a substantial part of software development. This ""urban legend"" is based on work that dates back decades, which throws up the question whether the advances in software development tools, techniques, and methodologies that have emerged since then may invalidate or confirm the claim. We present an empirical investigation which goal is to confirm or reject the claim, based on interaction data which captures the user interface activities of developers. We use interaction data to empirically quantify the distribution of different developer activities during software development: In particular, we focus on estimating the role of program comprehension. In addition, we investigate if and how different developers and session types influence the duration of such activities. We analyze interaction data from two different contexts: One comes from the ECLIPSE IDE on Java source code development, while the other comes from the PHARO IDE on Smalltalk source code development. We found evidence that code navigation and editing occupies only a small fraction of the time of developers, while the vast majority of the time is spent on reading & understanding source code. In essence, the importance of program comprehension was significantly underestimated by previous research.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958415","Program Comprehension;Program Understanding;Quantification;Interaction data;IDE","Navigation;History;Java;Inspection;Software;Browsers;Maintenance engineering","human computer interaction;Java;Smalltalk;software engineering;source code (software);user interfaces","program comprehension;interaction data;software development tools;software development techniques;software developer user interface activities;ECLIPSE IDE;Java source code development;PHARO IDE;Smalltalk source code development;code navigation;code editing","","12","","24","","","","","IEEE","IEEE Conferences"
"Program Comprehension: Past, Present, and Future","J. Siegmund","NA","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","","2016","5","","13","20","Program comprehension is the main activity of the software developers. Although there has been substantial research to support the programmer, the high amount of time developers need to understand source code remained constant over thirty years. Beside more complex software, what might be the reason? In this paper, I explore the past of program-comprehension research, discuss the current state, and outline what future research on program comprehension might bring.","","978-1-5090-1855","10.1109/SANER.2016.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476769","Program comprehension;empirical software engineering","Programming;Software;Computer languages;Protocols;Time factors;Atmospheric measurements;Particle measurements","software engineering;source code (software)","program comprehension research;source code;time developers;programmer;software developers","","3","","41","","","","","IEEE","IEEE Conferences"
"RTFM (Read the Factual Mails) - Augmenting Program Comprehension with Remail","A. Bacchelli; M. Lanza; V. Humpa","NA; NA; NA","2011 15th European Conference on Software Maintenance and Reengineering","","2011","","","15","24","The advent of globalization has led to the adoption of distributed software development as a common practice. One of its drawbacks-the absence of impromptu meetings - is tackled with other communication means, such as emails, instant messaging, or forums. Mailing lists have proven to be effective for enabling developers' collaboration and coordination: Being asynchronous, emails can evade time zone barriers, being public, mailing lists maintain developers' awareness, being recorded, email archives offer information on system evolution. Emails can provide information about a task, clarify implementation details, or reveal hidden connections among entities, always within the clear context of a discussion. As a result, we argue that emails might help program comprehension. We devised Remail, an Eclipse plug in to integrate email communication in the IDE. It allows developers to seamlessly handle code entities and emails concerning the source code. Discussions relevant to chosen entities can be retrieved easily, thus providing an updated and effective form of complementary documentation. We present design and implementation of Remail, and illustrate, through a number of scenarios, how it can augment program comprehension.","1534-5351;1534-5351","978-1-61284-259-2978-0-7695-4343","10.1109/CSMR.2011.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741255","email;program comprehension;remail;IDE plugin","Electronic mail;Databases;Context;Engines;Documentation;Programming;Joining processes","electronic mail;software maintenance","program comprehension;distributed software development;Remail plug-in;E-mail communication","","9","","24","","","","","IEEE","IEEE Conferences"
"Facilitating Scenario-Based Program Comprehension with Topic Models","T. Wang; Y. Liu","NA; NA","2017 24th Asia-Pacific Software Engineering Conference (APSEC)","","2017","","","642","647","Researchers and practitioners have been seeking automatic and semi-automatic approaches to support program comprehension. However, not too much attention has been given to the discussion about program comprehension scenarios and further exploration based on scenarios. In this paper, we explored program comprehension from the perspective of developers, analyzed the demands of developers, refined two program comprehension scenarios (Program Users Scenario and Program Owners Scenario), and mainly researched on the latter. In the Program Users Scenario, where developers need help to quickly understand a program and be able to use it fast, we found that topic modeling provides a promising way to facilitate program comprehension. Using topic modeling, features and structures can be discovered automatically from textual software assets. We also developed JSEA, a tool that provides semi-automatic program comprehension assistance. JSEA utilizes essential information automatically generated from Java projects to construct a project overview and give developers search capability. Experiments with 12 volunteers on two open source Java projects suggest that JSEA can support Java developers in comprehending programs in the Program Users Scenario.","","978-1-5386-3681-7978-1-5386-3682","10.1109/APSEC.2017.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305995","Mining software assets;Java program comprehension;Topic models","Java;Tools;Analytical models;Maintenance engineering;Task analysis;Software maintenance","Java;public domain software;reverse engineering;software maintenance","scenario-based program comprehension;program user scenario;program owner scenario;textual software assets;JSEA tool;open source Java projects;semiautomatic program comprehension assistance;topic modeling","","","","22","","","","","IEEE","IEEE Conferences"
"Maintenance of embedded systems: Supporting program comprehension using dynamic analysis","J. Trümper; S. Voigt; J. Döllner","Hasso-Plattner-Institute - University of Potsdam, Germany; Hasso-Plattner-Institute - University of Potsdam, Germany; Hasso-Plattner-Institute - University of Potsdam, Germany","2012 Second International Workshop on Software Engineering for Embedded Systems (SEES)","","2012","","","58","64","Maintenance of embedded software systems is faced with multiple challenges, including the exploration and analysis of the actual system's runtime behavior. As a fundamental technique, tracing can be used to capture data about runtime behavior as a whole, and represents one of the few methods to observe and record data about embedded systems within their production environments. In this paper we present a software-based, function-boundary tracing approach for embedded software systems. It uses static binary instrumentation, which implies only lightweight memory and performance overheads. To further reduce these overheads, instrumentation can be configured per trace, i.e., activated only for a specified group of functions without having to recompile the system. The technique can be characterized by its robust implementation and its versatile usage. It is complemented by a visualization framework that allows for analysis and exploration of a system's runtime behavior, e.g., to examine thread interaction. To show the technique's applicability, we conclude with a case study that has been applied to an industrial embedded software system.","","978-1-4673-1853-2978-1-4673-1852","10.1109/SEES.2012.6225492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225492","","Instruments;Runtime;Embedded systems;Libraries;Hardware","embedded systems;software maintenance","embedded system maintenance;program comprehension support;dynamic analysis;embedded software systems;fundamental technique;production environments;static binary instrumentation;lightweight memory","","4","","36","","","","","IEEE","IEEE Conferences"
"Assessing the contribution of the individual alpha frequency (IAF) in an EEG-based study of program comprehension","I. Crk; T. Kluthe","Department of Computer Science, Southern Illinois University Edwardsville, Edwardsville, IL 62026, USA; Department of Computer Science, Southern Illinois University Edwardsville, Edwardsville, IL 62026, USA","2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2016","","","4601","4604","Empirical studies of programming language learnability and usability have thus far depended on indirect measures of human cognitive performance, attempting to capture what is at its essence a purely cognitive exercise through various indicators of comprehension, such as the time spent working out the meaning of code and producing acceptable solutions. We present evidence of the relative contribution of experience and the individual alpha frequency (IAF) to achieving correct performance during program comprehension tasks, specifically that more experience and higher IAF are both associated with an increased likelihood of correct task performance, with experience playing the greater part.","1558-4615;1557-170X","978-1-4577-0220-4978-1-4577-0219","10.1109/EMBC.2016.7591752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7591752","","Electroencephalography;Atmospheric measurements;Particle measurements;Timing;Programming;Frequency conversion","cognition;electroencephalography;human computer interaction","individual alpha frequency;EEG-based study;program comprehension tasks;programming language learnability;programming language usability;human cognitive performance;empirical study;human-computer interaction","Alpha Rhythm;Comprehension;Electroencephalography;Humans;Logistic Models;Reproducibility of Results;Task Performance and Analysis;Time Factors","","","22","","","","","IEEE","IEEE Conferences"
"An Empirical Study on the Usage of SQL Execution Traces for Program Comprehension","N. Noughi; S. Hanenberg; A. Cleve","NA; NA; NA","2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)","","2017","","","47","54","Several studies have investigated dynamic analysis in the context of software maintenance and evolution, and most of them confirmed the positive impact of such analysis on program comprehension tasks. In this paper, we focus on the understanding of the database access behavior of a program, which has become an important (yet largely ignored) aspect of program comprehension. We empirically assess how developers/students are (not) able to understand interactions between the database and the application program. To this end, we used DAViS, a tool for Dynamic Analysis and Visualization of SQL execution traces. We present a controlled experiment that quantitatively evaluates to what extent DAViS can influence program comprehension in terms of duration and correctness of the tasks. The results of the study indicate that DAViS does reduce the response time and increases the correctness (with a large effect size), which means that we found a strong indicator that the chosen approach is truly able to help developers.","","978-1-5386-2072-4978-1-5386-2073","10.1109/QRS-C.2017.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004293","","Databases;Structured Query Language;Tools;Time factors;Performance analysis;Time measurement;Software","data visualisation;software maintenance;SQL","SQL execution traces;dynamic analysis;software maintenance;software evolution;program comprehension tasks;database access behavior;database;application program;DAViS;dynamic visualization","","","","25","","","","","IEEE","IEEE Conferences"
"A Controlled Experiment for Program Comprehension through Trace Visualization","B. Cornelissen; A. Zaidman; A. van Deursen","Software Improvement Group, Amsterdam; Delft University of Technology, Delft; Delft University of Technology, Delft","IEEE Transactions on Software Engineering","","2011","37","3","341","355","Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2010.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5441291","Program comprehension;dynamic analysis;controlled experiment.","Visualization;Computer Society;Time measurement;Programming;Documentation;Scalability;Software maintenance;Gain measurement;Control systems;Performance evaluation","data visualisation;software maintenance","program comprehension;software maintenance;execution trace visualization","","51","","56","","","","","IEEE","IEEE Journals & Magazines"
"Synchronized UML diagrams for object-oriented program comprehension","J. Yang; Y. Lee; D. Gandhi; S. G. Valli","Dept. of Computing and Cyber Security, Texas A&M University-San Antonio, San Antonio, U.S.A.; Dept. of Electrical Engineering and Computer Science, Texas A&M University-Kingsville, Kingsville, U.S.A.; Dept. of Electrical Engineering and Computer Science, Texas A&M University-Kingsville, Kingsville, U.S.A.; Dept. of Electrical Engineering and Computer Science, Texas A&M University-Kingsville, Kingsville, U.S.A.","2017 12th International Conference on Computer Science and Education (ICCSE)","","2017","","","12","17","We propose a novel approach for visualizing reverse-engineered Unified Modeling Language (UML) diagrams (class, object, and sequence) to improve Object-Oriented Program (OOP) comprehension on a web-based programming environment, JaguarCode. It aims to help students better understand static structure and dynamic behavior of Java programs and object-oriented programming concepts. This paper presents an evaluation of JaguarCode, supporting those UML diagrams to investigate its effectiveness and user satisfaction. The results of the experimental study revealed having synchronized UML diagrams positively impacted students' understanding of program execution. It was also observed that students were satisfied with the aspects of the synchronized visualizations of UML diagrams with source code.","2473-9464","978-1-5090-2508-4978-1-5090-2507-7978-1-5090-2509","10.1109/ICCSE.2017.8085455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085455","Program Comprehension;Object-Oriented Programming;UML Diagrams;Reverse Engineering","Java;Unified modeling language;Synchronization;Data visualization;Visualization;Programming;Runtime","computer science education;Internet;Java;object-oriented programming;program diagnostics;program visualisation;reverse engineering;Unified Modeling Language","synchronized UML diagrams;Java programs;program execution;synchronized visualizations;object-oriented program comprehension;reverse-engineered Unified Modeling Language diagram visualization;OOP comprehension;Web-based programming environment;static structure;dynamic behavior;JaguarCode evaluation;user satisfaction;source code","","","","19","","","","","IEEE","IEEE Conferences"
"Program Comprehension and Implications of Human Navigational Approaches","S. Moorthy; M. H. Samadzadeh","NA; NA","2011 21st International Conference on Systems Engineering","","2011","","","189","193","Cognitive scientists, psychologists, and other researchers have endeavored over the past three decades to identify the cognitive functions underpinning human navigation and its possible correlations with other characteristics. The answer to the basic question of how/why some people are good at following directions and some people are not, is yet to be determined conclusively. The scope of this research work included both theoretical and empirical studies of human direction sensitivity and the cognitive tests that attempt to test hypotheses about individual differences in spatial/temporal attention spans as well as a set of program comprehension questionnaire-based tests about the debugging/testing of computer programs and program comprehension. This work was done in the context of the relevant cognitive-based perceptual and spatial tests. The test results obtained suggest that programmers' directional detection skills appear to have some correlations with their program comprehension abilities.","","978-1-4577-1078","10.1109/ICSEng.2011.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6041560","navigation;program comprehension;perceptual and spatial tests","Navigation;Visualization;Humans;Correlation;Educational institutions;Particle measurements;Atmospheric measurements","cognition;navigation;program debugging;program testing;psychology","program comprehension;human navigational approach;human cognitive function;human direction sensitivity;cognitive test;spatial-temporal attention span;computer program debugging;computer program testing;cognitive-based perceptual test;cognitive-based spatial test;programmer directional detection skills","","","","17","","","","","IEEE","IEEE Conferences"
"A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers","A. Armaly; P. Rodeghero; C. McMillan","Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN","IEEE Transactions on Software Engineering","","2018","44","8","712","724","Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2017.2729548","National Science Foundation Graduate Research Fellowship Program; US National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987041","Program comprehension;accessibility technology;blindness","Tools;Software;Blindness;Navigation;Programming profession;Software engineering","programming environments;public domain software;software prototyping;source code (software)","source code;sighted programmers;program comprehension strategies;reading processes;stark;program comprehension;reading method signature","","1","","46","","","","","IEEE","IEEE Journals & Magazines"
"An empirical study on program comprehension task classification of novices","N. Saroni; S. A. Aljunid; S. M. Shuhidan; A. Shargabi","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia","2015 IEEE Conference on e-Learning, e-Management and e-Services (IC3e)","","2015","","","15","20","Program comprehension is difficult to novices. Tasks have substantial effect on program comprehension. This work is part of larger study aims at identifying tasks that can improve novices' program comprehension. In one of our previous studies, fourteen tasks were identified to be feasibly effective in improving novices' program comprehension. These tasks were also classified into cognitive categories using revised Bloom taxonomy. This particular study is to validate the classification of these tasks. An online survey was conducted to a number of programming instructors as well as developers. The respondents were asked to place each of the fourteen identified tasks into one of the six cognitive categories of revised Bloom taxonomy. The findings showed that most of the respondents agreed with our classification. In future, we plan to replicate this study with more respondents and also to conduct controlled experiments to investigate the effect of the classified tasks on novices.","","978-1-4673-9437-6978-1-4673-9436","10.1109/IC3e.2015.7403479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403479","program comprehensoin;novices;revised Bloom Taxonomy;task","Taxonomy;Programming profession;Debugging;Conferences;Electronic learning;Documentation","educational courses;programming","program comprehension task classification;novices;cognitive categories;revised Bloom taxonomy;programming instructors;programming course","","","","26","","","","","IEEE","IEEE Conferences"
"Comparing Programming Language Comprehension between Novice and Expert Programmers Using EEG Analysis","S. Lee; A. Matteson; D. Hooshyar; S. Kim; J. Jung; G. Nam; H. Lim","NA; NA; NA; NA; NA; NA; NA","2016 IEEE 16th International Conference on Bioinformatics and Bioengineering (BIBE)","","2016","","","350","355","For programming language comprehension, high cognitive skills (e.g., reading, writing, working memory, etc.) and information processing are required. However, there are few papers that approach this from a neuroscientific perspective. In this paper, we examine program comprehension neuroscientifically and also observe the differences between novice and expert programmers. We designed an EEG (electroencephalogram) experiment and observed 18 participants during a series of program comprehension tasks. We found clear differences in program comprehension ability between novice and expert programmers. Experts exhibited higher brainwave activation than novices in electrodes F3 and P8. These results indicate that experts have outstanding program comprehension-associated abilities such as digit encoding, coarse coding, short-term memory, and subsequent memory effect. Our findings can serve as a foundation for future research in this pioneering field.","2471-7819","978-1-5090-3834-3978-1-5090-3835","10.1109/BIBE.2016.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790010","program comprehension;electroencephalogram;neuroscientific;human factors;empirical study","Electroencephalography;Programming profession;Brain;Time factors;Information processing;Java","cognition;computer literacy;electroencephalography;medical signal processing","programming language comprehension;EEG analysis;electroencephalogram;program comprehension tasks;brainwave activation;F3 electrode;P8 electrode","","2","","16","","","","","IEEE","IEEE Conferences"
"InputTracer: A Data-Flow Analysis Tool for Manual Program Comprehension of x86 Binaries","U. Kargén; N. Shahmehri","NA; NA","2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation","","2012","","","138","143","Third-party security analysis of closed-source programs has become an important part of a defense-in-depth approach to software security for many companies. In the absence of efficient tools, the analysis has generally been performed through manual reverse engineering of the machine code. As reverse engineering is an extremely time-consuming and costly task, much research has been performed to develop more powerful methods for analysis of program binaries. One such popular method is dynamic taint analysis (DTA), which is a type of runtime data-flow analysis, where certain input data is marked as tainted. By tracking the flow of tainted data, DTA can, for instance, be used to determine which computations in a program are affected by a certain part of the input. In this paper we present Input Tracer, a tool that utilizes DTA for aiding in manual program comprehension and analysis of unmodified x86 executables running in Linux. A brief overview of dynamic taint analysis is given, followed by a description of the tool and its implementation. We also demonstrate the tool's ability to provide exact information on the origin of tainted data through a detailed use case, where the tool is used to find the root cause of a memory corruption bug.","","978-0-7695-4783-1978-1-4673-2398","10.1109/SCAM.2012.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392112","dynamic taint analysis;binary analysis;x86;program comprehension;Valgrind","Computer crashes;Security;Manuals;Performance analysis;Runtime;Instruments;Software","data flow analysis;Linux;program compilers;reverse engineering;security of data","InputTracer;data flow analysis tool;manual program comprehension;x86 binaries;security analysis;closed source programs;software security;reverse engineering;machine code;dynamic taint analysis;DTA;program comprehension;program analysis;Linux;memory corruption bug","","","","21","","","","","IEEE","IEEE Conferences"
"Program comprehension with four-layered mental model","M. Nosál'; J. Porubän","Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Letná 9, 042 00, Slovakia; Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Letná 9, 042 00, Slovakia","2015 13th International Conference on Engineering of Modern Electric Systems (EMES)","","2015","","","1","4","Program comprehension deals with an important problem of recreating mental model of the software system from the source code. This process is needed every time a new programmer joins the development team, or the old one forgets about the code. We designed the four-layered model of the developer's mental model that provides a deeper understanding of the mental model and of the program comprehension in general. The described model was verified by an observatory study with 3 subjects. This paper concludes with our findings and conclusions.","","978-1-4799-7650-8978-1-4799-7649-2978-1-4799-7648","10.1109/EMES.2015.7158420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158420","Program comprehension;comprehension model;mental model;study","Cognitive science;Object oriented modeling;Computational modeling;Computers;Java;Software systems;Programming","reverse engineering;software engineering","program comprehension;four-layered mental model;software system mental model;source code;developer mental model","","3","","10","","","","","IEEE","IEEE Conferences"
"Language-Independent Information Flow Tracking Engine for Program Comprehension Tools","M. R. Azadmanesh; M. L. Van De Vanter; M. Hauswirth","NA; NA; NA","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","","2017","","","346","355","Program comprehension tools are often developed for a specific programming language. Developing such a tool from scratch requires significant effort. In this paper, we report on our experience developing a language-independent framework that enables the creation of program comprehension tools, specifically tools gathering insight from deep dynamic analysis, with little effort. Our framework is language independent, because it is built on top of Truffle, an open-source platform, developed in Oracle Labs, for implementing dynamic languages in the form of AST interpreters. Our framework supports the creation of a diverse variety of program comprehension techniques, such as query, program slicing, and back-in-time debugging, because it is centered around a powerful information-flow tracking engine. Tools developed with our framework get access to the information-flow through a program execution. While it is possible to develop similarly powerful tools without our framework, for example by tracking information-flow through bytecode instrumentation, our approach leads to information that is closer to source code constructs, thus more comprehensible by the user. To demonstrate the effectiveness of our framework, we applied it to two of Truffle-based languages namely Simple Language and TruffleRuby, and we distill our experience into guidelines for developers of other Truffle-based languages who want to develop program comprehension tools for their language.","","978-1-5386-0535-6978-1-5386-0536","10.1109/ICPC.2017.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961535","Information flow;AST interpreter;Language-independent;Program comprehension;Tool;Dependency","Tools;Java;Runtime;Instruments;Program processors;Debugging;Libraries","program diagnostics;programming languages","language-independent information flow tracking engine;program comprehension tool;programming language;language-independent framework;deep dynamic analysis;Truffle open-source platform;Oracle Labs;AST interpreters;program execution;bytecode instrumentation;simple language;TruffleRuby","","","","25","","","","","IEEE","IEEE Conferences"
"A Novel Approach Based on Gestalt Psychology for Abstracting the Content of Large Execution Traces for Program Comprehension","H. Pirzadeh; A. Hamou-Lhadj","NA; NA","2011 16th IEEE International Conference on Engineering of Complex Computer Systems","","2011","","","221","230","The analysis of execution traces can reveal important information about the behavioral aspects of complex software systems, hence reducing the time and effort it takes to understand and maintain them. Traces, however, tend to be considerably large which hinders their effective analysis. Existing traces analysis tools rely on some sort of visualization techniques to help software engineers make sense of trace content. Many of these techniques have been studied and found to be limited in many ways. In this paper, we present a novel trace analysis technique that automatically divides the content of a large trace into meaningful segments that correspond to the program's main execution phases such as initializing variables, performing a specific computation, etc. These phases can simplify significantly the exploration of large traces by allowing software engineers to first understand the content of a trace at a high-level before they decide to dig into the details. Our phase detection method is inspired by Gestalt laws that characterize the proximity, similarity, and continuity of the elements of a data space. We model these concepts in the context of execution traces and show how they can be used as gravitational forces that yield the formation of dense groups of trace elements, which indicate candidate phases. We applied our approach to two software systems. The results are very promising.","","978-0-7695-4381-9978-1-61284-853","10.1109/ICECCS.2011.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5773396","Trace Analysis;Program Comprehension;Software Maintenance;Software Engineering","Gravity;Phase detection;Software;Complexity theory;Visualization;Context;Psychology","data visualisation;program diagnostics;psychology;reverse engineering;software maintenance","Gestalt psychology;large execution trace content analysis;program comprehension;complex software system;visualization technique;software engineers;phase detection method;software maintenance","","10","","29","","","","","IEEE","IEEE Conferences"
"An ontology toolkit for problem domain concept location in program comprehension","N. R. Carvalho","Department of Informatics, University of Minho, Campus de Gualtar - 4710-057 Braga, Portugal","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","1415","1418","Programmers are able to understand source code because they are able to relate program elements (e.g. modules, objects, or functions), with the real world concepts these elements are addressing. The main goal of this work is to enhance current program comprehension by systematically creating bidirectional mappings between domain concepts and source code. To achieve this, semantic bridges are required between natural language terms used in the problem domain and program elements written using formal programming languages. These bridges are created by an inference engine over a multi-ontology environment, including an ontological representation of the program, the problem domain, and the real world effects program execution produces. These ontologies are populated with data collected from both domains, and enriched using available Natural Language Processing and Information Retrieval techniques.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606731","","Ontologies;Conferences;Engines;Software maintenance;Natural languages;Data mining","formal languages;inference mechanisms;information retrieval;natural language processing;ontologies (artificial intelligence);programming languages;reverse engineering","ontology toolkit;problem domain concept location;program comprehension;source code;bidirectional mapping;semantic bridges;natural language terms;program elements;formal programming languages;inference engine;multiontology environment;ontological representation;program execution;natural language processing;information retrieval techniques","","3","","27","","","","","IEEE","IEEE Conferences"
"Comparing Trace Visualizations for Program Comprehension through Controlled Experiments","F. Fittkau; S. Finke; W. Hasselbring; J. Waller","NA; NA; NA; NA","2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","266","276","For efficient and effective program comprehension, it is essential to provide software engineers with appropriate visualizations of the program's execution traces. Empirical studies, such as controlled experiments, are required to assess the effectiveness and efficiency of proposed visualization techniques. We present controlled experiments to compare the trace visualization tools Extravis and Explor Viz in typical program comprehension tasks. We replicate the first controlled experiment with a second one targeting a differently sized software system. In addition to a thorough analysis of the strategies chosen by the participants, we report on common challenges comparing trace visualization techniques. Besides our own replication of the first experiment, we provide a package containing all our experimental data to facilitate the verifiability, reproducibility and further extensibility of our presented results. Although subjects spent similar time on program comprehension tasks with both tools for a small-sized system, analyzing a larger software system resulted in a significant efficiency advantage of 28 percent less time spent by using Explor Viz. Concerning the effectiveness (correct solutions for program comprehension tasks), we observed a significant improvement of correctness for both object system sizes of 39 and 61 percent with Explor Viz.","1092-8138","978-1-4673-8159-8978-1-4673-8158","10.1109/ICPC.2015.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181455","Software Visualization;Controlled Experiment;Trace Visualization;Program Comprehension","Visualization;Context;Cities and towns;Color;Software systems;Data visualization","data visualisation;software engineering","trace visualizations;program comprehension;controlled experiments;software engineers;program execution traces;visualization techniques;trace visualization tools;small sized system","","5","","46","","","","","IEEE","IEEE Conferences"
"Metrics to identify where object-oriented program comprehension benefits from the runtime structure","M. Abi-Antoun; R. Vanciu; N. Ammar","Department of Computer Science, Wayne State University, USA; Department of Computer Science, Wayne State University, USA; Department of Computer Science, Wayne State University, USA","2013 4th International Workshop on Emerging Trends in Software Metrics (WETSoM)","","2013","","","42","48","To evolve object-oriented code, developers often need to understand both the code structure in terms of classes and packages, as well as the runtime structure in terms of abstractions of objects. Recent empirical studies have shown that for some code modifications tasks, developers do benefit from having access to information about the runtime structure. However, there is no good sense of when object-oriented program comprehension clearly depends on information about the runtime structure. We propose using metrics to identify cases in object-oriented program comprehension that benefit from information about the runtime structure. The metrics relate properties observed on a statically extracted hierarchical object graph to the type structures declared in the code and highlight key differences between the runtime structure and the code structure.","2327-0969;2327-0950","978-1-4673-6331","10.1109/WETSoM.2013.6619335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619335","object-oriented runtime structure;metrics","Runtime;Measurement;Abstracts;Context;Concrete;Object recognition;Bismuth","object-oriented programming;software maintenance;software metrics","object-oriented program comprehension;runtime structure;object-oriented code;metrics;statically extracted hierarchical object graph;code structure;software maintenance","","1","","22","","","","","IEEE","IEEE Conferences"
"On Visualization and Comprehension of Scenario-Based Programs","N. Eitan; M. Gordon; D. Harel; A. Marron; G. Weiss","NA; NA; NA; NA; NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","189","192","We address the problem of comprehending cause and effect relationships between relatively independent behavior components of a single application. Our focus is on the paradigm of behavioral, scenario-based, programming, as captured by the language of live sequence charts (LSC) or its Java-based counterpart, BPJ. In this programming paradigm, multi-modal behaviors can be specified separately, and are integrated only at run time. We present a tool, with which the user can easily follow the decisions of the collective execution mechanism. It shows the behaviors and events that were executed at each point in time, and those that were delayed or abandoned, as well as the causes and reasons behind these run-time choices. The dynamic effects of such decisions on the system's behavior can be seen easily too.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970154","visualization;behavioral programming;scenario-based programming;BPJ","Visualization;Programming;Instruction sets;Synchronization;Aerodynamics","Java;program visualisation","scenario-based program comprehension;scenario-based program visualization;live sequence chart;LSC;Java-based counterpart;BPJ;multimodal behavior;collective execution mechanism","","4","","14","","","","","IEEE","IEEE Conferences"
"A case study of program comprehension effort and technical debt estimations","V. Singh; L. L. Pollock; W. Snipes; N. A. Kraft","University of Delaware, Newark, USA; University of Delaware, Newark, USA; ABB Corporate Research, Raleigh, NC, USA; ABB Corporate Research, Raleigh, NC, USA","2016 IEEE 24th International Conference on Program Comprehension (ICPC)","","2016","","","1","9","This paper describes a case study of using developer activity logs as indicators of a program comprehension effort by analyzing temporal sequences of developer actions (e.g., navigation and edit actions). We analyze developer activity data spanning 109,065 events and 69 hours of work on a medium-sized industrial application. We examine potential correlations between different measures of developer activity, code change metrics and code smells to gain insight into questions that could direct future technical debt interest estimation. To gain more insights into the data, we follow our analysis with commit message analysis and a developer interview. Our results indicate that developer activity as an estimate of program comprehension effort is correlated with both change proneness and static metrics for code smells.","","978-1-5090-1428","10.1109/ICPC.2016.7503710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503710","program comprehension effort;technical debt interest;developer activity logging;code smells","Measurement;Couplings;Navigation;Software;Maintenance engineering;Correlation;History","program diagnostics;software metrics;source code (software)","developer activity logs;program comprehension effort;temporal sequence analysis;developer activity data analysis;medium-sized industrial application;code change metrics;code smells;technical debt interest estimation;commit message analysis;change proneness;static metrics","","2","","24","","","","","IEEE","IEEE Conferences"
"The impact of tools supported in integrated-development environments on program comprehension","T. Kosar; M. Mernik; J. C. Carver","University of Maribor, Faculty of Electrical Engineering and Computer Science, Smetanova ulica 17, 2000, Slovenia; University of Maribor, Faculty of Electrical Engineering and Computer Science, Smetanova ulica 17, 2000, Slovenia; Department of Computer Science, University of Alabama, Tuscaloosa, USA","Proceedings of the ITI 2011, 33rd International Conference on Information Technology Interfaces","","2011","","","603","608","Program comprehension is a cognitive task done by a software programmer. Task is usually done manually, but in some cases tool support inside integrated development environment can facilitate the hard work. In order to test those tools, a study between manual program comprehension and program comprehension with tool support is presented in this paper. An empirical study that faces the alternatives has been done. In experiment, the same test has been used twice (with and without tools) and results have been compared and statistically analyzed in terms of test correctness and efficiency. The results regarding program comprehension with students confirm significant benefits in terms of correctness when using tools for program comprehension.","1330-1012","978-1-61284-897-6978-953-7138-21","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974091","program comprehension tools;integrated-development environments;controlled experiments","Manuals;Programming;Visualization;Atmospheric measurements;Software;Particle measurements","human factors;reverse engineering;software engineering","integrated-development environments;program comprehension tools;cognitive task","","","","11","","","","","IEEE","IEEE Conferences"
"Program comprehension levels of abstraction for novices","A. Shargabi; S. A. Aljunid; M. Annamalai; S. Mohamed Shuhidan; A. Mohd Zin","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam 40450, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam 40450, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam 40450, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam 40450, Malaysia; Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi 43600, Malaysia","2015 International Conference on Computer, Communications, and Control Technology (I4CT)","","2015","","","211","215","Although various program comprehension models exist, their levels of abstraction and the related terminologies do not fit the programming pedagogy perspective. In this study, we proposed five abstraction levels for novice's program comprehension: statement, block, module, program and domain. The expansion and the terminologies of the proposed abstraction levels are inspired by the application of decomposition concept in programming pedagogy. To comprehend a program, novices understand decomposed parts (i.e. statements, blocks, and modules) of that program and chunk those parts into a meaningful whole (i.e. program) within a specific domain. We supported our revised abstraction levels with a user study. The revised abstraction levels are able to describe hierarchical levels of novices' program comprehension that map appropriately with their mental model. The proposed abstraction levels can be applied in designing empirical studies of novices' program comprehension that distinctly capture their different abstraction levels.","","978-1-4799-7952-3978-1-4799-7951","10.1109/I4CT.2015.7219568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219568","program comprehension;mental model;abstraction level;novices;programming pedagogy","Error analysis;Adaptation models;Context;Education;Data models;Programming profession","computer science education;object-oriented methods","program comprehension levels;novice abstraction;program comprehension models;programming pedagogy perspective;novice program comprehension;decomposition concept","","1","","16","","","","","IEEE","IEEE Conferences"
"Conceptual interpretation of SQL execution traces for program comprehension","N. Noughi; A. Cleve","University of Namur, Belgium; University of Namur, Belgium","2015 IEEE 6th International Workshop on Program Comprehension through Dynamic Analysis (PCODA)","","2015","","","19","24","Modern data-intensive software systems manipulate an increasing amount of heterogeneous data usually stored in a database. Maintaining such systems became a crucial and complex task, which is especially true due to the lack of sufficient documentation. In this context, program comprehension became a primary and an important step in this task. Unfortunately, the highly dynamic nature of interactions between a system and its database makes it hard to analyze these interactions with static analysis techniques. To this end, we propose a novel approach that combines dynamic analysis techniques and visualization to ease understanding data-intensive systems, by focusing on their database manipulation behavior. The approach consists of defining the conceptual interpretation of SQL execution traces in terms of a domain-specific, platform-independent model.","","978-1-4673-6917","10.1109/PCODA.2015.7067179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067179","","Visualization;Databases;Cities and towns;Natural languages;Context;Software systems;Abstracts","data visualisation;distributed databases;program diagnostics;SQL","conceptual interpretation;SQL execution traces;program comprehension;data-intensive software systems;heterogeneous data;static analysis techniques;dynamic analysis techniques;data visualization;database manipulation behavior;domain-specific model;platform-independent model","","2","","12","","","","","IEEE","IEEE Conferences"
"Brain activity measurement during program comprehension with NIRS","Y. Ikutani; H. Uwano","Department of Information Engineering Nara National College of Technology Japan, Nara, Yamatokoriyama; Department of Information Engineering Nara National College of Technology Japan, Nara, Yamatokoriyama","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2014","","","1","6","Near infrared spectroscopy (NIRS) has been used as a low cost, noninvasive method to measure brain activity. In this paper, we experiment to measure the effects of variables and controls in a source code to the brain activity in program comprehension. The measurement results are evaluated after noise reduction and normalization to statistical analysis. As the result of the experiment, significant differences in brain activity were observed at a task that requires memorizing variables to understand a code snippet. On the other hand, no significant differences between different levels of mental arithmetic tasks were observed. We conclude that the frontal pole reflects workload to short-term memory caused by variables without affected from calculation.","","978-1-4799-5604","10.1109/SNPD.2014.6888727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888727","","Brain;Atmospheric measurements;Particle measurements;Noise;Blood;Noise reduction;Statistical analysis","blood;blood flow measurement;brain;infrared spectra;medical signal processing;neurophysiology;signal denoising;statistical analysis","brain activity measurement;program comprehension;near infrared spectroscopy;noninvasive method;source code;noise reduction;normalization;statistical analysis;mental arithmetic tasks;short-term memory;frontal pole;code snippet","","4","","10","","","","","IEEE","IEEE Conferences"
"Using cognitive easiness metric for program comprehension","M. Yin; B. Li; C. Tao","School of Computer Science and Engineering, Southeast University Nanjing, China; School of Computer Science and Engineering, Southeast University Nanjing, China; School of Computer Science and Engineering, Southeast University Nanjing, China","The 2nd International Conference on Software Engineering and Data Mining","","2010","","","134","139","Program comprehension is one of the most critical phases in software maintenance. During program designing, the codes having related function and behavior are often scattered in different parts of program, which increases the difficulty of understanding program, and further obfuscates programmers. In this paper, we propose an approach to assisting comprehension process using cognitive easiness metric, which can predicate the easiness degree of a method to understand in various comprehension phases. The approach uses coarse-grained call graph slicing technique and intra-procedural coarse-grained slicing technique to compute the cognitive easiness of non-understood methods based on those understood methods in the program. It can not only guide programmers to investigate the non-understood codes in the program from the easier parts to the harder parts, but also support different guidance for different programmers. Finally, we evaluate the effectiveness and practicability of our approach by applying to a case study.","","978-89-88678-22-0978-1-4244-7324","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542938","Cognitive Easiness Metric;Program Comprehension;Program Slicing;Call Graph Slice","Programming profession;Software maintenance;Software systems;Large-scale systems;Data mining;Computer science;Scattering;Aging;Humans;Research and development","program slicing;software maintenance;software metrics","cognitive easiness metric;program comprehension;software maintenance;assisting comprehension process;coarse grained call graph slicing technique;intraprocedural coarse grained slicing technique","","","","21","","","","","IEEE","IEEE Conferences"
"The effectiveness of Zoom Visual Flow (ZViF) technique in program comprehension activities","R. Kadar; S. Sulaiman","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Penang, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia","2010 International Symposium on Information Technology","","2010","1","","1","6","Many techniques and tools have been developed to improve program comprehension but most tools are unsuitable for novices. Some tools are not user friendly, some designs are more suitable to expert programmers and some integrated development environments (IDEs) are very simple and fail to support program comprehension needs among the users. These factors hinder the learning process and may become obstacles to users who have no programming background. This paper attempts to improve program comprehension by using Zoom Visual Flow (ZViF) technique that represents source code in graphical views. It also gives some insights on how to improve visual presentation method in a program editor or an IDE. A lab experiment was conducted to determine the effectiveness of the technique. The respondents were asked to determine their preference between ZViF technique and Control Structured Diagram (CSD). CSD is one of the best techniques that uses graphic to visualize a program. The result shows that users prefer the proposed technique compared to CSD as the ZViF helps to improve program comprehension among novices.","2155-899X;2155-8973;2155-897","978-1-4244-6718-1978-1-4244-6715-0978-1-4244-6717","10.1109/ITSIM.2010.5561292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5561292","program comprehension;program visualization;integrated development environment","Visualization;Color;Shape;Software;Programming;Switches;Computer languages","data visualisation;program visualisation","zoom visual flow technique;program comprehension activity;integrated development environments;learning process;source code;program editor;lab experiment;ZViF technique;control structured diagram;program visualization;visual presentation method;computer graphics","","","","14","","","","","IEEE","IEEE Conferences"
"Tasks that can improve novices' program comprehension","A. Shargabi; S. A. Aljunid; M. Annamalai; S. M. Shuhidan; A. M. Zin","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia","2015 IEEE Conference on e-Learning, e-Management and e-Services (IC3e)","","2015","","","32","37","This study aims at identifying tasks that can effectively promote novices' program comprehension. Based on literature of program comprehension and computer science education, we identified 14 tasks and classified them into homogeneous categories based on the Revised Bloom Taxonomy. We conducted a survey to rank these tasks within each Bloom category based on their potential effectiveness in developing the novices' program comprehension. The survey respondents were 154 programming and software engineering instructors from 13 universities. The results of the survey indicated that: a) for the remember category, recall received higher ranking than line documentation; b) for the understand category, representation received higher ranking than summarization; c) for the analyze category, the tasks ordered from the highest to the lowest rank are: tracing, debugging, search and reordering; d) for the create category, the tasks ordered from the highest to the lowest rank are: modification, extension, reuse and restructuring. The apply and evaluate categories contain one task each; and thus were not ranked. The outcomes indicated that tasks that were more commonly applied in teaching were received higher rankings. For example, tracing and representation received higher ranking as compared to other less common teaching tasks such as summarization and searching respectively. The findings of this study recommend that program comprehension skills can be improved through exercises on the different types of tasks identified in this survey. The next step is to conduct experiments of these novices in universities to validate the results of the conducted survey.","","978-1-4673-9437-6978-1-4673-9436","10.1109/IC3e.2015.7403482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403482","program comprehension;task;novices;Revised Bloom taxonomy","Taxonomy;Documentation;Debugging;Programming;Conferences;Electronic learning","computer science education;software engineering","novices program comprehension;program comprehension;computer science education;revised bloom taxonomy;software engineering instructors;remember category;line category;understand category;tracing;debugging;search;reordering;modification;extension;reuse;restructuring","","3","","36","","","","","IEEE","IEEE Conferences"
"A Concern Visualization Approach for Improving MATLAB and Octave Program Comprehension","I. D. M. Lessa; G. D. F. Carneiro; M. P. Monteiro; F. B. e. Abreu","NA; NA; NA; NA","2015 29th Brazilian Symposium on Software Engineering","","2015","","","130","139","The literature has pointed out the need for focusing efforts to better support comprehension of MATLAB and Octave programs. Despite being largely used in the industry and academia in the engineering domain, programs and routines written in those languages still require efforts to propose approaches and tools for its understanding. Considering the use of crosscutting concerns (CCCs) to support the comprehension of object-oriented programs, there is room of its use in the context of MATLAB and Octave programs. The literature has purpose and examples in this direction. Considering this scenario, we propose the use of visualization enriched with CCCs representation to support the comprehension of such programs. This paper discusses the use of a multiple view interactive environment called OctMiner in the context of two case studies to characterize how collected information relating to crosscutting concerns can foster the comprehension of MATLAB and GNU/Octave programs. As a result of the conducted case studies, we propose strategies based on OctMiner and tailored to support different comprehension activities of programs written in MATLAB and Octave.","","978-1-4673-9272","10.1109/SBES.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328017","MATLAB/Octave;software comprehension;crosscutting concerns;software visualization","MATLAB;Visualization;Scattering;Context;XML;Image color analysis","data visualisation;mathematics computing;program diagnostics","concern visualization approach;MATLAB;Octave program comprehension;engineering domain;crosscutting concerns;CCC;OctMiner;multiple view interactive environment;GNU-Octave programs","","","","19","","","","","IEEE","IEEE Conferences"
"Automatic Hierarchical Clustering of Static Call Graphs for Program Comprehension","G. Gharibi; R. Alanazi; Y. Lee","School of Computing and Enigeering, University of Missouri-Kansas City, Kansas City, USA; School of Computing and Enigeering, University of Missouri-Kansas City, Kansas City, USA; School of Computing and Enigeering, University of Missouri-Kansas City, Kansas City, USA","2018 IEEE International Conference on Big Data (Big Data)","","2018","","","4016","4025","Program comprehension is an imperative and indispensable prerequisite for several software tasks, including testing, maintenance, and evolution. In practice, understanding the software system requires investigating the high-level system functionality and mapping it to its low-level implementation, i. e. source code. The implementation of a software system can be captured using a call graph. A call graph represents the system's functions and their interactions at a single level of granularity. While call graphs can facilitate understanding the inner system functionality, developers are still required to manually map the high-level system functionality to its call graph. This manual mapping process is expensive, time-consuming and creates a cognitive gap between the system's highly-level functionality and its implementation. In this paper, we present an innovative approach that can automatically (1) construct and visualize the static call graph for a system written in Python, (2) cluster the execution paths of the call graph into hierarchal abstractions, and (3) label the clusters according to their major functional behaviors. The goal is to bridge the cognitive gap between the high-level system functionality and its call graph, which can further facilitate system comprehension. To validate our approach, we conducted four case studies including code2graph, Detectron, Flask, and Keras. The results demonstrated that our approach is feasible to construct call graphs and hierarchically cluster them into abstraction levels with proper labels.","","978-1-5386-5035-6978-1-5386-5034-9978-1-5386-5036","10.1109/BigData.2018.8622426","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8622426","static call graph;static analysis;clustering;topic modeling;program comprehension;Python","Tools;Python;Software systems;Task analysis;Visualization;Maintenance engineering","medical computing;pattern clustering;software maintenance","automatic hierarchical clustering;static call graph;program comprehension;software system;high-level system functionality;low-level implementation;call graphs;inner system functionality;system comprehension;code2graph","","","","41","","","","","IEEE","IEEE Conferences"
"Two User Perspectives in Program Comprehension: End Users and Developer Users","T. Roehm","NA","2015 IEEE 23rd International Conference on Program Comprehension","","2015","","","129","139","Recent empirical studies identified an interest of software developers in high-level usage information, i.e. Why and how end users employ a software application. Furthermore, recent empirical work found that developers of interactive applications put themselves in the role of users by interacting with the user interface during program comprehension. This paper presents an exploratory case study investigating these two user perspectives in detail. The study focuses on information needs regarding software usage and developers in the role of users during program comprehension. 21 developers from six software companies were observed during program comprehension tasks and interviewed. The resulting observation protocols and interview minutes were analyzed using coding. We found that developers are interested in information about use cases and user behavior, user goals and user needs, failure reproduction steps, and application domain concepts. But such information is rarely available to them during program comprehension. This mismatch indicates a potential to improve program comprehension practices by capturing such information and providing it to developers. Furthermore, we found that developers interact with the user interface of an interactive application to reproduce failures, to find relevant source code, to test changes, to trigger the debugger, and to familiarize with an unknown part of the application. Also, developers conceptually map elements of the user interface to source code, data structures, and algorithms. We call this behavior ""UI-based comprehension"" and argue that it is part of a broader comprehension strategy together with comprehension activities like reading source code or debugging.","1092-8138","978-1-4673-8159-8978-1-4673-8158","10.1109/ICPC.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181440","program comprehension;information needs;software usage;comprehension strategy;user interface (UI) comprehension;empirical study;case study;software maintenance;software evolution","Software;Interviews;Companies;User interfaces;Protocols;Observers;Java","data structures;program debugging;software engineering;source code (software);user interfaces","program comprehension;end users;developer users;software developers;user interface;coding;source code;data structures;debugging","","2","","29","","","","","IEEE","IEEE Conferences"
"A Comment Analysis Approach for Program Comprehension","J. L. Freitas; D. da Cruz; P. R. Henriques","NA; NA; NA","2012 35th Annual IEEE Software Engineering Workshop","","2012","","","11","20","Comments are interspersed by the Programmer among code lines, at software development phase, with two main purposes: to help himself during the development phase; to help other programmers later on, during the maintenance phase. The former are memos to help him remembering to do something; they are not useful for those willing to understand code. The latter are explanations about the ideas he has in mind when he wrote the code; they can be a relevant aid for others and should be taken into consideration as a first step in program comprehension. Comments are scattered all over the source code, sometimes wrapping a block of code (placed at the beginning or at its end), other times complementing a single statement. If comments are inserted to help in understanding the programmer ideas, they will contain for sure concepts associated with problem domain In this paper we discuss an approach to locate a relevant code chunk (one where the programmer should focus the attention for software maintenance), using information retrieval techniques to locate problem domain concepts within comments. In our approach, comments are isolated marking their type (inline, block or javadoc comment) and keeping their context (code lines to which they are associated). Picking up concepts from the ontology that describes the problem, it is possible to find all the comments that contain that concept (similar words) and rate them. Reading comments from the retrieved list, the programmer can select those that seem to him meaningful and dive directly into the associated chunk. In the paper, we also survey Comment Analysis techniques and describe an environment, Darius, that aims at automatizing the approach proposed. Moreover, Darius provides functionality to study comments frequency in the source files of a given project, to support the discussion weather it is worthwhile or not to apply this program comprehension step.","1550-6215;1550-6215","978-1-4673-5574-2978-0-7695-4947","10.1109/SEW.2012.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479798","program comprehension;comment analysis;concept location;problem domain;program domain","Portable document format;Semantics;Standards;Software maintenance;Natural languages;Data mining","information retrieval;ontologies (artificial intelligence);software maintenance","comment analysis approach;software development phase;code lines;source code;code chunk;software maintenance;information retrieval techniques;problem domain concepts;block comment;javadoc comment;inline comment;ontology;Darius;source files;discussion weather;program comprehension step","","1","","34","","","","","IEEE","IEEE Conferences"
"An Approach for Detecting Execution Phases of a System for the Purpose of Program Comprehension","H. Pirzadeh; A. Agarwal; A. Hamou-Lhadj","NA; NA; NA","2010 Eighth ACIS International Conference on Software Engineering Research, Management and Applications","","2010","","","207","214","Understanding the behavioural aspects of a software system is an important activity in many software engineering activities including program comprehension and reverse engineering. The behaviour of software is typically represented in the form of execution traces. Traces, however, tend to be considerably large which makes analyzing their content a complex task. There is a need for trace simplification techniques that can help software engineers make sense of the content of a trace despite the trace being massive. In this paper, we present a novel algorithm that aims to simplify the analysis of a large trace by detecting the execution phases that compose it. An example of a phase could be an initialization phase, a specific computation, etc. Our algorithm processes a trace generated from running the program under study and divides it into phases that can be later used by software engineers to understand where and why a particular computation appears. We also show the effectiveness of our approach through a case study.","","978-1-4244-7337-3978-1-4244-7336-6978-0-7695-4075","10.1109/SERA.2010.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5489832","Program comprehension;dynamic analysis;trace analysis;trace phase detection","Phase detection;Software maintenance;Software engineering;Reverse engineering;Algorithm design and analysis;Software algorithms;Runtime;Information analysis;Conference management;Engineering management","program diagnostics;reverse engineering;software engineering","execution phases detection;program comprehension;behavioural aspects;software engineering activities;reverse engineering;execution traces;trace simplification techniques","","15","","14","","","","","IEEE","IEEE Conferences"
"Awareness and Comprehension in Software/Systems Engineering Practice and Education: Trends and Research Directions","M. Schots; C. Werner; M. Mendonça","NA; NA; NA","2012 26th Brazilian Symposium on Software Engineering","","2012","","","186","190","The creation of tools, techniques and methodologies to support the manipulation of large data sets has been receiving special attention of both scientific and industrial communities, in order to discover new ways of dealing with the underlying information, including learning purposes, identification of patterns, decision making support, amongst others. However, making use of computing resources to enhance awareness and understanding of software information and the software itself is still a challenge in software/systems engineering, since it involves the identification of suitable mechanisms, adequate abstractions, and studies on stimulation of the human perceptive and cognitive abilities. This paper presents some of the challenges in this context, based on current trends of software development lifecycle, program comprehension, and software engineering education. At the end, a special focus is given on ongoing research on using and improving current mechanisms for supporting software reuse practices and software comprehension in general.","","978-0-7695-4868-5978-1-4673-4472","10.1109/SBES.2012.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6337872","software engineering;awareness;program comprehension;software visualization;software engineering education;grand challenges","Software;Software engineering;Education;Visualization;Conferences;Industries;Data visualization","computer science education;software reusability","software-system engineering practice;software-system engineering education;software information awareness;software information understanding;human perceptive stimulation;cognitive ability;software development lifecycle;program comprehension;software engineering education;software reuse practices;software comprehension","","","","40","","","","","IEEE","IEEE Conferences"
"Evaluation of user engagement and message comprehension in a pervasive software installation","M. Aasbakken; L. Jaccheri; K. Chorianopoulos","Department of Computer and Information Science, NTNU, Trondheim Norway; Department of Computer and Information Science, NTNU, Trondheim Norway; Ionian University, Corfu Greece","2012 Second International Workshop on Games and Software Engineering: Realizing User Engagement with Game Engineering Techniques (GAS)","","2012","","","27","30","The goal of this work is to explore the relationship between pervasive software and user engagement towards environmental issues. We study this relationship in the context of an art installation that concerns the water cycle in nature. The research question is: How can we design and evaluate software that becomes a medium to engage and inform the user? We have gathered empirical data during a two days exhibition of two versions of a pervasive art installation by: observations, questionnaires, and input logs. Data analysis reveals that the art installation engaged users, with focus on young children, and communicated the intended message. The results are organized according to five important factors for developing and evaluating interacting art installations. These are: 1) data collection method; 2) user interaction; 3) social interaction; 4) issues about children; 5) message comprehension. We suggest that these factors can inform engineering practices for engaging software like video-games.","","978-1-4673-1768-9978-1-4673-1769","10.1109/GAS.2012.6225923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225923","Social engagement;pervasive software;art installation;interactivity;evaluation","Art;Software;Games;Computers;Videos;Context;Educational institutions","art;computer games;environmental science computing;interactive systems;social sciences computing;ubiquitous computing","user engagement;message comprehension;pervasive software installation;environmental issues;pervasive art installation;data collection method;user interaction;social interaction;software like video-games;water cycle","","1","","11","","","","","IEEE","IEEE Conferences"
"Comprehension oriented software fault location","Wang Tiantian; Su Xiaohong; Ma Peijun; Wang Kechao","School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Software, Harbin University, China","Proceedings of 2011 International Conference on Computer Science and Network Technology","","2011","1","","340","343","Software errors can potentially lead to disastrous consequences. Unfortunately, debugging software errors can be difficult and time-consuming. A comprehension oriented software fault location approach (COFL) is proposed in this paper to provide automated assistance in bug location. It not only locates program predicates predicting bugs, but also provides high efficiency demand-driven data flow and control flow analysis to help developers understand the causes and contexts of bugs.","","978-1-4577-1587-7978-1-4577-1586-0978-1-4577-1585","10.1109/ICCSNT.2011.6181971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6181971","fault location;program comprehension;statistics-based;demand-driven","Software;Instruments;Computer bugs;Debugging;Fault location;Prediction algorithms;Conferences","data flow analysis;program debugging;software fault tolerance","comprehension oriented software fault location;software errors;COFL;bug location;high efficiency demand-driven data flow;flow analysis;software debugging","","","","24","","","","","IEEE","IEEE Conferences"
"Software Evolution Comprehension: Replay to the Rescue","L. Hattori; M. D'Ambros; M. Lanza; M. Lungu","NA; NA; NA; NA","2011 IEEE 19th International Conference on Program Comprehension","","2011","","","161","170","Developers often need to find answers to questions regarding the evolution of a system when working on its code base. While their information needs require data analysis spanning over different repository types, the source code repository has a pivotal role for program comprehension tasks. However, the coarse-grained nature of the data stored by commit-based software configuration management systems often makes it challenging for a developer to search for an answer. We present Replay, an Eclipse plug-in that allows one to explore the change history of a system by capturing the changes at a finer granularity level than commits, and by replaying the past changes chronologically inside the integrated development environment with the source code at hand. We conducted a controlled experiment to empirically assess whether Replay outperforms a baseline (SVN client in Eclipse) on helping developers to answer common questions related to software evolution. The experiment shows that Replay leads to a decrease in completion time with respect to a set of software evolution comprehension tasks.","1092-8138;1092-8138","978-1-61284-308-7978-0-7695-4398","10.1109/ICPC.2011.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970150","","Software;History;Java;Servers;Training;Context modeling;Timing","program diagnostics;software maintenance","software evolution;Replay;Eclipse plug-in;SVN client;source code repository;program comprehension","","14","","30","","","","","IEEE","IEEE Conferences"
"Reverse Engineering Software Tools Based on a Comprehension System","B. Tenovo; P. Mursanto; H. B. Santoso","Computer Science Faculty, Universitas Indonesia, Jakarta, Indonesia; Computer Science Faculty, Universitas Indonesia, Jakarta, Indonesia; Computer Science Faculty, Universitas Indonesia, Jakarta, Indonesia","2017 7th World Engineering Education Forum (WEEF)","","2017","","","202","209","In this research, we propose a comprehension system model. Our model is built based on 3 requirements: divergence of business process solutions within an organization as part of automation, the importance of two-way understanding between organizations and software developers, and supporting reverse engineering goals (reconstruction, validation, and part of forward engineering). An organization can have divergences in business process solutions: non-automation, semi-automation, and automation. What two-way understanding is, developer's understanding of the software to be developed and organization's understanding of the developer. Development team's understanding against the software is built with comprehension system. The system is developed with some Instructional Design System as a foundation reference. Organizational understanding is obtained by analyzing collected data from the interaction between comprehension system and a team member. Non-automation comprehension system prototype has been tested with 82 students to produce group model (students model) and to obtain more detailed specifications of the comprehension system model.","","978-1-5386-1523-2978-1-5386-1522-5978-1-5386-1524","10.1109/WEEF.2017.8467039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8467039","instructional design system;learning system;comprehension system;reverse engineering","Reverse engineering;Unified modeling language;Software;Prototypes;Interviews;Organizations;Tools","reverse engineering;software tools","organizational understanding;nonautomation comprehension system prototype;comprehension system model;reverse engineering software tools;business process solutions;automation;forward engineering;semiautomation;development team;reverse engineering goals;instructional design system","","","","58","","","","","IEEE","IEEE Conferences"
"Leveraging Crowd Knowledge for Software Comprehension and Development","L. Ponzanelli; A. Bacchelli; M. Lanza","NA; NA; NA","2013 17th European Conference on Software Maintenance and Reengineering","","2013","","","57","66","Question and Answer (Q&A) services, such as Stack Overflow, rely on a community of programmers who post questions, provide and rate answers, to create what is termed ""crowd knowledge"". As a consequence, these services archive voluminous and potentially useful information to help developers to solve programming-specific issues. Programmers tap into this crowd knowledge through web browsers. This requires them to step out of their integrated development environments (IDE), formulate a query, inspect the returned results and manually port the solution back to the IDE. We present an integrated and largely automated approach to assist programmers who want to leverage the crowd knowledge of Q&A services. We give a form to our approach by implementing Seahawk, an Eclipse plugin. Seahawk automatically formulates queries from the current context in the IDE, and presents a ranked and interactive list of results. Seahawk lets users identify individual discussion pieces and import code samples through simple drag & drop. Users can also link Stack Overflow discussions and source code persistently. We performed an evaluation of Seahawk, with promising results.","1534-5351","978-0-7695-4948-4978-1-4673-5833","10.1109/CSMR.2013.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498455","Q&amp;A services;recommendation systems","Engines;XML;Search engines;Java;Software;Navigation;User interfaces","application program interfaces;query processing;question answering (information retrieval);software engineering","crowd knowledge;software comprehension;software development;question-and-answer service;stack overflow service;Web browser;integrated development environment;IDE;query formulation;Seahawk Eclipse plugin","","25","","32","","","","","IEEE","IEEE Conferences"
"Hierarchical software landscape visualization for system comprehension: A controlled experiment","F. Fittkau; A. Krause; W. Hasselbring","Software Engineering Group, Kiel University, Germany; Software Engineering Group, Kiel University, Germany; Software Engineering Group, Kiel University, Germany","2015 IEEE 3rd Working Conference on Software Visualization (VISSOFT)","","2015","","","36","45","In many enterprises the number of deployed applications is constantly increasing. Those applications - often several hundreds - form large software landscapes. The comprehension of such landscapes is frequently impeded due to, for instance, architectural erosion, personnel turnover, or changing requirements. Therefore, an efficient and effective way to comprehend such software landscapes is required. The current state of the art often visualizes software landscapes via flat graph-based representations of nodes, applications, and their communication. In our ExplorViz visualization, we introduce hierarchical abstractions aiming at solving typical system comprehension tasks fast and accurately for large software landscapes. To evaluate our hierarchical approach, we conduct a controlled experiment comparing our hierarchical landscape visualization to a flat, state-of-the-art visualization. In addition, we thoroughly analyze the strategies employed by the participants and provide a package containing all our experimental data to facilitate the verifiability, reproducibility, and further extensibility of our results. We observed a statistically significant increase of 14 % in task correctness of the hierarchical visualization group compared to the flat visualization group in our experiment. The time spent on the system comprehension tasks did not show any significant differences. The results backup our claim that our hierarchical concept enhances the current state of the art in landscape visualization.","","978-1-4673-7526-9978-1-4673-7525","10.1109/VISSOFT.2015.7332413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332413","","Visualization;Software;Data visualization;Servers;Context;Computational modeling","data visualisation;program diagnostics","hierarchical software landscape visualization;ExplorViz visualization;hierarchical abstractions;system comprehension tasks;task correctness;hierarchical visualization group;flat visualization group","","2","","","","","","","IEEE","IEEE Conferences"
"Does the Documentation of Design Pattern Instances Impact on Source Code Comprehension? Results from Two Controlled Experiments","C. Gravino; M. Risi; G. Scanniello; G. Tortora","NA; NA; NA; NA","2011 18th Working Conference on Reverse Engineering","","2011","","","67","76","We present the results of a controlled experiment and a differentiated replication that have been carried out to assess the effect of the documentation of design patterns on the comprehension of source code. The two experiments involved Master Students in Computer Science at the University of Basilicas a and at University of Salerno, respectively. The participants to the original experiment performed a comprehension task with and without graphically-documented design patterns. Textually documented design patterns were provided or not to the participants to perform a comprehension task within the replication. The data analysis revealed that participants employing graphically documented design patterns achieved significantly better performances than the participants provided with source code alone. Conversely, the effect of textually documented design patterns was not statistically significant. A further analysis revealed that the documentation type (textual and graphical) does not significantly affect the performance, when participants correctly recognize design pattern instances.","2375-5369;1095-1350","978-1-4577-1948","10.1109/WCRE.2011.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6079776","Design Patterns;Controlled Experiment;Maintenance","Documentation;Unified modeling language;Educational institutions;Maintenance engineering;Software systems;Particle measurements","computer graphics;computer science education;data analysis;design of experiments;software engineering;source coding;system documentation","design pattern instances;source code comprehension;controlled experiment;differentiated replication;design patterns documentation;master students;computer science;University of Basilicas;University of Salerno;comprehension task;graphically-documented design patterns;textually documented design patterns;data analysis;documentation type","","2","","21","","","","","IEEE","IEEE Conferences"
"On Using Tree Visualisation Techniques to Support Source Code Comprehension","I. Bacher; B. M. Namee; J. D. Kelleher","NA; NA; NA","2016 IEEE Working Conference on Software Visualization (VISSOFT)","","2016","","","91","95","This paper presents a design study that investigates the use of compact tree visualisations to provide software developers with an overview of the static structure of a source code document within a code editor in order to facilitate source code understanding and navigation. A prototype is presented which utilises an icicle tree visualisation to encode the control structure hierarchy of a source code document, as well as a circular treemap visualisation to encode the scope hierarchy of a source code document. An overview of the prototype and its functionality is given as well as a detailed discussion on the design rationale behind the tool. Possible applications and future work plans are also discussed.","","978-1-5090-3850-3978-1-5090-3851","10.1109/VISSOFT.2016.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780163","Software visualisation;Tree visualisations techniques;Source code comprehension","Visualization;Software;Prototypes;Context;Navigation;Aerospace electronics;Data visualization","data visualisation;program control structures;software engineering;source code (software);trees (mathematics)","static structure;source code document;code editor;source code understanding;source code navigation;icicle tree visualisation;control structure hierarchy;circular treemap visualisation;scope hierarchy;design rationale;software development","","","","23","","","","","IEEE","IEEE Conferences"
"Users' Perception on the Use of MetricAttitude to Perform Source Code Comprehension Tasks: A Focus Group Study","R. Francese; M. Risi; G. Scanniello; G. Tortora","NA; NA; NA; NA","2017 21st International Conference Information Visualisation (IV)","","2017","","","8","13","MetricAttitude [18] is a visualization approach implemented in an environment that provides a mental picture of an object-oriented software by means of polymetric views of classes. In this paper, we describe a qualitative investigation we have conducted with a focus group involving developers aiming at evaluating their viewpoint on the relevance of the support MetricAttitude provides to perform comprehension tasks on source code. This investigation also allowed us to gather information on the developers' opinion on the MetricAttitude features and its software visualization metaphors. The discussion was animated and participants provided a number of useful suggestions for improving the visualization. The tool was considered very useful, while some usability problems have to be addressed. Specifically, the information provided has to be further filtered to easier software comprehension tasks.","2375-0138","978-1-5386-0831-9978-1-5386-0832","10.1109/iV.2017.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107941","Polymetric Views;Software Visualization;Program Comprehension;Focus Group;User Study;Qualitative Evaluation","Software;Measurement;Visualization;Computer bugs;Tools;Concrete","ergonomics;human factors;object-oriented methods;object-oriented programming;program visualisation;source code (software)","focus group study;visualization approach;mental picture;software visualization metaphors;users perception;MetricAttitude;source code comprehension tasks;object-oriented software;usability problems","","1","","24","","","","","IEEE","IEEE Conferences"
"An Early Investigation on the Contribution of Class and Sequence Diagrams in Source Code Comprehension","G. Scanniello; C. Gravino; G. Tortora","NA; NA; NA","2013 17th European Conference on Software Maintenance and Reengineering","","2013","","","367","370","We report the preliminary results of a controlled experiment conducted to analyze whether the combined use of UML class and sequence diagrams better supports source code comprehension with respect to the use of class and sequence diagrams alone. We also investigated which notation between class and sequence diagrams provides a better support in the execution of comprehension tasks on source code. The results suggest that it is better to use class and sequence diagrams together with respect to using either class or sequence diagrams alone. The difference in the source code comprehension is statistically significant with respect to the use of class diagrams alone, while is not statistically significant with respect to the sequence diagrams alone.","1534-5351","978-0-7695-4948-4978-1-4673-5833","10.1109/CSMR.2013.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498490","Comprehension;Controlled Experiment;UML","Unified modeling language;Object oriented modeling;Software engineering;Maintenance engineering;Software systems;Software maintenance","diagrams;Unified Modeling Language","UML class diagrams;sequence diagrams;source code comprehension","","2","","15","","","","","IEEE","IEEE Conferences"
"An Empirical Study on Code Comprehension: Data Context Interaction Compared to Classical Object Oriented","H. A. Valdecantos; K. Tarrit; M. Mirakhorli; J. O. Coplien","NA; NA; NA; NA","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","","2017","","","275","285","Source code comprehension affects software development - especially its maintenance - where code reading is one of the most time-consuming activities. A programming language, together with the programming paradigm it supports, is a strong factor that profoundly impacts how programmers comprehend code. We conducted a human-subject controlled experiment to evaluate comprehension of code written using the Data Context Interaction (DCI) paradigm relative to code written with commonly used Object-Oriented (OO) programming. We used a new research-level language called Trygve which implements DCI concepts, and Java, a pervasive OO language in the industry. DCI revisits lost roots of the OO paradigm to address problems that are inherent to Java and most other contemporary OO languages. We observed correctness, time consumption, and locality of reference during reading comprehension tasks. We present a method which relies on the Eigenvector Centrality metric from Social Network Analysis to study the locality of reference in programmers by inspecting their sequencing of reading language element declarations and their permanence time in the code. Results indicate that DCI code in Trygve supports more comprehensible code regarding correctness and improves the locality of reference, reducing context switching during the software discovery process. Regarding reading time consumption, we found no statistically significant differences between both approaches.","","978-1-5386-0535-6978-1-5386-0536","10.1109/ICPC.2017.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961524","Program comprehension;Controlled experiment;Human subjects;Programming languages;Programming paradigms;Data Context Interaction;Object-Oriented","Context;Java;Programming profession;Software;Sequential analysis","eigenvalues and eigenfunctions;Java;object-oriented languages;object-oriented programming;social networking (online);software engineering","data context interaction;source code comprehension;software development;programming language;object-oriented programming;OO language;research-level language;Trygve;Java;DCI concepts;Social Network Analysis;eigenvector centrality metric;software discovery process","","","","39","","","","","IEEE","IEEE Conferences"
"Incremental Annotate-Generalize-Search Framework for Interactive Source Code Comprehension","K. Nakayama; S. Tano; T. Hashiyama; E. Sakai","NA; NA; NA; NA","2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)","","2017","1","","311","316","Understanding unfamiliar source code is inherently difficult for a software engineer, despite its importance. Thus, an experienced engineer prefers to guess the intended behavior, rather than to trace it line-by-line, by combining semantic chunks found in the source code. It is, however, still hard for a system to help in this activity, for lack of ways of both representing semantic chunks and of preparing a rich dictionary of chunks. In this paper, an integrated framework for annotating and searching source code is presented. Since the research is still in its early stage, this paper focuses on the framework itself, together with a brief description of our prototype implementation. In the framework, each engineer gathers (annotates) semantic chunks that have the same meaning and interactively generalizes them to get a search pattern. As a result, a dictionary of semantic chunks together with their search patterns is incrementally created through engineer collaboration. To realize this, two representations are used: a tuple of nodes of an abstract syntax tree (AST) for a semantic chunk and a classifier on generative attribute vectors for search patterns.","0730-3157","978-1-5386-0367-3978-1-5386-0368","10.1109/COMPSAC.2017.147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029623","semantic chunk;abstract syntax tree","Semantics;Knowledge engineering;Syntactics;Natural languages;Software;Collaboration;Systems support","computational linguistics;software maintenance;source code (software);tree data structures","incremental annotate-generalize-search framework;interactive source code comprehension;unfamiliar source code understanding;semantic chunks;search pattern;abstract syntax tree;AST;generative attribute vectors","","","","18","","","","","IEEE","IEEE Conferences"
"On the comprehension of code clone visualizations: A controlled study using eye tracking","M. S. Uddin; V. Gaur; C. Gutwin; C. K. Roy","Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada; Department of Computer Science, University of Saskatchewan, Saskatoon, Canada","2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)","","2015","","","161","170","Code clone visualizations (CCVs) are graphical representations of clone detection results provided by various state-of-the-art command line and graphical analysis tools. In order to properly analyze and manipulate code clones within a target system, these visualizations must be easily and efficiently comprehensible. We conducted an eye-tracking study with 20 participants (expert, intermediate, and novice) to assess how well people can comprehend visualizations such as Scatter plots, Treemaps, and Hierarchical Dependency Graphs provided by VisCad, a recent clone visualization tool. The goals of the study were to find out what elements of the visualizations (e.g., colors, shapes, object positions) are most important for comprehension, and to identify common usage patterns for different groups. Our results help us understand how developers with different levels of expertise explore and navigate through the visualizations while performing specific tasks. Distinctive patterns of eye movements for different visualizations were found depending on the expertise of the participants. Color, shape and position information were found to play vital roles in comprehension of CCVs. Our results provide recommendations that can improve the implementation of visualization techniques in VisCad and other clone visualization systems.","","978-1-4673-7529-0978-1-4673-7528","10.1109/SCAM.2015.7335412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335412","Clone visualization;comprehension;code clone;VisCad;eye tracking;human-computer interaction","Cloning;Visualization;Gaze tracking;Data visualization;Color;Shape;Tracking","data visualisation;gaze tracking;program diagnostics;source code (software)","code clone visualizations comprehension;eye tracking;clone detection graphical representations;command line analysis tools;graphical analysis tools;scatter plots;treemaps;hierarchical dependency graphs;VisCad;clone visualization tool;CCV","","1","","35","","","","","IEEE","IEEE Conferences"
"Linear, Quadratic, and Semidefinite Programming Massive MIMO Detectors: Reliability and Complexity","R. Masashi Fukuda; T. Abrão","Electrical Engineering Department, State University of Londrina, Londrina, Brazil; Electrical Engineering Department, State University of Londrina, Londrina, Brazil","IEEE Access","","2019","7","","29506","29519","One of the downsides of the massive multiple-input-multiple-output (M-MIMO) system is its computational complexity. Considering that techniques and different algorithms proposed in the literature applied to conventional MIMO may not be well suited or readily applicable to M-MIMO systems, in this paper, the application of different formulations inside the convex optimization framework is investigated. This paper is divided into two parts. In the first part, linear programming, quadratic programming (QP), and semidefinite programming are explored in an M-MIMO environment with high-order modulation and under realistic channel conditions, i.e., considering spatial correlation, error in the channel estimation, as well as different system loading. The bit error rate is evaluated numerically through Monte Carlo simulations. In the second part, algorithms to solve the QP formulation are explored, and computational complexity in terms of floating-point operations (flops) is compared with linear detectors. Those algorithms have interesting aspects when applied to our specific problem (M-MIMO detection formulated as QP), such as the exploitation of the structure of the problem (simple constraints) and the improvement of the rate of convergence due to the well-conditioned Gram matrix (channel hardening). The number of iterations is higher when the number of users $K$ becomes similar to the number of base station antennas $M$ (i.e., $K\approx M$ ) than the case $K\ll M$ ; the number of iterations increases slowly as the number of users $K$ and base station antennas $M$ increases while keeping a low system loading. The QP with projected algorithms presented better performance than minimum mean square error detector when $K\approx M$ and promising computational complexity for scenarios with increasing $K$ and low system loading.","2169-3536","","10.1109/ACCESS.2019.2902521","Conselho Nacional de Desenvolvimento Científico e Tecnológico; State University of Londrina (UEL); State Government of Parana; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8656462","Massive MIMO communication;low-complexity detectors;convex optimization;linear programming;quadratic programming;semidefinite programming","Detectors;MIMO communication;Antennas;Channel estimation;Programming;Computational complexity;Linear programming","antenna arrays;channel estimation;computational complexity;convex programming;error statistics;iterative methods;linear programming;MIMO communication;Monte Carlo methods;quadratic programming;telecommunication network reliability","Monte Carlo simulations;QP formulation;computational complexity;floating-point operations;Gram matrix;channel hardening;base station antennas;low system loading;projected algorithms;M-MIMO systems;convex optimization framework;M-MIMO environment;high-order modulation;realistic channel conditions;channel estimation;bit error rate;massive multiple-input-multiple-output system;system loading;linear programming massive MIMO detector;semidefinite programming massive MIMO detector;quadratic programming massive MIMO detector;spatial correlation","","","","46","","","","","IEEE","IEEE Journals & Magazines"
"Approximation structures with moderate complexity in functional optimization and dynamic programming","M. Gaggero; G. Gnecco; T. Parisini; M. Sanguineti; R. Zoppoli","Institute of Intelligent Systems for Automation, National Research Council of Italy, Genova, Italy; DIBRIS Department, University of Genoa, Genova, Italy; Dept. of Electrical and Electronic Engineering at the Imperial College London, UK; DIBRIS Department, University of Genoa, Genova, Italy; DIBRIS Department, University of Genoa, Genova, Italy","2012 IEEE 51st IEEE Conference on Decision and Control (CDC)","","2012","","","1902","1908","Connections between function approximation and classes of functional optimization problems, whose admissible solutions may depend on a large number of variables, are investigated. The insights obtained in this context are exploited to analyze families of nonlinear approximation schemes containing tunable parameters and enjoying the following property: when they are used to approximate the (unknown) solutions to optimization problems, the number of parameters required to guarantee a desired accuracy grows at most polynomially with respect to the number of variables in admissible solutions. Both sigmoidal neural networks and networks with kernel units are considered as approximation structures to which the analysis applies. Finally, it is shown how the approach can be applied for the solution of finite-horizon optimal control problems via approximate dynamic programming enhancing the potentialities of recent developments in nonlinear approximation in the framework of the solution of sequential decision problems with continuous state spaces.","0191-2216;0743-1546;0743-1546","978-1-4673-2066-5978-1-4673-2065-8978-1-4673-2063-4978-1-4673-2064","10.1109/CDC.2012.6426656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6426656","","Function approximation;Optimization;Complexity theory;Optimal control;Vectors;Stochastic processes","dynamic programming;function approximation;neural nets;optimal control","functional optimization;dynamic programming;function approximation;nonlinear approximation;optimization problems;sigmoidal neural networks;finite-horizon optimal control;sequential decision problems;continuous state spaces","","","","34","","","","","IEEE","IEEE Conferences"
"Exact Complexity Certification of Active-Set Methods for Quadratic Programming","G. Cimini; A. Bemporad","ODYS S.r.l., Universit&#x00E0; Politecnica delle Marche, Ancona, Lucca, ItalyItaly; IMT School for Advanced Studies Lucca, Lucca, Italy","IEEE Transactions on Automatic Control","","2017","62","12","6094","6109","Active-set methods are recognized to often outperform other methods in terms of speed and solution accuracy when solving small-size quadratic programming (QP) problems, making them very attractive in embedded linear model predictive control (MPC) applications. A drawback of active-set methods is the lack of tight bounds on the worst-case number of iterations, a fundamental requirement for their implementation in a real-time system. Extensive simulation campaigns provide an indication of the expected worst-case computation load, but not a complete guarantee. This paper solves such a certification problem by proposing an algorithm to compute the exact bound on the maximum number of iterations and floating point operations required by a state-of-the-art dual active-set QP solver. The algorithm is applicable to a given QP problem whose linear term of the cost function and right-hand side of the constraints depend linearly on a vector of parameters, as in the case of linear MPC. In addition, a new solver is presented that combines explicit and implicit MPC ideas, guaranteeing improvements of the worst-case computation time. The ability of the approach to exactly quantify memory and worst-case computation requirements is tested on a few MPC examples, also highlighting when online optimization should be preferred to explicit MPC.","0018-9286;1558-2523;2334-3303","","10.1109/TAC.2017.2696742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907269","Active-set methods;complexity certification;linear model predictive control (MPC);quadratic programming (QP)","Complexity theory;Predictive control;Quadratic programming;Cost function;Memory management;Real-time systems;Computational modeling","computational complexity;gradient methods;iterative methods;linear systems;predictive control;quadratic programming","embedded linear model predictive control applications;worst-case computation load;certification problem;floating point operations;linear MPC;worst-case computation time;worst-case computation requirements;small-size quadratic programming problem solving;QP problem solver","","2","","44","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Exact solutions of time difference of arrival source localisation based on semi-definite programming and Lagrange multiplier: complexity and performance analysis","V. Heidari; M. Amidzade; K. Sadeghi; A. M. Pezeshk","Sharif University of Technology, Iran; Sharif University of Technology, Iran; Sharif University of Technology, Iran; Sharif University of Technology, Iran","IET Signal Processing","","2014","8","8","868","877","In this study, the authors investigate the problem of source localisation based on the time difference of arrival (TDOA) in a group of sensors. Aiming to minimise the squared range-difference errors, the problem leads to a quadratically constrained quadratic programme. It is well known that this approach results in a non-convex optimisation problem. By proposing a relaxation technique, they show that the optimisation problem would be transformed to a convex one which can be solved by semi-definite programming (SDP) and Lagrange multiplier methods. Moreover, these methods offer the exact solution of the original problem and the affirmation of its uniqueness. In contrast to other complicated state-of-the-art SDP algorithms presented in the TDOA localisation literature, the authors methods are derived in a few straightforward reformulations and insightful steps; thus, there are no confusing and unjustifiable changes in the main optimisation problem. Furthermore, complexity analysis and a new approach for performance analysis, which show the merit of their methods, are introduced. Simulations and numerical results demonstrate that the positioning estimators resulted from the proposed algorithms outperform existing SDP-based methods presented so far.","1751-9675;1751-9683","","10.1049/iet-spr.2013.0457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955141","","","convex programming;sensor fusion;sensor placement;time-of-arrival estimation","time difference of arrival;source localisation;semidefinite programming;performance analysis;TDOA;sensors;squared range-difference errors;quadratically constrained quadratic programme;nonconvex optimisation problem;relaxation technique;SDP-based methods;complexity analysis;SDP algorithms;Lagrange multiplier methods;optimisation problem","","2","","37","","","","","IET","IET Journals & Magazines"
"Sensor Placement for Fault Isolability Using Low Complexity Dynamic Programming","G. Chi; D. Wang; T. Le; M. Yu; M. Luo","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Singapore Institute of Manufacturing Technology, Singapore","IEEE Transactions on Automation Science and Engineering","","2015","12","3","1080","1091","In this paper, a novel approach of sensor placement is proposed for the purpose of maximizing fault detectability and isolability. This new approach rests on the basic fact that faults are embedded in the analytical redundancy relations (ARRs) and that the occurrence of a fault will change the consistency of the corresponding ARRs. Based on these basic facts, the minimal isolating (MI) set is introduced to formulate the full/maximal isolability which is the constraint for sensor placement. Consequently, the optimization problem for sensor placement is reformulated as searching an MI set which is related to the least number of candidate sensors. To find the optimal MI set, a low complexity dynamic programming (LCDP) algorithm is developed on the fault set F that consists of system faults and sensor faults. However, sensor faults are varied as different candidate sensors are used. Therefore, another dedicated procedure is proposed to handle this issue. A case study shows that the proposed approach outperforms an existing sensor placement approach in terms of efficiency.","1545-5955;1558-3783","","10.1109/TASE.2014.2372792","National Natural Science Foundation of China; Singapore ASTAR SERC GRANT on Industrial Robotics Programme M4070209040 AStar 1225100004; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010953","Analytical redundancy relations;dynamic programming;fault detectability and isolability;sensor placement","Vectors;Robot sensing systems;Dynamic programming;Complexity theory;Optimization;Indexes;Fault detection","dynamic programming;fault diagnosis;sensor placement","fault detectability maximization;fault isolability maximization;analytical redundancy relation;ARR;optimization problem;MI set;LCDP algorithm;low complexity dynamic programming algorithm;sensor placement approach;minimal isolation set","","3","","35","","","","","IEEE","IEEE Journals & Magazines"
"Reuse historic costs in dynamic programming to reduce computational complexity in the context of model predictive optimization","T. Guan; C. W. Frey","Department of Measurement, Control and Diagnosis, Fraunhofer IOSB, 76131 Karlsruhe, Germany; Department of Measurement, Control and Diagnosis, Fraunhofer IOSB, 76131 Karlsruhe, Germany","2015 IEEE International Conference on Vehicular Electronics and Safety (ICVES)","","2015","","","256","263","Energy efficiency has become a major issue in trade, transportation and environment protection. While the next generation of zero emission propulsion systems still have difficulties in reaching similar travel distances as combustion engine propulsion systems, it is already possible to increase fuel efficiency in regular vehicles by applying a more fuel efficient driving behaviour. An adapted Dynamic Programming approach is used to calculate optimal behaviour profiles for the road ahead within a finite optimization horizon. The main purpose of this publication is the development of a strategy to reuse historic minimal costs in order to reduce the computational complexity of future optimization steps. The percent reduction is deterministic and increases with the discretization degree of the optimization horizon.","","978-1-4673-9114-6978-1-4673-9113","10.1109/ICVES.2015.7396927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396927","","Optimization;Engines;Fuels;Computational modeling;Dynamic programming;Vehicles;Gears","computational complexity;dynamic programming;internal combustion engines;transportation","finite optimization horizon;adapted dynamic programming approach;combustion engine propulsion system;zero emission propulsion system;model predictive optimization;computational complexity","","4","","20","","","","","IEEE","IEEE Conferences"
"Information-Based Complexity, Feedback and Dynamics in Convex Programming","M. Raginsky; A. Rakhlin","Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; Department of Statisics, Wharton School of Business, University of Pennsylvania, Philadelphia, PA, USA","IEEE Transactions on Information Theory","","2011","57","10","7036","7056","We study the intrinsic limitations of sequential convex optimization through the lens of feedback information theory. In the oracle model of optimization, an algorithm queries an oracle for noisy information about the unknown objective function and the goal is to (approximately) minimize every function in a given class using as few queries as possible. We show that, in order for a function to be optimized, the algorithm must be able to accumulate enough information about the objective. This, in turn, puts limits on the speed of optimization under specific assumptions on the oracle and the type of feedback. Our techniques are akin to the ones used in statistical literature to obtain minimax lower bounds on the risks of estimation procedures; the notable difference is that, unlike in the case of i.i.d. data, a sequential optimization algorithm can gather observations in a controlled manner, so that the amount of information at each step is allowed to change in time. In particular, we show that optimization algorithms often obey the law of diminishing returns: the signal-to-noise ratio drops as the optimization algorithm approaches the optimum. To underscore the generality of the tools, we use our approach to derive fundamental lower bounds for a certain active learning problem. Overall, the present work connects the intuitive notions of “information” in optimization, experimental design, estimation, and active learning to the quantitative notion of Shannon information.","0018-9448;1557-9654","","10.1109/TIT.2011.2154375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766746","Convex optimization;Fano's inequality;feedback information theory;hypothesis testing with controlled observations;information-based complexity;information-theoretic converse;minimax lower bounds;sequential optimization algorithms;statistical estimation","Optimization;Complexity theory;Markov processes;Convex functions;Accuracy;Noise measurement;Random variables","convex programming;feedback;information theory;minimax techniques;sequential estimation","information-based complexity;sequential convex optimization;feedback information theory;statistical literature;minimax lower bound;sequential optimization algorithm;signal-to-noise ratio;active learning problem;quantitative notion;Shannon information","","12","","34","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity polynomial approximation of explicit MPC via linear programming","M. Kvasnica; J. Löfberg; M. Herceg; L. čirka; M. Fikar","Institute of Information Engineering, Automation, and Mathematics, Faculty of Chemical and Food Technology, Slovak University of Technology in Bratislava, Slovakia; Automatic Control Laboratory, Linköpings universitet, Sweden; Institute of Information Engineering, Automation, and Mathematics, Faculty of Chemical and Food Technology, Slovak University of Technology in Bratislava, Slovakia; Institute of Information Engineering, Automation, and Mathematics, Faculty of Chemical and Food Technology, Slovak University of Technology in Bratislava, Slovakia; Institute of Information Engineering, Automation, and Mathematics, Faculty of Chemical and Food Technology, Slovak University of Technology in Bratislava, Slovakia","Proceedings of the 2010 American Control Conference","","2010","","","4713","4718","This paper addresses the issue of the practical implementation of Model Predictive Controllers (MPC) to processes with short sampling times. Given an explicit solution to an MPC problem, the main idea is to approximate the optimal control law defined over state space regions by a single polynomial of pre-specified degree which, when applied as a state-feedback, guarantees closed-loop stability, constraint satisfaction, and a bounded performance decay. It is shown how to search for such a polynomial by solving a single linear program.","2378-5861;0743-1619;0743-1619","978-1-4244-7427-1978-1-4244-7426-4978-1-4244-7425","10.1109/ACC.2010.5531092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5531092","","Polynomials;Linear programming;Sampling methods;Optimal control;Stability;Table lookup;Automatic control;Feedback;State-space methods;Chemical technology","linear programming;polynomial approximation;predictive control;state feedback","low complexity polynomial approximation;linear programming;model predictive controller;optimal control law;state space region;state feedback;closed loop stability;constraint satisfaction;single linear program","","5","","18","","","","","IEEE","IEEE Conferences"
"Sensor selection and placement using low complexity dynamic programming","G. Chi; T. Le; D. Wang; M. Yu; M. Luo","EXQUISITUS, Centre for E-City, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798; Manufacturing Execution and Control Group, SIMTech, A∗STAR, Singapore, 638075; EXQUISITUS, Centre for E-City, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798; EXQUISITUS, Centre for E-City, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798; Manufacturing Execution and Control Group, SIMTech, A∗STAR, Singapore, 638075","2012 IEEE Conference on Prognostics and Health Management","","2012","","","1","6","In this paper, a novel approach is proposed for sensor selection and placement in systems for the purpose of fault detection and isolation (FDI). This new approach benefits from the basic fact that faults are embedded in the analytical redundancy relations (ARRs) and that the occurrence of a fault will cause the corresponding ARRs to change. For FDI purposes, each ARR is connected to a set of sensors that represent the measurable variables. New concepts of fault associated sets and fault distinguishable sets are introduced to develop a low complexity dynamic programming algorithm to minimize the number of sensors needed and simultaneously to guarantee all possible faults being detectable and isolable. A case study of a fuel-cell system shows that the proposed method performs well when the numbers of faults and sensors are moderate.","","978-1-4673-0358-3978-1-4673-0356-9978-1-4673-0357","10.1109/ICPHM.2012.6299519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6299519","Sensor selection and placement;fault detection and isolation;analytical redundancy relations;dynamic programming","Circuit faults;Graphical models;Mathematical model;Vectors;Complexity theory;Dynamic programming;Optimization","dynamic programming;fault diagnosis;fuel cells;sensors","sensor selection;sensor placement;low complexity dynamic programming;fault detection and isolation;analytical redundancy relation;fault occurrence;FDI purpose;fuel-cell system","","3","","12","","","","","IEEE","IEEE Conferences"
"On the complexity of the convex liftings-based solution to inverse parametric convex programming problems","N. A. Nguyen; S. Olaru; P. Rodriguez-Ayerbe","Laboratory of Signals and Systems (L2S, UMR CNRS 8506), CentraleSupélec-CNRS-Université Paris Sud, 3 rue Joliot Curie, Plateau de Moulon, 91192, Gif-sur-Yvette, France; Laboratory of Signals and Systems (L2S, UMR CNRS 8506), CentraleSupélec-CNRS-Université Paris Sud, 3 rue Joliot Curie, Plateau de Moulon, 91192, Gif-sur-Yvette, France; Laboratory of Signals and Systems (L2S, UMR CNRS 8506), CentraleSupélec-CNRS-Université Paris Sud, 3 rue Joliot Curie, Plateau de Moulon, 91192, Gif-sur-Yvette, France","2015 European Control Conference (ECC)","","2015","","","3428","3433","The link between linear model predictive control (MPC) and parametric linear/quadratic programming has reached maturity in terms of the characterization of the structural properties and the numerical methods available for the effective resolution. The computational complexity is one of the current bottlenecks for these control design methods and inverse optimality has been recently shown to provide a new perspective for this challenge. However, the question of the minimal complexity of inverse optimality formulation is still open and much under discussion. In this paper we revisit some recent results by pointing out unnecessary geometrical complications which can be avoided by the interpretation of the optimality conditions. Two algorithms for fine-tuning inverse optimality formulation will be proposed and the results will be interpreted via two illustrative examples in comparison with existing formulations.","","978-3-9524-2693-7978-3-9524-2694","10.1109/ECC.2015.7331064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331064","","Programming;Computational complexity;Cost function;Redundancy;Europe","computational complexity;convex programming;linear programming;predictive control;quadratic programming","parametric linear programming;fine-tuning inverse optimality formulation;computational complexity;parametric quadratic programming;MPC;linear model predictive control;inverse parametric convex programming problem;convex lifting-based solution","","5","","12","","","","","IEEE","IEEE Conferences"
"Complexity measures in Genetic Programming learning: A brief review","N. Le; H. N. Xuan; A. Brabazon; T. P. Thi","Hanu IT Research and Development Center, Hanoi University; Hanu IT Research and Development Center, Hanoi University; University College Dublin, Ireland; Hanu IT Research and Development Center, Hanoi University","2016 IEEE Congress on Evolutionary Computation (CEC)","","2016","","","2409","2416","Model complexity of Genetic Programming (GP) as a learning machine is currently attracting considerable interest from the research community. Here we provide an up-to-date overview of the research concerning complexity measure techniques in GP learning. The scope of this review includes methods based on information theory techniques, such as the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC); plus those based on statistical machine learning theory on generalization error bound, namely, Vapnik-Chervonenkis (VC) theory; and some based on structural complexity. The research contributions from each of these are systematically summarized and compared, allowing us to clearly define existing research challenges, and to highlight promising new research directions. The findings of this review provides valuable insights into the current GP literature and is a good source for anyone who is interested in the research on model complexity and applying statistical learning theory to GP.","","978-1-5090-0623-6978-1-5090-0622-9978-1-5090-0624","10.1109/CEC.2016.7744087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744087","Genetic Programming;Complexity measure;Model Selection;Vaknik-Chervonenkis dimension;Statistical Machine Learning;Rademacher complexity","Complexity theory;Data models;Training;Statistical learning;Genetic programming;Training data;Bayes methods","genetic algorithms;learning (artificial intelligence);statistical analysis","genetic programming learning;brief review;complexity model;machine learning;research community;GP learning;information theory techniques;Akaike information criterion;AIC;Bayesian information criterion;BIC;statistical machine learning theory;Vapnik-Chervonenkis theory;VC;structural complexity;statistical learning theory","","4","","63","","","","","IEEE","IEEE Conferences"
"Complexity comparison of integer programming and genetic algorithms for resource constrained scheduling problems","R. Čorić; M. Đumić; D. Jakobović","Department of Mathematics, University of Osijek, Croatia; Department of Mathematics, University of Osijek, Croatia; Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2017","","","1182","1188","Resource constrained project scheduling problem (RCPSP) is one of the most intractable combinatorial optimization problems. RCPSP belongs to the class of NP hard problems. Integer Programming (IP) is one of the exact solving methods that can be used for solving RCPSP. IP formulation uses binary decision variables for generating a feasible solution and with different boundaries eliminates some of solutions to reduce the solution space size. All exact methods, including IP, search through entire solution space so they are impractical for very large problem instances. Due to the fact that exact methods are not applicable to all problem instances, many heuristic approaches are developed, such as genetic algorithms. In this paper we compare the time complexity of IP formulations and genetic algorithms when solving the RCPSP. We present two different solution representations for genetic algorithms, permutation vector and vector of floating point numbers. Two formulations of IP and and their time and convergence results are compared for the aforementioned approaches.","","978-953-233-090-8978-953-233-092-2978-1-5090-4969","10.23919/MIPRO.2017.7973603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973603","","Genetic algorithms;IP networks;Sociology;Statistics;Schedules;Linear programming;Silicon","combinatorial mathematics;computational complexity;genetic algorithms;integer programming;scheduling;vectors","integer programming;genetic algorithms;resource constrained project scheduling problem;RCPSP;intractable combinatorial optimization problems;NP hard problems;exact solving methods;IP formulation;binary decision variables;heuristic approaches;time complexity;permutation vector;floating point number vector","","","","23","","","","","IEEE","IEEE Conferences"
"Sparse Semidefinite Programs with Near-Linear Time Complexity","R. Y. Zhang; J. Lavaei","Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, 94720, USA; Department of Industrial Engineering and Operations Research, University of California, Berkeley, CA, 94720, USA","2018 IEEE Conference on Decision and Control (CDC)","","2018","","","1624","1631","Some of the strongest polynomial-time relaxations to NP-hard combinatorial optimization problems are semidefinite programs (SDPs), but their solution complexity of up to O(n6.5 L) time and O(n4) memory for L accurate digits limits their use in all but the smallest problems. Given that combinatorial SDP relaxations are often sparse, a technique known as chordal conversion can sometimes reduce complexity substantially. In this paper, we describe a modification of chordal conversion that allows any general-purpose interior-point method to solve a certain class of sparse SDPs with a guaranteed complexity of O(nl.5L) time and O(n) memory. To illustrate the use of this technique, we solve the MAX k- CUT relaxation and the Lovasz Theta problem on power system models with up to n = 13659 nodes in 5 minutes, using SeDuMi v1.32 on a 1.7 GHz CPU with 16 GB of RAM. The empirical time complexity for attaining L decimal digits of accuracy is ≈ 0.001nl.l L seconds.","2576-2370;0743-1546","978-1-5386-1395-5978-1-5386-1394-8978-1-5386-1396","10.1109/CDC.2018.8619478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8619478","","Symmetric matrices;Sparse matrices;Time complexity;Power systems;Standards;Optimization","combinatorial mathematics;computational complexity;mathematical programming;polynomials;sparse matrices","sparse semidefinite programs;near-linear time complexity;NP-hard combinatorial optimization problems;combinatorial SDP relaxations;chordal conversion;general-purpose interior-point method;Lovasz Theta problem;empirical time complexity;MAX k-CUT relaxation;sparse SDP;polynomial-time relaxations;time 5.0 min;frequency 1.7 GHz;memory size 16.0 GByte","","","","39","","","","","IEEE","IEEE Conferences"
"The Program-Enumeration Bottleneck in Average-Case Complexity Theory","L. Trevisan","NA","2010 IEEE 25th Annual Conference on Computational Complexity","","2010","","","88","95","Three fundamental results of Levin involve algorithms or reductions whose running time is exponential in the length of certain programs. We study the question of whether such dependency can be made polynomial. 1) Levin's ""optimal search algorithm"" performs at most a constant factor more slowly than any other fixed algorithm. The constant, however, is exponential in the length of the competing algorithm. We note that the running time of a universal search cannot be made ""fully polynomial"" (that is, the relation between slowdown and program length cannot be made polynomial), unless P=NP. 2) Levin's ""universal one-way function"" result has the following structure: there is a polynomial time computable function f<sub>Levin</sub> such that if there is a polynomial time computable adversary A that inverts f<sub>Levin</sub> on an inverse polynomial fraction of inputs, then for every polynomial time computable function g there also is a polynomial time adversary A<sub>g</sub> that inverts g on an inverse polynomial fraction of inputs. Unfortunately, again the running time of A<sub>g</sub> depends exponentially on the bit length of the program that computes g in polynomial time. We show that a fully polynomial uniform reduction from an arbitrary one-way function to a specific one-way function is not possible relative to an oracle that we construct, and so no ""universal one-way function"" can have a fully polynomial security analysis via relativizing techniques. 3) Levin's completeness result for distributional NP problems implies that if a specific problem in NP is easy on average under the uniform distribution, then every language L in NP is also easy on average under any polynomial time computable distribution. The running time of the implied algorithm for L, however, depends exponentially on the bit length of the non-deterministic polynomial time Turing machine that decides L. We show that if a completeness result for distributional NP can be proved via a ""fully uniform"" and ""fully polynomial"" time reduction, then there is a worst-case to average-case reduction for NP-complete problems. In particular, this means that a fully polynomial completeness result for distributional NP is impossible, even via randomized truth-table reductions, unless the polynomial hierarchy collapses.","1093-0159","978-1-4244-7215-4978-1-4244-7214","10.1109/CCC.2010.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5497896","Average-case Complexity;Universal Search","Complexity theory;Polynomials;Computational complexity;Distributed computing;Security;Turing machines;NP-complete problem;Search problems","computational complexity;Turing machines","average-case complexity theory;program enumeration;Levin optimal search algorithm;inverse polynomial fraction;polynomial time computable function;universal one-way function;distributional NP hard problems;nondeterministic polynomial time Turing machine;randomized truth-table reductions","","","","14","","","","","IEEE","IEEE Conferences"
"Reduced complexity dynamic programming solution for Kalman filtering of linear discrete time descriptor systems","A. Al-Matouq; T. Vincent; L. Tenorio","Colorado School of Mines, USA; Department of Electrical Engineering and Computer Science, Colorado School of Mines 1600 Illinois St., Golden, 80401, USA; Department of Applied Mathematics and Statistics, Colorado School of Mines 1600 Illinois St., Golden, 80401, USA","2013 American Control Conference","","2013","","","340","345","We consider linear discrete time descriptor systems that are described by state and measurement equations that have both stochastic and purely deterministic components. We suggest an estimation algorithm that operates by decomposing the system into stochastic and deterministic parts, and processing each part separately. It solves the deterministic subsystem using the pseudo-inverse according to the Moore-Penrose definition [1], and then minimizes the Kalman filter objective function by exploiting the orthogonal subspace defined by the deterministic subsystem. A simulation example is given for estimating tray composition for a distillation column by linearization over a trajectory of a non-linear differential algebraic model. Compared to the method of R. Nikoukhah et.al [2], the reduction in time produced by our method for this example is 87%. The reason is that our algorithm requires only 1-block matrix inversion that does not involve any singular blocks, whereas the algorithm in [2] requires 3-block matrix inversions containing possibly singular matrix blocks arising from singular covariance matrices.","0743-1619;2378-5861","978-1-4799-0178-4978-1-4799-0177-7978-1-4799-0175","10.1109/ACC.2013.6579860","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579860","","Mathematical model;Equations;Kalman filters;Estimation;Vectors;Stochastic processes;Temperature measurement","computational complexity;covariance matrices;differential algebraic equations;discrete time systems;distillation equipment;dynamic programming;Kalman filters;linear systems;linearisation techniques;minimisation;nonlinear differential equations;state estimation;stochastic processes","reduced complexity dynamic programming solution;Kalman filtering;linear discrete time descriptor system;state equation;measurement equation;stochastic components;purely deterministic components;estimation algorithm;system decomposition;deterministic subsystem;Moore-Penrose definition;Kalman filter objective function minimization;orthogonal subspace;tray composition;distillation column;linearization;nonlinear differential algebraic model;1-block matrix inversion;3-block matrix inversion;singular matrix blocks;singular covariance matrix","","1","","25","","","","","IEEE","IEEE Conferences"
"Managing the complexity of a telecommunication power systems equipment replacement program","E. H. Lim; F. Bodi","Mission Critical Assets, Silcar Pty Ltd, Melbourne, Australia; Mission Critical Assets, Silcar Pty Ltd, Melbourne, Australia","Intelec 2012","","2012","","","1","9","Telecommunication operators must manage many types and models of power equipment from many manufacturers each with different characteristics. With Telco's globally reporting strong pressure on CAPEX reduction, it is important to develop capability not only of predicting end-of-life of equipment, but also of managing the vast number of types, makes and models used in a variety of environments. Delayed replacement results in higher service disruption risk while early replacement places considerable pressure on CAPEX budgets. We have developed an Equipment Reliability Analysis (ERA) program for telecommunication power equipment life cycle replacement. Rectifiers, batteries and UPS life data are grouped by features including technology, manufacturer make/model, life-data analysis from field failure data, and equipment tear-down analysis for life cycle replacement decisions. Replacement decisions are based on a number of inputs, including field experience, type of application, defect data and life data analysis.","2158-5210;0275-0473;2158-5210","978-1-4673-1000-0978-1-4673-0999-8978-1-4673-0998","10.1109/INTLEC.2012.6374535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6374535","equipment reliability analysis;life cycle replacement","Batteries;Power system reliability;Telecommunication network reliability;Rectifiers","electrical maintenance;power apparatus;rectifiers;secondary cells;telecommunication power supplies;uninterruptible power supplies","telecommunication power systems equipment replacement program;telecommunication operators;CAPEX;equipment end-of-life;equipment reliability analysis program;ERA;telecommunication power equipment life cycle replacement;rectifiers;batteries;uninterruptible power supplies;UP;field failure data;equipment tear-down analysis;life cycle replacement decisions;life data analysis","","2","","11","","","","","IEEE","IEEE Conferences"
"Difference constraints: an adequate abstraction for complexity analysis of imperative programs","M. Sinn; F. Zuleger; H. Veith","TU Wien, Austria; TU Wien, Austria; TU Wien, Austria","2015 Formal Methods in Computer-Aided Design (FMCAD)","","2015","","","144","151","Difference constraints have been used for termination analysis in the literature, where they denote relational inequalities of the form x' ≤ y + c, and describe that the value of x in the current state is at most the value of y in the previous state plus some constant c ∈ ℤ. In this paper, we argue that the complexity of imperative programs typically arises from counter increments and resets, which can be modeled naturally by difference constraints.We present the first practical algorithm for the analysis of difference constraint programs and describe how C programs can be abstracted to difference constraint programs. Our approach contributes to the field of automated complexity and (resource) bound analysis by enabling automated amortized complexity analysis for a new class of programs and providing a conceptually simple program model that relates invariant- and bound analysis.We demonstrate the effectiveness of our approach through a thorough experimental comparison on real world C code: our tool Loopus computes the complexity for considerably more functions in less time than related tools from the literature.","","978-0-9835-6785-1978-1-5090-4151","10.1109/FMCAD.2015.7542264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542264","","Algorithm design and analysis;Complexity theory;Analytical models;Radiation detectors;Standards;Cognition;Upper bound","C language;program diagnostics","difference constraints;imperative programs;program complexity analysis;termination analysis;counter increments;resets;C programs;amortized complexity analysis","","6","","21","","","","","IEEE","IEEE Conferences"
"Complexity simulation of DMC based on quadratic programming","T. Zou; H. Li; X. Zhang; D. Zhao","Zhejiang University of Technology, College of Information Engineering, Hangzhou 310023, China; Zhejiang University of Technology, College of Information Engineering, Hangzhou 310023, China; Shanghai University, School of Mechatronics Engineering and Automation, 200444, China; China University of Petroleum, College of Mechanical and Electronic Engineering, Shanghai 257061, China","Proceedings of the 30th Chinese Control Conference","","2011","","","3335","3339","Constrained dynamic matrix control(DMC)is essentially a standard quadratic programming problem with high complexity and long on-line solving time. The Karush-Kuhn-Tucker (KKT) conditions for optimization problems are used to analyze the complexity of DMC algorithm. Therefore, the number of manipulated variables and the length of control horizons are found out to be the mainly restricted two factors of computational efficiency in algorithm, and the time complexity of the algorithm is proportional to the cube of the product of the two factors. Then standard quadratic programming (QP) algorithm was applied to three classical industrial cases which simulated and verified the result. Finally, a curve fitting method was used to compute the maximum size of control system in standard model predictive control implementation time. Thus a theoretical basis was provided for properly choosing the number of manipulated variables and the length of control horizons, reducing the computational complexity of the dynamic matrix control algorithm.","2161-2927;1934-1768;1934-1768","978-988-17255-9-2978-1-4577-0677","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6000466","Quadratic programming;Model predictive control;Dynamic matrix control;Computational complexity","Predictive control;Predictive models;Mathematical model;Quadratic programming;Electronic mail;Complexity theory;Prediction algorithms","computational complexity;control system synthesis;curve fitting;predictive control;quadratic programming","complexity simulation;DMC;quadratic programming;constrained dynamic matrix control;Karush-Kuhn-Tucker conditions;curve fitting method;model predictive control;computational complexity","","","","14","","","","","IEEE","IEEE Conferences"
"Framing core and advanced competencies for undergraduate information systems program courses: Does the nature, level, complexity and audience of a course matter?","I. Baumgartner; V. Shankararaman","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore","Proceedings of IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE) 2012","","2012","","","H2A-12","H2A-17","This contribution reports on one of the cycles of an on-going, multi-cycle, multi-year effort to refine a learning outcomes framework designed and implemented for the Bachelor of Science (Information Systems Management) degree program offered by the School of Information Systems (SIS) at Singapore Management University (SMU). To further improve the within-course alignment and to further refine cross-course alignment for all courses offered within this program a set of competencies for each course of the program was derived and integrated into a program-wide yet course-level competencies framework. Subsequently, the competencies framework was integrated with Learning Outcomes Framework defined at the program level. Numerous challenges were encountered and several important insights were gained while extracting and framing competencies for all core and elective courses of the program. This paper describes the competencies extraction and definition process particularly focusing on the impact which the nature, level, complexity and the audience of an undergraduate course have on this process. In addition, this paper highlights the benefits which the implementation of the competencies framework brings to the School, students, teaching staff and potential employers.","","978-1-4673-2418-2978-1-4673-2417-5978-1-4673-2416","10.1109/TALE.2012.6360331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6360331","learning outcomes;learning outcomes framework;competencies;competencies framework;information systems management program;core competencies;advanced competencies;prerequisite competencies","Information systems;Educational institutions;Complexity theory;Conferences;Java;Business","educational courses;information systems","undergraduate information systems program courses;Bachelor of Science Information Systems Management;School of Information Systems;Singapore Management University;cross-course alignment;learning outcomes framework","","","","14","","","","","IEEE","IEEE Conferences"
"Assessing your algorithm: A program complexity metrics for basic algorithmic thinking education","M. Kayam; M. Fuwa; H. Kunimune; M. Hashimoto; D. K. Asano","Shinshu University, Nagano, Japan; Graduate School of Shinshu University, Nagano, Japan; Shinshu University, Nagano, Japan; Shinshu University, Nagano, Japan; Shinshu University, Nagano, Japan","2016 11th International Conference on Computer Science & Education (ICCSE)","","2016","","","309","313","We have explored educational methods for algorithmic thinking conceptual modeling for novices and implemented a block programming editor and a simple learning management system. This system has been used in our algorithmic thinking course since 2008. Based on this experience, in this paper, we propose a program/algorithm complexity metric specified for novice learners. This metric is based on the variable usage in arithmetic and relational formulas in learner's algorithms. Moreover, to evaluate the applicability of this metric for novice education, we discuss the differences between three previous program complexity metrics and our proposed metric.","","978-1-5090-2218-2978-1-5090-2217-5978-1-5090-2219","10.1109/ICCSE.2016.7581599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581599","algorithmic thinking;program complexity metric;novice education;variable usage;educational assessment","Measurement;Complexity theory;Education;Vocabulary;Programming profession;Algorithm design and analysis","arithmetic;educational computing;learning management systems;software metrics;text editing","program complexity metrics;basic algorithmic thinking education;educational methods;algorithmic thinking conceptual modeling;block programming editor;learning management system;algorithmic thinking course;algorithm complexity metric;arithmetic variable usage;relational formulas;novice education","","","","29","","","","","IEEE","IEEE Conferences"
"Concealing the complexity of node programming in wireless sensor networks","S. Bader; B. Oelmann","Department of Information Technology and Media, Mid Sweden University, Holmgatan 10, 85170 Sundsvall, Sweden; Department of Information Technology and Media, Mid Sweden University, Holmgatan 10, 85170 Sundsvall, Sweden","2013 IEEE Eighth International Conference on Intelligent Sensors, Sensor Networks and Information Processing","","2013","","","177","182","There is a significant potential for Wireless sensor networks to be used as a general distributed measurement and monitoring system. The integration of computation, communication and sensing enables smart sensors to be built that can be adapted to a plethora of application requirements and allow for automated data collection throughout the network. However, the potential end users of this systems are domain experts, who usually do not possess the technical expertise to program, and thus operate, wireless sensor nodes, which prohibits the technology from becoming off-the-shelf equipment. In this paper, we present a method which enables the complexity of programming sensor nodes to be concealed in order to allow domain experts to use wireless sensor networks in basic applications without the requirement of technical assistance. We propose to use a computer-based specification entry, which generates a configuration parameter set to adjust the sensor node's application behavior. The method has been implemented in a proof-of-concept system and evaluated with test subjects who possess limited programming skills. The results show that users without any prior programming knowledge, or experience with embedded systems, are capable of configuring a sensor node according to a given application scenario within minutes.","","978-1-4673-5501-8978-1-4673-5499-8978-1-4673-5500","10.1109/ISSNIP.2013.6529785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6529785","","Wireless sensor networks;Programming;Computer languages;Monitoring;Hardware;Sensors;Universal Serial Bus","embedded systems;intelligent sensors;wireless sensor networks","node programming;wireless sensor networks;distributed measurement;monitoring system;smart sensors;automated data collection;wireless sensor nodes;programming sensor nodes;embedded systems","","","","19","","","","","IEEE","IEEE Conferences"
"A low-computation-complexity, energy-efficient, and high-performance linear program solver using memristor crossbars","R. Cai; A. Ren; Y. Wang; S. Soundarajan; Q. Qiu; B. Yuan; P. Bogdan","Electrical Engineering and Computer Science, Syracuse University, NY, USA; Electrical Engineering and Computer Science, Syracuse University, NY, USA; Electrical Engineering and Computer Science, Syracuse University, NY, USA; Electrical Engineering and Computer Science, Syracuse University, NY, USA; Electrical Engineering and Computer Science, Syracuse University, NY, USA; Electrical Engineering, City University of New York, USA; Electrical Engineering, University of Southern California, Los Angeles, USA","2016 29th IEEE International System-on-Chip Conference (SOCC)","","2016","","","317","322","Linear programming is required in a wide variety of application including routing, scheduling, and various optimization problems. The primal-dual interior point (PDIP) method is state-of-the-art algorithm for solving linear programs, and can be decomposed to matrix-vector multiplication and solving systems of linear equations, both of which can be conducted by the emerging memristor crossbar technique in O(1) time complexity in the analog domain. This work is the first to apply memristor crossbar for linear program solving based on the PDIP method, which has been reformulated for memristor crossbars to compute in the analog domain. The proposed linear program solver can overcome limitations of memristor crossbars such as supporting only non-negative coefficients, and has been extended for higher scalability. The proposed solver is iterative and achieves O(N) computation complexity in each iteration. Experimental results demonstrate that reliable performance with high accuracy can be achieved under process variations.","2164-1706","978-1-5090-1367-8978-1-5090-1366-1978-1-5090-1368","10.1109/SOCC.2016.7905500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905500","","Memristors;Mathematical model;Linear systems;Algorithm design and analysis;Handheld computers;Complexity theory;Scalability","analogue circuits;circuit optimisation;computational complexity;energy conservation;linear programming;matrix decomposition;matrix multiplication;memristor circuits","low-computation-complexity;high-performance linear program solver;energy-efficiency;optimization problems;primal-dual interior point method;PDIP method;matrix-vector multiplication decomposition;linear equations;O(1) time complexity;memristor crossbar technique;analog domain;nonnegative coefficients","","2","","22","","","","","IEEE","IEEE Conferences"
"Dynamic Complexity of the Temporal Transcriptional Regulation Program in Human Endotoxemia","T. T. Nguyen; P. T. Foteinou; I. P. Androulakis; S. E. Calvano; S. F. Lowry","NA; NA; NA; NA; NA","2010 IEEE International Conference on BioInformatics and BioEngineering","","2010","","","112","117","Human endotoxemia is a well-accepted surrogate model for studying the acute inflammatory responses. In order to discover the complex underlying dynamics, identifying biologically relevant transcriptional regulators as well as their putative regulatory interactions with target genes is an essential step. However, prediction of relevant transcriptional regulators in higher eukaryotes remains a challenge both in silico and in vivo. In this study, we analyzed gene expression data from human blood leukocytes to extract four significant patterns of highly coexpressed genes that capture the essence of inflammatory phases. Upon identification of these patterns, a number of inflammation-specific pathways are selected by evaluating the enrichment of the corresponding subsets. Subsequently, statistically significant cis-regulatory modules (CRMs) are selected and decomposed into a list of relevant transcription factors (34 TFs) which are further validated from prior experiments and computational studies in literature. Additionally, our analysis also allows for the construction of a putative dynamic representation of the transcriptional regulatory program, making it become a critical enabler for unraveling regulatory interactions which is an essential step towards a quantification of dynamic transcriptional regulatory networks.","","978-1-4244-7495-0978-1-4244-7494-3978-0-7695-4083","10.1109/BIBE.2010.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521703","transcription factors;cis-regulatory modules;alternative promoters;gene expression clustering;human endotoxemia;human acute inflammtion","Humans;Regulators;Gene expression;Biomedical engineering;Pattern analysis;Surgery;Pathogens;Bioinformatics;Computational biology;USA Councils","blood;cellular biophysics;genetics","dynamic complexity;temporal transcriptional regulation program;human endotoxemia;acute inflammatory response;eukaryote;gene expression;human blood leukocyte;cis-regulatory module;transcription factor","","","","44","","","","","IEEE","IEEE Conferences"
"Program complexity metrics and programmer opinions","B. Katzmarski; R. Koschke","Fachbereich Mathematik und Informatik, University of Bremen, Bremen, Germany; Fachbereich Mathematik und Informatik, University of Bremen, Bremen, Germany","2012 20th IEEE International Conference on Program Comprehension (ICPC)","","2012","","","17","26","Various program complexity measures have been proposed to assess maintainability. Only relatively few empirical studies have been conducted to back up these assessments through empirical evidence. Researchers have mostly conducted controlled experiments or correlated metrics with indirect maintainability indicators such as defects or change frequency. This paper uses a different approach. We investigate whether metrics agree with complexity as perceived by programmers. We show that, first, programmers' opinions are quite similar and, second, only few metrics and in only few cases reproduce complexity rankings similar to human raters. Data-flow metrics seem to better match the viewpoint of programmers than control-flow metrics, but even they are only loosely correlated. Moreover we show that a foolish metric has similar or sometimes even better correlation than other evaluated metrics, which raises the question how meaningful the other metrics really are. In addition to these results, we introduce an approach and associated statistical measures for such multi-rater investigations. Our approach can be used as a model for similar studies.","1092-8138","978-1-4673-1216-5978-1-4673-1213-4978-1-4673-1215","10.1109/ICPC.2012.6240486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240486","control-flow metrics;data-flow metrics;program complexity","Complexity theory;Measurement;Instruments;Correlation;Programming;Strontium;Semantics","computational complexity;software maintenance;software metrics","program complexity metrics;programmer opinions;program complexity measurement;maintainability assess;empirical evidence;controlled experiments;correlated metrics;indirect maintainability indicators;human raters;complexity rankings;data flow metrics","","10","","58","","","","","IEEE","IEEE Conferences"
"Tikhonov Regularization as a Complexity Measure in Multiobjective Genetic Programming","J. Ni; P. Rockett","Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, U.K.; Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, U.K.","IEEE Transactions on Evolutionary Computation","","2015","19","2","157","166","In this paper, we propose the use of Tikhonov regularization in conjunction with node count as a general complexity measure in multiobjective genetic programming. We demonstrate that employing this general complexity yields mean squared test error measures over a range of regression problems, which are typically superior to those from conventional node count (but never statistically worse). We also analyze the reason that our new method outperforms the conventional complexity measure and conclude that it forms a decision mechanism that balances both syntactic and semantic information.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2014.2306994","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6746085","Complexity measure;genetic programming;Pareto dominance;Tikhonov regularization","Complexity theory;Vectors;Data models;Semantics;Syntactics;Training;Sociology","computational complexity;genetic algorithms;mean square error methods;regression analysis","Tikhonov regularization;node count;general complexity measure;multiobjective genetic programming;mean squared test error measures;regression problems;conventional complexity measure;decision mechanism;syntactic information;semantic information","","8","","26","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Parallelizable Numerical Algorithm for Sparse Semidefinite Programming","R. Madani; A. Kalbat; J. Lavaei","Department of Electrical Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Electrical Engineering, United Arab Emirates University, Abu Dhabi, UAE; Department of Industrial Engineering and Operations Research, University of California, Berkeley, Berkeley, CA, USA","IEEE Transactions on Control of Network Systems","","2018","5","4","1898","1909","In the past two decades, the semidefinite programming (SDP) technique has been proven to be extremely successful in the convexification of hard optimization problems appearing in graph theory, control theory, polynomial optimization theory, and many areas in engineering. In particular, major power optimization problems, such as optimal power flow, state estimation, and unit commitment, can be formulated or well approximated as SDPs. However, the inability to efficiently solve large-scale SDPs is an impediment to the deployment of such formulations in practice. Motivated by the significant role of SDPs in revolutionizing the decision-making process for real-world systems, this paper designs a low-complexity numerical algorithm for solving sparse SDPs, using the alternating direction method of multipliers and the notion of tree decomposition in graph theory. The iterations of the designed algorithm are highly parallelizable and enjoy closed-form solutions, whose most expensive computation amounts to eigenvalue decompositions over certain submatrices of the SDP matrix. The proposed algorithm is a general-purpose parallelizable SDP solver for sparse SDPs, and its performance is demonstrated on the SDP relaxation of the optimal power flow problem for real-world benchmark systems with more than 13 600 nodes.","2325-5870;2372-2533","","10.1109/TCNS.2017.2774008","ONR YIP; DARPA Young Faculty; AFOSR YIP; NSF CAREER; NSF EECS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8110723","Large-scale optimization;numerical algorithms;power systems;semidefinite programming;tree decomposition","Optimization;Power systems;Convex functions;Algorithm design and analysis;Programming;Matrix decomposition;Convergence","approximation theory;computational complexity;convex programming;load flow;mathematical programming;matrix algebra;power generation dispatch;power system state estimation;trees (mathematics)","eigenvalue decompositions;SDP matrix;general-purpose parallelizable SDP solver;SDP relaxation;optimal power flow problem;real-world benchmark systems;low-complexity parallelizable numerical algorithm;convexification;hard optimization problems;graph theory;control theory;polynomial optimization theory;major power optimization problems;state estimation;unit commitment;decision-making process;real-world systems;tree decomposition;closed-form solutions;sparse SDP;sparse semidefinite programming technique;large-scale SDP;alternating direction method-of-multipliers","","","","61","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity detection for large MIMO systems using partial ML detection and genetic programming","P. Svac; F. Meyer; E. Riegler; F. Hlawatsch","Institute of Telecommunications, Vienna University of Technology, Austria; Institute of Telecommunications, Vienna University of Technology, Austria; Institute of Telecommunications, Vienna University of Technology, Austria; Institute of Telecommunications, Vienna University of Technology, Austria","2012 IEEE 13th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2012","","","585","589","We propose a low-complexity detector for multiple-input multiple-output (MIMO) systems using BPSK or QAM constellations. The detector operates at the bit level and is especially advantageous for large MIMO systems. It consists of three stages performing partial ML detection, generation of soft values, and soft-input genetic optimization. For the last stage, we present a genetic programming algorithm that uses the soft values computed by the second stage. Simulation results demonstrate that for large systems, our detector can outperform state-of-the-art methods, and its complexity scales roughly cubically with the system dimension.","1948-3252;1948-3244;1948-3244","978-1-4673-0971-4978-1-4673-0970-7978-1-4673-0969","10.1109/SPAWC.2012.6292977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292977","","Detectors;MIMO;Cascading style sheets;Complexity theory;Vectors;Genetic algorithms;Signal to noise ratio","genetic algorithms;maximum likelihood detection;MIMO communication;phase shift keying;quadrature amplitude modulation","low-complexity detection;MIMO system;partial ML detection;genetic programming;multiple-input multiple-output system;BPSK;QAM constellation;soft values generation;soft-input genetic optimization","","4","","26","","","","","IEEE","IEEE Conferences"
"Complexity reduced explicit model predictive control by solving approximated mp-QP program","Y. Chen; S. Li; N. Li","Department of Automation, Shanghai Jiao Tong University, Key Laboratory of System, Control and Information Processing, Ministry of Education, Shanghai, 200240, P.R. China; Department of Automation, Shanghai Jiao Tong University, Key Laboratory of System, Control and Information Processing, Ministry of Education, Shanghai, 200240, P.R. China; Department of Automation, Shanghai Jiao Tong University, Key Laboratory of System, Control and Information Processing, Ministry of Education, Shanghai, 200240, P.R. China","2015 10th Asian Control Conference (ASCC)","","2015","","","1","6","In this paper, two methods to reduce the complexity of multi-parametric programming model predictive control are proposed. We show that the standard multi-parametric programming problem can be modified by approximating the quadratic programming constraints. For a certain control sequence, only constraints on the first element is considered, while constraints on future elements are ignored or approximated to a simple saturation function. Both the number of critical regions and the computation time are proven to be reduced. Geometric interpretations is given and complexity analysis is conducted. The result is tested on an illustrating example to show its effectiveness.","","978-1-4799-7862","10.1109/ASCC.2015.7244434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244434","","Complexity theory;Aerospace electronics;Optimization;Approximation methods;Space exploration;Optimal control;Partitioning algorithms","approximation theory;predictive control;quadratic programming","complexity reduced explicit model predictive control;approximated mp-QP program;multiparametric programming model predictive control;quadratic programming constraints;geometric interpretations;complexity analysis","","","","13","","","","","IEEE","IEEE Conferences"
"Variant design process programming of assembly based on dimension constraint complexity","X. Xu; Dan Li","Institute of Industrial Engineering, China Jiliang University, Hangzhou, Zhejiang Province, China; Institute of Industrial Engineering, China Jiliang University, Hangzhou, Zhejiang Province, China","2010 8th World Congress on Intelligent Control and Automation","","2010","","","5145","5149","The implementation process of assembly variant design involves complicated task planning. A reasonable part variant design sequence is imperative for successfully executing assembly variant design which characterizes a dynamic uncertainty reducing process. This paper adopts dimension constraint complexity of part to find the most appropriate sequence of part variant design. Firstly, a dimension constraint network among parts was constructed at dimension level. Then, the dimension constraint complexity of part in dimension constraint network was presented and used as an evidence from which the task complexity of executing part variant design can be measured and further converted to the variant design sequence of part. Finally, a case study demonstrates that dimension constraint complexity can effectively represent the task complexity of part variant design, and leverage part variant design activities allocation at the entire assembly product.","","978-1-4244-6712-9978-1-4244-6711","10.1109/WCICA.2010.5554926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5554926","Variant design automation;Process programming;Dimension constraint network;Complexity","Assembly;Complexity theory;Programming;Uncertainty;Design automation;Planning;Product design","design engineering;product customisation","variant design process programming;dimension constraint complexity;variant design sequence;assembly product","","","","10","","","","","IEEE","IEEE Conferences"
"A Token-based Illicit Copy Detection Method Using Complexity for a Program Exercise","M. Iwamoto; S. Oshima; T. Nakashima","NA; NA; NA","2013 Eighth International Conference on Broadband and Wireless Computing, Communication and Applications","","2013","","","575","580","The conducts to copy using other person's source codes and submit as reports are regarded as a problem for program exercises of programming subjects in universities or colleges. An automatic detection algorithm to detect illicit copies is required in these educational organizations. In previous researches, these methods based on the detection standard of the token length have been proposed. These methods use the threshold simply using the character length. In these cases, miss detections occur in the case of the simple program such as the sequence of the print statement or the case that token sequences appear in the middle of a statement. This paper proposes the detection method using the program complexity and the complete token sequence. As the results of experiments, our method can improve the recall R adopting the complexity as the detection standard and the precision P adopting the complete token sequence for exercise programs submitted by students.","","978-0-7695-5093","10.1109/BWCCA.2013.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690950","copy detection;code clone;programming exercise;token-based;complexity","Cloning;Complexity theory;Educational institutions;Syntactics;Feature extraction;Measurement;Electronic mail","copy protection;educational institutions;programming;software metrics;source code (software)","token-based illicit copy detection method;program exercise complexity;source codes;programming subjects;universities;colleges;automatic illicit copy detection algorithm;educational organizations;token sequence;recall;precision","","","","11","","","","","IEEE","IEEE Conferences"
"Cognitive complexity: A model for distributing equivalent programming problems","S. K. Dey; S. S. M. Tariq; M. S. Islam; G. M. M. Bashir","Dept. of CSE, Military Institute of Science and Technology, Dhaka, Bangladesh; BJIT Ltd., Dhaka, Bangladesh; Dept. of CSE, Dhaka University of Engineering and Technology, Dhaka, Bangladesh; Dept. of CSE, Patuakhali Science and Technology University, Patuakhali, Bangladesh","2017 International Conference on Electrical, Computer and Communication Engineering (ECCE)","","2017","","","831","837","Traditional distribution of programming problems seems to create dissatisfaction among students. To overcome this problem, complexity measurement of programming problems is necessary. Although software is the outcome of human ingenious activity, cognitive informatics plays a significant role in understanding its ultimate characteristics. In this article, we have proposed a software complexity measurement algorithm based on cognitive weight of basic control structure that shrink the limitations of existing measures. Cognitive weight concept of any basic control structure are morally based on the rational capacity of Human Brain. According to cognitive data, we have developed a new software tool using Java Standard Edition and MySQL to measure the cognitive complexity by following our developed algorithm. This software is structured and developed based on the outcome of our research data which is capable of determining the complexity value of several programming languages. This will assist the instructors allocating the problems among the learners by preserving the equivalent level of difficulty. Therefore, the developed complexity measurement tool will ensure the students to get problems with identical difficulty level for assessment.","","978-1-5090-5627-9978-1-5090-5628","10.1109/ECACE.2017.7913018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913018","cognitive weight;complexity measurement;equivalent distribution;programming problems;java SE;MySQL database;problems bank","Complexity theory;Software;Programming;Software measurement;Weight measurement;Cognitive informatics;Software algorithms","Java;software metrics;SQL","programming problem complexity measurement tool;programming problems;human ingenious activity;cognitive informatics;software complexity measurement algorithm;human brain rational capacity;cognitive data;software tool;Java standard edition;MySQL;cognitive complexity","","1","","21","","","","","IEEE","IEEE Conferences"
"A survey on metric of software complexity","S. Yu; S. Zhou","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","2010 2nd IEEE International Conference on Information Management and Engineering","","2010","","","352","356","With the evolution of the software development, the scale of the software is increasingly growing to the extent that we cannot hand it easily. Some metrics are proposed to measure the complexity of software in last a few years. This article aims at a comprehensive survey of the metric of software complexity. Some classic and efficient software complexity metrics, such as Lines of Codes (LOC), Halstead Complexity Metric (HCM) and Cyclomatic Complexity Metric (CCM), are discussed and analyzed first. Then, some other approaches driven from above classic metrics are also discussed. The comparison and the relationship of these metrics of software complexity are also presented.","","978-1-4244-5263-7978-1-4244-5265","10.1109/ICIME.2010.5477581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477581","complexity measure;software complexity;software measurement;project managementt","Software measurement;Costs;Software metrics;Computer science;Lab-on-a-chip;Project management;Application software;Programming profession;Usability;Stability","software metrics","software complexity metric;software development;lines of codes;Halstead complexity metric;cyclomatic complexity metric","","11","","22","","","","","IEEE","IEEE Conferences"
"Managing Software Complexity and Variability in Coupled Climate Models","S. Rugaber; R. Dunlap; l. mark; S. Ansari","Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology","IEEE Software","","2011","28","6","43","48","Coupled climate models exhibit scientific, numerical, and architectural variability. This variability introduces requirements that give rise to complexity. However, techniques exist that can tame this complexity; one such technique is feature analysis. As climate model fidelity and complexity increase, the climate-modeling community should adopt a systematic way to deal with software variability.","0740-7459;1937-4194","","10.1109/MS.2011.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999646","climate modeling;earth and atmospheric sciences;automatic programming;domain engineering;reusable software;software engineering","Meteorology;Atmospheric modeling;Global warming;Software development;Atmospheric measurements;Analytical models;Data models","computational complexity;software engineering","software complexity;software variability;coupled climate models;architectural variability;numerical variability;scientific variability;feature analysis;climate modeling community","","1","","13","","","","","IEEE","IEEE Journals & Magazines"
"Software complexity measurement of water poverty mapping application with function point method","T. Wahyono; B. Soewito; S. W. H. L. Hendric; F. L. Gaol","Faculty of IT, Satya Wacana Christian University, Salatiga, Indonesia; Binus Graduate Program, Bina Nusantara University, Jakarta, Indonesia; Doctor of Computer Science Program, Bina Nusantara University, Jakarta, Indonesia; Doctor of Computer Science Program, Bina Nusantara University, Jakarta, Indonesia","2017 International Conference on Applied Computer and Communication Technologies (ComCom)","","2017","","","1","5","The emergence of various problems of the water crisis in Central Java, urge on need for developing the mapping application of the water poverty level in that area to help the Government and related parties in executing the decision making required. This application will map the poverty levels of the water from 8,576 villages, 573 subdistricts and 35 regencies in Central Java by using the technique of spatial data mining and the approach of the water poverty index (WPI). Seeing the magnitude of that application scope, then it needs to do the software complexity measurement to find out further the resources needed for the development of the system in the future. The measurement of the software complexity is held with the Function Point (FP) method that produces the estimation of the resource requirements such as project work effort, project duration and speed of delivery. The results of the research showed that the software water poverty mapping owns the complexity level with FP at 136.64, the project work effort at 1,273 hours, the project duration in 4.037 months and the speed of delivery at 33.85 FP per person month.","","978-1-5090-4048-3978-1-5090-4047-6978-1-5386-3824","10.1109/COMCOM.2017.8167102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8167102","Function Point Analysis;Water poverty mapping;Software Complexity measurement","","data mining;decision making;software cost estimation;software metrics;water resources;water supply","software complexity measurement;water poverty mapping application;function point method;water crisis;Central Java;water poverty level;decision making;poverty levels;spatial data mining;water poverty index;FP;resource requirements;project duration;software water poverty mapping;complexity level;project work effort","","3","","23","","","","","IEEE","IEEE Conferences"
"Towards Validating Complexity-Based Metrics for Software Product Line Architectures","A. Marcolino; E. Oliveira; I. Gimenes; T. U. Conte","NA; NA; NA; NA","2013 VII Brazilian Symposium on Software Components, Architectures and Reuse","","2013","","","69","79","Software product line (PL) is an approach that focuses on software reuse and has been successfully applied for specific domains. The PL architecture (PLA) is one of the most important assets, and it represents commonalities and variabilities of a PL. The analysis of the PLA, supported by metrics, can be used as an important indicator of the PL quality and return on investment (ROI). This paper presents the replication of a controlled experiment for validating complexity metrics for PLAs. In particular, in this replication we are focused on evaluating how subjects less-qualified than the subjects from the original experiment evaluate complexity of a PLA by means of generated specific products. It was applied a PLA variability resolution model of a given PL to a sample of subjects from at least basic knowledge on UML modeling, PL and variability management. Apart of the selection of different subjects, the same original experiment conditions were kept. The proposed PLA complexity metrics were experimentally validated based on their application to a set of 35 derived products from the Arcade Game Maker (AGM) PL. Normality tests were applied to the metrics observed values, thus, pointing out their non-normality. Therefore, the non-parametric Spearman's correlation ranking technique was used to demonstrate the correlation between the CompPLA metric and the complexity rate given by the subjects to each derived product. Such a correlation was strong and positive. The results obtained in this replication shown that even less-qualified subjects, compared to the subjects from the original experiment, are able to rate the complexity of a PLA by means of its generated products, thus corroborating the results of the original experiment and providing more evidence that the composed metric for complexity (CompPLA) can be used as a relevant indicator for measuring the complexity of PLA based on their derived products.","","978-1-4799-2531","10.1109/SBCARS.2013.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685792","Correlation Analysis;Emprical Validation;Metrics;Replication;Software Product Line Architecture","Programmable logic arrays;Measurement;Complexity theory;Correlation;Unified modeling language;Computer architecture;Software","nonparametric statistics;program verification;software architecture;software metrics;software quality;software reusability","complexity-based metrics validation;software product line architectures;software reuse;software PLA;PL quality;return on investment;ROI;PLA variability resolution model;UML modeling;variability management;PLA complexity metrics;Arcade Game Maker;AGM PL;nonparametric Spearman correlation ranking technique;CompPLA metric;complexity rate","","","","20","","","","","IEEE","IEEE Conferences"
"Hardware complexity metrics for high level synthesis of software functions","S. Sinha; T. Srikanthan","Center for High Performance Embedded Systems, Nanyang Technological University, Singapore-637553; Center for High Performance Embedded Systems, Nanyang Technological University, Singapore-637553","Proceedings of 2011 International Symposium on VLSI Design, Automation and Test","","2011","","","1","4","High level synthesis using C/C++ code of applications is rapidly gaining ground. Code profiling has traditionally been used to select code blocks to implement in hardware by using ASICs, FPGAs or other programmable logic. We present a set of new hardware complexity metrics to reason about the suitability of a software function for its implementation by hardware synthesis. Our proposed metrics bring in the hardware aware perspective in the design process and overcome certain specific but important limitations inherent in code profiling based analysis. The metrics defined in the current work also help in design management and are applicable to innovative and future applications which are not fully developed to enable their code profiling.","","978-1-4244-8499-7978-1-4244-8500-0978-1-4244-8498","10.1109/VDAT.2011.5783553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783553","","Complexity theory;Hardware;Measurement;Software;Field programmable gate arrays;Finite impulse response filter;Mathematical model","C++ language;high level synthesis","hardware complexity metrics;high level synthesis;software functions;C/C++ code;code profiling","","1","","11","","","","","IEEE","IEEE Conferences"
"Optimizing software integration by considering integration test complexity and test effort","M. Steindl; J. Mottok","University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS3), Seybothstr. 2, D-93053 Regensburg, Germany; University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS3), Seybothstr. 2, D-93053 Regensburg, Germany","Proceedings of the 10th International Workshop on Intelligent Solutions in Embedded Systems","","2012","","","63","68","Software integration testing is often a bottleneck in the development process. Selecting the next component to integrate often depends heavily on integrators expertise or solely on the integration schedule. This may lead to an increasing number of stubs and makes testing more difficult. In this work we present a novel metric to calculate the test complexity of a certain integration order and provide an approach for optimizing it with respect to the integration test complexity and the integration test effort using simulated annealing.","","978-3-902463-09-8978-1-4673-2464","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6273606","","Complexity theory;Testing;Software;Simulated annealing;Measurement;Couplings","program testing;simulated annealing","integration test complexity;software integration testing;development process;integration schedule;integration order;integration test effort;simulated annealing","","","","26","","","","","IEEE","IEEE Conferences"
"An Embedded Software Power Model Based on Algorithm Complexity Using Back-Propagation Neural Networks","Q. Li; B. Guo; Y. Shen; J. Wang; Y. Wu; Y. Liu","NA; NA; NA; NA; NA; NA","2010 IEEE/ACM Int'l Conference on Green Computing and Communications & Int'l Conference on Cyber, Physical and Social Computing","","2010","","","454","459","Nowadays as low carbon economy is greatly advocated worldwide, the electricity consumption caused by a huge number of embedded computer systems is gaining more and more attention. Different instruction set, software algorithm and high-level software architecture can significantly affect the system energy consumption. In this paper, we first analyze the relations between software power consumption and some software characteristics on algorithm level. Through measuring three algorithm complexity characteristics, i.e., time complexity, space complexity and input scale, we propose an embedded software power model based on algorithm complexity. Then, we design and train a back propagation neural network to fit the power model accurately based on a sample training function set and more than 400 software power data. Simulation results show that the error between the estimation values of this model and the real measured values is below 10 percent, and this model can effectively estimate the power consumption of software in an early stage of software design.","","978-1-4244-9779-9978-0-7695-4331","10.1109/GreenCom-CPSCom.2010.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5724868","software power consumption;power model;algorithm complexity;back-propagation neural network","Energy consumption;Artificial neural networks;Complexity theory;Software algorithms;Analytical models;Embedded software","backpropagation;embedded systems;instruction sets;neural nets;power consumption;power engineering computing;software architecture","embedded software power model;back propagation neural networks;low carbon economy;electricity consumption;embedded computer systems;instruction set;software algorithm;high-level software architecture;system energy consumption;algorithm complexity;software design","","2","","9","","","","","IEEE","IEEE Conferences"
"A Process Refactoring for Software Development with Process Complexity and Activity Priority Lists","N. Hanakawa","NA","2011 Joint Conference of the 21st International Workshop on Software Measurement and the 6th International Conference on Software Process and Product Measurement","","2011","","","209","214","We propose a process refactoring technique and we show experience reports. The process refactoring is based onprocess complexity and activity priority lists. Values of process complexity are calculated by additional processes. The priority list presents priority levels of all activities that do not finish. In an industrial large-scale project, the process refactoring was executed. The period of the project was 2 years and 6 months. Four times process refactoring were executed. As a result, customers' satisfactions were high although several customers' requests were not realized. In addition, vendor's atisfactions also were high. The vendor felt glad that resources about all activities were sufficiently prepared by the process refactoring. In addition, we confirmed that process refactoring was useful to narrow all customers' requests down into actual indispensable requests.","","978-1-4577-1930","10.1109/IWSM-MENSURA.2011.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6113061","Software process complexity;process refactoring;workflow management table;requirement acquisition","Complexity theory;Schedules;Humans;Programming;Graphical user interfaces;Timing;Postal services","project management;software development management;software maintenance;software process improvement","software development;process complexity;activity priority list;process refactoring technique;industrial large-scale project;customer satisfaction;customer request;vendor satisfaction","","1","","11","","","","","IEEE","IEEE Conferences"
"A Complexity Theory Viewpoint on the Software Development Process and Situational Context","P. Clarke; R. V. O’Connor; B. Leavy","NA; NA; NA","2016 IEEE/ACM International Conference on Software and System Processes (ICSSP)","","2016","","","86","90","The research literature informs us that a software development process should be appropriate to its software development context but there is an absence of explicit guidance on how to achieve the harmonization of a development process with the corresponding situational context. Whilst this notion of harmonization may be intuitively appealing, in this paper we argue that interaction between a software development process and its situational context is an instance of a complex system. In Complexity Theory, complex systems consist of multiple agents that interact in a multitude of diverse ways, with system outcomes being non-deterministic. Complex systems are therefore noted to be difficult to control, such as is the case with many software development endeavors. If the interaction of software processes with situational contexts is representative of a complex system, then we should not be surprised that the task of software development has proven so resistant to attempts to produce generalized software processes. We should also seek to ameliorate the software development challenge through the adoption of techniques recommended for use in managing complex systems, not as a replacement for the many software process approaches presently in use, but as complement that can aid the task of process definition and evolution.","","978-1-4503-4188-2978-1-5090-2244","10.1109/ICSSP.2016.019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831579","Software and its engineering;Software creation and management;Software development process management;Software development methods","Software;Context;Complexity theory;Complex systems;Capability maturity model;Agile software development;Business","software development management;software metrics","software development process;complexity theory;complex system;situational context","","","","45","","","","","IEEE","IEEE Conferences"
"Reconfigurable Low Area Complexity Filter Bank Architecture Based on Frequency Response Masking for Nonuniform Channelization in Software Radio Receivers","R. Mahesh; A. P. Vinod","NA; Nanyang Technological University","IEEE Transactions on Aerospace and Electronic Systems","","2011","47","2","1241","1255","The most computationally demanding block in the digital front-end of a software defined radio (SDR) receiver is the channelizer which operates at the highest sampling rate. Channelizers are employed in the SDR receivers for extracting individual channels (frequency bands) from the digitized wideband input signal. Reconfigurability and low complexity are the two key requirements in an SDR channelizer. A new reconfigurable filter bank (FB) architecture based on frequency response masking (FRM) for SDR channelizers is proposed. The proposed FB offers reconfigurability at the architectural level and at the channel filter level and is capable of extracting channels of nonuniform bandwidths corresponding to multiple wireless communication standards from the digitized wideband input signal. Design examples show that the proposed FB offers multiplier complexity reduction of 84% over the conventional per-channel (PC) approach, which is best suitable for the extraction of channels of nonuniform bandwidth. The proposed FB has been synthesized on 0.18 μm complementary metal oxide semiconductor (CMOS) technology and compared with the PC approach. Synthesis results show that the proposed FB offers area reduction of 85%, power reduction of 48.5%, and improvement in speed of 56.7% over the PC approach. The proposed FB has been implemented on Xilinx Virtex 2v3000ff1152-4 FPGA and tested using real-time inputs from a vector signal generator.","0018-9251;1557-9603;2371-9877","","10.1109/TAES.2011.5751255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5751255","","Finite impulse response filter;Filter banks;Complexity theory;Bandwidth;Receivers;Delay","channel bank filters;field programmable gate arrays;frequency response;radio receivers;reconfigurable architectures;software radio;wireless channels","reconfigurable low area complexity filter bank architecture;frequency response masking;nonuniform channelization;software defined radio receiver;channel extraction;multiple wireless communication standard;multiplier complexity reduction;complementary metal oxide semiconductor technology;power reduction;FPGA;vector signal generator;field programmable gate array","","22","","24","","","","","IEEE","IEEE Journals & Magazines"
"How Simple is It to Measure Software Size and Complexity for an IT Practitioner?","G. Robiolo","NA","2011 International Symposium on Empirical Software Engineering and Measurement","","2011","","","40","48","An empirical study was conducted in order to evaluate the simplicity of FPA, COSMIC and Paths, from the IT practitioners' viewpoint. The results have shown that P are the simplest measure because they presented a significantly lower measurement time. The study has also been useful to see which aspects of Paths should be clarified in order to facilitate the practitioners' application of this measure and to demonstrate in a mathematical form how the complexity of an application which is measured with P increases if the number of ""transactions"" increases. The results obtained cannot be generalized because the number of measurements performed was small, and the subjects had particular characteristics, but a new empirical study using use case textual descriptions of different domains and subjects with different characteristics is expected to solve this limitation.","1949-3789;1949-3770;1938-6451","978-1-4577-2203-5978-0-7695-4604","10.1109/ESEM.2011.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092552","functional size measurement;functional size measurement simplicity;Function Points;COSMIC;Paths;empirical study","Particle measurements;Atmospheric measurements;Time measurement;Size measurement;Complexity theory;Software;Software measurement","computational complexity;software development management","software size;software complexity;IT practitioner;FPA;COSMIC;Paths;textual descriptions","","4","","26","","","","","IEEE","IEEE Conferences"
"Impact of Product Complexity on Actual Effort in Software Developments: An Empirical Investigation","Z. Li; L. O'Brien; Y. Yang","NA; NA; NA","2014 23rd Australian Software Engineering Conference","","2014","","","170","179","[Background:] Software effort prediction methods and models typically assume positive correlation between software product complexity and development effort. However, conflicting observations, i.e. negative correlation between product complexity and actual effort, have been witnessed from our experience with the COCOMO81 dataset. [Aim:] Given our doubt about whether the observed phenomenon is a coincidence, this study tries to investigate if an increase in product complexity can result in the abovementioned counter-intuitive trend in software development projects. [Method:] A modified association rule mining approach is applied to the transformed COCOMO81 dataset. To reduce noise of analysis, this approach uses a constant antecedent (Complexity increases while Effort decreases) to mine potential consequents with pruning. [Results:] The experiment has respectively mined four, five, and seven association rules from the general, embedded, and organic projects data. The consequents of the mined rules suggested two main aspects, namely human capability and product scale, to be particularly concerned in this study. [Conclusions:] The negative correlation between complexity and effort is not a coincidence under particular conditions. In a software project, interactions between product complexity and other factors, such as Programmer Capability and Analyst Capability, can inevitably play a ""friction"" role in weakening the practical influences of product complexity on actual development effort.","1530-0803;2377-5408","978-1-4799-3149","10.1109/ASWEC.2014.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824122","Product Complexity;Software Development;Software Effort Estimation;Empirical Software Engineering","Complexity theory;Software;Correlation;Association rules;Market research;Software algorithms","data mining;software cost estimation;software metrics","product complexity;software developments;software effort prediction methods;software product complexity;COCOMO81 dataset;software development projects;modified association rule mining approach;organic project data;programmer capability;analyst capability;software effort estimation","","1","","29","","","","","IEEE","IEEE Conferences"
"Assessing the complexity of upgrading software modules","B. Schoenmakers; N. van den Broek; I. Nagy; B. Vasilescu; A. Serebrenik","ASML Netherlands B.V., De Run 6501, 5504 DR, Veldhoven, The Netherlands; ASML Netherlands B.V., De Run 6501, 5504 DR, Veldhoven, The Netherlands; ASML Netherlands B.V., De Run 6501, 5504 DR, Veldhoven, The Netherlands; Technische Universiteit Eindhoven, Den Dolech 2, P.O. Box 513, 5600 MB Eindhoven, The Netherlands; Technische Universiteit Eindhoven, Den Dolech 2, P.O. Box 513, 5600 MB Eindhoven, The Netherlands","2013 20th Working Conference on Reverse Engineering (WCRE)","","2013","","","433","440","Modern software development frequently involves developing multiple codelines simultaneously. Improvements to one codeline should often be applied to other codelines as well, which is typically a time consuming and error-prone process. In order to reduce this (manual) effort, changes are applied to the system's modules and those affected modules are upgraded on the target system. This is a more coarse-grained approach than upgrading the affected files only. However, when a module is upgraded, one must make sure that all its dependencies are still satisfied. This paper proposes an approach to assess the ease of upgrading a software system. An algorithm was developed to compute the smallest set of upgrade dependencies, given the current version of a module and the version it has to be upgraded to. Furthermore, a visualization has been designed to explain why upgrading one module requires upgrading many additional modules. A case study has been performed at ASML to study the ease of upgrading the TwinScan software. The analysis shows that removing elements from interfaces leads to many additional upgrade dependencies. Moreover, based on our analysis we have formulated a number improvement suggestions such as a clear separation between the test code and the production code as well as an introduction of a structured process of symbols deprecation and removal.","1095-1350;2375-5369","978-1-4799-2931","10.1109/WCRE.2013.6671319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671319","","Software;Heating;Visualization;Algorithm design and analysis;Stress;Image color analysis;Complexity theory","software metrics","software modules upgrading complexity;software development;codelines development;coarse-grained approach;upgrade dependencies;TwinScan software","","","","34","","","","","IEEE","IEEE Conferences"
"Software Complexity Analysis Based on Shannon Entropy Theory and C&K Metrics","K. Hirama","NA","IEEE Latin America Transactions","","2016","14","5","2485","2490","This work presents an analysis of software complexity based on Shannon Communication Theory using Chidamber and Kemerer (C and K) Metrics. The complexity affects effort and cost of software development and maintenance. The main software artifact is its architecture which depends on good balance of structural decisions based on coupling and cohesion concepts. The entropy concept can help us to measure the disorder level of the system. This work proposes using entropy as reference to control software design based on C&K metrics. I also define thresholds for some metrics and calculate the entropy of jUnit software used as an example. The results are good that suggest next steps in software complexity.","1548-0992","","10.1109/TLA.2016.7530449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530449","C and K metrics;complex systems;entropy theory;Software architecture","Software;Entropy;Complexity theory;Measurement;Couplings;Java;Loading","information theory;software architecture;software maintenance;software metrics","software complexity analysis;Shannon entropy theory;C-and-K metrics;Shannon communication theory;Chidamber-and-Kemerer metrics;software development;software maintenance;software architecture;cohesion concepts;entropy concept;jUnit software","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity flexible filter banks for uniform and non-uniform channelisation in software radios using coefficient decimation","R. Mahesh; A. P. Vinod","School of Computer Engineering, Nanyang Technological University; School of Computer Engineering, Nanyang Technological University","IET Circuits, Devices & Systems","","2011","5","3","232","242","A new approach to implement computationally efficient reconfigurable filter banks (FBs) is presented. If the coefficients of a finite impulse response filter are decimated byM, that is, if everyMth coefficient of the filter is kept unchanged and remaining coefficients are replaced by zeros, a multi-band frequency response will be obtained. The frequency response of the decimated filter will have bands with centre frequencies at 2πk/M, wherekis an integer ranging from 0 toM-1. If these multi-band frequency responses are subtracted from each other or selectively masked using inherently low complex wide transition-band masking filters, different low-pass, high-pass, band-pass and band-stop frequency bands are obtained. The resulting FB, whose bands- centre frequencies are located at integer multiples of 2π/M, is a low complexity alternative to the well-known uniform discrete Fourier transform FBs (DFTFBs). It is shown that the channeliser based on the proposed FB does not require any DFT for its implementation unlike a DFTFB. It is also shown that the proposed FB is more flexible and easily reconfigurable than the DFTFB. Furthermore, the proposed FB is able to receive channels of multiple standards simultaneously, whereas separate FBs would be required for simultaneous reception of multi-standard channels in a DFTFB-based receiver. This is achieved through a second stage of coefficient decimation. Implementation result shows that the proposed FB offers an area reduction of 41% and improvement in the speed of 50.8% over DFTFBs.","1751-858X;1751-8598","","10.1049/iet-cds.2010.0010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5893985","","","band-pass filters;band-stop filters;discrete Fourier transforms;FIR filters;high-pass filters;low-pass filters;radio receivers;software radio","low complexity flexible filter banks;non-uniform channelisation;uniform channelisation;software radios;coefficient decimation;finite impulse response filter;FIR filter;multiband frequency response;decimated filter;wide transition-band masking filters;band-stop frequency bands;bandpass frequency bands;high-pass frequency bands;low-pass frequency bands;uniform discrete Fourier transform;DFTFB-based receiver","","22","","","","","","","IET","IET Journals & Magazines"
"Dribbling complexity in model driven development using Naked Objects, domain driven design, and software design patterns","S. A. Soares; M. Brandão; M. I. Cortés; E. S. S. Freire","Universidade Estadual do Ceará, Fortaleza - Brasil; Universidade Estadual do Ceará, Fortaleza - Brasil; Universidade Estadual do Ceará, Fortaleza - Brasil; Instituto Federal do Ceará, Iguatu - Brasil","2015 Latin American Computing Conference (CLEI)","","2015","","","1","11","Systems modeling and transformations that are necessary for code generation in the model driven development approach need to realize user interface aspects and persistence infrastructure to create executable software. The developer does not work just with the domain application and also the modeling is more complex whenever more details are needed in the model. Therefore, we propose a development tool where the developer just models the business objects, the associations between objects, and their behaviors using Domain Patterns and Design Patterns. The code is generated based on these Design Patterns and a framework, that implements the architectural patterns Naked Objects, has the responsibility by the infrastructure.","","978-1-4673-9143-6978-1-4673-9142","10.1109/CLEI.2015.7360022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360022","model driven development;naked objects;domain-driven design;domain patterns;design patterns","Unified modeling language;Software;Computational modeling;Java;Manuals;Computer architecture;Context modeling","program compilers;software engineering;user interfaces","dribbling complexity;naked object;domain driven design;software design pattern;code generation;model driven development approach;user interface;executable software;domain pattern;design pattern","","","","13","","","","","IEEE","IEEE Conferences"
"Empirical Validation of Complexity and Extensibility Metrics for Software Product Line Architectures","E. A. Oliveira Junior; J. C. Maldonado; I. M. S. Gimenes","NA; NA; NA","2010 Fourth Brazilian Symposium on Software Components, Architectures and Reuse","","2010","","","31","40","The software product line (PL) architecture (PLA) is one of the most important PL core assets as it is the abstraction of the products that can be generated, and it represents similarities and variabilities of a PL. Its quality attributes analysis and evaluation can serve as a basis for analyzing the managerial and economical values of a PL. We proposed metrics for PLA complexity and extensibility quality attributes. This paper is concerned with the empirical validation of such metrics. As a result of the experimental work we can conclude that the metrics are relevant indicators of complexity and extensibility of PLA by presenting their correlation analysis.","","978-1-4244-8707-3978-0-7695-4259","10.1109/SBCARS.2010.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631689","complexity;empirical validation;extensibility;metrics;product line architecture;software product line","Programmable logic arrays;Measurement;Complexity theory;Correlation;Unified modeling language;Computer architecture;Software","software architecture;software metrics;statistical analysis","software product line architecture;extensibility quality attribute;empirical validation;extensibility metric;correlation analysis","","2","","15","","","","","IEEE","IEEE Conferences"
"Measuring Business Logic Complexity in Software Systems","M. Kamimura; A. Matsuo; Y. Maeda","NA; NA; NA","2015 Asia-Pacific Software Engineering Conference (APSEC)","","2015","","","370","376","Many organizations adopt software to support their business process and business logic is embedded in their systems. To meet ever-changing business needs, business logic in software must be continuously changed and renewing the whole system as known as software modernization is performed for systems running for a long time. When modernizing especially large scale system, many developers are involved to deal with large amount of code. To support software modernization, project management is needed and managers use volume information such as software metrics to find hard things, estimate efforts and qualities though when analyzing business logic, existing metrics are not useful because existing metrics do not distinguish business logic from other infrastructure activities. To solve this problem we define business logic complexity to capture business logic from the source code and to quantify how hard it is to understand. First we identify business logic by excluding format checks and by using input and output data in condition and calculation statements. Next, we quantify the degree of complexity of business logic. When making decisions about improving applications, our method makes it possible to prioritize the aspects of the business logic that are complex and hard to understand.","1530-1362","978-1-4673-9644","10.1109/APSEC.2015.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467323","business logic;complexity;prioritization","Business;Complexity theory;Software;Software metrics;Large-scale systems","business data processing;organisational aspects;software metrics","measuring business logic complexity;software systems;business process;software modernization;project management;software metrics;business logic;source code","","","","24","","","","","IEEE","IEEE Conferences"
"Evaluate How Cyclomatic Complexity Changes in the Context of Software Evolution","H. Liu; X. Gong; L. Liao; B. Li","NA; NA; NA; NA","2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)","","2018","02","","756","761","Cyclomatic complexity (CC) is often used as a factor to evaluate the quality of source code. Many researchers have studied the relationships between CC and LOC, between CC and basic testing paths, and between CC and code maintainability etc. However, few researchers studied how software evolution affects CC. In this paper, we propose a methodology based on source code change analysis and develop a supporting tool, called CCEvaluator, to evaluate CC variation during software evolution. By empirical studies on six pieces of typical open source projects, a series of interesting findings including six commonness and five differences have been obtained. To explain why these commonness and differences are produced during software evolution, code change information among successive versions are captured and used in this paper.","0730-3157","978-1-5386-2667","10.1109/COMPSAC.2018.10332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377960","Cyclomatic Complexity;Software Evolution;Evaluation","Complexity theory;Software;Java;Detectors;Tools;Measurement;Testing","program testing;public domain software;software maintenance;software metrics;software quality;software tools;source code (software)","source code change analysis;software evolution;code change information;cyclomatic complexity changes;basic testing paths;code maintainability;open source projects;CCEvaluator","","","","10","","","","","IEEE","IEEE Conferences"
"A Model-Based Framework to Support Complexity Analysis Service for Regression Testing of Component-Based Software","C. Tao; J. Gao; B. Li","NA; NA; NA","2015 IEEE Symposium on Service-Oriented System Engineering","","2015","","","326","331","Today, software components have been widely used in software construction to reduce the cost of project and speed up software development cycle. During software maintenance, various software change approaches can be used to realize specific change requirements of software components. Different change approaches lead to diverse regression testing complexity. Such complexity is one of the key contributors to the cost and effectiveness of software maintenance. However, there is a lack of research work addressing regression testing complexity analysis service for software components. This paper proposes a framework to measure and analyze regression testing complexity based on a set of change and impact complexity models and metrics. The framework can provide services for complexity modeling, complexity factor classification, and regression testing complexity measurements. The initial study results indicate the proposed framework is feasible and effective in measuring the complexity of regression testing for component-based software.","","978-1-4799-8356","10.1109/SOSE.2015.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133549","testing service;component-based software regression testing;software maintenance;regression testing complexity","Complexity theory;Testing;Measurement;Computational modeling;Analytical models;Software maintenance","object-oriented programming;program testing;software maintenance;software metrics","model-based framework;component-based software;software components;software construction;project cost reduction;software development cycle;software maintenance;software change approach;regression testing complexity analysis service;complexity modeling;complexity factor classification;regression testing complexity measurements","","","","15","","","","","IEEE","IEEE Conferences"
"Exploring Complexity in Open Source Software: Evolutionary Patterns, Antecedents, and Outcomes","D. P. Darcy; S. L. Daniel; K. J. Stewart","NA; NA; NA","2010 43rd Hawaii International Conference on System Sciences","","2010","","","1","11","Software complexity is important to researchers and managers, yet much is unknown about how complexity evolves over the life of a software application and whether different dimensions of software complexity may exhibit similar or different evolutionary patterns. Using cross-sectional and longitudinal data on a sample of 108 open source projects, this research investigated how the complexity of open source project releases varied throughout the life of the project. Functional data analysis was applied to the release histories of the projects and recurring evolutionary patterns were derived. There were projects that saw little evolution, according to their measures of size and structural complexity. However, projects that displayed some evolution often differed on the pattern of evolution depending on whether size or structural complexity was examined. Factors that contribute to and result from the patterns of complexity were evaluated, and implications for research and practice are presented.","1530-1605","978-1-4244-5510-2978-1-4244-5509","10.1109/HICSS.2010.198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428486","","Open source software;Documentation;Application software;Project management;Conference management;Data analysis;Licenses;Open systems;History;Size measurement","computational complexity;data analysis;public domain software;software engineering;software maintenance;software metrics","open source software;evolutionary patterns;software complexity;functional data analysis","","5","","28","","","","","IEEE","IEEE Conferences"
"A quantitative cohesion complexity measure to enhancing software quality","P. Charoenporn; P. Sophatsathit","Advanced Virtual and Intelligent Computing (AVIC) Center, Department of Mathematics and Computer Science, Faculty of Science, Chulalongkorn University, Bangkok, Thailand; Advanced Virtual and Intelligent Computing (AVIC) Center, Department of Mathematics and Computer Science, Faculty of Science, Chulalongkorn University, Bangkok, Thailand","2014 International Computer Science and Engineering Conference (ICSEC)","","2014","","","360","365","This paper proposes a quantitative approach to measure module cohesion. The relatedness of elements within a module is quantified in the form of cohesion complexity. We first identify variable relatedness using variable dependence graph. Cohesion complexity is then analyzed and mathematically formulated in accordance with standard definitions. Variable relatedness being analyzed are data, selection, and loop. As such, traditional ordinal measure can be objectively clarified to distinguish the differences of design cohesion classification, reflecting the desired software quality. The result so obtained will help developers achieve better cohesive design of software.","","978-1-4799-4963-2978-1-4799-4965","10.1109/ICSEC.2014.6978223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6978223","cohesion;cohesion complexity;software quality;design cohesion","Complexity theory;Computer science;Software quality;Software measurement;Standards;Decision trees","graph theory;software quality","quantitative cohesion complexity measure;software quality;variable dependence graph","","","","13","","","","","IEEE","IEEE Conferences"
"A complexity metric for concurrent finite state machine based embedded software","L. Guo; A. S. Vincentelli; A. Pinto","University of California, Berkeley, USA; University of California, Berkeley, USA; United Technologies Research Center, Inc, USA","2013 8th IEEE International Symposium on Industrial Embedded Systems (SIES)","","2013","","","189","195","The development cost of safety-critical embedded systems is dominated today by the cost of software including verification and validation. This cost is typically related to the complexity of the software functions implementing the desired system behavior in nominal and off-nominal conditions. A widely used measure of complexity is the cyclomatic number, which is computed on the implementation code. However this technique is not effective when model-based development and code generation are used because the complexity of the software also depends on the communication and execution semantics of the models. This paper proposes a model-based complexity number that is defined on the decision diagram (DD) representation of the system functionality. The proposed complexity number gives an upper bound on the number of tests that are necessary to achieve Condition/Decision (C/D) coverage (which is required for safety critical systems). We show that the number of tests is related to the min-flow/max-cut computed on the DD. By comparing the proposed metric with the cyclomatic complexity, we show that the former seems to be better suited for capturing the complexity of the model than the latter. A case study on an aircraft power system shows that the complexity metric has applications in functional partitioning and architecture selection.","2150-3109;2150-3117","978-1-4799-0658","10.1109/SIES.2013.6601491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601491","","Complexity theory;Measurement;Computational modeling;Software;Switches;Unified modeling language;Semantics","computational complexity;embedded systems;finite state machines;safety-critical software","complexity metric;concurrent finite state machine;embedded software;safety-critical embedded system;software verification;software validation;cyclomatic number;model-based development;code generation;DD representation;decision diagram representation;condition-decision coverage;cyclomatic complexity;aircraft power system;functional partitioning;architecture selection","","1","","17","","","","","IEEE","IEEE Conferences"
"Automated tool for the calculation of cognitive complexity of a software","Dinuka Rukshani Wijendra; K. P. Hewagamage","Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Systems Engineering, University of Colombo School of Computing, Sri Lanka","2016 2nd International Conference on Science in Information Technology (ICSITech)","","2016","","","163","168","Software Complexity Metrics play a major role of determining the complexity of a given software quantitatively with respective to one or more software complexity attributes in which the metric is willing to compute with. Among the continuation of the possible ways of computing the complexity of a software, the Cognitive Complexity calculation can be considered as a prominent factor since it indicates how the human brain identifies the internal logic and the structure behind the source code based on Cognitive Informatics. The proposed Cognitive Complexity metric evaluates the complexity in terms of the Architectural and Spatial aspects in which it determines the amount of information inside the software through Cognitive weights and the way of information scattering in terms of Lines of Codes (LOC) respectively. Then it is going to be analyzed how the proposed Cognitive Complexity calculation can be automated to provide a collaborative workspace to the end users with high efficiency and to prevent the manual calculation.","","978-1-5090-1721-8978-1-5090-1720-1978-1-5090-1722","10.1109/ICSITech.2016.7852627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852627","architectural aspect;automated tool;cognitive complexity;spatial aspect;software complexity","Complexity theory;Software;Measurement;Information technology;Standards;Data structures;Cognitive informatics","software architecture;software metrics;source code (software)","software cognitive complexity;software complexity metrics;source code;Cognitive Informatics;information scattering;lines of codes;architectural aspects","","","","33","","","","","IEEE","IEEE Conferences"
"A framework for the estimation of OO software reliability using design complexity metrics","R. Bharathi; R. Selvarani","Department of ISE, PES Institute of Technology-BSC, Bangalore, India; Department of CSE, Alliance College of Engineering and Design, Alliance University, Bangalore, India","2015 International Conference on Trends in Automation, Communications and Computing Technology (I-TACT-15)","","2015","","","1","7","Reliability of a software product essentially denotes its trustworthiness or dependability. The stakeholder's requirement is normally a robust and highly reliable software product. Reliability is one of the dynamic characteristics of software, which can be quantitatively measured by analyzing the failure data. In order to estimate as well as to predict the reliability of software systems, failure data need to be properly measured by various means during software development. Most of the software failures happen due to poor design quality. Controlled and well-monitored design with respect to reliability can be achieved by quantitative analysis and appropriate mapping of design metrics. This analysis leads to the prediction of reliability at the design stage of Software Development Life Cycle (SDLC) of Object Oriented (OO) software, which will provide early information for software architect about the quality of the design with respect to reliability. Chidamber and Kemerer (CK) design metrics play a dynamic role to understand the design aspects of object-oriented software. In this context, we have proposed a framework for predicting reliability at the design phase of software development life cycle. Through empirical analysis, a valid hypothesis is established relating the design complexity metrics and reliability. A functional relationship is established through polynomial regression technique for selected design metrics. With these equations, an estimation model called R-MODEL is formulated through weighted linear combination of multifunctional parameters, which predicts reliability for any module of the OO software project. The R-MODEL is evaluated and validated with commercial software, which is found to be satisfactory. This quantitative empirical analysis provides an idea for the percentage influence of design metrics on reliability at the design level. The approach is to make, measurement as an integral part of the software design process to assist software designer to predict the software reliability at early stage and to improve the product quality.","","978-1-4673-6667-0978-1-4673-6666","10.1109/ITACT.2015.7492648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492648","Reliability;Software quality;Object Oriented Design;CK metrics;Estimation","Software reliability;Measurement;Software;Reliability engineering;Estimation;Mathematical model","object-oriented methods;software metrics;software reliability","OO software reliability estimation;object-oriented software;design complexity metrics;software product reliability;software trustworthiness;software dependability;stakeholder requirement;software development life cycle;SDLC;Chidamber-Kemerer design metrics;CK design metrics;R-model;quantitative empirical analysis","","1","","21","","","","","IEEE","IEEE Conferences"
"A reduced complexity cross-correlation interference mitigation technique on a real-time software-defined radio GPS L1 receiver","E. Schmidt; Z. A. Ruble; D. Akopian; D. J. Pack","Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Electrical Engineering, The University of Tennessee at Chattanooga, Chattanooga, TN, USA; Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX, USA; Department of Electrical Engineering, The University of Tennessee at Chattanooga, Chattanooga, TN, USA","2018 IEEE/ION Position, Location and Navigation Symposium (PLANS)","","2018","","","931","939","The U.S. global position system (GPS) is one of the existing global navigation satellite systems (GNSS) that provides position and time information for users in civil, commercial and military backgrounds. Because of its reliance on many applications nowadays, it's crucial for GNSS receivers to have robustness to intentional or unintentional interference. Because most commercial GPS receivers are not flexible, software-defined radio emerged as a promising solution for fast prototyping and research on interference mitigation algorithms. This paper provides a proposed minimum mean-squared error (MMSE) interference mitigation technique which is enhanced for computational feasibility and implemented on a real-time capable GPS L1 SDR receiver. The GPS SDR receiver SW has been optimized for real-time operation on National Instruments' LabVIEW (LV) platform in conjunction with C/C++ dynamic link libraries (DLL) for improved efficiency. Performance results of said algorithm with real signals and injected interference are discussed. The proposed SDR receiver gains in terms of BER curves for several interferers are demonstrated.","2153-3598","978-1-5386-1647-5978-1-5386-1648","10.1109/PLANS.2018.8373471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8373471","Global Positioning System;software-defined radio;interference mitigation;mean-squared error;real-time receiver","Interference;Receivers;Global Positioning System;Satellites;Real-time systems;Global navigation satellite system","error statistics;Global Positioning System;interference suppression;least mean squares methods;radio receivers;software radio","time information;GNSS receivers;unintentional interference;commercial GPS receivers;software-defined radio;interference mitigation algorithms;GPS SDR receiver SW;real-time operation;injected interference;SDR receiver gains;reduced complexity cross-correlation interference mitigation technique;global navigation satellite systems;position information;real-time software-defined GPS L1 receiver;minimum mean-squared error interference mitigation technique;real-time capable GPS L1 SDR receiver;BER curves;dynamic link libraries","","","","25","","","","","IEEE","IEEE Conferences"
"Design level metrics to measure the complexity across versions of AO software","Parthipan S; Senthil Velan S; C. Babu","Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","","2014","","","1708","1714","Software metric plays a vital role in quantitative assessment of any specific software development methodology and its impact on the maintenance of software. It can also be used to indicate the degree of interdependence among the components by providing valuable feedback about quality attributes such as maintainability, modifiability and understandability. The effort for software maintenance normally has a high correlation with the complexity of its design. Aspect Oriented Software Design is an emerging methodology that provides powerful new techniques to improve the modularity of software from its design. In this paper, evaluation model to capture the symptoms of complexity has been defined consisting of metrics, artifacts and elements of complexity. A tool to automatically capture these metrics across different versions of a case study application, University Automation System has been developed. The values obtained for the proposed metrics are used to infer on the complexity of Java and AspectJ implementations of the case study application. These measurements indicate that AspectJ implementations are less complex compared to the Java implementations and there by positively influencing the maintainability of software.","","978-1-4799-3914-5978-1-4799-3913","10.1109/ICACCCT.2014.7019400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019400","","Complexity theory;Java;Software","aspect-oriented programming;computational complexity;educational administrative data processing;Java;software maintenance;software metrics;software quality","design level metrics;version complexity;AO software;software metric;quantitative assessment;software development methodology;software maintenance;quality attributes;aspect oriented software design;university automation system;Java;AspectJ","","6","","9","","","","","IEEE","IEEE Conferences"
"A low complexity and flexible implementation of 2n-QAM for software defined radio applications","H. Alasti","School of Polytechnic, Indiana Purdue University, Fort Wayne (IPFW), Fort Wayne, IN, 46805 USA","2018 15th IEEE Annual Consumer Communications & Networking Conference (CCNC)","","2018","","","1","6","A low complexity and flexible implementation of Gray-coded 2n-QAM with rectangular constellation is proposed and discussed, where it drastically saves the required storage at the cost of very low complexity. The proposed approach introduces a unified implementation algorithm for Gray-coded 2n QAM, V n, where it is favorable when the radio needs to adaptively increase or decrease the modulation code-word size based on the required data rate and the channel status. The performance of a successive interference cancellation algorithm is evaluated as optimal detector using computer simulations. The proposed approach has flexibility for selectively loading the codeword's bits with unequal error rates. It is shown that for the odd values of n, stretching the constellation simultaneously reduces the peak to average power ratio (PAPR), and improves the bit error rate. An experimental implementation of the modulation on a low power ARM microcontroller is presented.","2331-9860","978-1-5386-4790-5978-1-5386-4791","10.1109/CCNC.2018.8319203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319203","Software-defined radio (SDR);digital to analog converter (DAC);peak to average power ratio;digital modulation schemes;performance analysis","Quadrature amplitude modulation;Peak to average power ratio;Complexity theory;Binary phase shift keying;Conferences;Microcontrollers","error statistics;Gray codes;interference suppression;modulation coding;OFDM modulation;quadrature amplitude modulation;signal detection;software radio","modulation code-word size;successive interference cancellation algorithm;low power ARM microcontroller;software defined radio applications;rectangular constellation;data rate;Gray-coded 2n-QAM;channel status;optimal detector;computer simulations;unequal error rates;codeword bits;peak to average power ratio;PAPR;bit error rate","","","","18","","","","","IEEE","IEEE Conferences"
"Exploration on software complexity metrics for business process model and notation","I. Solichah; M. Hamilton; P. Mursanto; C. Ryan; M. Perepletchikov","Faculty of Computer Science, Universitas Indonesia, Depok, ID; School of Computer Science and Information Technology, RMIT University, Melbourne, AU; Faculty of Computer Science, Universitas Indonesia, Depok, ID; School of Computer Science and Information Technology, RMIT University, Melbourne, AU; School of Computer Science and Information Technology, RMIT University, Melbourne, AU","2013 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","","2013","","","31","37","Business Process Model and Notation (BPMN) is a graphical representation and notation for modeling complex business processes in diagrams. A simple BPMN diagram is easier to understand by all of the business stakeholders than a complex one. It is also easier for the developers to implement the corresponding systems. Complexity metrics can measure the complexity of a diagram. Only a few BPMN complexity metrics are found in the literature as BPMN is a recent development. To propose a new BPMN complexity metric, it is important to find suitable software complexity metrics which can be further adapted to develop a complexity metric for BPMN. This research surveys the existing software complexity metrics and the existing BPMN complexity metrics (i.e. McCabe Cyclomatic Complexity, Control-flow Complexity, and Halstead-based Process Complexity Metrics) to compare their performance and suitability in measuring the complexity of BPMN diagrams. The BPMN diagrams of the business processes of two Enterprise Resource Planning (ERP) open-source systems (i.e. Compiere and Openbravo ERP systems) are used in this research. The metrics values obtained are compared with empirical application and code measurement values (i.e. number of form-fields, number of files of code, and number of classes) of the two open-source systems. This research finds that the Halstead-based Process Complexity that has been proposed in the literature is useful in measuring the data complexity of BPMN diagrams. This means that the Halstead-based Process Complexity can be further elaborated to produce a BPMN complexity measure.","","978-1-4799-4692","10.1109/ICACSIS.2013.6761549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761549","","Complexity theory;Software;Business;Process control;Software metrics","business data processing;diagrams;enterprise resource planning;public domain software;software metrics","software complexity metrics;business process model and notation;diagram complexity;BPMN complexity metrics;McCabe cyclomatic complexity;control-flow complexity;Halstead-based process complexity metrics;enterprise resource planning open-source systems;Compiere ERP systems;Openbravo ERP systems;code measurement values","","","","17","","","","","IEEE","IEEE Conferences"
"Effects of Feature Complexity on Software Effort Estimates -- An Exploratory Study","A. Magazinius; R. B. Svensson","NA; NA","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","","2014","","","301","304","Software estimates are an important input to project selection, planning, monitoring as well assessment of projects' success. However, the practice of estimation is also a challenging activity for many practitioners, and there is a continuing need to understand and improve it. To contribute to the existing knowledge of this process, an in-depth study was conducted where 18 practitioners from eight sub domains at a company producing embedded systems were interviewed. The contribution of this article is twofold: First, it reports the estimation challenges as perceived by the practitioners where the two most prominent were organizational dependencies and complexity of requested features. Second, since feature complexity was noted as one of the estimation challenges, this paper also reports on factors that are driving that complexity, two of the most common being dependencies on functionality developed by other parts of the organization, and functionality that affects the entire product.","1089-6503;2376-9505","978-1-4799-5795","10.1109/SEAA.2014.69","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928826","","Complexity theory;Estimation;Software;Interviews;Hardware;Companies;Software engineering","embedded systems;planning;project management;software cost estimation","project selection;project planning;project monitoring;project success assessment;embedded systems;feature complexity;software effort estimates","","","","14","","","","","IEEE","IEEE Conferences"
"Hardware-software collaborative complexity reduction scheme for the emerging HEVC intra encoder","M. U. K. Khan; M. Shafique; M. Grellert; J. Henkel","Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","125","128","High Efficiency Video Coding (HEVC/H.265) is an emerging standard for video compression that provides almost double compression efficiency at the cost of major computational complexity increase as compared to current industry-standard Advanced Video Coding (AVC/H.264). This work proposes a collaborative hardware and software scheme for complexity reduction in an HEVC Intra encoding system, with run-time adaptivity. Our scheme leverages video content properties which drive the complexity management layer (software) to generate a highly probable coding configuration. The intra prediction size and direction are estimated for the prediction unit which provides reduced computational-complexity. At the hardware layer, specialized coprocessors with enhanced reusability are employed as accelerators. Additionally, depending upon the video properties, the software layer administers the energy management of the hardware coprocessors. Experimental results show that a complexity reduction of up to 60 % and the energy reduction up to 42 % are achieved.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513485","","Hardware;Encoding;Complexity theory;Video coding;Software;Clocks;Registers","","","","20","","13","","","","","IEEE","IEEE Conferences"
"Hardware/Software Mechanisms for Protecting an IDS against Algorithmic Complexity Attacks","G. S. Shenoy; J. Tubella; A. Gonz'lez","NA; NA; NA","2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum","","2012","","","1190","1196","Intrusion Detection Systems (IDS) have emerged as one of the most promising ways to secure systems in the network. An IDS like the popular Snort[17] detects attacks on the network using a database of previous attacks. So in order to detect these attack strings in the packet, Snort uses the Aho-Corasick algorithm. This algorithm first constructs a Finite State Machine (FSM) from the attack strings, and subsequently traverses the FSM using bytes from the packet. We observe that there are input bytes that result in a traversal of a series of FSM states (also viewed as pointers). This chain of pointer traversal significantly degrades (22X) the processing time of an input byte. Such a wide variance in the processing time of an input byte can be exploited by an adversary to throttle the IDS. If the IDS is unable to keep pace with the network traffic, the IDS gets disabled. So in the process the network becomes vulnerable. Attacks done in this manner are referred to as algorithmic complexity attacks, and arise due to weaknesses in IDS processing. In this work, we explore defense mechanisms to the above outlined algorithmic complexity attack. Our proposed mechanisms provide over 3X improvement in the worst-case performance.","","978-1-4673-0974","10.1109/IPDPSW.2012.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270773","Intrusion Detection Systems;Defense Mechanisms;Hardware Support","Payloads;Hardware;Databases;Clocks;Optimization;Software;Complexity theory","computational complexity;computer network security;finite state machines;string matching","software mechanisms;IDS protection;algorithmic complexity attacks;hardware mechanisms;intrusion detection systems;Aho-Corasick algorithm;finite state machine;FSM states;processing time;network traffic;IDS processing;intrusion defense mechanisms;worst-case performance;string detection","","","","25","","","","","IEEE","IEEE Conferences"
"Project Complexity and Knowledge Transfer in Global Software Outsourcing Project Teams: A Transactive Memory Systems Perspective","G. Qu; S. Ji; A. Nsakanda","NA; NA; NA","2012 45th Hawaii International Conference on System Sciences","","2012","","","3776","3785","The paper presents results based on a study that examined the relationship between transactive memory systems (TMS) and knowledge transfer performance (KTP) in the context of global software outsourcing project teams. Based on a cross sectional sample of 107 project managers and using structural equation modeling method, the study verifies the moderating effect of project complexity on the relationship between TMS and KTP. It is concluded that TMS is an important facilitator to KTP and project complexity, as a moderating factor, plays an important role in specialization on team knowledge transfer.","1530-1605;1530-1605","978-1-4577-1925-7978-0-7695-4525","10.1109/HICSS.2012.488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149256","Transactive memory systems;knowledge transfer;IS project complexity;global software outsourcing","Software;Complexity theory;Knowledge transfer;Outsourcing;Uncertainty;Context;Correlation","computational complexity;knowledge management;outsourcing;software management","project complexity;global software outsourcing project teams;transactive memory systems perspective;TMS;knowledge transfer performance;KTP;structural equation modeling method;team knowledge transfer","","1","","39","","","","","IEEE","IEEE Conferences"
"Task Environment Complexity, Global Team Dispersion, Process Capabilities, and Coordination in Software Development","G. Lee; J. A. Espinosa; W. H. DeLone","American University, Washington; American University, Washington; American University, Washington","IEEE Transactions on Software Engineering","","2013","39","12","1753","1771","Software development teams are increasingly global. Team members are separated by multiple boundaries such as geographic location, time zone, culture, and organization, presenting substantial coordination challenges. Global software development becomes even more challenging when user requirements change dynamically. However, little empirical research has investigated how team dispersion across multiple boundaries and user requirements dynamism, which collectively increase task environment complexity, influence team coordination and software development success in the global context. Further, we have a limited understanding of how software process capabilities such as rigor, standardization, agility, and customizability mitigate the negative effects of global team dispersion and user requirements dynamism. To address these important issues, we test a set of relevant hypotheses using field survey data obtained from both project managers and stakeholders. Our results show that global team dispersion and user requirements dynamism have a negative effect on coordination effectiveness. We find that the negative effect of global team dispersion on coordination effectiveness decreases as process standardization increases and that the negative effect of user requirements dynamism on coordination effectiveness decreases as process agility increases. We find that coordination effectiveness has a positive effect on global software development success in terms of both process and product aspects.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583162","Global boundaries;global software development;user requirements dynamism;software process capability;task environment complexity;team coordination;team dispersion","User centered design;Complexity theory;Dispersion;Global communication;Software development;Process capability;Globalization","software development management;team working","task environment complexity;global team dispersion;process capabilities;coordination;software development teams;geographic location;time zone;culture;organization;global software development;user requirements dynamism;rigor capability;standardization capability;agility capability;customizability capability;coordination effectiveness;process aspect;product aspect","","10","","101","","","","","IEEE","IEEE Journals & Magazines"
"A Novel Method to Measure Comprehensive Complexity of Software Based on the Metrics Statistical Model","A. Sepasmoghaddam; H. Rashidi","NA; NA","2010 Fourth UKSim European Symposium on Computer Modeling and Simulation","","2010","","","520","525","Calculating software complexity is one of the most challenging problems in the Software Engineering due to using them in estimating errors, having a landscape of software reliability, approximating costs of software implementation and maintenance, and delivering software with better quality. Most of the recent researches on calculating the software's complexity focus on special directions and goals. This paper presents a novel method for measuring comprehensive complexity of software based on Statistical model evaluation of the existing complexity metrics through modules. To reach this purpose, the amount of comprehensive complexity is achieved for every module by identifying statistical distribution of complexity metric quantities, normalization and their combination. Afterward, the comprehensive complexity of the software is calculated by composition of the module's complexity amounts. This method is applied on some samples of the ""NASA Software Engineering laboratory"" and some of its positive results are presented.","","978-1-4244-9313-5978-0-7695-4308","10.1109/EMS.2010.92","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703738","Software Complexity;Comprehensive Complexity;Module;Exponential Distribution;NASA Metrics Data Program","Complexity theory;Software;Computer languages;NASA;Indexes;Software measurement","software maintenance;software metrics;software quality;software reliability;statistical distributions","software complexity;metrics statistical model;software engineering;software reliability;software implementation;software maintenance;software quality;statistical distribution","","2","","13","","","","","IEEE","IEEE Conferences"
"The analysis of software metrics for design complexity and its impact on reusability","A. P. Singh; P. Tomar","Ajay Kumar Garg Engineering College, Ghaziabad 201001, U.P. India; School of ICT, Gautam Buddha University, Greater Noida 201312, U.P. India","2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)","","2016","","","3808","3812","Reusability and complexity of software are key features of software quality. Such design metrics are considered to have potential for improvement of software quality and developer productivity. This study performs analysis of Object-Oriented design complexity metrics and its relation with reusability. To be precise the paper considers the most cited Chidamber and Kemerer (CK) metric suite. This study provides empirical evidence in support of the role of design metrics specially CK metrics in estimating reusability of software components. In this paper the competence and effectiveness of machine learning regression techniques are also examined. An experiment is performed to analyze comparative study of Multi linear regression, Model Tree M5P, Meta-learning scheme Additive Regression and Isotonic Regression. This experiment is performed by using data values from projects existing in real world. The results indicate that the complexity is considerably associated with reusability of software. For this study the paper uses Weka tool. The paper believes that the results from this study provide significant suggestions for designing high quality software applications using Object-Oriented and Component-Based approach and identifies the better regression algorithm for reusability estimation using complexity metrics.","","978-9-3805-4421-2978-9-3805-4420-5978-1-4673-9417","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724972","Reusability;complexity;regression analysis;object-oriented approach","Decision support systems;Handheld computers","learning (artificial intelligence);object-oriented programming;regression analysis;software metrics;software quality;software reusability;trees (mathematics)","software metrics analysis;software complexity;software quality improvement;developer productivity improvement;object-oriented design complexity metrics;Chidamber-and-Kemerer metric suite;software component reusability estimation;machine learning regression techniques;multilinear regression;model tree MSP;meta-learning scheme;additive regression;isotonic regression;object-oriented approach;component-based approach","","","","23","","","","","IEEE","IEEE Conferences"
"Framework for evaluation and validation of software complexity measures","S. Misra; I. Akman; R. Colomo-Palacios","Atilim University, Ankara, Turkey; Atilim University, Ankara, Turkey; Universidad Carlos III de Madrid, Madrid, Spain","IET Software","","2012","6","4","323","334","This study proposes a framework for the evaluation and validation of software complexity measure. This framework is designed to analyse whether or not software metric qualifies as a measure from different perspectives. Unlike existing frameworks, it takes into account the practical usefulness of the measure and includes all the factors that are important for theoretical and empirical validation including measurement theory. The applicability of the framework is tested by using cognitive functional size measure. The testing process shows that in the same manner the proposed framework can be applied to any software measure. A comparative study with other frameworks has also been performed. The results reflect that the present framework is a better representation of most of the parameters that are required to evaluate and validate a new complexity measure.","1751-8806;1751-8814","","10.1049/iet-sen.2011.0206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6322854","","","computational complexity;software quality","software complexity measurement;software metric;measurement theory;software measurement;software quality","","2","","","","","","","IET","IET Journals & Magazines"
"A Theory of Software Complexity","A. Ghazarian","NA","2015 IEEE/ACM 4th SEMAT Workshop on a General Theory of Software Engineering","","2015","","","29","32","The need for a theory of software complexity to serve as a rigorous, scientific foundation for software engineering has long been recognized. However, unfortunately, the complexity measures proposed thus far have only resulted in rough heuristics and rules of thumb. In this paper, we propose a new information theoretic measure of software complexity that, unlike previous measures, captures the volume of design information in software modules. By providing proof outlines for a number of theorems that collectively represent our current understanding and intuitions about software complexity, we demonstrate that this new, information-based formulation of software complexity is not only capable of explaining our current understanding of software complexity, but also is resilient to the factors that cause inaccuracies in previous measures.","","978-1-4673-7052","10.1109/GTSE.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169392","Software Complexity;Metrics;Theory;Information Volume;Design Decisions;Software Design","Complexity theory;Software measurement;Software systems;Volume measurement;Current measurement","information theory;software architecture;software metrics","software complexity;scientific foundation;software engineering;information theoretic measure;design information;software modules","","3","","8","","","","","IEEE","IEEE Conferences"
"An Empirical Study on the Structural Complexity Introduced by Core and Peripheral Developers in Free Software Projects","A. Terceiro; L. R. Rios; C. Chavez","NA; NA; NA","2010 Brazilian Symposium on Software Engineering","","2010","","","21","29","Background: Several factors may impact the process of software maintenance and evolution of free software projects, including structural complexity and lack of control over its contributors. Structural complexity, an architectural concern, makes software projects more difficult to understand, and consequently more difficult to maintain and evolve. The contributors in a free software project exhibit different levels of participation in the project, and can be categorized as core and peripheral developers. Research aim: This research aims at characterising the changes made to the source code of 7 web server projects written in C with respect to the amount of structural complexity added or removed and the developer level of participation. Method: We performed a observational study with historical data collected from the version control repositories of those projects, recording structural complexity information for each change as well as identifying each change as performed by a core or a peripheral developer. Results and conclusions: We have found that core developers introduce less structural complexity than peripheral developers in general, and that in the case of complexity-reducing activities, core developers remove more structural complexity than peripheral developers. These results demonstrate the importance of having a stable and healthy core team to the sustainability of free software projects.","","978-1-4244-8917-6978-0-7695-4273","10.1109/SBES.2010.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631694","Empirical Software Engineering;Structural Complexity;Free Software;Open Source software;FLOSS;Core and Periphery","Software;Complexity theory;Web server;Couplings;Aging;Measurement","file servers;public domain software;software maintenance","free software project;structural complexity;peripheral developers;core developers;software maintenance;source code;Web server","","9","","30","","","","","IEEE","IEEE Conferences"
"Measuring the constraint complexity of automotive embedded software systems","M. Garg; R. Lai","Department of Computer Science &amp; Computer Engineering, La Trobe University, Melbourne, Australia; Department of Computer Science &amp; Computer Engineering, La Trobe University, Melbourne, Australia","2014 International Conference on Data and Software Engineering (ICODSE)","","2014","","","1","6","The rapid growth of software-based functionalities has made automotive Electronic Control Units (ECUs) significantly complex. Factors affecting software complexity of components embedded in an ECU depend not only on the interface and interaction properties, but also on the added constraints associated with their usage and stringent timing requirements. Traditional constraint complexity measures are not adequate for embedded real-time systems as they do not yet sufficiently provide a measure of the timing constraints described by AUTOSAR languages. This paper presents a method for measuring the structural and timing constraint complexity of automotive embedded software systems at the specification level.","","978-1-4799-7996-7978-1-4799-8175-5978-1-4799-7995","10.1109/ICODSE.2014.7062701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062701","Automotive embedded software system;timing dependency;constraint complexity;software measure","Software;Complexity theory;Timing;Unified modeling language;Automotive engineering;Real-time systems;Software measurement","automotive engineering;embedded systems;formal specification;software metrics","constraint complexity;automotive embedded software systems;software-based functionalities;electronic control units;ECU;software complexity;stringent timing requirements;AUTOSAR languages;specification level","","1","","50","","","","","IEEE","IEEE Conferences"
"Software Application to Evaluate the Complexity Theory of the RSA and Elliptic Curves Asymmetric Algorithms","W. Ayala; W. Fuertes; F. Galárraga; H. Aules; T. Toulkeridis","NA; NA; NA; NA; NA","2017 International Conference on Software Security and Assurance (ICSSA)","","2017","","","87","93","In the current study, we performed a quantitative evaluation of digital signature algorithms between the asymmetric cryptographic scheme RSA (Rivest, Shamir, and Adleman) and ECDSA (Elliptic Curve Digital Signature Algorithm) with the purpose to limit or block illegal digital interferences. Mathematical foundations of asymmetric digital signature algorithms have been analyzed, giving a special focus to the mentioned algorithms. RSA and ECDSA have been coded in Java Development Environment, with their respective libraries. In addition, a Java software application has been designed and implemented with the respective algorithms of key generation and verification. We have used Scrum by articulating each of its phases with the architecture and extensible security elements of the Java platform. Thus, all of these processes have been applied, in order to establish the RSA or ECDSA with the most suitable characteristics for the performance and confidentiality of transmitted information. The own standards of asymmetric digital signature algorithms and elliptic curves have been taken into account, so that the comparison appears adequate and produces data that, besides of being measurable, are also sustainable. The results obtained have been visualized through a statistical process as products of the determination of the response times obtained during this process. To verify these results, we have used a mathematical validation, based on the Least Squares method.","","978-1-5386-4808-7978-1-5386-4809","10.1109/ICSSA.2017.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392624","Cryptography;Digital signature;RSA;ECDSA","Cryptography;Digital signatures;Java;Software algorithms;Elliptic curves;Software","digital signatures;Java;public key cryptography","ECDSA;Java Development Environment;Java software application;asymmetric digital signature algorithms;elliptic curves;quantitative evaluation;asymmetric cryptographic scheme RSA;Elliptic Curve Digital Signature Algorithm;illegal digital interferences;Least Squares method;complexity theory;elliptic curves asymmetric algorithms","","","","30","","","","","IEEE","IEEE Conferences"
"Using evolutionary computation to shed light on the effect of scale and complexity on object-oriented software design","C. L. Simons; J. Smith","Department of Computer Science &amp; Creative Technology, University of the West of England, Bristol BS16 1QY United Kingdom; Department of Computer Science &amp; Creative Technology, University of the West of England, Bristol BS16 1QY United Kingdom","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2014","","","441","446","Early lifecycle software design is an intensely human activity in which design scale and complexity can place a high cognitive load on the software designer. Recently, the use of evolutionary search has been suggested to yield insights in the nature of software engineering problems generally, and so we have applied dynamic evolutionary computation using self-adaptive mutation to the object-oriented software design search space. Using three design problem instances of varying scale and complexity, initial investigations of the discrete search landscape reveal a redundancy in genotype-to-phenotype mapping enabling flexible and effective exploration. In further experiments, mutation probabilities and population diversity are observed to significantly increase in the face of increasing problem scale, but not for increasing complexity (in problems of the same scale). Based on these findings, we conclude that design problem scale rather than complexity has an effect on the software design process, emphasizing the role of decomposition as a design technique.","1062-922X","978-1-4799-3840","10.1109/SMC.2014.6973947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973947","software design;evolutionary computation","Complexity theory;Software design;Sociology;Statistics;Economic indicators;Search problems;Evolutionary computation","computational complexity;evolutionary computation;object-oriented methods;probability;software engineering","dynamic evolutionary computation;scale effect;complexity effect;object-oriented software design search space;early lifecycle software design;cognitive load;software engineering problems;genotype-to-phenotype mapping;mutation probabilities;population diversity","","1","","14","","","","","IEEE","IEEE Conferences"
"Software Product Complexity Estimation Using Grey Measurement","S. Y. Tastekin; Y. M. Erten; S. Bilgen","NA; NA; NA","2013 39th Euromicro Conference on Software Engineering and Advanced Applications","","2013","","","308","312","A new understanding of product complexity is described as a determinant of software development time. A complexity estimation method for software is proposed taking different factors into consideration. Grey measurement technique is used for complexity estimation and complexity measurements of five software products are presented. The results of these measurement show that grey complexity measurement results are linearly related to the number of main functions of software products.","1089-6503;2376-9505","978-0-7695-5091","10.1109/SEAA.2013.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619526","software product complexity;grey measurement;product development time estimation","Complexity theory;Software;Software measurement;Estimation;Product development;Size measurement;Time measurement","grey systems;software development management;software metrics;software prototyping","software product complexity estimation method;software development time;grey complexity measurement technique;software product function","","","","24","","","","","IEEE","IEEE Conferences"
"Model-Driven Software Engineering in Robotics: Models Are Designed to Use the Relevant Things, Thereby Reducing the Complexity and Cost in the Field of Robotics","D. Brugali","School of Engineering, Universita Degli Studi di Bergamo, Bergamo BG, 24129, Italy","IEEE Robotics & Automation Magazine","","2015","22","3","155","166","A model is an abstract representation of a real system or phenomenon [1]. The idea of a model is to capture important properties of reality and to eglect irrelevant details. The properties that are relevant and that can be neglected depend on the purpose of creating a model. A model can make a particular system or phenomenon easier to understand, quantify, visualize, simulate, or predict.","1070-9932;1558-223X","","10.1109/MRA.2015.2452201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7254324","","Tutorials;Software development;Atmospheric modeling;Robot sensing systems;Ports (Computers);Analytical models;Unified modeling language","formal specification;robot programming","model-driven software engineering;robotics;abstract representation","","9","","53","","","","","IEEE","IEEE Journals & Magazines"
"Software-based performance and complexity analysis for the design of embedded classification systems","M. Ring; U. Jensen; P. Kugler; B. Eskofier","Digital Sports Group, Pattern Recognition Lab, University of Erlangen-Nuremberg, Germany; Digital Sports Group, Pattern Recognition Lab, University of Erlangen-Nuremberg, Germany; Digital Sports Group, Pattern Recognition Lab, University of Erlangen-Nuremberg, Germany; Digital Sports Group, Pattern Recognition Lab, University of Erlangen-Nuremberg, Germany","Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)","","2012","","","2266","2269","Embedded microcontrollers are employed in an increasing number of applications as a target for the implementation of classification systems. This is true for example for the fields of sports, automotive and medical engineering. However, important challenges arise when implementing classification systems on embedded microcontrollers, which is mainly due to limited hardware resources. In this paper, we present a solution to the two main challenges, namely obtaining a classification system with low computational complexity and at the same time high classification accuracy. For the first challenge, we propose complexity measures on the mathematical operation and parameter level, because the abstraction level of the commonly used Landau notation is too high in the context of embedded system implementation. For the second challenge, we present a software toolbox that trains different classification systems, compares their classification rate, and finally analyzes the complexity of the trained system. To give an impression of the importance of such complexity measures when dealing with limited hardware resources, we present the example analysis of the popular Pima Indians Diabetes data set, where considerable complexity differences between classification systems were revealed.","1051-4651;1051-4651","978-4-9906441-0-9978-1-4673-2216","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460616","","Support vector machines;Algorithm design and analysis;Accuracy;Complexity theory;Memory management;Software;Hardware","computational complexity;embedded systems;microcontrollers;pattern classification;resource allocation","software-based performance;complexity analysis;embedded classification system design;embedded microcontrollers;hardware resources;computational complexity;classification accuracy;mathematical operation;parameter level;abstraction level;Landau notation;embedded system implementation;software toolbox;Pima Indian diabetes data set","","2","","15","","","","","IEEE","IEEE Conferences"
"Estimating encoding complexity of a real-time embedded software HEVC codec","A. Mercat; W. Hamidouche; M. Pelcat; D. Menard","IETR, INSA Rennes, CNRS UMR 6164, UEB, France; IETR, INSA Rennes, CNRS UMR 6164, UEB, France; IETR, INSA Rennes, CNRS UMR 6164, UEB, France; IETR, INSA Rennes, CNRS UMR 6164, UEB, France","2016 Conference on Design and Architectures for Signal and Image Processing (DASIP)","","2016","","","26","33","The High Efficiency Video Coding (HEVC) standard provides up to 40% bitrate savings compared to the state-of-art H.264/AVC standard for the same perceptual video quality. Power consumption constraints represent a serious challenge for embedded applications based on a software design. A large number of systems are likely to integrate the HEVC codec in the long run and will need to be energy aware. In this context, we carry out a complexity study of the HEVC coding trees encoding process. This study shows that the complexity of encoding a Coding Unit (CU) of a given size has a non trivial probability density shape and thus can hardly be predicted with accuracy. However, we propose a model that linearly links the ratios between the complexities of coarse-grain and lower-grain CU encodings with a precision error under 6%. This model is valid for a wide range of video contents coded in Intra configurations at different bitrates. This information is useful to control encoder energy during the encoding process on battery limited devices.","","979-1-0922-7915-3978-1-5090-3085","10.1109/DASIP.2016.7853792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7853792","Complexity prediction;HEVC;quad-tree partitioning;embedded platforms","Encoding;Mathematical model;Computational modeling;Real-time systems;Computational complexity;Video coding","computational complexity;embedded systems;power aware computing;video coding","encoding complexity;real-time embedded software;HEVC codec;high efficiency video coding standard;bitrate savings;perceptual video quality;power consumption constraints;software design;embedded applications;HEVC coding trees;coding unit;CU;probability density shape;coarse-grain CU encodings;lower-grain CU encodings;precision error;video contents;intra configurations;encoder energy;battery limited devices","","","","25","","","","","IEEE","IEEE Conferences"
"Comparing computational thinking development assessment scores with software complexity metrics","J. Moreno-León; G. Robles; M. Román-González","Programamos.es, Sevilla, Spain; Universidad Rey Juan Carlos, Madrid, Spain; Universidad Nacional de Educación a Distancia, Madrid, Spain","2016 IEEE Global Engineering Education Conference (EDUCON)","","2016","","","1040","1045","The development of computational thinking skills through computer programming is a major topic in education, as governments around the world are introducing these skills in the school curriculum. In consequence, educators and students are facing this discipline for the first time. Although there are many technologies that assist teachers and learners in the learning of this competence, there is a lack of tools that support them in the assessment tasks. This paper compares the computational thinking score provided by Dr. Scratch, a free/libre/open source software assessment tool for Scratch, with McCabe's Cyclomatic Complexity and Halstead's metrics, two classic software engineering metrics that are globally recognized as a valid measurement for the complexity of a software system. The findings, which prove positive, significant, moderate to strong correlations between them, could be therefore considered as a validation of the complexity assessment process of Dr. Scratch.","2165-9567","978-1-4673-8633-3978-1-4673-8632","10.1109/EDUCON.2016.7474681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474681","computational thinking;Scratch;programming;assessment tools;Dr. Scratch;software metrics;complexity","Measurement;Complexity theory;Correlation;Vocabulary;Programming profession;Fitting","computer science education;programming;public domain software;software metrics;teaching","computational thinking development assessment score;software complexity metrics;computational thinking skills;computer programming;education;school curriculum;teaching;learning;Dr. Scratch;free software;libre software;open source software assessment tool;McCabe Cyclomatic Complexity metrics;Halstead metrics;software engineering metrics;complexity assessment process","","8","","25","","","","","IEEE","IEEE Conferences"
"A self-adaptation framework for dealing with the complexities of software changes","J. Wan; Q. Li; L. Wang; L. He; Y. Li","Software Engineering Institute, Xidian University, Xi'an Province, China; Software Engineering Institute, Xidian University, Xi'an Province, China; Software Engineering Institute, Xidian University, Xi'an Province, China; Software Engineering Institute, Xidian University, Xi'an Province, China; Software Engineering Institute, Xidian University, Xi'an Province, China","2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2017","","","521","524","Software Self-adaption (SA) is a promising technology to reduce the cost of software maintenance. However, the complexities of software changes such as various and producing different effects, interrelated and occurring in an unpredictable context challenge the SA. The current methods may be insufficient to provide the required self-adaptation abilities to handle all the existent complexities of changes. Thus, this paper presents a self-adaptation framework which can provide a multi-agent system for self-adaptation control to equip software system with the required adaptation abilities. we employ the hybrid control mode and construct a two-layer MAPE control structure to deal with changes hierarchically. Multi-Objective Evolutionary Algorithm and Reinforcement Learning are applied to plan an adequate strategy for these changes. Finally, in order to validate the framework, we exemplify these ideas with a meta-Search system and confirm the required self-adaptive ability.","2327-0594","978-1-5386-0497-7978-1-5386-0496-0978-1-5386-0498","10.1109/ICSESS.2017.8342969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342969","Self-adaptive systems;Search-based software engineering;Multi-agent systems;Reinforcement Learning;Multi-Objective Evolutionary Algorithm","Planning;Google;Learning (artificial intelligence);Servers;Task analysis;Complexity theory;Software","evolutionary computation;learning (artificial intelligence);multi-agent systems;software maintenance","SA;Multi-Objective Evolutionary Algorithm;Reinforcement Learning;meta-Search system;software maintenance;software Self-adaption;required self-adaptive ability;two-layer MAPE control structure;hybrid control mode;required adaptation abilities;software system;self-adaptation control;multiagent system;self-adaptation framework;existent complexities;required self-adaptation abilities;current methods;unpredictable context challenge;software changes","","","","10","","","","","IEEE","IEEE Conferences"
"Multi agent paradigm used to complexity measure for perfective software maintenance","A. Mishra; V. Srivastava","Computer Science and Engineering Department, Thapar University, Patiala, Punjab, India; Computer Engineering Department, IIT-BHU, Varanasi, UP, India","Asia-Pacific World Congress on Computer Science and Engineering","","2014","","","1","9","Agent is a computing entity which mimics the behavior of a human being in problem solving strategy. Multi agents systems are the group of agents which have different tasks and work in cooperation, coordination and in communication to each other. Apart from wide applications in software engineering tasks, design, development and testing, it has a vital role in software maintenance also. Few effective attempts have been made in this direction. We made an attempt to develop multi agent system for perfective maintenance using JADE-architecture. These results will helps to assess and estimate the complexity of change in a particular method, particular class and particular file with their effect to other set of methods, classes, and files (clusters). This knowledge can be deployed for adaptive maintenance in which a method or class or file are being changed or replaced. We are in process of developing and adding two more agents one adaptive and another evaluating in the context of software maintenance.","","978-1-4799-1954-3978-1-4799-1955","10.1109/APWCCSE.2014.7053834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053834","Data Mining;Agents;Source code;Cognitive measure","Complexity theory;Maintenance engineering;Data mining;Principal component analysis;Software maintenance;Databases;Computer architecture","multi-agent systems;problem solving;software architecture;software maintenance","multi agent paradigm;complexity measure;perfective software maintenance;human being;problem solving strategy;multi agents systems;JADE-architecture;adaptive maintenance","","1","","13","","","","","IEEE","IEEE Conferences"
"Software complexity metric-based defect classification using FARM with preprocessing step CFS and SMOTE a preliminary study","M. F. Naufal; S. Rochimah","Informatics Department, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Informatics Department, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2015 International Conference on Information Technology Systems and Innovation (ICITSI)","","2015","","","1","6","One criteria for assessing the software quality is ensuring that there is no defect in the software which is being developed. Software defect classification can be used to prevent software defects. More earlier software defects are detected in the software life cycle, it will minimize the software development costs. This study proposes a software defect classification using Fuzzy Association Rule Mining (FARM) based on complexity metrics. However, not all complexity metrics affect on software defect, therefore it requires metrics selection process using Correlation-based Feature Selection (CFS) so it can increase the classification performance. This study will conduct experiments on the NASA MDP open source dataset that is publicly accessible on the PROMISE repository. This datasets contain history log of software defects based on software complexity metric. In NASA MDP dataset the data distribution between defective and not defective modules are not balanced. It is called class imbalanced problem. Class imbalance problem can affect on classification performance. It needs a technique to solve this problem using oversampling method. Synthetic Minority Oversampling Technique (SMOTE) is used in this study as oversampling method. With the advantages possessed by FARM in learning on dataset which has quantitative data attribute and combined with the software complexity metrics selection process using CFS and oversampling using SMOTE, this method is expected has a better performance than the previous methods.","","978-1-4673-6664-9978-1-4673-6663-2978-1-4673-6662","10.1109/ICITSI.2015.7437685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437685","Bugs;Defect;Fault;Fuzzy Association Rule Mining;Machine Learning;Software Defect Classification;Correlation-based Feature Selection;Synthetic Minority Oversampling Technique","Software;Complexity theory;NASA;Data mining;Training;Software metrics","aerospace computing;data mining;feature selection;fuzzy set theory;learning (artificial intelligence);program testing;sampling methods;software metrics;software quality","software complexity metric-based defect classification;FARM;CFS;SMOTE;software quality assessment;software life cycle;software development cost minimization;fuzzy association rule mining;correlation-based feature selection;NASA-MDP open source dataset;PROMISE repository;history log;data distribution;defective module;nondefective module;class imbalanced problem;synthetic minority oversampling technique;learning;quantitative data attribute;software complexity metric selection process","","1","","16","","","","","IEEE","IEEE Conferences"
"Measuring complexity, effectiveness and efficiency in software course projects","W. P&#x0E1;dua","Federal University of Minas Gerais, Nova Lima -- MG -- Brazil","2010 ACM/IEEE 32nd International Conference on Software Engineering","","2010","1","","545","554","This paper discusses results achieved in measuring complexity, effectiveness and efficiency, in a series of related software course projects, spanning a period of seven years. We focus on how the complexity of those projects was measured, and how the success of the students in effectively and efficiently taming that complexity was assessed. This required defining, collecting, validating and analyzing several indicators of size, effort and quality; their rationales, advantages and limitations are discussed. The resulting findings helped to improve the process itself.","1558-1225;0270-5257","978-1-60558-719","10.1145/1806799.1806878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062058","software complexity","Complexity theory;Appraisal;Java;Standards;Software;Size measurement;Materials","computer science education;software metrics","software course project;software complexity measurement;software effectiveness measurement;software efficiency measurement","","7","","20","","","","","IEEE","IEEE Conferences"
"An Empirical Study on the Estimation of Size and Complexity of Software Applications with Function Points Analysis","L. M. Alves; S. Oliveira; P. Ribeiro; R. J. Machado","NA; NA; NA; NA","2014 14th International Conference on Computational Science and Its Applications","","2014","","","27","34","Empirical studies are important in software engineering to evaluate new tools, techniques, methods and technologies in a structured way before they are introduced in the industrial (real) software process. Perform empirical studies in a real context is very difficult due to various obstacles. An interesting alternative is perform empirical studies in an educational context using students as subjects and share the results with the academia and the industry. This paper describes a case study with two teams that developed a software system (Web application) for a real customer. In this study we used a model based on Function Points Analysis (FPA) to estimate the size and complexity of software system.","","978-1-4799-4264","10.1109/ICCSA.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976659","empirical studies;software engineering management;software engineering process;software quality;function point;function points analysis","Complexity theory;Estimation;Software measurement;Software systems;Software engineering","software cost estimation;software metrics;software quality","software size estimation;software complexity;function points analysis;software engineering;Web application","","3","","41","","","","","IEEE","IEEE Conferences"
"Design and performance analysis of a low complexity digital clock recovery algorithm for software-defined radio applications","A. Montazeri; K. Kiasaleh","Telecommunications at the University of Texas at Dallas; Telecommunications at the University of Texas at Dallas","IEEE Transactions on Consumer Electronics","","2010","56","3","1258","1263","In this paper, we propose and study a low-complexity digital clock recovery scheme suitable for implementation on programmable platforms, such as digital signal processing (DSP) or field-programmable gate-array (FPGA) platforms. Performance is established in terms of mean-square timing error and the required computational complexity, a key factor in the successful implementation of efficient software-defined radios (SDR). It is shown that the proposed algorithm achieves a superior performance when compared with the existing algorithms for a wide range of operating parameters. To assess complexity in terms of resource utilization, the FPGA platform is used to study the proposed algorithm along with other well-known algorithms.","0098-3063;1558-4127","","10.1109/TCE.2010.5606256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606256","Synchronization, Symbols, Pulse Amplitude Modulation, Software Defined Radio, FPGA","Approximation methods;Signal to noise ratio;Field programmable gate arrays;Synchronization;Approximation algorithms;Clocks","clocks;field programmable gate arrays;least mean squares methods;logic design;signal processing;software radio","low complexity digital clock recovery algorithm;software-defined radio applications;programmable platforms;digital signal processing;field-programmable gate-array platforms;mean-square timing error;computational complexity;operating parameters;resource utilization;FPGA platform","","3","","20","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Parity Check Code for Futuristic Wireless Networks Applications","S. A. Alabady; F. Al-Turjman","Computer Engineering Department, University of Mosul, Mosul, Iraq; Computer Engineering Department, Antalya Bilim University, Antalya, Turkey","IEEE Access","","2018","6","","18398","18407","The Internet of Things refers to the aptitude of remotely connecting and monitoring anything, anytime, and anywhere via futuristic wireless networks. Due to the unreliable wireless links, broadcast nature of wireless transmissions, interference and noisy transmission channels, frequent topology changes, and the various quality of wireless channel, there are challenges in providing high data rate service, high throughput, high packet delivery ratio, low end-to-end delay, and reliable services. In wireless network and real time application systems, low complexity and shorter codeword length in channel coding scheme are preferred. Consequently, in order to address these challenges, we propose a novel error detection and correction codes called the low-complexity parity-check (LCPC) codes with short codeword lengths for futuristic wireless networks applications. The proposed codes have less complexity and lower memory requirement in comparison to turbo and low-density parity-check (LDPC) codes. Simulation results demonstrated that the proposed LCPC codes outperform the Hamming and Reed-Solomon codes, in addition to the renowned LDPC codes. It offers up to 3-dB coding gain.","2169-3536","","10.1109/ACCESS.2018.2818740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8331070","Error detection and correction;LCPC;LDPC;short codeword;Internet of Things","Parity check codes;Encoding;Complexity theory;Wireless networks;Decoding;Error correction codes","channel coding;error correction codes;error detection codes;parity check codes;wireless channels","low complexity parity check code;futuristic wireless networks applications;unreliable wireless links;wireless transmissions;noisy transmission channels;wireless channel;high data rate service;high packet delivery ratio;low end-to-end delay;wireless network;channel coding scheme;low-complexity parity-check codes;low-density parity-check codes;LCPC codes;renowned LDPC codes;error detection codes;error correction codes;Internet of Things;real time application systems;Hamming codes;Reed-Solomon codes","","","","36","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Recursive Approach Toward Code-Domain NOMA for Massive Communications","M. V. Jamali; H. Mahdavifar","Electrical Engineering and Computer Science Department, University of Michigan, Ann Arbor, MI, 48109, USA; Electrical Engineering and Computer Science Department, University of Michigan, Ann Arbor, MI, 48109, USA","2018 IEEE Global Communications Conference (GLOBECOM)","","2018","","","1","6","Nonorthogonal multiple access (NOMA) is a promising technology to meet the demands of the next generation wireless networks on massive connectivity, high throughput and reliability, improved fairness, and low latency. In this context, code-domain NOMA which attempts to serve K users in M ≤ K orthogonal resource blocks, using a pattern matrix, is of utmost interest. However, extending the pattern matrix dimensions severely increases the detection complexity and hampers on the significant advantages that can be achieved using large pattern matrices. In this paper, we propose a novel approach toward code-domain NOMA which factorizes the pattern matrix as the Kronecker product of some other factor matrices each with a smaller dimension. Therefore, both the pattern matrix design at the transmitter side and the mixed symbols' detection at the receiver side can be performed over much smaller dimensions and with a remarkably reduced complexity and latency. As a consequence, the system can significantly be overloaded to effectively support the requirements of the next generation wireless networks without any considerable increase on the system complexity.","2576-6813;1930-529X","978-1-5386-4727-1978-1-5386-6976-1978-1-5386-4728","10.1109/GLOCOM.2018.8647982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8647982","","NOMA;Complexity theory;Receivers;Uplink;Downlink;Throughput;Reliability","matrix algebra;multi-access systems;next generation networks;radio networks;radio receivers;radio transmitters;resource allocation;signal detection;telecommunication network reliability","pattern matrix dimensions;detection complexity;code-domain NOMA;pattern matrix design;low-complexity recursive approach;massive communications;nonorthogonal multiple access;massive connectivity;high throughput;reliability;next generation wireless networks;Kronecker product;transmitter side;mixed symbols detection;receiver side","","1","","21","","","","","IEEE","IEEE Conferences"
"Digital Code-Division Multiplexing Channel Aggregation for Mobile Fronthaul Architecture With Low Complexity","H. Li; Q. Yang; S. Fu; M. Luo; X. Li; Z. He; P. Jiang; Y. Liu; S. Yu","Wuhan National Laboratory for Optoelectronics & School of Optical and Electronic Information, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Optical Communication Technologies and Networks, Wuhan Research Institute of Posts & Telecommunications, Wuhan, China; Wuhan National Laboratory for Optoelectronics & School of Optical and Electronic Information, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics & School of Optical and Electronic Information, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Optical Communication Technologies and Networks, Wuhan Research Institute of Posts & Telecommunications, Wuhan, China; State Key Laboratory of Optical Communication Technologies and Networks, Wuhan Research Institute of Posts & Telecommunications, Wuhan, China; Wuhan Hongxin Telecommunication Technologies Co., Ltd., Wuhan, China; Wuhan Hongxin Telecommunication Technologies Co., Ltd., Wuhan, China; Wuhan National Laboratory for Optoelectronics & School of Optical and Electronic Information, Huazhong University of Science and Technology, Wuhan, China","IEEE Photonics Journal","","2018","10","2","1","10","A low-complexity mobile fronthaul architecture via digital code-division multiplexing (CDM) is proposed to enable channel aggregation of 4G-LTE signals. In comparison with traditional frequency division multiplexing based aggregation scheme, the fast Fourier transformation/inverse fast Fourier transformation operations are replaced by simple sign selection and addition, leading to the significant reduction of computational complexity. Moreover, synchronous transmission of both the I/Q waveforms of wireless signals and the control words (CWs) used for the purpose of control and management using the CDM approach is also presented to be compliant with the common public radio interface (CPRI). In a proof-of-concept experiment, we demonstrate the transmission of 48 × 20 MHz LTE signals with CPRI equivalent data rate of 59 Gb/s, achieving an average error vector magnitude (EVM) of ~3.6% and ~4.3% after 5 and 20 km transmission over standard single-mode fiber (SSMF), respectively. Furthermore, we successfully demonstrate the transmission of 32 × 20 MHz LTE signals together with CPRI-compliant CWs, corresponding to CPRI-equivalent data rate of 39.32 Gb/s, only using single optical wavelength channel with analog bandwidth of ~1.96 GHz. After transmission over 5 km SSMF, CWs can be error-free recovered while the LTE signals are recovered with an EVM of ~3.6%.","1943-0655;1943-0647","","10.1109/JPHOT.2017.2751538","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031892","Mobile fronthaul;digital code-division multiplexing (CDM);control words (CWs)","Frequency division multiplexing;Mobile communication;Digital signal processing;Quadrature amplitude modulation;Optical fiber communication;Long Term Evolution;Bandwidth","code division multiple access;fast Fourier transforms;Long Term Evolution;OFDM modulation;optical modulation;radio-over-fibre;wavelength division multiplexing","CPRI-compliant CWs;single optical wavelength channel;digital code-division multiplexing channel aggregation;low-complexity mobile fronthaul architecture;4G-LTE signals;traditional frequency division;fast Fourier transformation/inverse fast Fourier transformation operations;computational complexity;synchronous transmission;wireless signals;control words;management;CDM approach;common public radio interface;average error vector magnitude;sign selection;equivalent data rate;sign addition","","1","","19","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Method for Channel-Dependent Construction of Polar Codes Using Partial Order","B. Feng; R. Liu; B. Dai; K. Tian; H. Sun","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","IEEE Access","","2018","6","","67404","67414","In this paper, a novel low-complexity method for channel-dependent construction of polar codes is designed by exploiting the partial order (PO) relation to eliminate redundant computations. The proposed method selects the information set with a threshold, thus avoiding the reliability computation of unnecessary synthetic channels and the cost of sorting. We first introduce two key iterative strategies based on the PO relation of the synthetic channels. Then, we analyze the iteration process of the proposed method through a binary tree and a linear table structure and further derive the theoretical complexity of the proposed method. We validate the theoretical analysis through simulation results. Finally, considering the practical application, we propose a suboptimal simplified version of the proposed method to significantly reduce the complexity. Compared with the sorting method, the proposed simplified method needs only at most 10% of the comparison numbers in the process of the information set selection with negligible performance loss under the cyclic redundancy check-aided successive cancellation list decoding.","2169-3536","","10.1109/ACCESS.2018.2879376","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528435","Polar codes;code construction;partial order;low-complexity","Sorting;Reliability theory;Computational complexity;Channel capacity;Reliability engineering","","","","","","22","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Syndrome-Based Decoding Algorithm Applied to Block Turbo Codes","B. Ahn; S. Yoon; J. Heo","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Access","","2018","6","","26693","26706","This paper presents a technique for reducing the decoding complexity of block turbo code with an extended Hamming code as a component code. In conventional decoding algorithms, when an input vector has a zero syndrome, complexity can be reduced by using the hard-input soft-output (HISO) algorithm. Although sufficient error correction can be achieved using hard decision decoding (HDD) of a component code, conventional methods have used the soft-input soft-output (SISO) algorithm for input vectors with a single error. However, when HDD is applied to all input vectors in which the syndrome is detected as a single error, performance loss occurs owing to the occasional presence of input vectors with triple errors. To solve this problem, we used two criteria for distinguishing between instances of single and triple errors. We maximized the applied rates of the HDD-based HISO algorithm depending on whether the criteria were satisfied. The SISO algorithm was applied when the two criteria were not met. In this case, the number of HDD usages can be reduced to half by removing duplicates or unnecessary candidate codewords. Simulation results show that the proposed algorithm can considerably reduce decoding complexity without performance loss compared with conventional algorithms.","2169-3536","","10.1109/ACCESS.2018.2829087","MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program; IITP (Institute for Information and Communications Technology Promotion); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8350318","Block turbo code (BTC);soft-input soft-output (SISO);hard-input soft-output (HISO);hard decision decoding (HDD);extended Hamming code","Decoding;Complexity theory;Iterative decoding;Euclidean distance;Degradation;Turbo codes","block codes;decoding;error correction codes;Hamming codes;turbo codes;vectors","component code;conventional decoding algorithms;input vector;zero syndrome;hard decision decoding;HDD;single error;triple errors;HISO algorithm;SISO algorithm;low complexity syndrome;block turbo code;extended Hamming code;error correction;hard-input soft-output algorithm;decoding complexity reduction","","","","29","","","","","IEEE","IEEE Journals & Magazines"
"Bandwidth Overhead-Free Data Reconstruction Scheme for Distributed Storage Code With Low Decoding Complexity","M. Dai; X. Wang; H. Wang; X. Lin; B. Chen","Shenzhen Key Lab of Advanced Communication and Information Processing, Shenzhen Key Lab of Media Security, College of Information Engineering, Shenzhen University, Guangdong, China; Shenzhen Key Lab of Advanced Communication and Information Processing, Shenzhen Key Lab of Media Security, College of Information Engineering, Shenzhen University, Guangdong, China; Shenzhen Key Lab of Advanced Communication and Information Processing, Shenzhen Key Lab of Media Security, College of Information Engineering, Shenzhen University, Guangdong, China; Shenzhen Key Lab of Advanced Communication and Information Processing, Shenzhen Key Lab of Media Security, College of Information Engineering, Shenzhen University, Guangdong, China; Shenzhen Key Lab of Advanced Communication and Information Processing, Shenzhen Key Lab of Media Security, College of Information Engineering, Shenzhen University, Guangdong, China","IEEE Access","","2017","5","","6824","6832","The (n, k) combination property (CP) is defined as follows: k source packets are mapped into n ≥ k packets and any k out of these n packets are able to recover the information of the original k packets. This (n, k) CP is extensively needed by cloud storage service providers. Reed-Solomon (RS) codes possess CP at the cost of high encoding and decoding complexity for two reasons: operation over a large-size finite field and time-consuming matrix inversion operation. By operating within the binary field and by allowing only zigzag decoding at the decoder, binary zigzag decoding that possesses CP lowers the decoding complexity significantly. The drawback is that storage room overhead is needed. Corresponding to this storage room overhead, in the data reconstruction process, intuitively fetching k whole stored packets will consume overhead bandwidth. In this paper, a data reconstruction scheme that is optimal in terms of bandwidth consumption is designed, where optimal means the bandwidth consumption is equal to the volume of data to be reconstructed, namely, no overhead bandwidth is needed. To do that, a universal method of fetching sub-packet is proposed, and its corresponding decoding method is also designed.","2169-3536","","10.1109/ACCESS.2017.2699170","Natural Science Foundation of China; Specialized Research Fund for the Doctoral Program of Higher Education from the Ministry of Education; Key Project of Department of Education of Guangdong Province, from Guangdong Natural Science Foundation; Foundation of Shenzhen City; Natural Science Foundation of Shenzhen University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913577","Distributed storage;network code;zigzag decoding;data reconstruction bandwidth","Decoding;Bandwidth;Systematics;Complexity theory;Encoding;Distributed databases;Indexes","binary codes;cloud computing;decoding;network coding;Reed-Solomon codes","bandwidth overhead-free data reconstruction scheme;distributed storage code;(n, k) combination property;(n, k) CP;source packet mapping;cloud storage service providers;Reed-Solomon codes;RS codes;decoding complexity;binary zigzag decoding;storage room overhead","","","","35","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Low-Complexity Decoding of CCSDS Reed–Solomon Codes Based on Justesen’s Concatenation","R. S. Elagooz; A. Mahran; S. Gasser; M. Aboul-Dahab","Electronics and Communications Department, Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Avionics Department, Military Technical College, Cairo, Egypt; Electronics and Communications Department, Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt; Electronics and Communications Department, Arab Academy for Science, Technology and Maritime Transport, Cairo, Egypt","IEEE Access","","2019","7","","49596","49603","Forward error correction (FEC) is a key capability in modern satellite communications that provide the system designer with the needed flexibility to comply with the different applications' requirements. Reed-Solomon (RS) codes are well known for their ability to optimize between the system power, bandwidth, data rate, and the quality of service. This paper introduces an efficient decoding scheme for decoding the RS codes adhering to the Consultative Committee for Space Data Systems (CCSDS) standards based on Justesen's construction of concatenation. To maintain the standard output size, the proposed scheme first encodes every m - 1 bits using the single-parity-check (SPC) code, while the RS code encodes K SPC codewords into N symbols that are of the same size as CCSDS standard. Decoding on the inner SPC code is based on maximum-likelihood decoding Kaneko algorithm, while for the proposed coding scheme, the reduced test-pattern Chase algorithm is adapted for decoding the outer RS code. The simulation results show the coding gains of 1.4 and 7 dB compared with the algebraic decoding of RS codes over the AWGN and Rayleigh fading channels, respectively. Moreover, the adopted reduced test-pattern Chase algorithm for decoding the RS code achieves an overall complexity reduction of 40% compared with the conventional Chase decoding algorithm.","2169-3536","","10.1109/ACCESS.2019.2905966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8692348","CCSDS;chase algorithm;concatenated codes;Justesen code;Reed-Solomon code;single-parity-check","Maximum likelihood decoding;Encoding;Reliability;Standards;Concatenated codes;Error probability","algebraic codes;AWGN channels;channel coding;concatenated codes;error correction codes;forward error correction;maximum likelihood decoding;parity check codes;Rayleigh channels;Reed-Solomon codes;satellite communication","system power;efficient decoding scheme;standard output size;single-parity-check code;CCSDS standard;inner SPC code;maximum-likelihood decoding Kaneko algorithm;coding scheme;outer RS code;coding gains;algebraic decoding;conventional Chase decoding algorithm;efficient low-complexity decoding;Justesen concatenation;CCSDS Reed-Solomon codes;forward error correction;FEC;satellite communications;quality of service;data rate;RS codes;reduced test-pattern Chase algorithm;Consultative Committee for Space Data Systems standards;Rayleigh fading channels;AWGN fading channels","","","","19","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity multi-standard variable length coding decoder using tree-based partition and classification","C. Lo; C. Hsu; M. Shieh","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","IET Image Processing","","2013","7","3","185","190","MPEG-2 and H.264/AVC use variable length coding (VLC) to remove statistical redundancy. Representing the codeword table efficiently is thus an important issue to reduce hardware complexity, especially for multi-standard applications. In this study, the VLC tree is decomposed using sub-tree classification. The proposed algorithm reduces the amount of storage required for codewords. The proposed MPEG-2/H.264 VLC decoder has 11.9 K gates when synthesised to operate at 180 MHz. The gate count is 20% lower than the sum of gate count of individual H.264 and MPEG-2 VLC decoders.","1751-9659;1751-9667","","10.1049/iet-ipr.2011.0195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530967","","","adaptive codes;decoding;image classification;tree codes;variable length codes;video coding","MPEG-2;variable length coding;H.264;AVC;codeword table;multistandard application;subtree classification;VLC decoder;tree-based partition;frequency 180 MHz","","","","24","","","","","IET","IET Journals & Magazines"
"Low-complexity image coder/decoder with an approaching-entropy quad-tree search code for embedded computing platforms","T. Ma; P. Shrestha; M. Hempel; D. Peng; H. Sharif","Department of Computer and Electronics Engineering, University of Nebraska Lincoln; Department of Computer and Electronics Engineering, University of Nebraska Lincoln; Department of Computer and Electronics Engineering, University of Nebraska Lincoln; Department of Computer and Electronics Engineering, University of Nebraska Lincoln; Department of Computer and Electronics Engineering, University of Nebraska Lincoln","2011 18th IEEE International Conference on Image Processing","","2011","","","297","300","In this paper, we propose a fast, simple and efficient image codec applicable for embedded processing systems. Among the existing image coding methods, wavelet quad-tree is a foundation leading to an efficient structure to encode images. By searching significant coefficients along quadtrees, an embedded efficient code can be obtained. In this work, we exploit hierarchical relations of the quad-tree structure in terms of searching entropy and present a quadtree searching model that is very close to the searching entropy. By applying this model, our codec surpasses SPIHT [1] by 0.2-0.4 db over wide code rates, and its performance is comparable to SPIHT with arithmetic coding and JPEG2000 [2]. With no additional overhead of arithmetic coding, our code is much faster and simpler than SPIHT with adaptive arithmetic coding and the more complicated JPEG2000 algorithms. This is a critical factor sought in embedded processing in communication systems where energy consumption and speed are priority concerns. Our simulation results demonstrate that the proposed codec is about twice as fast with very low computational overheads and comparable coding performances than existing algorithms.","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116287","Image coding;quad-tree search model;embedded communication system","Image coding;Entropy;Codecs;Transform coding;Algorithm design and analysis;Computational modeling;Communication systems","adaptive codes;arithmetic codes;codecs;computational complexity;embedded systems;energy consumption;entropy;image coding;quadtrees;tree searching;wavelet transforms","low-complexity image coder-decoder;approaching-entropy quadtree search code;embedded computing platforms;image codec;embedded processing systems;image coding methods;wavelet quadtree;adaptive arithmetic coding;JPEG2000 algorithms;energy consumption","","2","","12","","","","","IEEE","IEEE Conferences"
"Comprehensive study on a 2 × 2 full-rate and linear decoding complexity space–time block code","S. S. Hosseini; S. Talebi; J. Abouei","Islamic Azad University, Iran; Shahid Bahonar University of Kerman, Iran; Yazd University, Iran","IET Communications","","2015","9","1","122","132","This paper presents a comprehensive study on the Full-Rate and Linear-Receiver (FRLR) STBC proposed as a newly coding scheme with the low decoding complexity for a 2×2 MIMO system. It is shown that the FRLR code suffers from the lack of the non-vanishing determinant (NVD) property that is a key parameter in designing a full-rate STBC with a good performance in higher data rates, across QAM constellation. To overcome this drawback, we show that the existence of the NVD feature for the FRLR code depends on the type of the modulation. In particular, it is analytically proved that the FRLR code fulfills the NVD property across the PAM constellation but not for the QAM scheme. Simulation results show that, at a BER equal to 10-4, utilising the PAM modulation for the FRLR-STBC, provides about 2 dB gain over a use of the QAM when the bandwidth efficiency is 6b/s/Hz. In addition, for the PAM constellation, the FRLR code significantly outperforms some existing full-rate STBCs. Finally, we utilise the moment generating function approach to derive an exact closed-form expression for the average error probability of the FRLR code with the BPSK modulation.","1751-8628;1751-8636","","10.1049/iet-com.2014.0367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001284","","","block codes;error statistics;MIMO communication;phase shift keying;pulse amplitude modulation;quadrature amplitude modulation","signal-to-noise ratio;binary phase shift keying modulation;closed-form expression;BER;bit error rate;pseudo global-local search;PAM modulation scheme;pulse amplitude modulation;QAM;quadrature amplitude modulation;NVD;nonvanishing determinant;MIMO system;multiple-input-multiple-output system;FRLR;full-rate and linear-receiver;STBC;space-time block code;linear decoding","","2","","38","","","","","IET","IET Journals & Magazines"
"LORD: LOw-complexity, Rate-controlled, Distributed Video Coding System","R. Cohen; D. Malah","NA; NA","2012 Eighth International Conference on Signal Image Technology and Internet Based Systems","","2012","","","21","28","Distributed Video Coding (DVC) is an emerging coding scheme that employs principles of source coding with side information (SI) at the decoder. In this paper, we present a new DVC encoder, named LORD (LOw-complexity, Rate-controlled, Distributed video coding system). No feedback channel is used in our encoder, and an adaptive noise model that varies both spatially and temporally is employed. The SI is created using motion extrapolation, resulting in a low delay encoding process. We extend LORD for encoding videos acquired by Bayer sensors in endoscopy, in which only partial information of the colors in each pixel is known. This special video format has not been addressed yet in the DVC framework. We show that, using LORD, a significant improvement in performance is achieved over a standard intra-coding method with a similar complexity, on a set of examined videos.","","978-1-4673-5152-2978-0-7695-4911","10.1109/SITIS.2012.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395068","Bayer format;distributed video coding;endoscopy videos;motion extrapolation;wyner-ziv coding","Decoding;Discrete cosine transforms;Endoscopes;Encoding;Transform coding;Quantization;Video coding","endoscopes;source coding;video coding","distributed video coding system;source coding;side information;LORD;feedback channel;adaptive noise model;motion extrapolation;low delay encoding process;encoding videos;Bayer sensors;endoscopy;intra-coding method","","2","","12","","","","","IEEE","IEEE Conferences"
"Construction of multi-rate high performance QC-LDPC codes with low implementation complexity","Y. Liu; K. Peng","Tsinghua National Laboratory for Information Science and Technology, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Beijing 100084, China","2014 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting","","2014","","","1","6","Multi-rate LDPC codes with near Shannon limit performance are highly demanded in digital television terrestrial broadcasting (DTTB) system. Several proposals have been carried out to construct multi-rate LDPC codes, among which, Row-combining and Row-splitting with Edge Variation (RCSEV) helps construct LDPC codes with constant code length and simple hardware architecture. Besides, Column Extension (CE) is also an effective method for rate-compatible LDPC code design. This paper proposes an improved scheme of Row-Combining and splitting with Edge Variation and Column Deletion (RCSEV-CD), which is the rational combination of RCSEV and CE. RCSEV is carried out to generate multi-rate LDPC codes sharing the same code length in basic code group. The codes in extended code group are generated via CD, ensuring same code length in each extended code groups. Multi-rate codes within a code group improves the flexibility of a service while the code groups meet the requirement of multiple services. EXIT chart analysis helps guide the trade-off between column degree distributions among multiple rates. A comparison with multi-rate codes in DVB-S2 is performed via bit error rate (BER) simulation and the result shows the superior performance of our designed codes at typical code rates.","2155-5044;2155-5052","978-1-4799-1654","10.1109/BMSB.2014.6873508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6873508","low-density parity-check (LDPC);quasi-cyclic (QC)-LDPC;Multi-rate;extrinsic information transfer (EXIT)","Parity check codes;Digital video broadcasting;Complexity theory;Proposals;Decoding;Digital TV;Hardware","digital television;error statistics;parity check codes;television broadcasting","multirate high performance QC-LDPC codes;low implementation complexity;near Shannon limit performance;digital television terrestrial broadcasting system;DTTB system;row-combining and row-splitting with edge variation;RCSEV;constant code length;simple hardware architecture;column extension;rate-compatible LDPC code design;row-combining and splitting with edge variation and column deletion;RCSEV-CD;EXIT chart analysis;bit error rate;column degree distributions;DVB-S2;BER","","2","","13","","","","","IEEE","IEEE Conferences"
"Low Complexity Decoding for Spinal Codes: Sliding Feedback Decoding","S. Xu; S. Wu; J. Luo; J. Jiao; Q. Zhang","NA; NA; NA; NA; NA","2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)","","2017","","","1","5","As a type of newly invented rateless codes, Spinal codes are characterized by capacity achieving over both additive white Gaussian noise (AWGN) and binary symmetric channel (BSC) with short message length and pseudo-random like codewords. For the emerging ultra- reliable low-latency communication (URLLC) scenarios such as information exchanging between self-drive cars, Spinal codes hold great prospects. However, the high decoding complexity of Spinal codes remains a bottleneck for its practical applications. In this work, a novel low complexity decoding algorithm named sliding feedback decoding (SFD) for Spinal codes is proposed. By 'sliding', the decoding tree is layered by a sliding window. By 'feedback decoding', the optimal parent node decision for each layer located by the sliding window is made by the feedback from the best leaf node in the located layer. And the final decoding path is composed of all the optimal parent nodes selected layer by layer. The complexity of the proposed algorithm is analyzed theoretically, and the results show that it is lower than the complexity of other algorithm . Extensive simulations are carried out to verify the effectiveness of the proposed algorithm. Compared with the bubble decoder and the forward stack decoding (FSD) proposed in the literature, SFD can significantly reduce the decoding complexity without any harming to the rate performance.","","978-1-5090-5935-5978-1-5090-5934-8978-1-5090-5936","10.1109/VTCFall.2017.8287919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8287919","","Complexity theory;Algorithm design and analysis;Maximum likelihood decoding;Microsoft Windows;Encoding;Measurement","AWGN channels;binary codes;channel coding;decoding;feedback;trees (mathematics)","sliding feedback decoding;decoding tree;sliding window;final decoding path;bubble decoder;high decoding complexity;spinal codes;low complexity decoding algorithm;invented rateless codes;additive white Gaussian noise channel;binary symmetric channel;AWGN channel;BSC;ultra-reliable low-latency communication;URLLC scenarios","","1","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity near-optimal codes for Gaussian relay networks","T. K. Dikaliotis; H. Yao; A. S. Avestimehr; S. Jaggi; T. Ho","California Institute of Technology; California Institute of Technology; Cornell University; Chinese University of Hong Kong; California Institute of Technology","2012 International Conference on Signal Processing and Communications (SPCOM)","","2012","","","1","5","We consider the problem of information flow over Gaussian relay networks. Similar to the recent work by Avestimehr et al. [1], we propose network codes that achieve up to a constant gap from the capacity of such networks. However, our proposed codes are also computationally tractable. Our main technique is to use the codes of Avestimehr et al. as inner codes in a concatenated coding scheme.","2165-0608","978-1-4673-2014-6978-1-4673-2013-9978-1-4673-2012","10.1109/SPCOM.2012.6290236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6290236","","Noise;Decoding;Relays;Encoding;Complexity theory;Random variables;Vectors","concatenated codes;Gaussian processes;network coding;relays","low-complexity near-optimal codes;Gaussian relay network code;information flow;inner code;concatenated coding scheme","","","","14","","","","","IEEE","IEEE Conferences"
"Improved Network-Coded Cooperative Transmission with Low-Complexity Adaptation to Wireless Channels","D. Kim; H. Kim; G. Im","Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang 790-784, Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang 790-784, Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang 790-784, Korea","IEEE Transactions on Communications","","2011","59","10","2916","2927","The relay employs network coding to transmit the packets from the source nodes simultaneously, for increasing spectral efficiency in wireless environments. The cooperative transmission based on network coding usually works on decode-and-forward (DF) protocols. However, detection errors at the relay cause error propagation, which degrades the performance of cooperative communications. To overcome this problem, we model the error propagation effect of the DF-based system at the destination as the addition of virtual noise, and then design a low complexity detection method. We derive the achievable diversity gain to evaluate the proposed model and corresponding detection scheme. To extend the proposed model to network-coded systems, we first express the channel conditions between the sources and relay as a single equivalent channel gain. Then, we develop low complexity detection schemes for the network-coded systems. From the error propagation model, we propose a dual mode network coding technique, which exploits different network coding schemes adaptively according to channel qualities. Simulation results show that the proposed model and detection scheme effectively reduce the error propagation effects. Also, the proposed dual mode network coding has gains for all channel conditions and thus gives better BER performance than conventional methods.","0090-6778;1558-0857","","10.1109/TCOMM.2011.081111.110059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5997291","Cooperative diversity;multi-node wireless relay networks;network coding","Relays;Network coding;Noise;Complexity theory;Bit error rate;Diversity methods;Demodulation","channel coding;cooperative communication;diversity reception;error statistics;network coding;radiowave propagation;wireless channels","network-coded cooperative transmission;wireless channel;relay;packet transmission;source node;spectral efficiency;wireless environment;error detection;error propagation;cooperative communication;DF-based system;virtual noise;low complexity detection;diversity gain;channel condition;channel gain;dual mode network coding;channel quality;BER performance","","18","","32","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive low-complexity motion estimation algorithm for high efficiency video coding encoder","A. Medhat; A. Shalaby; M. S. Sayed; M. Elsabrouty; F. Mehdipour","Egypt-Japan University of Science and Technology (E-JUST), Egypt; Egypt-Japan University of Science and Technology (E-JUST), Egypt; Egypt-Japan University of Science and Technology (E-JUST), Egypt; Egypt-Japan University of Science and Technology (E-JUST), Egypt; Kyushu University and WaioMio Ltd., New Zealand","IET Image Processing","","2016","10","6","438","447","High quality videos became an essential requirement in recent applications. High efficiency video coding (HEVC) standard provides an efficient solution for high quality videos at lower bit rates. On the other hand, HEVC comes with much higher computational cost. In particular, motion estimation (ME) in HEVC, consumes the largest amount of computations. Therefore, fast ME algorithms and hardware accelerators are proposed in order to speed-up integer ME in HEVC. This study presents a fast centre search algorithm (FCSA) and an adaptive search window algorithm (ASWA) for integer pixel ME in HEVC. In addition, centre adaptive search algorithm, a combination of the two proposed algorithms FCSA and ASWA, is proposed in order to achieve the best performance. Experimental results show notable speed-up in terms of encoding time and bit rate saving with tolerable peak signal-to-noise ratio (PSNR) quality degradation. The proposed fast search algorithms reduce the computational complexity of the HEVC encoder by 57%. This improvement is accompanied with a modest average PSNR loss of 0.014 dB and an increase by 0.6385% in terms of bit rate when compared with related works.","1751-9659;1751-9667","","10.1049/iet-ipr.2015.0666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470330","","","computational complexity;image resolution;motion estimation;search problems;video coding","adaptive low-complexity motion estimation algorithm;high efficiency video coding encoder;HEVC standard;bit rate saving;computational cost;fast ME algorithms;fast centre search algorithm;FCSA;adaptive search window algorithm;ASWA;centre adaptive search algorithm;encoding time;peak signal-to-noise ratio quality degradation;computational complexity reduction","","3","","25","","","","","IET","IET Journals & Magazines"
"Complexity-Performance Tradeoff for Intersymbol Interference Channels—Random Coding Analysis","A. Trofimov; C. K. Sann","$^{1}$ St. Petersburg State University of Aerospace Instrumentation,, St. Petersburg,, Russia; Data Storage Institute, Singapore","IEEE Transactions on Magnetics","","2010","46","4","1077","1091","In this work, we look at symmetric cutoff rates for intersymbol interference (ISI) channels with additive white Gaussian noise (AWGN), as well as with colored noise. The derived symmetric cutoff rates are used to estimate the signal-to-noise ratio (SNR) losses expected over channels with various reduced complexity channels. The techniques used in this work are generalizations on established approaches to computing the cutoff rate for linear ISI channels. The generalizations mentioned consist of an extension to traditional schemes by considering: (a) general (e.g., nonlinear) ISI; (b) colored Gaussian noise; and (c) suboptimal decoding approaches (reduced number of trellis states). Examples involving some real ISI channels are examined and the performance differences of real LDPC coded channels are compared with the results obtained via the symmetric cutoff rate analysis.","0018-9464;1941-0069","","10.1109/TMAG.2009.2037811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5352344","Cholesky decomposition;colored noise;equalization;generalized cutoff rate;intersymbol interference;LDPC codes;Perron-Frobenius theorem","Intersymbol interference;Detectors;AWGN;Signal to noise ratio;Maximum likelihood decoding;Performance loss;Additive white noise;Colored noise;Parity check codes;Maximum likelihood detection","AWGN channels;intersymbol interference;parity check codes;random codes","complexity-performance tradeoff;intersymbol interference channels;random coding analysis;colored additive white Gaussian noise;signal-to-noise ratio estimation;complexity channels;linear ISI channels;suboptimal decoding approaches;ISI channels;LDPC coded channels;cutoff rate analysis","","2","","30","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity iteration-based interference cancellation in asynchronous physical-layer network coding","Y. Li; F. Zheng","The University of Reading, UK; The University of Reading, UK","IET Communications","","2015","9","4","576","583","When two terminals exchange information through an intermediate relay, physical-layer network coding (PLNC) improves the spectrum efficiency by allowing cocurrent packet transmission. Synchronisation is one of the most important issues in distributed wireless communications systems. In time-domain (TD)-based PLNC, signals transmitted from user terminals may arrive the relay at different time. The fractional symbol level delay will introduces inter-symbol interference because of the use of practical pulse-shaping and matched-filter. Orthogonal-frequency division-multiplexing (OFDM) can be used to deal with time asynchrony. However, OFDM systems are very sensitive to carrier frequency offsets, which will introduce inter-carrier interference. In this study, a novel low-complexity symbol-based decoding iterative interference cancellation schemes are proposed for TD-based and OFDM-based PLNC. Signals from two sources are separately decoded, and interference is reconstructed and eliminated. Monte Carlo simulations show that the proposed scheme with over just one iteration can significantly improve the bit error rate performance.","1751-8628;1751-8636","","10.1049/iet-com.2014.0571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7055415","","","intercarrier interference;interference suppression;intersymbol interference;iterative decoding;matched filters;network coding;OFDM modulation;pulse shaping;relay networks (telecommunication);synchronisation","bit error rate performance;Monte Carlo simulations;low-complexity symbol-based decoding iterative interference cancellation schemes;intercarrier interference;carrier frequency offsets;OFDM systems;time asynchrony;orthogonal-frequency division-multiplexing;matched-filter;pulse-shaping;intersymbol interference;fractional symbol level delay;user terminals;TD;time-domain-based PLNC;distributed wireless communications systems;synchronisation;cocurrent packet transmission;spectrum efficiency;intermediate relay;asynchronous physical-layer network coding","","4","","25","","","","","IET","IET Journals & Magazines"
"Relaxed channel polarization for reduced complexity polar coding","M. El-Khamy; H. Mahdavifar; G. Feygin; J. Lee; I. Kang","Modem R&D, Samsung Electronics, San Diego, CA 92121; Modem R&D, Samsung Electronics, San Diego, CA 92121; Modem R&D, Samsung Electronics, San Diego, CA 92121; Modem R&D, Samsung Electronics, San Diego, CA 92121; Modem R&D, Samsung Electronics, San Diego, CA 92121","2015 IEEE Wireless Communications and Networking Conference (WCNC)","","2015","","","207","212","Arıkan's polar codes are proven to be capacity-achieving error correcting codes while having explicit constructions. They are characterized to have encoding and decoding complexities of l log l, for code length l. In this work, we construct another family of capacity-achieving codes that have even lower encoding and decoding complexities, by relaxing the channel polarizations for certain bit-channels. We consider schemes for relaxing the polarization of both sufficiently good and sufficiently bad bit-channels, in the process of channel polarization. We prove that, similar to conventional polar codes, relaxed polar codes also achieve the capacity of binary memoryless symmetric channels. We analyze the complexity reductions achievable by relaxed polarization for asymptotic and finite-length codes, both numerically and analytically. We show that relaxed polar codes can have better bit error probabilities than conventional polar codes, while having reduced encoding and decoding complexities.","1525-3511;1558-2612","978-1-4799-8406","10.1109/WCNC.2015.7127470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127470","","Complexity theory;Decoding;Encoding;Error probability;Conferences;Upper bound;Channel capacity","channel coding;error correction codes;error statistics;polarisation","bit error probability;finite-length code;binary memoryless symmetric channel;bit-channel;capacity-achieving code;code length;decoding complexity;encoding complexity;error correcting codes;Arıkan polar code;polar coding complexity reduction;relaxed channel polarization","","6","","11","","","","","IEEE","IEEE Conferences"
"Low-Complexity Z_4 LDPC Code Design under a Gaussian Approximation","M. Ferrari; S. Bellini; A. Tomasoni","IEIIT Institute of the Italian National Research Council (CNR), via Ponzio 34/5, 20133 Milano, Italy; Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133 Milano, Italy; IEIIT Institute of the Italian National Research Council (CNR), via Ponzio 34/5, 20133 Milano, Italy","IEEE Wireless Communications Letters","","2012","1","6","589","592","In this paper we propose low complexity LDPC code design and decoding in Z<sub>4</sub> which may be useful to combat phase ambiguities in wireless links affected by strong phase noise. We approximate messages exchanged on the Tanner graph using separable probability density functions. This allows a substantial reduction of decoder memory and complexity, with a negligible performance penalty, compared to ideal Z<sub>4</sub> decoding. Furthermore, we show that the Density Evolution analysis of this suboptimal decoder leads to irregular LDPC designs matching the criteria of binary LDPC codes.","2162-2337;2162-2345","","10.1109/WCL.2012.082712.120471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6290311","LDPC codes;density evolution;Z_4","Parity check codes;Decoding;Vectors;Binary codes;Equations;Convolution","binary codes;codecs;Gaussian processes;parity check codes;probability;radio links","low-complexity Z<sub>4</sub> LDPC code design;Gaussian approximation;decoding;combat phase ambiguities;wireless links;phase noise;Tanner graph;separable probability density functions;decoder memory reduction;performance penalty;density evolution analysis;binary LDPC codes","","4","","8","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity side information for distributed compressed video coding","Y. Baig; E. M. Lai; P. Amal","School of Engineering and Advanced Technology, Massey University, New Zealand; School of Engineering and Advanced Technology, Massey University, New Zealand; School of Engineering and Advanced Technology, Massey University, New Zealand","2012 IEEE International Conference on Signal Processing, Communication and Computing (ICSPCC 2012)","","2012","","","436","441","Distributed Video coding based on compressed sensing is considered in this paper. Side information plays an important role in the quality of decoded non-key video frames. Existing systems generate side information based on the decoded key frames and the processes are quite complicated, increasing the computation burden at the decoder. We propose a side information generation method that is founded on the high statistical correlation between compressed sensing measurements of key frames and non-key frames. The proposed technique is simple and simulation results show that better rate distortion performance can be obtained in comparison with motion compensated interpolation.","","978-1-4673-2193-8978-1-4673-2192-1978-1-4673-2191","10.1109/ICSPCC.2012.6335688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335688","Distributed Video Coding;Side Information Generation;Compressed Sensing","Correlation;Decoding;Video coding;Video sequences;Vectors;Complexity theory;Interpolation","compressed sensing;decoding;statistical analysis;video coding","low complexity side information;distributed compressed video coding;decoded nonkey video frames;side information generation method;high statistical correlation;compressed sensing measurements;rate distortion performance;motion compensated interpolation","","2","","23","","","","","IEEE","IEEE Conferences"
"A unified and complexity scalable entropy coding scheme for video compression","M. Preiss; D. Marpe; B. Bross; V. George; H. Kirchhoffer; T. Nguyen; M. Siekmann; J. Stegemann; T. Wiegand","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany","2012 19th IEEE International Conference on Image Processing","","2012","","","729","732","The state-of-the-art hybrid video coding standard H.264/AVC defines two entropy-coding schemes with different complexity-performance trade-offs. Supporting these two schemes within a single standard raises several problems ranging from higher efforts for product development to increased silicon costs for hardware implementations. To overcome these issues, this work proposes a unified and complexity-scalable entropy-coding framework that is based on PIPE/V2V. The proposed framework uses a single set of tools for all configurations and achieves the same complexity-performance trade-offs as the existing entropy-coding schemes through scalability.","1522-4880;1522-4880;2381-8549","978-1-4673-2533-2978-1-4673-2534-9978-1-4673-2532","10.1109/ICIP.2012.6466963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466963","Entropy Coding;PIPE/V2V;CABAC","Decoding;Encoding;Context;Video coding;Hardware;Context modeling;Throughput","data compression;entropy codes;video codecs;video coding","unified scalable entropy coding scheme;complexity scalable entropy coding scheme;video compression;hybrid video coding standard;H.264/AVC;complexity-performance trade-offs;product development;complexity-scalable entropy-coding framework;PIPE/V2V;entropy-coding schemes","","","","9","","","","","IEEE","IEEE Conferences"
"Low Complexity Hybrid ARQ Using Extended Turbo Product Codes Self-Detection","H. Mukhtar; A. Al-Dweik; M. Al-Mualla","NA; NA; NA","2015 IEEE Global Communications Conference (GLOBECOM)","","2015","","","1","6","This paper presents a hybrid automatic repeat request (HARQ) system using a parity error checking (PEC) technique with low processing power requirements. The proposed technique is applied to extended turbo product codes (TPC) where the parity check bits used for extending the component codes of TPC, are exploited to replace the conventional cyclic redundancy check (CRC) error detection in HARQ systems. Consequently, the required processing power can be reduced substantially while the throughput is almost unchanged for long TPC codes, or increased for short TPC codes. The proposed PEC technique is also compared to the state-of-the-art syndrome error checking (SEC) as well as conventional CRC. Monte Carlo simulation results reveal that PEC- HARQ can provide equivalent throughput to SEC-HARQ and higher throughput than CR-HARQ systems. Moreover, numerical results show that the PEC technique has lower computational complexity than both SEC and CRC error detection. In particular cases, the complexity of the proposed system is reduced by more than 50% as compared to the state- of-the-art, and by more than 80% when compared to the CRC error detection.","","978-1-4799-5952-5978-1-4799-5951","10.1109/GLOCOM.2015.7417538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417538","","Forward error correction;Complexity theory;Iterative decoding;Automatic repeat request;Decoding;Throughput;Product codes","automatic repeat request;cyclic redundancy check codes;Monte Carlo methods;parity check codes;product codes;turbo codes","hybrid automatic repeat request system;HARQ system;parity error checking technique;PEC technique;extended turbo product codes;TPC;parity check bits;component codes;cyclic redundancy check error detection;CRC error detection;syndrome error checking;SEC;Monte Carlo simulation results","","1","","17","","","","","IEEE","IEEE Conferences"
"Low-complexity concatenated polar codes with excellent scaling behavior","Sung Whan Yoon; Jaekyun Moon","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, 34141, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, 34141, Republic of Korea","2017 IEEE International Conference on Communications Workshops (ICC Workshops)","","2017","","","948","954","In this paper, highly efficient practical concatenated coding schemes with multiple short length polar codes and single-parity-check codes are proposed. As for hardware complexity, required memory space is significantly reduced by utilizing small decoding units geared to serialized decoding of short-length component polar codes. In theoretic analysis, each component short polar code shows much improved error-rate scaling-behavior thanks to simple single-parity-check decoding; the error rate decays with increasing overall code length as fast as in single stand alone code while retaining considerable hardware-memory advantage. Moreover, by applying list decoding and cyclic-redundancy-checking to each short component polar code, finite-length error-rate performance is comparable to list decoding of CRC-aided long polar codes while offering much lower memory-complexity implementation options.","2474-9133","978-1-5090-1525-2978-1-5090-1526","10.1109/ICCW.2017.7962781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962781","","Iterative decoding;Encoding;Maximum likelihood decoding;Concatenated codes;Resource management;5G mobile communication","concatenated codes;parity check codes","low-complexity concatenated polar codes;concatenated coding;single-parity-check codes;single-parity-check decoding;error rate decays;cyclic-redundancy-checking;finite-length error-rate;CRC-aided long polar codes","","","","11","","","","","IEEE","IEEE Conferences"
"Modified GII-BCH Codes for Low-Complexity and Low-Latency Encoders","W. Li; J. Tian; J. Lin; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China","IEEE Communications Letters","","2019","23","5","785","788","Generalized integrated interleaved Bose-Chaudhuri-Hocquenghem (GII-BCH) codes show great potentials in some storage applications, such as flash memory programming, since they need less redundancy than the simple interleaved BCH codes for a target reliability. The coefficient vectors that are precomputed from the conversion matrix play a very important role in the GII-BCH encoding and also characterize its hardware architecture. However, they require a large amount of computations, including matrix inversions and multiplications both modulo polynomials. In addition, the GII-BCH encoders also suffer from high complexity and latency. In this letter, we modify the original conversion matrix and directly obtain the coefficient vectors without sacrificing error-correction performance. The modified coefficient vectors are much simpler than the original ones due to their independence on the error-correction capabilities. Furthermore, encoder architectures for the original and the modified GII-BCH codes are developed and compared. It is shown that the encoders for the modified GII-BCH codes perform significantly better than those for the original ones in both hardware complexity and latency.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2019.2908867","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680037","Generalized integrated interleaved (GII) codes;Bose-Chaudhuri-Hocquenghem (BCH) codes;conversion matrix;low-complexity and low-latency encoders","Encoding;Generators;Decoding;Redundancy;Complexity theory;Systematics;Hardware","BCH codes;error correction codes;interleaved codes;matrix inversion;matrix multiplication;polynomials;reliability;vectors","modified GII-BCH codes;low-latency encoders;generalized integrated interleaved Bose-Chaudhuri-Hocquenghem codes;simple interleaved BCH codes;GII-BCH encoding;matrix inversions;GII-BCH encoders;original conversion matrix;modified coefficient vectors;modulo polynomials;low-complexity encoders;hardware architecture;matrix multiplications;error-correction performance;hardware complexity;target reliability","","","","7","","","","","IEEE","IEEE Journals & Magazines"
"Identifying complex functions: By investigating various aspects of code complexity","V. Antinyan; M. Staron; J. Derehag; M. Runsten; E. Wikström; W. Meding; A. Henriksson; J. Hansson","Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Department of Computer Science and Engineering, Chalmers | University of Gothenburg, Gothenburg, Sweden; Ericsson; AB Volvo, Gothenburg, Sweden, University of Skövde, Skövde, Sweden; Ericsson; Ericsson; AB Volvo, Gothenburg, Sweden, University of Skövde, Skövde, Sweden; NA","2015 Science and Information Conference (SAI)","","2015","","","879","888","The complexity management of software code has become one of the major problems in software development industry. With growing complexity the maintenance effort of code increases. Moreover, various aspects of complexity create difficulties for complexity assessment. The objective of this paper is to investigate the relationships of various aspects of code complexity and propose a method for identifying the most complex functions. We have conducted an action research project in two software development companies and complemented it with a study of three open source products. Four complexity metrics are measured, and their nature and mutual influence are investigated. The results and possible explanations are discussed with software engineers in industry. The results show that there are two distinguishable aspects of complexity of source code functions: Internal and outbound complexities. Those have an inverse relationship. Moreover, the product of them does not seem to be greater than a certain limit, regardless of software size. We present a method that permits identification of most complex functions considering the two aspects of complexities. The evaluation shows that the use of the method is effective in industry: It enables identification of 0.5% most complex functions out of thousands of functions for reengineering.","","978-1-4799-8547-0978-1-4799-8546","10.1109/SAI.2015.7237246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237246","code;complexity;management;risk;trade-off","Complexity theory;Software;Software measurement;Industries;Fluid flow measurement;Phase measurement","computational complexity;public domain software;software development management;software maintenance;software metrics","complex function identification;code complexity;complexity management;software code;software development industry;code maintenance effort;complexity assessment;open source products;complexity metrics;software engineers;source code functions;inverse relationship;software size","","","","28","","","","","IEEE","IEEE Conferences"
"Low-Complexity Detector in Sparse Code Multiple Access Systems","Y. Du; B. Dong; Z. Chen; J. Fang; P. Gao; Z. Liu","National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communications, University of Electronic Science and Technology of China, Chengdu, China","IEEE Communications Letters","","2016","20","9","1812","1815","One of the challenges in the design of sparse code multiple access systems is developing low-complexity detectors. To achieve this goal, we propose a novel low-complexity detector based on an edge selection approach, which remarkably reduces the computational complexity. First, the proposed detector applies adaptive Gaussian approximation to the unselected edges that have smaller modulus of the channel coefficients, on the basis of the different channel qualities. As a result, the original factor graph can be simplified. In addition, a mean and variance feedback mechanism is employed to further compensate the information loss brought by unselected edges. Simulations show that, compared with the original message passing algorithm-based detector, the computational complexity is reduced substantially with negligible bit error rate performance degradation.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2592912","Huawei HIRP project; Sichuan Youth Science and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516621","Sparse code multiple access (SCMA);multiuser detection;message passing algorithm (MPA);adaptive Gaussian approximation;mean and variance feedback","Detectors;Computational complexity;Image edge detection;Gaussian approximation;Bit error rate;5G mobile communication","approximation theory;computational complexity;Gaussian processes;graph theory;multi-access systems;multiuser detection","low-complexity detector;sparse code multiple access systems;edge selection approach;computational complexity;adaptive Gaussian approximation;channel coefficient modulus;channel quality;factor graph;variance feedback mechanism;mean feedback mechanism;information loss compensation;multiuser detection","","18","","9","","","","","IEEE","IEEE Journals & Magazines"
"Fast and low complexity method for content accessing and extracting DC-pictures from H.264 coded videos","M. Mehrabi; F. Zargari; M. Ghanbari","Department of Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Iran Telecom Research Center (ITRC), Ministry of Telecommunications and Information Technology, Iran; Video Networking at the University of Essex, United Kingdom","IEEE Transactions on Consumer Electronics","","2010","56","3","1801","1808","A fast and simple method for content accessing and extracting the DC-pictures of H.264 coded video without full decompression is presented. Since DC-pictures are used in various applications such as compressed domain analysis or indexing of visual data, several methods for extracting DC-pictures of non-integer DCT codecs have been proposed. However, these methods are not efficient for the H.264 codec that uses integer DCT transform, which employs only shift and add operations and already has lower computational load than the non-integer DCT transform. In this paper we propose a fast and low complexity method to extract DC-pictures of H.264 coded videos with significant lower computational cost compared to its full decompression.","0098-3063;1558-4127","","10.1109/TCE.2010.5606329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5606329","DC-Picture, content access, integer DCT, H.264 video coding standard, compressed domain analysis","Discrete cosine transforms;Videos;Decoding;Visualization;Pixel;Image coding","data compression;discrete cosine transforms;video codecs;video coding","H.264 coded videos;complexity method;content accessing;DC-picture extraction;compressed domain analysis;visual data;noninteger DCT codecs;H.264 codec;integer DCT transform;computational cost","","6","","18","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity two-dimensional weight-constrained codes","E. Ordentlich; R. M. Roth","Hewlett-Packard Laboratories, Palo Alto, CA 94304, USA; Computer Science Department, Technion, Haifa, Israel","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","149","153","We describe two low complexity coding techniques for mapping arbitrary data to and from n × n binary arrays in which the Hamming weight of each row and column is at most n/2. One technique is based on flipping rows and columns of an arbitrary binary array until the Hamming weight constraint is satisfied in all rows and columns, and the other is based on a certain explicitly constructed “antipodal” matching between layers of the Boolean lattice. Both codes have a redundancy of roughly 2n and may have applications in next generation resistive memory technologies.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6033792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6033792","","Encoding;Zinc;Complexity theory;Decoding;Redundancy;Indexes;Iterative decoding","Boolean algebra;computational complexity;Hamming codes","low complexity two-dimensional weight-constrained codes;arbitrary data mapping;flipping rows;arbitrary binary array;Hamming weight constraint;flipping columns;antipodal matching;Boolean lattice;next generation resistive memory technologies","","3","","14","","","","","IEEE","IEEE Conferences"
"Low-Complexity Soft-Decoding Algorithms for Reed–Solomon Codes—Part I: An Algebraic Soft-In Hard-Out Chase Decoder","J. Bellorado; A. Kavcic","Link-A-Media Devices, Santa Clara; Department of Electrical Engineering, University of Hawaii, Honolulu","IEEE Transactions on Information Theory","","2010","56","3","945","959","In this paper, we present an algebraic methodology for implementing low-complexity, Chase-type, decoding of Reed-Solomon (RS) codes of lengthn. In such, a set of2¿test-vectors that are equivalent on all except¿ ¿ ncoordinate positions is first produced. The similarity of the test-vectors is utilized to reduce the complexity of interpolation, the process of constructing a set of polynomials that obey constraints imposed by each test-vector. By first considering the equivalent indices, a polynomial common to all test-vectors is constructed. The required set of polynomials is then produced by interpolating the final¿dissimilar indices utilizing a binary-tree structure. In the second decoding step (factorization) a candidate message is extracted from each interpolation polynomial such that one may be chosen as the decoded message. Although an expression for the direct evaluation of each candidate message is provided, carrying out this computation for each polynomial is extremely complex. Thus, a novel, reduced-complexity, methodology is also given. Although suboptimal, simulation results affirm that the loss in performance incurred by this procedure is decreasing with increasing code lengthn, and negligible for long(n> 100) codes. Significant coding gains are shown to be achievable over traditional hard-in hard-out decoding procedures (e.g., Berlekamp-Massey) at an equivalent (and, in some cases, lower) computational complexity. Furthermore, these gains are shown to be similar to the recently proposed soft-in-hard-out algebraic techniques (e.g., Sudan, Ko¿tter-Vardy) that bear significantly more complex implementations than the proposed algorithm.","0018-9448;1557-9654","","10.1109/TIT.2009.2039073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429128","Reed–Solomon (RS) codes;chase decoding;polynomial factorization;polynomial interpolation;reduced-complexity factorization;hard-decision decoding;soft decoding","Decoding;Polynomials;Testing;Interpolation;Computational modeling;Performance loss;Computational complexity;Optical feedback;Magnetic memory;Artificial satellites","binary codes;decoding;polynomials;Reed-Solomon codes","soft-decoding algorithms;Reed-Solomon Codes;soft-In hard-out chase decoder;algebraic methodology;polynomials;test vector;equivalent indices;polynomial common;test-vectors;binary-tree structure","","27","","20","","","","","IEEE","IEEE Journals & Magazines"
"Achieving Near Capacity of Non-Binary LDPC Coded Large MIMO Systems with a Novel Ultra Low-Complexity Soft-Output Detector","P. Suthisopapan; K. Kasai; A. Meesomboon; V. Imtawil","Department of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand; Department of Communications and Integrated Systems, Graduate School of Science and Engineering, Tokyo Institute of Technology, Tokyo 152-8550, Japan; Department of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand; Department of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand","IEEE Transactions on Wireless Communications","","2013","12","10","5185","5199","Recently, it has been proved that both MMSE and MF detectors are near optimal detection for large scale MIMO systems, e.g., MIMO systems with hundreds of antennas. In order to attain near capacity region with reasonable complexity, low-complexity detector with soft-output generation is necessary for coded large MIMO systems. We show in this paper that the non-binary LDPC codes and well-known soft-output MMSE detector can be utilized to significantly reduce the gap to capacity. We also propose a novel soft-output MF-based detector for the non-binary LDPC coded large MIMO systems. With this proposed detector, capacity approaching performance, i.e., the gap to capacity of 1.6 dB, can be achieved with ultra low-complexity detection, e.g., just 0.28% of MMSE detection. Moreover, use of the proposed scheme in large MIMO systems is found to be robust to the presence of imperfect channel estimation and spatial fading correlation which are both the realistic scenarios for large MIMO systems.","1536-1276;1558-2248","","10.1109/TWC.2013.090513.122056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6596069","Large MIMO systems;hundreds of antennas;non-binary LDPC codes;MMSE detection;MF detection;soft-output detection","MIMO;Detectors;Decoding;Turbo codes;Signal to noise ratio;Parity check codes;Receiving antennas","channel capacity;channel estimation;least mean squares methods;MIMO communication;parity check codes;signal detection","nonbinary LDPC coded large MIMO systems;ultra low-complexity soft-output detector;near optimal detection;near capacity region;soft-output generation;MMSE detector;soft-output MF-based detector;imperfect channel estimation;spatial fading correlation;antennas","","12","","46","","","","","IEEE","IEEE Journals & Magazines"
"Complexity-optimized concatenated LDGM-staircase codes","L. M. Zhang; F. R. Kschischang","Dept. of Electrical &amp; Computer Engineering, University of Toronto; Dept. of Electrical &amp; Computer Engineering, University of Toronto","2017 IEEE International Symposium on Information Theory (ISIT)","","2017","","","1688","1692","A concatenated soft-decision channel coding scheme consisting of an inner LDGM code and an outer staircase code is proposed. The soft-decision LDGM code is used for error reduction while the majority of bit errors are corrected by the low complexity hard-decision staircase code. Decoding complexity of the concatenated code is quantified by a score based on the number of edges in the LDGM code Tanner graph, the number of decoding iterations, and the number of staircase code decoding operations. The inner LDGM ensemble is designed by solving an optimization problem, which minimizes the product of the average node degree and an estimate of the required number of decoding iterations. A search procedure is used to find the inner and outer code pair with lowest complexity. The design procedure results in a Pareto-frontier characterization of the trade-off between net coding-gain and complexity for the concatenated code. Simulations of code designs at rate 5/6 show that the proposed scheme achieves net coding-gains equivalent to existing soft-decision codes, with up to 57% reduction in complexity.","2157-8117","978-1-5090-4096-4978-1-5090-4097","10.1109/ISIT.2017.8006817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006817","","Iterative decoding;Decoding;Complexity theory;Encoding;Concatenated codes;Bit error rate","channel coding;communication complexity;concatenated codes;decision theory;error statistics;graph theory;iterative decoding;Pareto optimisation","complexity-optimized concatenated LDGM-staircase codes;concatenated soft-decision channel coding;soft-decision LDGM code;error reduction;bit errors;low complexity hard-decision staircase code;decoding complexity;Tanner graph;decoding iterations;inner LDGM ensemble;optimization problem;Pareto-frontier characterization;net coding-gain;low-density generator-matrix code","","2","","22","","","","","IEEE","IEEE Conferences"
"Low-complexity layered joint detection and decoding for LDPC coded large-MIMO systems","Xiaomu Zhao; Yabo Li; Jie Zhong; Minjian Zhao; Chen Zheng","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China","2013 International Conference on Wireless Communications and Signal Processing","","2013","","","1","6","High performance and low complexity detectors are of great importance for MIMO systems, especially large MIMO systems, where there could be tens or hundreds of antennas. In this paper, we propose a layered factor graph based joint detection and decoding algorithm for LDPC coded large MIMO system. The joint factor graph which combines both detection and decoding is adopted, and Gaussian approximation is used to reduce the complexity. In the proposed algorithm, different from existing algorithms, in one iteration, the information of the variable nodes is updated one by one, instead of being updated as a whole. It is shown that the proposed algorithm can achieve better performance than the well-known MMSE based algorithm, but with much lower complexity. To reduce the complexity further, the half layered factor graph based algorithm and QRD pre-processing based algorithm are also proposed.","","978-1-4799-0308","10.1109/WCSP.2013.6677178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6677178","Factor graph;joint detection and decoding;large MIMO;layered detection;LDPC code;message passing","Complexity theory;Joints;MIMO;Iterative decoding;Vectors;Antennas","decoding;Gaussian processes;graph theory;MIMO communication;parity check codes","QRD preprocessing based algorithm;half layered factor graph based algorithm;MMSE based algorithm;variable nodes;Gaussian approximation;decoding algorithm;multiple-input multiple-output systems;LDPC coded large-MIMO systems;low-complexity layered joint detection","","2","","14","","","","","IEEE","IEEE Conferences"
"Interpolation-Based Low-Complexity Chase Decoding Algorithms for Hermitian Codes","S. Wu; L. Chen; M. Johnston","School of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, China; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.","IEEE Transactions on Communications","","2018","66","4","1376","1385","Algebraic-geometric (AG) codes have good error-correction capability due to their generally large code word length. However, their decoding remains complex, preventing practical applications. Addressing the challenge, this paper proposes two interpolation-based low-complexity Chase (LCC) decoding algorithms for one of the most popular AG codes-Hermitian codes. By choosing η unreliable symbols and realizing them with the two most likely decisions, 2ηdecoding test-vectors can be formulated. The first LCC algorithm performs interpolation for the common elements of the test-vectors, producing an intermediate outcome that will be shared by the uncommon element interpolation. It eliminates the redundant computation for decoding each test-vector, resulting in a low-complexity. With an interpolation multiplicity of one, the decoding is further facilitated by removing the requirement of pre-calculating the Hermitian curve's corresponding coefficients. The second LCC algorithm is an adaptive variant of the first algorithm, where the number of test-vectors is determined by the reliability of received information. When the channel condition improves, it can reduce the complexity without compromising the decoding performance. Simulation results show that the both LCC algorithms outperform a number of existing algebraic decoding algorithms for Hermitian codes. Finally, our complexity analysis will reveal the proposals' low-complexity feature.","0090-6778;1558-0857","","10.1109/TCOMM.2017.2786667","National Natural Science Foundation of China (NSFC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239693","Algebraic-geometric codes;adaptive decoding;Chase decoding;Hermitian codes;interpolation","Decoding;Interpolation;Complexity theory;Reliability;Algorithm design and analysis;Hamming distance;Simulation","channel coding;decoding;error correction codes;geometric codes;interpolation;vectors","uncommon element interpolation;interpolation multiplicity;decoding performance;algebraic decoding algorithms;complexity analysis;low-complexity feature;algebraic-geometric codes;error-correction capability;AG codes;LCC algorithm;Hermitian curve;code word length;unreliable symbols;interpolation-based low-complexity Chase decoding algorithms;Hermitian codes;2ηdecoding test-vectors;channel condition","","","","19","","","","","IEEE","IEEE Journals & Magazines"
"Rate-complexity-distortion evaluation for hybrid video coding","X. Li; M. Wien; J. Ohm","Institute of Communications Engineering, RWTH Aachen University, Germany; Institute of Communications Engineering, RWTH Aachen University, Germany; Institute of Communications Engineering, RWTH Aachen University, Germany","2010 IEEE International Conference on Multimedia and Expo","","2010","","","685","690","To objectively evaluate the coding efficiency of video codecs, Bjøntegaard Delta PSNR (BD-PSNR) was proposed. Based on the rate-distortion (R-D) curve fitting, BD-PSNR is able to provide a good evaluation of the R-D performance. However, BD-PSNR has a critical drawback: It doesn't take the coding complexity into account. Clearly for practical video applications, especially for those on handheld devices, coding complexity has to be considered when evaluating the overall coding performance. Therefore in this paper, a new coding efficiency measurement is developed by generalizing BD-PSNR from R-D curve fitting to rate-complexity-distortion (R-C-D) surface fitting. Simulations show that a comprehensive performance evaluation can easily be obtained with the proposed method. Moreover, the idea can be used for rate-distortion optimization for complexity-constrained video coding.","1945-788X;1945-7871;1945-7871","978-1-4244-7493-6978-1-4244-7491-2978-1-4244-7492","10.1109/ICME.2010.5582589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582589","Hybrid video coding;rate-complexity-distortion optimization;BD-PSNR","Encoding;Complexity theory;PSNR;Distortion measurement;Automatic voltage control;Performance evaluation;Surface fitting","curve fitting;optimisation;video coding","rate-complexity-distortion evaluation;hybrid video coding;Bjontegaard Delta PSNR;peak signal-to-noise ratio;rate-distortion curve fitting;coding complexity;rate-distortion optimization","","9","","20","","","","","IEEE","IEEE Conferences"
"A Survey and Tutorial on Low-Complexity Turbo Coding Techniques and a Holistic Hybrid ARQ Design Example","H. Chen; R. G. Maunder; L. Hanzo","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu 610054, China; School of Electronics and Computer Science, University of Southampton, UK; School of Electronics and Computer Science, University of Southampton, UK","IEEE Communications Surveys & Tutorials","","2013","15","4","1546","1566","Hybrid Automatic Repeat reQuest (HARQ) has become an essential error control technique in communication networks, which relies on a combination of arbitrary error correction codes and retransmissions. When combining turbo codes with HARQ, the associated complexity becomes a critical issue, since conventionally iterative decoding is immediately activated after each transmission, even though the iterative decoder might fail in delivering an error-free codeword even after a high number of iterations. In this scenario, precious battery-power would be wasted. In order to reduce the associated complexity, we will present design examples based on Multiple Components Turbo Codes (MCTCs) and demonstrate that they are capable of achieving an excellent performance based on the lowest possible memory octally represented generator polynomial (2, 3)o. In addition to using low-complexity generator polynomials, we detail two further techniques conceived for reducing the complexity. Firstly, an Early Stopping (ES) strategy is invoked for curtailing iterative decoding, when its Mutual Information (MI) improvements become less than a given threshold. Secondly, a novel Deferred Iteration (DI) strategy is advocated for the sake of delaying iterative decoding, until the receiver confidently estimates that it has received sufficient information for successful decoding. Our simulation results demonstrate that the MCTC aided HARQ schemes are capable of significantly reducing the complexity of the appropriately selected benchmarkers, which is achieved without degrading the Packet Loss Ratio (PLR) and throughput.","1553-877X;2373-745X","","10.1109/SURV.2013.013013.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463373","Multiple-component turbo codes;ARQ;automatic repeat request;EXIT chart;iterative detection","Forward error correction;Iterative decoding;Decoding;Complexity theory;Turbo codes;Automatic repeat request;Tutorials","automatic repeat request;computational complexity;error correction codes;iterative decoding;polynomials;telecommunication network reliability;turbo codes","arbitrary error correction codes;arbitrary error correction retransmissions;iterative decoding;error-free codeword;associated complexity reduction;multiple components turbo codes;MCTC;low-complexity generator polynomials;early stopping strategy;mutual information improvements;deferred iteration strategy;packet loss ratio;PLR;telecommunication networks;communication networks;error control technique;hybrid automatic repeat request;hybrid ARQ design example;low-complexity turbo coding techniques","","25","","75","","","","","IEEE","IEEE Journals & Magazines"
"Novel low complexity coherence estimation and synthesis algorithms for parametric stereo coding","Y. Lang; D. Virette; C. Faller","Huawei European Research Center, Germany; Huawei European Research Center, Germany; Illusonic GmbH, Switzerland","2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)","","2012","","","2427","2431","In this paper, we present novel low complexity coherence estimation and synthesis algorithms and their application to parametric stereo coding. Inter-channel correlation /coherence (IC) is an important parameter for parametric stereo coding as it represents the degree of similarity of the channels and is strongly related to the perception of width and diffuseness of the stereo image. It is relevant for most audio and music contents to re-generate ambience, stereo reverberation, source width, and other perceptions related to spatial impression. In the state of the art parametric stereo codec, the IC estimation and corresponding synthesis algorithms are very complex which prevents their use for complexity-constrained applications. Hence, we introduce novel low complexity coherence estimation and synthesis algorithms for stereo coding. A subjective listening test shows that with the proposed algorithms, the perceived quality for very low bit rate parametric stereo is improved with a limited computational complexity cost.","2076-1465;2219-5491;2219-5491","978-1-4673-1068","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6334027","Inter-channel correlation;parametric stereo coding;de-correlation","Decision support systems;Iron;Europe;Signal processing","codecs;computational complexity;image coding;stereo image processing","low complexity coherence estimation;synthesis algorithms;parametric stereo coding;novel low complexity coherence estimation;interchannel correlation-coherence;stereo image;music contents;audio contents;source width;stereo reverberation;parametric stereo codec;complexity-constrained applications;very low bit rate parametric stereo;computational complexity cost","","2","","9","","","","","IEEE","IEEE Conferences"
"A new candidate adding algorithm for coded MIMO systems with fixed-complexity detection","T. Tsubaki; H. Ochiai","Department of Electrical and Computer Engineering, Yokohama National University, 79-5 Tokiwadai, Hodogaya, Yokohama, Kanagawa 240-8501, Japan; Department of Electrical and Computer Engineering, Yokohama National University, 79-5 Tokiwadai, Hodogaya, Yokohama, Kanagawa 240-8501, Japan","2013 IEEE International Conference on Communications (ICC)","","2013","","","4525","4529","We propose a new approach that achieves near maxlog optimal performance by improving soft information of the eliminated bits in reduced and fixed complexity coded multiple-input multiple-output (MIMO) systems. As a fixed-complexity MIMO detection, we consider both the QR decomposition with M-algorithm (QRM) and the fixed-complexity sphere decoding (FSD). In general, for the MIMO system with channel coding, the detector needs to calculate soft information for each coded bit. However, the reduced complexity detection often eliminates the candidate symbols that are required for accurate soft information calculation. We propose a novel symbol recovery approach that can retrieve the coded bits and thus recalculate the soft information with improved error performance. Simulation results demonstrate the effectiveness of the proposed soft information calculation approach under the example system with 4 × 4 MIMO-OFDM employing 16-QAM and 64-QAM.","1550-3607;1938-1883","978-1-4673-3122","10.1109/ICC.2013.6655281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655281","","MIMO;Principal component analysis;Complexity theory;Bit error rate;Detectors;Decoding;Parity check codes","channel coding;MIMO communication;OFDM modulation;quadrature amplitude modulation;signal detection","quadrature amplitude modulation;QAM;MIMO-OFDM;accurate soft information calculation;reduced complexity detection;channel coding;fixed-complexity sphere decoding;QR decomposition with M-algorithm;multiple input multiple output system;near maxlog optimal performance;fixed-complexity detection;MIMO system;candidate adding algorithm","","1","","11","","","","","IEEE","IEEE Conferences"
"Minimizing the Update Complexity of Facebook HDFS-RAID Locally Repairable Code","M. Mehrabi; M. Ardakani; M. Khabbazian","NA; NA; NA","2017 IEEE 86th Vehicular Technology Conference (VTC-Fall)","","2017","","","1","5","Erasure codes are recently used in real-world distributed storage systems (DSSs) such as Google File System,Microsoft Azure Storage, and Facebook HDFS-RAID for data reliability. When designing erasure codes for DSSs, special attention is given to the associated costs of data handling operations such as repair or update. For example, locally repairable codes (LRC) are designed and used in DSSs to allow for low-cost repair of failed blocks. Update complexity (defined as the number of blocks that need to be updated when an information block is changed) is yet another design parameter. This parameter can be seen as a measure of the computation, I/O and networking costs associated with updating an information block in a DSS. Since information is frequently updated by many applications, lowering update complexity can result in lower power consumptions in DSSs. In this work, we study the update complexity of LRCs. Based on our study, we propose an improvement over the LRC used by Facebook HDFS-RAID. Keeping the same code parameters including length, storage overhead, minimum distance and cost of repair (locality), we improve the update complexity by more than 16%. Moreover, we show that with these parameters achieving a lower update complexity is impossible.","","978-1-5090-5935-5978-1-5090-5934-8978-1-5090-5936","10.1109/VTCFall.2017.8288123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288123","","Complexity theory;Facebook;Systematics;Spread spectrum communication;Data centers;Block codes;Distributed databases","cloud computing;data handling;RAID;social networking (online);storage management","DSSs;locally repairable codes;low-cost repair;information block;distributed storage systems;erasure codes;code parameters;update complexity;Facebook HDFS-RAID locally repairable code;Google File System;Microsoft Azure Storage;data reliability","","","","20","","","","","IEEE","IEEE Conferences"
"A reduced complexity decoding algorithm for NB-LDPC codes","S. Song; J. Lin; J. Tian; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China","2017 IEEE 17th International Conference on Communication Technology (ICCT)","","2017","","","127","131","Non-binary low-density parity-check (NB-LDPC) codes perform much better than their binary counterparts, when codeword length is moderate or high-order modulation is used. However, the implementation of a NB-LDPC decoder usually suffers from excessive hardware complexity and large memory requirement. Many new algorithms and decoding schedules have been introduced in recent literatures to reduce the decoding complexity in further. However, the complexity of current decoding algorithms for NB-LDPC codes is still high. In this paper, a two-extra-column trellis min-sum algorithm (TEC-TMSA) is proposed. The TEC-TMSA combines the two-extra-column method and the trellis min-sum algorithm in an intelligent way. Since only the minimum value of a row needs to be sorted out for configuration constructions, the computational complexity of the proposed TEC-TMSA is much lower than that of the TMSA. Furthermore, we develop a novel location-fixed selection scheme, which significantly decreases the number of required comparison operations in the TEC-TMSA with negligible performance loss.","2576-7828","978-1-5090-3944-9978-1-5090-3942-5978-1-5090-3945","10.1109/ICCT.2017.8359617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359617","Nonbinary low-density parity-check codes;two-extra-column trellis min-sum;location-fixed selection scheme;low complexity","Complexity theory;Decoding;Parity check codes;Reliability engineering;Indexes;Approximation algorithms","binary codes;computational complexity;decoding;parity check codes;trellis coded modulation","high-order modulation;NB-LDPC decoder;TEC-TMSA;computational complexity;reduced complexity decoding algorithm;nonbinary low-density parity-check codes;hardware complexity;decoding scheduling;two-extra-column trellis minsum algorithm;location-fixed selection scheme","","1","","18","","","","","IEEE","IEEE Conferences"
"Low-complexity symbol timing error detection for quasi-orthogonal space-time block codes","P. A. Dmochowski","Victoria University of Wellington, Wellington, New Zealand","IET Communications","","2013","7","3","206","216","The author presents the design and analysis of low-complexity symbol timing error detectors (TEDs) for timing synchronisation in quasi-orthogonal space-time block code (QOSTBC) receivers. The estimators operate on data symbols and approximate decision variables, producing timing error measurements which are shown to be robust to channel fading. In evaluating the detector S-curve for the general form of the estimator, the author shows that the result is independent of the constellation rotation angle employed by the code. The expressions for the estimation error variance and TED signal-to-noise ratio are also obtained, with the analysis carried out under the assumptions of perfect data and channel knowledge at the receiver. Through system simulations, the effects of decision errors on the detector characteristics are examined, and the overall system performance is evaluated, where the proposed TEDs are incorporated into the receiver timing loop. Receivers with perfect channel knowledge and pilot-based channel estimation are considered. Symbol error rate results show timing synchronisation loss of less than 0.5 dB for a receiver with perfect channel information. In addition, it is shown that the receiver is able to track the timing variations two orders of magnitude faster than required by the present-day hardware oscillators.","1751-8628;1751-8636","","10.1049/iet-com.2012.0244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519379","","","error detection codes;fading channels;oscillators;space-time block codes;synchronisation","low-complexity symbol timing error detection;quasiorthogonal space-time block codes;TED signal-to-noise ratio;QOSTBC receivers;low-complexity symbol timing error detectors;data symbols;channel fading;timing error measurements;S-curve detector;decision variable approximation;channel information;hardware oscillators;timing synchronisation","","1","","14","","","","","IET","IET Journals & Magazines"
"Low-complexity and high-efficiency background modeling for surveillance video coding","X. Zhang; Y. Tian; T. Huang; W. Gao","National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China; National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China; National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China; National Engineering Laboratory for Video Technology, Peking University, Beijing 100871, China","2012 Visual Communications and Image Processing","","2012","","","1","6","Recently, background modeling (shortly BgModeling) plays a more and more important role in high-efficiency surveillance video coding. Meanwhile, many practical video coding applications also present some specific requirements for BgModeling, such as the low memory cost and low computational complexity. However, existing BgModeling methods are mostly designed for video content analysis such as object detection. Thus they may be not directly applicable for video coding. In this paper, we firstly present an analysis for the features of BgModeling in surveillance video coding and make a comparison of the performances of existing BgModeling methods. Then we propose a segment-and-weight based running average (SWRA) method for surveillance video coding. SWRA firstly divides pixels at each position in the training frames into several temporal segments, and then calculate their corresponding mean values and weights. After that, a running and weighted average procedure is used to reduce the influence of foreground pixels and finally obtain the modeling results. Experimental results show that, the SWRA-based encoder achieves the best performance over several state-of-the-art methods, with much less cost of memory and modeling time.","","978-1-4673-4407-4978-1-4673-4405-0978-1-4673-4406","10.1109/VCIP.2012.6410796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6410796","background modeling;surveillance video coding;Gaussian mixture model;complexity;memory","Video coding;Surveillance;Training;Encoding;Memory management;Algorithm design and analysis;Computational modeling","computational complexity;object detection;video coding;video surveillance","background modeling;surveillance video coding;computational complexity;video content analysis;object detection;segment-and-weight based running average method;temporal segment;weighted average procedure;foreground pixel;encoder","","16","","11","","","","","IEEE","IEEE Conferences"
"On low repair complexity storage codes via group divisible designs","B. Zhu; K. W. Shum; H. Li; S. R. Li","Shenzhen Eng. Lab of Converged Networks Technology, Shenzhen Graduate School, Peking University, China; Institute of Network Coding, The Chinese University of Hong Kong, China; Shenzhen Eng. Lab of Converged Networks Technology, Shenzhen Graduate School, Peking University, China; Institute of Network Coding, The Chinese University of Hong Kong, China","2014 IEEE Symposium on Computers and Communications (ISCC)","","2014","","","1","5","Fractional repetition (FR) codes are a family of storage codes that provide efficient node repair at the minimum bandwidth regenerating point. Specifically, the repair process is exact and uncoded, but table-based. Existing constructions of FR codes are primarily based on combinatorial designs such as Steiner systems, resolvable designs, etc. In this paper, we present a new explicit construction of FR codes, which adopts the theory of uniform group divisible designs, termed GDDFR codes. Our codes achieve the storage capacity of random access and are available for a wide range of parameters. In addition, our techniques allow for constructing FR codes with parameters that are not covered by Steiner systems, which answers an open question put forward in prior work.","1530-1346","978-1-4799-4277","10.1109/ISCC.2014.6912604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912604","Distributed storage systems;fractional repetition codes;combinatorial designs;group divisible designs","Maintenance engineering;Decision support systems;Educational institutions;Bandwidth;Complexity theory;Equations;Network coding","codes;combinatorial mathematics;storage management","low repair complexity storage codes;fractional repetition codes;FR codes;minimum bandwidth regenerating point;combinatorial designs;Steiner systems;resolvable designs;uniform group divisible designs;GDDFR codes;random access storage capacity","","7","","21","","","","","IEEE","IEEE Conferences"
"Low-complexity and robust coding mode decision in the EVS coder","E. Ravelli; C. R. Helmrich; G. Fuchs; M. Multrus","Fraunhofer Institut f&#x00FC;r Integrierte Schaltungen (IIS), Am Wolfsmantel 33, 91058 Erlangen, Germany; International Audio Laboratories Erlangen, Am Wolfsmantel 33, 91058, Germany; Fraunhofer Institut f&#x00FC;r Integrierte Schaltungen (IIS), Am Wolfsmantel 33, 91058 Erlangen, Germany; Fraunhofer Institut f&#x00FC;r Integrierte Schaltungen (IIS), Am Wolfsmantel 33, 91058 Erlangen, Germany","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","5888","5892","Several state-of-the-art switched audio codecs employ the closed-loop mode decision to select the best coding mode at every frame. The closed-loop mode selection is known to have good performance but also high complexity. The new approach we propose in this paper is a low-complexity version of the closed-loop approach, based on similar decisions which compute the coding distortion of each mode and select the one with the lowest distortion. Our approach differs mainly in the way the coding distortions are calculated. We are able to notably reduce the complexity by only estimating the distortions without encoding and decoding the input for each mode. The new approach was implemented in the EVS codec standard and evaluated both objectively and subjectively. Compared to the closed-loop approach, it yields similar performance and lower complexity.","1520-6149;2379-190X","978-1-4673-6997","10.1109/ICASSP.2015.7179101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7179101","Speech and audio coding;switched coding;mode decision;mode selection;closed-loop;open-loop","Speech;Distortion;Codecs;Signal to noise ratio;Switches;Speech coding","audio coding;speech codecs;speech coding","robust coding mode decision;state-of-the-art switched audio codecs;closed-loop mode decision;closed-loop mode selection;EVS codec;coding distortions;speech coding","","2","","12","","","","","IEEE","IEEE Conferences"
"Reduced Complexity Sum-Product Algorithm for Decoding Nonlinear Network Codes and In-Network Function Computation","A. Gupta; B. S. Rajan","Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, India","IEEE Transactions on Communications","","2016","64","10","4070","4082","While the capacity, feasibility, and methods to obtain codes for network coding problems are well studied, the decoding procedure and complexity have not garnered much attention. In this paper, we pose the decoding problem at a sink node in a network as a marginalize product function (MPF) problem over the Boolean semiring and use the sum product (SP) algorithm on a suitably constructed factor graph to perform iterative decoding. The number of operations required to perform SP decoding is reduced using traceback. The number of operations required to perform SP decoding with and without traceback is obtained. For nonlinear network codes, we define fast decodability of a network code at sinks demanding all the messages and identify a sufficient condition for the same. Next, we consider the network function computation problem wherein the sink nodes demand a function of the messages. We present an MPF formulation for function computation at the sink nodes and use the SP algorithm to obtain the value of the demanded function. Though the proposed method can be used for decoding both linear and nonlinear network codes, it is advantageous only for the case of nonlinear network codes.","0090-6778;1558-0857","","10.1109/TCOMM.2016.2602347","Science and Engineering Research Board (SERB) of Department of Science and Technology (DST), Government of India, through J. C. Bose National Fellowship to B. Sundar Rajan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551154","Decoding;in-network function computation;network coding;sum-product algorithm;traceback","Encoding;Iterative decoding;Decoding;Network coding;Complexity theory;Routing;Network topology","iterative decoding;network coding;nonlinear codes","sum-product algorithm;decoding nonlinear network codes;in-network function computation;network coding;sink node;marginalize product function;Boolean semiring;sum product algorithm;iterative decoding","","1","","34","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Two-Dimensional Weight-Constrained Codes","E. Ordentlich; R. M. Roth","Hewlett–Packard Laboratories, Palo Alto, CA, USA; Computer Science Department, Technion, Haifa, Israel","IEEE Transactions on Information Theory","","2012","58","6","3892","3899","Two low complexity coding techniques are described for mapping arbitrary data to and from m × n binary arrays in which the Hamming weight of each row (respectively, column) is at most n/2 (respectively, m/2). One technique is based on flipping rows and columns of an arbitrary binary array until the Hamming weight constraint is satisfied in all rows and columns, and the other is based on a certain explicitly constructed “antipodal” matching between layers of the Boolean lattice. Both codes have a redundancy of roughly m+n and may have applications in next generation resistive memory technologies.","0018-9448;1557-9654","","10.1109/TIT.2012.2190380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6166887","Boolean lattice;resistive memory;two-dimensional coding;weight-constrained codes","Encoding;Complexity theory;Decoding;Redundancy;Arrays;Indexes;Resistance","encoding;Hamming codes","low complexity two-dimensional weight-constrained codes;flipping rows;flipping columns;arbitrary binary array;Hamming weight constraint;antipodal matching;Boolean lattice;next generation resistive memory technologies","","9","","16","","","","","IEEE","IEEE Journals & Magazines"
"Parallel low-complexity MIMO detection algorithm using QR decomposition and Alamouti space-time code","M. Arar; A. Yongacoglu","NA; NA","2010 European Wireless Conference (EW)","","2010","","","141","148","Fourth generation (4G) and beyond-4G wireless standards must support sub-1Gbps data rates with peak bandwidth efficiencies of up to 30 bit/s/Hz. It is widely recognized that the use of multiple antennas on both ends of the wireless link, commonly referred to as MIMO, is the technology that will make this a reality. The problem is that most existing MIMO algorithms are either too complex to be able to support such rates or do not lend themselves to a parallel implementation, a must-have feature that will enable the use of low-power multi-core processors. In this paper we propose a parallel architecture and associated algorithm that has reasonable complexity. Most importantly, the algorithm possesses a performance-on-demand feature that allows for complexity, performance and power consumption to be traded off on a packet-by-packet basis.","","978-1-4244-6001-4978-1-4244-5999-5978-1-4244-6000","10.1109/EW.2010.5483405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5483405","Alamouti code;QR decomposition;V-BLAST;successive interference cancellation","MIMO;Detection algorithms;Space time codes;Detectors;Maximum likelihood detection;Multicore processing;Energy consumption;Interference cancellation;Filtering algorithms;Payloads","4G mobile communication;communication complexity;MIMO communication;parallel architectures;space-time codes","parallel low-complexity MIMO detection algorithm;QR decomposition;Alamouti space-time code;4G wireless standards;multiple antennas;wireless link;parallel architecture","","2","","16","","","","","IEEE","IEEE Conferences"
"Complexity reduction in joint decoding of block coded signals in overloaded MIMO-OFDM system","Y. Doi; M. Inamori; Y. Sanada","Dept. of Electronics and Electrical Engineering, Keio University, Yokohama, Japan; Dept. of Electronics and Electrical Engineering, Keio University, Yokohama, Japan; Dept. of Electronics and Electrical Engineering, Keio University, Yokohama, Japan","2013 International Symposium on Intelligent Signal Processing and Communication Systems","","2013","","","590","595","This paper presents a low complexity joint decoding scheme of block coded signals in an overloaded multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) system. In previous literature, a joint maximum likelihood decoding scheme of block coded signals has been evaluated through theoretical analysis. The diversity gain with block coding prevents performance degradation induced by signal multiplexing. However, the computational complexity of the joint decoding scheme increases exponentially with the number of multiplexed signal streams. Thus, this paper proposes a two step joint decoding scheme for the block coded signals. The first step of the proposed scheme calculates metrics to reduce the number of the candidate codewords using decoding based on joint maximum likehood symbol detection. The second step of the proposed scheme carries out joint decoding on the reduced candidate codewords. It is shown that the proposed scheme reduces the complexity by about 1/174 for 4 signal stream transmission.","","978-1-4673-6361-7978-1-4673-6360","10.1109/ISPACS.2013.6704619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704619","","Joints;Maximum likelihood decoding;Measurement;Receiving antennas;Computational complexity;Multiplexing","block codes;communication complexity;diversity reception;maximum likelihood decoding;maximum likelihood detection;MIMO communication;OFDM modulation","complexity reduction;block coded signals;overloaded MIMO-OFDM system;low complexity joint decoding scheme;overloaded multiple-input multiple-output orthogonal frequency division multiplexing system;joint maximum likelihood decoding scheme;diversity gain;block coding;signal multiplexing;computational complexity;multiplexed signal streams;joint maximum likehood symbol detection;candidate codewords;signal stream transmission","","2","","10","","","","","IEEE","IEEE Conferences"
"Low-Complexity Concatenated LDPC-Staircase Codes","M. Barakatain; F. R. Kschischang","Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada","Journal of Lightwave Technology","","2018","36","12","2443","2449","A low-complexity soft-decision concatenated FEC scheme, consisting of an inner LDPC code and an outer staircase code, is proposed. The inner code is tasked with reducing the bit error probability below the outer-code threshold. The concatenated code is obtained by optimizing the degree distribution of the inner-code ensemble to minimize estimated data-flow, for various choices of outer staircase codes. A key feature that emerges from this optimization is that it pays to leave some inner codeword bits completely uncoded, thereby greatly reducing a significant portion of the decoding complexity. The tradeoff between required signal-to-noise ratio and decoding complexity of the designed codes is characterized by a Pareto frontier. Computer simulations of the resulting codes reveals that the net coding-gains of existing designs can be achieved with up to 71% reduction in complexity. A hardware-friendly quasi-cyclic construction is given for the inner codes, which can realize an energy-efficient decoder implementation, and even further complexity reductions via a layered message-passing decoder schedule.","0733-8724;1558-2213","","10.1109/JLT.2018.2812738","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307171","Concatenated codes;fiber-optic communications;forward error correction;low-density parity-check codes;optical transport network;pareto frontier;quasi-cyclic codes;staircase codes","Decoding;Complexity theory;Signal to noise ratio;Forward error correction;Optimization;Iterative decoding","concatenated codes;cyclic codes;decoding;error correction codes;error statistics;forward error correction;Pareto distribution;parity check codes","bit error probability;outer-code threshold;inner-code ensemble;outer staircase code;inner codeword bits;decoding complexity;net coding-gains;low-complexity concatenated LDPC-staircase codes;soft-decision concatenated FEC scheme;Pareto frontier;quasicyclic construction;energy-efficient decoder;layered message-passing decoder schedule","","5","","18","","","","","IEEE","IEEE Journals & Magazines"
"On the complexity of Unary Error Correction codes for the near-capacity transmission of symbol values from an infinite set","W. Zhang; R. G. Maunder; L. Hanzo","Electronics and Computer Science, University of Southampton, SO17 1BJ, United Kingdom; Electronics and Computer Science, University of Southampton, SO17 1BJ, United Kingdom; Electronics and Computer Science, University of Southampton, SO17 1BJ, United Kingdom","2013 IEEE Wireless Communications and Networking Conference (WCNC)","","2013","","","2795","2800","Unary Error Correction (UEC) codes have recently been proposed for the near-capacity Joint Source and Channel Coding (JSCC) of symbol values that are selected from a set having an infinite cardinality. In this paper, we characterize the computational complexity of UEC decoders and use complexity analysis for striking a desirable trade-off between the contradictory requirements of low complexity and near-capacity operation. We investigate a wide range of application scenarios and offer a deep insight into their beneficial parameterizations. In particular, we introduce puncturing for controlling the scheme's throughput and for facilitating fair comparisons with a Separate Source and Channel Coding (SSCC) benchmarker. The UEC scheme is found to offer almost 1.3 dB gain, when operating within 1.6 dB of the capacity bound. This is achieved without any increase in transmission energy, bandwidth, transmit duration or decoding complexity.","1525-3511;1525-3511;1558-2612","978-1-4673-5939-9978-1-4673-5938-2978-1-4673-5937","10.1109/WCNC.2013.6555003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6555003","Source coding;Video coding;Channel coding;Channel capacity;Iterative decoding;EXIT chart;Unary Error Correction Codes","Decoding;Vectors;Encoding;Complexity theory;Iterative decoding;Phase shift keying;Benchmark testing","","","","6","","14","","","","","IEEE","IEEE Conferences"
"A capacity-achieving coding scheme for the AWGN channel with polynomial encoding and decoding complexity","S. Vatedka; N. Kashyap","Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India","2016 Twenty Second National Conference on Communication (NCC)","","2016","","","1","6","A fundamental problem in coding theory is the design of an efficient coding scheme that achieves the capacity of the additive white Gaussian (AWGN) channel. In this article, we study a simple capacity-achieving nested lattice coding scheme whose encoding and decoding complexities are polynomial in the blocklength. Specifically, we show that by concatenating an inner nested lattice code with an outer Reed-Solomon code over an appropriate finite field, we can achieve the capacity of the AWGN channel. The main feature of this technique is that the encoding and decoding complexities grow as O(N<sup>2</sup>), while the probability of error decays exponentially in N, where N denotes the blocklength. We also show that this gives us a recipe to extend a high-complexity nested lattice code for a Gaussian channel to low-complexity concatenated code without any loss in the asymptotic rate. As examples, we describe polynomial-time coding schemes for the wiretap channel, and the compute-and-forward scheme for computing integer linear combinations of messages.","","978-1-5090-2361-5978-1-5090-2362","10.1109/NCC.2016.7561091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7561091","","Lattices;Complexity theory;AWGN channels;Channel coding;Maximum likelihood decoding","AWGN channels;channel capacity;channel coding;concatenated codes;decoding;error statistics;integer programming;linear programming;Reed-Solomon codes","AWGN channel;polynomial encoding-decoding complexity;additive white Gaussian channel capacity;capacity-achieving nested lattice coding scheme;Reed-Solomon code;error probability;high-complexity nested lattice code;low-complexity concatenated code;polynomial-time coding scheme;wiretap channel;compute-and-forward scheme;integer linear message combination","","1","","21","","","","","IEEE","IEEE Conferences"
"Dimensionality reduced decoding for the golden code with the worst-case complexity of O(m1.5) for low range of SNR","S. Kahraman; M. E. Çelebi","National Research Institute of Electronics and Cryptology (UEKAE), TÜBİTAK, 41470 Kocaeli, Turkey; Electronics and Communication Eng., Istanbul Technical University, 34469 Istanbul, Turkey","2012 IEEE Wireless Communications and Networking Conference (WCNC)","","2012","","","246","250","In this paper we introduce an efficient decoding method which is based on the dimensionality reduction of the search tree in the sphere decoder for the golden code in a low SNR regime. A codeword of the golden code has four independent m-QAM data symbols, hence, the required complexity of the exhaustive-search decoder is m4. An efficient implementation of the maximum-likelihood decoder for the golden code with a worst-case complexity is known to be proportional to m2.5. Additionally, in low range of SNR, sphere decoding has significantly high expected decoding complexity. Our motivation is for an efficient decoder with a worst-case complexity of no more than m2for a low SNR regime. In this purpose, we show that our proposed method has m1.5complexity in the worst-case with a loss of only 1 dB with respect to optimal decoding.","1558-2612;1525-3511;1525-3511","978-1-4673-0437-5978-1-4673-0436-8978-1-4673-0435","10.1109/WCNC.2012.6214212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6214212","Golden code;fast decoding;sphere decoding","Complexity theory;Maximum likelihood decoding;Signal to noise ratio;Phase shift keying;Bit error rate;Delay","maximum likelihood decoding;quadrature amplitude modulation;trees (mathematics)","dimensionality reduced decoding;golden code;worst-case complexity;efficient decoding method;dimensionality reduction;search tree;sphere decoder;independent m-QAM data symbols;exhaustive-search decoder;maximum-likelihood decoder;sphere decoding;decoding complexity","","2","","18","","","","","IEEE","IEEE Conferences"
"Low-Complexity Single-Channel Blind Separation of Co-Frequency Coded Signals","X. Liu; Y. L. Guan; S. N. Koh; Z. Liu; P. Wang","School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore; School of EEE, Nanyang Technological University, Singapore; School of EEE, Nanyang Technological University, Singapore","IEEE Communications Letters","","2018","22","5","990","993","Single-channel blind source separation (SCBSS) of uncoordinated, non-spread, co-frequency interfering signals with non-zero carrier frequency offset and timing offset, and without training sequence for channel estimation, is a challenging task. Iterative SCBSS of coded signals leads to good performance but is computationally expensive as it involves the turbo processing of multi-user per-survivor processing and soft-input soft-output channel decoding. In this letter, we propose a low-complexity SCBSS (LC-SCBSS) scheme, which reduces the computational complexity of the conventional iterative SCBSS by using interference-cancellation from the second iteration onward, and adaptive channel truncation for certain users. Simulation results show that the proposed LC-SCBSS reduces the computational complexity by more than 99%, with only a marginal degradation in performance.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2805332","Temasek Laboratories@NTU Signal Research Programme 2 Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8290696","Single-channel blind source separation;per-survivor processing;interference-cancellation;channel truncation","Detectors;Receivers;Indexes;Complexity theory;Channel estimation;Iterative decoding;Decoding","blind source separation;channel coding;channel estimation;computational complexity;decoding;interference suppression;iterative methods;radiofrequency interference;signal denoising;turbo codes","soft-input soft-output channel decoding;low-complexity SCBSS;computational complexity;conventional iterative SCBSS;adaptive channel truncation;low-complexity single-channel blind separation;single-channel blind source separation;nonzero carrier frequency;training sequence;channel estimation;turbo processing;co-frequency coded signals;uncoordinated nonspread co-frequency interfering signals;timing offset;multiuser per-survivor processing;low complexity LC-SCBSS scheme;interference-cancellation;marginal degradation","","2","","12","","","","","IEEE","IEEE Journals & Magazines"
"A low-complexity improved successive cancellation decoder for polar codes","O. Afisiadis; A. Balatsoukas-Stimming; A. Burg","Telecommunications Circuits Laboratory, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Switzerland; Telecommunications Circuits Laboratory, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Switzerland; Telecommunications Circuits Laboratory, &#x00C9;cole Polytechnique F&#x00E9;d&#x00E9;rale de Lausanne, Switzerland","2014 48th Asilomar Conference on Signals, Systems and Computers","","2014","","","2116","2120","Under successive cancellation (SC) decoding, polar codes are inferior to other codes of similar blocklength in terms of frame error rate. While more sophisticated decoding algorithms such as list- or stack-decoding partially mitigate this performance loss, they suffer from an increase in complexity. In this paper, we describe a new flavor of the SC decoder, called the SC flip decoder. Our algorithm preserves the low memory requirements of the basic SC decoder and adjusts the required decoding effort to the signal quality. In the waterfall region, its average computational complexity is almost as low as that of the SC decoder.","1058-6393","978-1-4799-8297-4978-1-4799-8295","10.1109/ACSSC.2014.7094848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7094848","","Decoding;Computational complexity;Signal to noise ratio;Memory management;Error analysis","computational complexity;decoding;error statistics;signal processing","low-complexity improved SC flip decoder;polar codes;frame error rate;signal quality;average computational complexity;successive cancellation decoding","","23","","10","","","","","IEEE","IEEE Conferences"
"Configurable low complexity decoder architecture for Quasi-Cyclic LDPC codes","S. A. Zied; A. T. Sayed; R. Guindi","Varkon Semiconductors, Cairo, Egypt; Varkon Semiconductors, Cairo, Egypt; Nile University, Giza, Egypt","2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013)","","2013","","","1","5","In this paper, we present a fully pipelined QC-LDPC decoder for 802.11n standard that supports variable block sizes and multiple code rates. The proposed architecture utilizes features of Quasi-Cyclic LDPC codes and layered decoding to reduce memory bits and interconnection complexity through efficient utilization of permutation network for forward and backward interconnection routing. Permutation network reorganization and small check node granularity reduced the overall resources required for routing, thus reducing the overall decoder dynamic power consumption. Proposed architecture has been synthesized using Virtex-6 FPGA and achieved 19% reduction in dynamic power consumption, 5% less logic resources and 12% increase in throughput.","","978-953-290-040","10.1109/SoftCOM.2013.6671861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671861","","Decoding;Iterative decoding;Routing;Equations;Complexity theory;IEEE 802.11n Standard","cyclic codes;decoding;field programmable gate arrays;parity check codes;wireless LAN","configurable low complexity decoder architecture;quasi-cyclic LDPC codes;fully pipelined QC-LDPC decoder;802.11n standard;variable block sizes;multiple code rates;memory bits;interconnection complexity;backward interconnection routing;forward interconnection routing;permutation network reorganization;small check node granularity;Virtex-6 FPGA;dynamic power consumption","","2","","17","","","","","IEEE","IEEE Conferences"
"Low-Complexity Transformed Encoder Architectures for Quasi-Cyclic Nonbinary LDPC Codes Over Subfields","X. Zhang; Y. Tai","Western Digital Corporation, San Jose, CA, USA; Western Digital Corporation, San Jose, CA, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","25","4","1342","1351","Quasi-cyclic low-density parity-check (QC-LDPC) codes are adopted in many digital communication and storage systems. The encoding of these codes is traditionally done by multiplying the message vector with a generator matrix consisting of dense circulant submatrices. To reduce the encoder complexity, this paper introduces two schemes making use of finite Fourier transform. We focus on QC-LDPC codes whose circulant submatrices are of dimension (2r- 1) × (2r- 1) and the entries are elements of GF(2p), where p divides r, and hence, GF(2p) is a subfield of GF(2r). These cover a broad range of codes, and binary LDPC codes are a special case. Making use of conjugacy constraints, low-complexity architectures are developed for finite Fourier and inverse transforms over subfields in this paper. In addition, composite field arithmetic is exploited to eliminate the computations associated with message mapping and reduce the complexity of Fourier transform. For a (2016, 1074) nonbinary QC-LDPC code whose generator matrix consists of circulants of dimension 63 × 63 with GF(22) entries, the proposed encoders achieve 22% area reduction compared with the conventional encoders without sacrificing the throughput.","1063-8210;1557-9999","","10.1109/TVLSI.2016.2630055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779152","Encoder;finite field;Fourier transform;low-density parity-check (LDPC) codes;nonbinary (NB);quasi-cyclic (QC);VLSI architecture","Encoding;Fourier transforms;Computer architecture;Generators;Complexity theory;Parity check codes;Decoding","cyclic codes;Fourier transforms;inverse transforms;matrix algebra;parity check codes","composite field arithmetic;inverse transforms;conjugacy constraints;circulant submatrices;finite Fourier transform;encoder complexity;message vector;generator matrix;storage system;digital communication;QC-LDPC code;quasicyclic nonbinary low-density parity-check code;low-complexity transformed encoder architecture","","","","12","","","","","IEEE","IEEE Journals & Magazines"
"On t-designs and bounds relating query complexity to error resilience in locally correctable codes","V. Lalitha; N. Prakash; G. M. Kamath; P. V. Kumar","Dept. of ECE, Indian Institute of Science, Bangalore - 560012, India; Dept. of ECE, Indian Institute of Science, Bangalore - 560012, India; Dept. of ECE, Indian Institute of Science, Bangalore - 560012, India; Dept. of ECE, Indian Institute of Science, Bangalore - 560012, India","2012 National Conference on Communications (NCC)","","2012","","","1","5","An n-length block code C is said to be r-query locally correctable, if for any codeword x ∈ C, one can probabilistically recover any one of the n coordinates of the codeword x by querying at most r coordinates of a possibly corrupted version of x. It is known that linear codes whose duals contain 2-designs are locally correctable. In this article, we consider linear codes whose duals contain t-designs for larger t. It is shown here that for such codes, for a given number of queries r, under linear decoding, one can, in general, handle a larger number of corrupted bits. We exhibit to our knowledge, for the first time, a finite length code, whose dual contains 4-designs, which can tolerate a fraction of up to 0.567/r corrupted symbols as against a maximum of 0.5/r in prior constructions. We also present an upper bound that shows that 0.567 is the best possible for this code length and query complexity over this symbol alphabet thereby establishing optimality of this code in this respect. A second result in the article is a finite-length bound which relates the number of queries r and the fraction of errors that can be tolerated, for a locally correctable code that employs a randomized algorithm in which each instance of the algorithm involves t-error correction.","","978-1-4673-0816-8978-1-4673-0815-1978-1-4673-0814","10.1109/NCC.2012.6176752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176752","","Decoding;Linear code;Error correction codes;Complexity theory;Parity check codes;Polynomials","block codes;decoding;dual codes;error correction codes;linear codes","t-designs;query complexity;error resilience;locally-correctable codes;n-length block code;codeword;linear codes;dual code;linear decoding;finite length code;finite-length bound;error fraction;t-error correction","","","","7","","","","","IEEE","IEEE Conferences"
"A pragmatic means for measuring the complexity of source code ensembles","O. Hummel; S. Burger","Software Design and Quality Group, Karlsruhe Institute of Technology (KIT), Germany; Software Engineering Group, University of Mannheim, Germany","2013 4th International Workshop on Emerging Trends in Software Metrics (WETSoM)","","2013","","","76","79","Most of the software metrics known and applied today are measured on a per file or even per function basis so that it is difficult to interpret their results for higher-order code ensembles such as components or whole systems. In order to overcome this weakness, we propose the hm-Index as a simple metric to condense the dependencies, i.e. the Fan-out, between source units in such code ensembles into a single number. As it is inspired by the h-Index in bibliometrics, it is based on a well-known procedure that already had significant impact in a different field. We expect the hm-Index to become a simple metric for comparing the code complexity of different components or systems in software engineering and present promising preliminary results from real-world systems confirming our assumption in this paper.","2327-0969;2327-0950","978-1-4673-6331","10.1109/WETSoM.2013.6619340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619340","Software Metrics;Code Quality and Complexity","Complexity theory;Software;Software metrics;Indexes;Software engineering","software metrics","source code ensembles complexity measurement;software metrics;higher-order code ensembles;hm-Index;software engineering;bibliometrics","","1","","20","","","","","IEEE","IEEE Conferences"
"Low complexity syndrome based decoding of Turbo codes","J. Geldmacher; K. Hueske; J. Götze; M. Kosakowski","Information Processing Lab, TU Dortmund University, Otto-Hahn-Strasse 4, 44227 Dortmund, Germany; Information Processing Lab, TU Dortmund University, Otto-Hahn-Strasse 4, 44227 Dortmund, Germany; Information Processing Lab, TU Dortmund University, Otto-Hahn-Strasse 4, 44227 Dortmund, Germany; Research In Motion Deutschland GmbH, Universitätsstr. 140","2012 IEEE International Symposium on Information Theory Proceedings","","2012","","","2371","2375","A new syndrome trellis based decoding approach for Turbo coded data is described in this paper. Based on estimating error symbols instead of code symbols, it inherently features options for reducing the computational complexity of the decoding process. After deriving required transition metrics, numerical results in terms of block error rate and required equivalent iterations are presented to demonstrate the efficiency of the approach.","2157-8117;2157-8095;2157-8095","978-1-4673-2579-0978-1-4673-2580-6978-1-4673-2578","10.1109/ISIT.2012.6283938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6283938","","Iterative decoding;Signal to noise ratio;Maximum likelihood decoding;Systematics;Convolutional codes;Complexity theory","computational complexity;decoding;error detection codes;trellis codes;turbo codes","low complexity syndrome based decoding;turbo codes;syndrome trellis based decoding approach;estimating error symbols;computational complexity;block error rate;equivalent iterations","","1","","19","","","","","IEEE","IEEE Conferences"
"A low complexity cryptosystem based on nonsystematic turbo codes","K. Ghavami; M. Naraghi-Pour","Division of Electrical and Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA 70803, United States; Division of Electrical and Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA 70803, United States","2015 IEEE International Conference on Communications (ICC)","","2015","","","7388","7393","A novel symmetric cryptosystem based on turbo codes with nonsystematic constituent codes is proposed. The proposed system introduces an interleaver for each constituent code and secures the interleavers with a secret key. It is verified by simulation results that without affecting the performance of the intended receiver, complete security for the unintended receiver can be achieved. More specifically, while the bit-error rate (BER) of the intended receiver remains the same as that of the insecure turbo code, for the unintended receiver BER is close to 0.5. The implementation issues for key-dependent interleaver design based on linear feedback shift registers are also discussed.","1550-3607;1938-1883","978-1-4673-6432-4978-1-4673-6431","10.1109/ICC.2015.7249507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7249507","Turbo codes;cryptosystem;data encryption;physical-layer security","Bit error rate;Security;Systematics;Turbo codes;Decoding;Receivers;Signal to noise ratio","computational complexity;cryptography;error statistics;interleaved codes;shift registers;telecommunication security;turbo codes","low complexity cryptosystem;nonsystematic turbo codes;symmetric cryptosystem;nonsystematic constituent codes;intended receiver;unintended receiver BER;bit-error rate;insecure turbo code;key-dependent interleaver design;linear feedback shift registers","","","","16","","","","","IEEE","IEEE Conferences"
"Design and Complexity Analysis of Reed Solomon Code Algorithm for Advanced RAID System in Quaternary Domain","V. Vasudevan; V. Sheshadri; S. K. R.; V. P. K.S.","NA; NA; NA; NA","2011 IEEE Computer Society Annual Symposium on VLSI","","2011","","","307","312","The complexity of programmable logic design has increased ever since its conception because of the increased focus on seamless integration of design creation. Interconnections, which occupy 60 to 90% of the chip area, play a vital role in deciding the power and delay in such designs. Hence there is an increased focus on Multi-Valued Logic, because of its inherent ability to reduce the number of interconnections. In this paper, we present the design of (n=15, k=13) Reed Solomon code algorithm for advanced Redundant Array of Independent Disks (RAID) system in quaternary domain, to tolerate multiple disk failures. We present two designs to implement the algorithm on a FPGA- a heterogeneous design consisting of quaternary and binary circuits, a complete quaternary design. The first design performs all computations using only binary EX-OR gates. This design requires more EX-OR operations than the binary counterpart. This number is a function of the generator polynomial used for encoding. The second design based on quaternary look-up tables is efficient and can be easily implemented. The look-up tables are in turn based on quaternary multiplexers that can be further optimized by reducing the feature size.","2159-3477;2159-3469;2159-3469","978-1-4577-0803-9978-0-7695-4447","10.1109/ISVLSI.2011.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992524","quaternary logic;RAID;Galois;Reed Solomon;erasure;failure;look-up table;encoder;decoder;disk failure","Polynomials;Algorithm design and analysis;Table lookup;Generators;Reed-Solomon codes;Integrated circuit interconnections;Logic gates","field programmable gate arrays;integrated circuit interconnections;logic design;logic gates;programmable logic devices;RAID;Reed-Solomon codes;table lookup","Reed Solomon code algorithm;advanced RAID system;quaternary domain;programmable logic design;multivalued logic;redundant array of independent disks;FPGA;binary EX-OR gates;quaternary look-up tables;quaternary multiplexers","","","","11","","","","","IEEE","IEEE Conferences"
"Rate-Distortion-Complexity Optimized Coding Mode Decision for HEVC","B. Huang; Z. Chen; Q. Cai; M. Zheng; D. Wu","Department of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian 350108, China.; Department of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian 350108, China.; Department of Electrical and Computer Engineering, University of Florida, Gainesville, Florida, USA.; Department of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian 350108, China.; Department of Electrical and Computer Engineering, University of Florida, Gainesville, Florida, USA.","IEEE Transactions on Circuits and Systems for Video Technology","","2019","PP","99","1","1","The newest generation of video coding standard, high efficiency video coding (HEVC), significantly improves the video compression efficiency by introducing more flexible block partitioning structures and richer coding modes than those of previous coding standards; however, the encoders suffer from high computational complexity, which greatly hinders their extensive application. Extensive studies on optimizing the complexity of HEVC encoders have been conducted. However, most studies do not effectively achieve a trade-off between the rate-distortion (RD) performance loss and complexity during rate-distortion optimization (RDO). In this paper, we mathematically define the complexity-constrained RDO problem as a constrained optimization problem of subset selection. Next, based on the classification methodology, the derivation process for this optimization problem is simplified to finding the adaptive threshold function in the feature space with extremely low complexity. The proposed method is also highly general and is applicable to algorithm design for various coding mode decisions, such as coding unit (CU) splitting, prediction unit (PU) partitioning and transform unit (TU) tree decision, and the global optimum can be achieved. Compared with existing methods, the experimental results show that the proposed method can reduce the coding time by 2-16% with the same RD performance loss and can decrease the BD rate by 0.1-1.2% under the same complexity. In addition, this method is capable of flexibly adjusting the complexity under different RDC trade-off requirements.","1051-8215;1558-2205","","10.1109/TCSVT.2019.2893396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620268","Rate-distortion-complexity;HEVC;coding mode decision","Encoding;Complexity theory;Optimization;Video coding;Standards;Distortion;Bit rate","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"Low Complexity, High Efficiency Probability Model for Hyper-spectral Image Coding","F. Auli-Llinas; J. Bartrina-Rapesta; J. Serra-Sagrista; M. W. Marcellin","NA; NA; NA; NA","2011 First International Conference on Data Compression, Communications and Processing","","2011","","","229","235","This paper describes a low-complexity, high-efficiency lossy-to-lossless coding scheme for hyper-spectral images. Together with only a 2D wavelet transform on individual image components, the proposed scheme achieves coding performance similar to that achieved by a 3D transform strategy that adds one level of wavelet decomposition along the depth axis of the volume. The proposed schemes operates by means of a probability model for symbols emitted by the bit plane coding engine. This probability model captures the statistical behavior of hyper-spectral images with high precision. The proposed method is implemented in the core coding system of JPEG2000 reducing computational costs by 25%.","","978-1-4577-1458-0978-0-7695-4528","10.1109/CCP.2011.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061130","Hyper-spectral image coding;bitplane image coding;JPEG2000","Encoding;Transform coding;Image coding;Sensors;Wavelet transforms;Table lookup","geophysical image processing;image coding;probability;wavelet transforms","low complexity high efficiency probability model;hyperspectral image coding;high-efficiency lossy-to-lossless coding scheme;2D wavelet transform;3D transform strategy;wavelet decomposition;bit plane coding engine;statistical behavior;JPEG2000 core coding system","","","","19","","","","","IEEE","IEEE Conferences"
"Low-complexity separable multiplier-less loop filter for video coding","A. Saxena; M. Aabed; M. Budagavi","Samsung Research America, 1301 E. Lookout Drive, Richardson, TX-75082; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA-30332; Samsung Research America, 1301 E. Lookout Drive, Richardson, TX-75082","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","3715","3719","In this paper, we present a low-complexity loop filter for video coding. We begin by presenting a set of non-Wiener based loop filters that can complement the Wiener-based adaptive loop filter which was considered as a tool for possible adoption in the HEVC standard. We devise filters which can be adaptively operated on different regions in an image. We devise a quad-tree based signaling for the filters, and present various loop filters: such as bilateral and Gaussian, as well as a separable 3-tap filter which can be implemented by just shifts and adds. The proposed 3-tap filter is thus hardware friendly with minimal complexity. In terms of compression performance, the proposed 3-tap filter can approach other sophisticated filters, albeit at a substantially reduced complexity; and can provide compression gains of 2.3% on average; and upto 7.0% for Low-Delay-P configuration over a data-set of 19 diverse HD, and UHD sequences of upto 8K resolution when implemented on top of the HM14.0 software for HEVC. Finally, we also present results for combining our proposed filters with the Wiener-based adaptive loop filter considered in HEVC, and illustrate that there is a significant amount of compression gain that can be achieved by loop filters for the next generation of video coding standard beyond HEVC.","","978-1-4799-8339-1978-1-4799-8338","10.1109/ICIP.2015.7351498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351498","Loop filter;bilateral filtering;separable filter;H.265/HEVC;video compression;Wiener filter;ALF","Decoding;Standards;Complexity theory;Encoding;Distortion;Video coding;Hardware","quadtrees;video coding;Wiener filters","low complexity separable multiplier-less loop filter;non-Wiener based loop filters;Wiener-based adaptive loop filter;HEVC standard;quad-tree based signaling;separable 3-tap filter;minimal complexity;sophisticated filters;compression gains;low-delay-P configuration;UHD sequences;video coding standard","","","","8","","","","","IEEE","IEEE Conferences"
"Reduced-complexity collaborative decoding of interleaved Reed-Solomon and Gabidulin codes","H. Kurzweil; M. Seidl; J. B. Huber","Department Mathematik, Universit&#x00E4;t Erlangen-N&#x00FC;rnberg, Germany; Lehrstuhl f&#x00FC;r Informations&#x00FC;bertragung, Universit&#x00E4;t Erlangen-N&#x00FC;rnberg, Germany; Department Mathematik, Universit&#x00E4;t Erlangen-N&#x00FC;rnberg, Germany","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","2557","2561","An alternative method for collaborative decoding of interleaved Reed-Solomon codes as well as Gabidulin codes for the case of high interleaving degree is proposed. As an example of application, simulation results are presented for a concatenated coding scheme using polar codes as inner codes.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6034030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034030","","Decoding;Polynomials;Reed-Solomon codes;Hafnium;Digital video broadcasting;Error analysis","concatenated codes;decoding;interleaved codes;Reed-Solomon codes","reduced-complexity collaborative decoding;interleaved Reed-Solomon codes;Gabidulin codes;concatenated coding;polar codes;inner codes","","6","","13","","","","","IEEE","IEEE Conferences"
"A Texture Complexity Based Fast Prediction Unit Size Selection Algorithm for HEVC Intra-coding","Y. Liu; X. Liu; P. Wang","NA; NA; NA","2014 IEEE 17th International Conference on Computational Science and Engineering","","2014","","","1585","1588","The latest High Efficiency Video Coding (HEVC) standard could achieve the highest coding efficiency compared with all existing video coding standards. However, the computational complexity is increased dramatically because all the possible combinations of the mode candidates are calculated in order to find the optimal rate distortion (RD) cost using Lagrange multiplier. In order to save the coding time of HEVC encoder, this paper proposes a texture complexity based fast prediction unit size selection algorithm for HEVC intra coding. The proposed algorithm could reduce around 38% encoding time in average with only 0.2% increase in BD-rate compared to the test model HM10.0 of HEVC.","","978-1-4799-7981-3978-1-4799-7980","10.1109/CSE.2014.292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023803","HEVC;intra prediction;video coding;texture complexity","Encoding;Video coding;Complexity theory;Prediction algorithms;Standards;Algorithm design and analysis;Partitioning algorithms","computational complexity;image texture;video codecs;video coding","texture complexity;fast prediction unit size selection algorithm;high efficiency video coding;HEVC standard;video coding standards;optimal rate distortion;Lagrange multiplier;HEVC encoder;HEVC intra coding;BD-rate","","2","","14","","","","","IEEE","IEEE Conferences"
"A Coded MIMO Spatial Multiplexing Approach with Low-Complexity Matched-Filter Detector and CRC-Assisted Interference Cancellation","K. Masukawa; H. Ochiai","Yokohama National University, Department of Electrical and Computer Engineering, Yokohama, 79–5 Tokiwadai, Hodogaya, Kanagawa, 240-8501, Japan; Yokohama National University, Department of Electrical and Computer Engineering, Yokohama, 79–5 Tokiwadai, Hodogaya, Kanagawa, 240-8501, Japan","MILCOM 2018 - 2018 IEEE Military Communications Conference (MILCOM)","","2018","","","541","546","Even though the benefit of multiple-input multiple-output (MIMO) spatial multiplexing systems is significant, its optimal detection becomes challenging as the number of antennas increases. In general, there is a trade-off relationship between the complexity and the achievable error rate performance for MIMO detection, and the maximum-likelihood detector (MLD) is known to be optimal with highest complexity, whereas that based on the matched filter (MF) has the lowest complexity but the remaining interference leads to poor error rate performance. Therefore, in this work, we propose a two-stage MIMO detection system that uses MF with parallel interference cancellation (PIC)at the first stage, followed by the MLD after cancellation of successfully detected streams using cyclic redundancy-check (CRC) codes. Simulation results demonstrate that the proposed two-stage approach may reduce the complexity of MIMO detection with better frame error rate (FER) performance compared to the single-stage MLD.","2155-7586;2155-7578","978-1-5386-7185-6978-1-5386-7186","10.1109/MILCOM.2018.8599727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599727","","MIMO communication;Detectors;Decoding;Complexity theory;Transmitting antennas;Interference cancellation","cyclic redundancy check codes;error statistics;interference suppression;matched filters;maximum likelihood detection;MIMO communication;space division multiplexing","low-complexity matched-filter detector;CRC-assisted interference cancellation;multiple-input multiple-output spatial multiplexing systems;maximum-likelihood detector;two-stage MIMO detection system;parallel interference cancellation;cyclic redundancy-check codes;frame error rate performance;single-stage MLD;coded MIMO spatial multiplexing approach","","","","13","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Delay-Tunable Coding Scheme for Visible Light Communication Systems","S. Zhao; X. Ma","College of Information Science and Technology, Jinan University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Photonics Technology Letters","","2016","28","18","1964","1967","Visible light communication (VLC) systems are expected to support a variety of applications, such as common-information broadcasting, real-time multimedia streaming, and large file downloading. Typically, these applications have different delay requirements. Hence, it is desirable to design high-performance coding scheme capable of supporting a wide range of delays but with acceptable hardware complexity. To achieve this, we propose a delay-tunable coding scheme for VLC systems based on block Markov superposition transmission of short non-binary low-density parity-check (NBLDPC) codes. The proposed coding scheme includes the following advantages: 1) it is easily configurable to fulfill different delay requirements while keeping the code rate constant; 2) it requires essentially the same hardware modules to implement the encoder/decoder as the involved short NBLDPC code; and 3) it can have a larger coding gain if a longer delay is tolerated. Numerical results are presented to show the expected tradeoff between delay and energy in a VLC system.","1041-1135;1941-0174","","10.1109/LPT.2016.2580562","National Natural Science Foundation of China; National Science Foundation through the Basic Research Project of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7491294","Block Markov superposition transmission;delay-tunable;light emitting diode (LED);non-binary LDPC codes;visible light communication (VLC)","Encoding;Delays;Decoding;Demodulation;Hardware;Visible light communication","encoding;free-space optical communication;Markov processes","low-complexity delay-tunable coding scheme;visible light communication systems;block Markov superposition transmission;short nonbinary low-density parity-check codes;constant code rate;encoder/decoder","","4","","20","","","","","IEEE","IEEE Journals & Magazines"
"DAVINCI Non-Binary LDPC codes: Performance and complexity assessment","I. Gutierrez; G. Bacci; J. Bas; A. Bourdoux; H. Gierszal; A. Mourad; S. Pleftschinger","Samsung Electronics Research Institute, Communications House, TW18-4QE, Staines (UK); Wireless Systems Engineering and Research (WISER), Livorno, 57123, Italy; Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Parc Mediterrani de la Tecnologia, 08860, Castelldefels (Spain); Wireless Research, Interuniversity MicroElectronics Center (IMEC), Kapeldreef 75, 3001 Leuven, Belgium; ITTI SP. Z.O.O. Rubiez 46, 61-612 Poznan, Poland; Samsung Electronics Research Institute, Communications House, TW18-4QE, Staines (UK); Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Parc Mediterrani de la Tecnologia, 08860, Castelldefels (Spain)","2010 Future Network & Mobile Summit","","2010","","","1","8","Non-Binary Low Density Parity Check (NB-LDPC) codes are expected to reduce the performance gap between binary LDPC and Turbo Codes for both short and medium length codewords, retaining in both cases the best of each. This paper presents the performance evaluation of NB-LDPC when compared to state-of-art channel coding techniques, namely Convolutional Turbo Codes (CTC) and Quasi-Cyclic LDPC. The performance evaluation is twofold: first at the link level and then at system level. In both analyses, it has been shown that the NB-LDPC codes outperform the benchmark codes giving a Signal to Noise Ratio gain up to 0.5 dB and improving the average throughput 5%. Besides, a complexity analysis is also provided for NB-LDPC showing that, using a low complexity decoder, the complexity is approximately one order of magnitude above that required in case of CTC or QC-LDPC.","","978-1-905824-18-2978-1-905824-16","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5722335","NB-LDPC;DBTC;LDPC;FEC;IMT-Advanced;WiMAX;LTE","Parity check codes;Decoding;Throughput;Complexity theory;Gain;AWGN;Encoding","channel coding;computational complexity;parity check codes;performance evaluation;turbo codes","DAVINCI nonbinary LDPC codes;complexity assessment;performance assessment;low density parity check codes;medium length codewords;performance evaluation;convolutional turbo codes;quasi-cyclic LDPC;complexity analysis;complexity decoder;channel coding techniques","","5","","19","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Viterbi Decoder for Space-Time Trellis Codes","K. Shr; H. Chen; Y. Huang","NA; NA; NA","IEEE Transactions on Circuits and Systems I: Regular Papers","","2010","57","4","873","885","Space-time trellis code (STTC) has been widely applied to coded multiple-input multiple-output (MIMO) systems because of its gains in coding and diversity; however, its great decoding complexity makes it less promising in chip realization compared to the space-time block code (STBC). The complexity of STTC decoding lies in the branch metric calculation in the Viterbi algorithm and increases significantly along with the number of antennas and the modulation order. Consequently, a low-complexity algorithm to mitigate the computational burden is proposed. The results show that more than 70%, 78%, and 83% of the computational complexity is reduced for 2 × 2, 3 × 3, and 4 × 4 MIMO configurations, respectively. Based on the proposed algorithm, a reconfigurable MISO STTC Viterbi decoder is designed and implemented using 0.18 ¿m 1P6M CMOS technology. The decoder achieves 11.14 Mbps, 8.36 Mbps, and 5.75 Mbps for 4-PSK, 8-PSK, and 16-QAM modulations, respectively.","1549-8328;1558-0806","","10.1109/TCSI.2009.2027648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5357411","Branch metrics;MIMO;space-time trellis code;Viterbi decoder","Viterbi algorithm;Decoding;Convolutional codes;MIMO;Wireless communication;CMOS technology;Space technology;Transmitting antennas;Block codes;Receiving antennas","antenna arrays;block codes;CMOS integrated circuits;computational complexity;MIMO communication;phase shift keying;space-time codes;trellis codes;Viterbi decoding","low-complexity Viterbi decoder;space-time trellis codes;coded multiple-input multiple-output systems;MIMO systems;space-time block code;computational complexity;reconfigurable MISO STTC Viterbi decoder;1P6M CMOS technology;4-PSK modulation;8-PSK modulation;16-QAM modulations;size 0.18 mum","","7","","32","","","","","IEEE","IEEE Journals & Magazines"
"RAID-6 reed-solomon codes with asymptotically optimal arithmetic complexities","S. Lin; A. Alloum; T. Y. Al-Naffouri","School of Information Science and Technology, University of Science and Technology of China (USTC); Nokia Bell Labs; Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST)","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","","2016","","","1","5","In computer storage, RAID 6 is a level of RAID that can tolerate two failed drives. When RAID-6 is implemented by Reed-Solomon (RS) codes, the penalty of the writing performance is on the field multiplications in the second parity. In this paper, we present a configuration of the factors of the second-parity formula, such that the arithmetic complexity can reach the optimal complexity bound when the code length approaches infinity. In the proposed approach, the intermediate data used for the first parity is also utilized to calculate the second parity. To the best of our knowledge, this is the first approach supporting the RAID-6 RS codes to approach the optimal arithmetic complexity.","2166-9589","978-1-5090-3254-9978-1-5090-3255","10.1109/PIMRC.2016.7794681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794681","RAID-6 codes;computational complexity;Distributed storage systems;erasure codes","Complexity theory;Decoding;Land mobile radio;Reflective binary codes;Reed-Solomon codes;STEM;Electronic mail","computational complexity;Reed-Solomon codes","RAID-6 Reed-Solomon codes;asymptotic optimal arithmetic complexity;RS codes;field multiplications;second-parity formula;optimal complexity bound;code length","","1","","14","","","","","IEEE","IEEE Conferences"
"Reduced-Complexity Intra Block Copy (IntraBC) Mode With Early CU Splitting and Pruning for HEVC Screen Content Coding","S. Tsang; Y. Chan; W. Kuang; W. Siu","Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Electronic and Information Engineering, Centre for Signal Processing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong","IEEE Transactions on Multimedia","","2019","21","2","269","283","A screen content coding (SCC) extension to high efficiency video coding has been developed to incorporate many new coding tools in order to achieve better coding efficiency for videos mixed with camera-captured content and graphics/text/animation. For instance, the Intra Block Copy (IntraBC) mode helps to encode repeating patterns within the same frame while the Palette mode aims at encoding screen content with a few major colors. However, the IntraBC mode brings along high computational complexity due to the exhaustive block matching within the same frame though there are already some constraints and fast approaches applied to the IntraBC mode to reduce its complexity. Thus, we propose a fast intracoding scheme to reduce the complexity of using the IntraBC mode in SCC. Screen content always contains no sensor noise resulting in the characteristics with pixel exactness along both horizontal and vertical directions. These characteristics pave the way for mode skipping and early coding unit (CU) splitting. Besides, early CU pruning and early termination are proposed based on the rate distortion cost to further reduce encoder complexity. Moreover, we also propose reducing the complexity of the IntraBC mode by checking the hash value of each block candidate and the current block during block matching. With our proposed scheme, the encoding time is reduced compared with the SCC while the coding efficiency can still be maintained with a minor increase in the bjontegaard delta bitrate.","1520-9210;1941-0077","","10.1109/TMM.2018.2856078","Council of the Hong Kong Special Administrative Region, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410679","Hash search;HEVC;intra block copy;screen content coding;video coding","Encoding;Copper;Color;Complexity theory;Indexes;High efficiency video coding;Videos","block codes;computational complexity;computer animation;image colour analysis;image matching;video cameras;video coding","exhaustive block matching;reduced-complexity intra block copy mode;early CU pruning;repeating pattern encoding;fast intracoding scheme;pixel exactness;early coding unit splitting;early termination;rate distortion cost;hash value;high computational complexity;Palette mode;camera-captured content;coding tools;high efficiency video coding;screen content coding extension;HEVC screen content coding;early CU splitting;coding efficiency;encoder complexity;early coding unit splitting;mode skipping;SCC;IntraBC mode;exhaustive block","","","","50","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Method for Evaluating the Distance Spectrum of Polar Codes","G. T. Chen; L. Cao; K. Qin; Z. Zhang","School of Electrical and Information Engineering, Fuqing Branch of Fujian Normal University, Fuzhou, 350300, China; University of Mississippi University, MS; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China","2018 IEEE 5G World Forum (5GWF)","","2018","","","383","388","Different from linear block codes that may use one single generator matrix for different channel conditions, polar codes have different generator matrices for different channel conditions. Few papers investigated polar codes from the view of different distance spectrum associated with different generator matrices. Successive cancellation list (SCL) decoding was proposed to evaluate the distance spectrum of polar codes. However, the large memory requirement of SCL decoding using a large number of survival paths can be beyond the memory constraint of normal computers. One practical method to evaluate the distance spectrum of polar codes with SCL decoding was implemented with the aid of hard disk to store the immediately generated data. In this paper, we modify the algorithm of SCL decoding proposed by Tal and Vardy to largely reduce the time complexity and the required space in hard disk. With this method, we further investigate the distance spectrum of polar codes with length N = 128 and N = 256, respectively.","","978-1-5386-4982-4978-1-5386-4983","10.1109/5GWF.2018.8516712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516712","","Generators;Maximum likelihood decoding;Measurement;Memory management;Block codes;Computers","block codes;channel coding;computational complexity;decoding;linear codes;matrix algebra","polar codes;linear block codes;low-complexity method;distance spectrum;generator matrices;SCL decoding;successive cancellation list decoding","","","","11","","","","","IEEE","IEEE Conferences"
"On the Achievable Diversity-Complexity Tradeoffs of Joint Network-Channel Coded Cooperative Communication","M. J. Chaudhry; A. Ul-Haq; S. Narayanan; F. Graziosi; N. Ashra; S. Rabbani","NA; NA; NA; NA; NA; NA","2015 International Conference on Computational Science and Computational Intelligence (CSCI)","","2015","","","674","679","In this paper, a novel mathematical framework for the analysis and optimization of Joint-Network -- Channel Coded Diversity (JNCCD) protocol is presented. The analysis is applicable to relay -- aided protocols based on the error propagation model, which rely on appropriately designed diversity combining demodulators at the destination. Wireless networks with an arbitrary number of sources and relays are considered. Arbitrary multilevel modulation schemes and network codes constructed over Galois Field (GF) are analyzed and JNCCD protocols are investigated. Our results show that if the modulation order is smaller than the Galois field, we are still able to achieve the higher diversity order with lower decoding complexity.","","978-1-4673-9795-7978-1-4673-9794","10.1109/CSCI.2015.178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424176","","Demodulation;Relays;Protocols;Decoding;Error probability;Diversity reception","channel coding;cooperative communication;diversity reception;Galois fields;network coding;protocols","achievable diversity-complexity tradeoffs;joint network-channel coded cooperative communication;relay-aided protocols;error propagation model;arbitrary multilevel modulation schemes;Galois field","","","","23","","","","","IEEE","IEEE Conferences"
"An Effective Low-Complexity Error-Floor Lowering Technique for High-Rate QC-LDPC Codes","H. Lee; P. Chou; Y. Ueng","Department of Electrical Engineering, Chang Gung University, Taoyuan, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering and the Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan","IEEE Communications Letters","","2018","22","10","1988","1991","This letter presents a low-complexity redecoding-based error-floor lowering technique for quasi-cyclic low-density parity-check codes, where a predetermined set of variable nodes are attenuated before the redecoding. Using a two-stage off-line search, the attenuation set is determined based on the error patterns collected from the standard decoding simulation. It is shown that the error floor can be effectively lowered, and only a negligible amount of complexity is introduced.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2018.2864982","Ministry of Science and Technology of the R.O.C.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434230","Low-complexity decoder;low-density parity-check (LDPC) codes;error floor","Attenuation;Decoding;Standards;Parity check codes;Indexes;Electrical engineering;Signal to noise ratio","cyclic codes;decoding;parity check codes;search problems;set theory","high-rate QC-LDPC codes;low-complexity redecoding-based error-floor lowering technique;quasicyclic low-density parity-check codes;error patterns;error floor;two-stage off-line search;attenuation set;standard decoding simulation","","1","","13","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-complexity maximum-likelihood decoding for 3D MIMO code","M. Liu; J. Hélard; M. Crussière; M. Hélard","Université Européenne de Bretagne (UEB), INSA, IETR, UMR 6164, F-35708, Rennes, France; Université Européenne de Bretagne (UEB), INSA, IETR, UMR 6164, F-35708, Rennes, France; Université Européenne de Bretagne (UEB), INSA, IETR, UMR 6164, F-35708, Rennes, France; Université Européenne de Bretagne (UEB), INSA, IETR, UMR 6164, F-35708, Rennes, France","2013 IEEE Wireless Communications and Networking Conference (WCNC)","","2013","","","4015","4020","The 3D MIMO code is a robust and efficient spacetime coding scheme for the distributed MIMO broadcasting. However, it suffers from the high computational complexity if the optimal maximum-likelihood (ML) decoding is used. In this paper we first investigate the unique properties of the 3D MIMO code and consequently propose a simplified decoding algorithm without sacrificing the ML optimality. Analysis shows that the decoding complexity is reduced from O(M8) to O(M4.5) in quasi-static channels when M-ary square QAM constellation is used. Moreover, we propose an efficient implementation of the simplified ML decoder which achieves a much lower decoding time delay compared to the classical sphere decoder with Schnorr-Euchner enumeration.","1525-3511;1525-3511;1558-2612","978-1-4673-5939-9978-1-4673-5938-2978-1-4673-5937","10.1109/WCNC.2013.6555219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6555219","","MIMO;Complexity theory;Maximum likelihood decoding;Signal to noise ratio;Matrix decomposition;Measurement","broadcast communication;computational complexity;maximum likelihood decoding;MIMO communication;quadrature amplitude modulation;space-time block codes;wireless channels","reduced-complexity maximum-likelihood decoding;3D MIMO code;spacetime coding scheme;distributed MIMO broadcasting;computational complexity;optimal maximum-likelihood decoding;M-ary square QAM constellation;quasi-static channels;ML decoder;lower decoding time delay;STBC","","1","","17","","","","","IEEE","IEEE Conferences"
"Low-Complexity Enhancement Layer Compression for Scalable Lossless Video Coding Based on HEVC","A. Heindel; E. Wige; A. Kaup","Institute of Multimedia Communications and Signal Processing, Friedrich-Alexander University Erlangen–Nürnberg, Erlangen, Germany; Institute of Multimedia Communications and Signal Processing, Friedrich-Alexander University Erlangen–Nürnberg, Erlangen, Germany; Institute of Multimedia Communications and Signal Processing, Friedrich-Alexander University Erlangen–Nürnberg, Erlangen, Germany","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","8","1749","1760","Lossless compression is desired, especially for professional applications like medical imaging. However, scalability may be necessary to transmit a fast preview of pictures when the channel capacity is limited. Furthermore, there may be the need for random access to single frames of a losslessly reconstructed video. We present and evaluate a lossy-to-lossless scalable video coding system in which the scalability is achieved by using a lossy base layer (BL) in conjunction with lossless compression of the reconstruction error in the enhancement layer (EL). High Efficiency Video Coding (HEVC) is employed for encoding the BL. For compression of the EL, we propose a low-complexity scheme called sample-based weighted prediction for EL coding (SELC). Furthermore, EL compression using JPEG-LS and the scalable extension of HEVC (SHVC) are evaluated. The performance of scalable coding using each of these three methods is compared with lossless single-layer (SL) coding using the intra-main RExt configuration. Experimental results show that the scalable coding with SELC achieves average bitrate savings of 7.3% compared with the lossless SL coding, while average bitrate reductions of only 6.3% and 3.9% are obtained using JPEG-LS and SHVC, respectively. Concerning runtime, the additional encoder runtimes of SELC and JPEG-LS compared with the BL coding are negligible. In contrast to that, the runtime of the SHVC EL encoder is much higher and similar to the runtime of the BL encoder.","1051-8215;1558-2205","","10.1109/TCSVT.2016.2556338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457283","Base layer (BL);enhancement layer (EL);High Efficiency Video Coding (HEVC);sample-based weighted prediction (SWP);sample-based weighted prediction for enhancement layer coding (SELC);scalable coding","Encoding;Image coding;Image reconstruction;Video coding;Streaming media;Codecs;Decoding","video coding","low-complexity enhancement layer compression;scalable lossless video coding;HEVC;lossy base layer;lossless compression;reconstruction error;high efficiency video coding;sample-based weighted prediction","","5","","28","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity coding scheme to approach multiple-access channel capacity","G. Song; J. Cheng","Dept. of Intelligent Information Eng. and Sci., Doshisha University, Kyoto, Japan 610-0321; Dept. of Intelligent Information Eng. and Sci., Doshisha University, Kyoto, Japan 610-0321","2015 IEEE International Symposium on Information Theory (ISIT)","","2015","","","2106","2110","A very simple coding scheme, called multi-user repetition-aided irregular repeat-accumulate (IRA) code, is proposed to approach the multiple-access channel (MAC) capacity. The main idea is that not only parity checks, which are generated by an IRA encoder, but also repetitions are used in each user's codeword to reduce the coding and decoding complexities. Repetition is a simple way to construct a low-rate code and is shown to be beneficial for multi-user decoding iteration. It is shown that there is a maximum allowable fraction of repetitions in codewords, below which channel capacity can always be approached by optimizing the degree distribution of the IRA encoder. As users increase, the maximum allowable fraction of repetitions increases, and therefore, very low encoding and decoding complexities are required.","2157-8117;2157-8095","978-1-4673-7704-1978-1-4673-7703","10.1109/ISIT.2015.7282827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7282827","","Encoding;Iterative decoding;Decoding;Complexity theory;Error correction;Error correction codes","access protocols;channel capacity;channel coding;communication complexity;iterative decoding;multiuser channels;optimisation","low complexity coding scheme;multiple access channel capacity;multiuser repetition aided irregular repeat accumulate;MAC capacity;user codeword;multiuser decoding iteration;degree distribution optimisation;IRA encoder;maximum allowable fraction;decoding complexity;encoding complexity","","8","","9","","","","","IEEE","IEEE Conferences"
"Flexible Mode Selection and Complexity Allocation in High Efficiency Video Coding","T. Zhao; Z. Wang; S. Kwong","Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, Canada; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong","IEEE Journal of Selected Topics in Signal Processing","","2013","7","6","1135","1144","To improve compression performance, High Efficiency Video Coding (HEVC) employs a quad-tree based block representation, namely Coding Tree Unit (CTU), which can support larger partitions and more coding modes than a traditional macroblock. Despite its high compression efficiency, the number of combinations of coding modes increases dramatically, which results in high computational complexity at the encoder. Here we propose a flexible framework for HEVC coding mode selection, with a user-defined global complexity factor. Based on linear programming, a hierarchical complexity allocation scheme is developed to allocate computational complexities among frames and Coding Units (CUs) to maximize the overall Rate-Distortion (RD) performance. In each CU, with the allocated complexity factor, a mode mapping based approach is employed for coding mode selection. Extensive experiments demonstrate that, with a series of global complexity factors, the proposed model can achieve good trade-offs between computational complexity and RD performance.","1932-4553;1941-0484","","10.1109/JSTSP.2013.2271421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548058","Complexity allocation;HEVC;linear programming;mode decision;video coding","Standards;Video coding;Linear programming","computational complexity;linear programming;quadtrees;video coding","flexible mode selection;hierarchical complexity allocation;high efficiency video coding;HEVC coding mode selection;compression performance improvement;quad-tree based block representation;coding tree unit;CTU;user-defined global complexity factor;linear programming;overall rate-distortion performance maximization;global complexity factors","","42","","42","","","","","IEEE","IEEE Journals & Magazines"
"Memory and complexity analysis of on-the-fly coding schemes for multimedia multicast communications","G. Smith; E. Lochin; J. Lacan; R. Boreli","Universite de Toulouse - ISAE UNSW - NICTA; Universite de Toulouse - ISAE; Universite de Toulouse - ISAE; UNSW - NICTA","2012 IEEE International Conference on Communications (ICC)","","2012","","","2070","2074","A new class of erasure codes for delay-constraint applications, called on-the-fly coding, have recently been introduced for their improvements in terms of recovery delay and achievable capacity. Despite their promising characteristics, little is known about the complexity of the systematic and non-systematic variants of this code, notably for live multicast transmission of multimedia content which is their ideal use case. Our paper aims to fill this gap and targets specifically the metrics relevant to mobile receivers with limited resources: buffer size requirements and computation complexity of the receiver. As our contribution, we evaluate both code variants on uniform and bursty erasure channels. Results obtained are unequivocal and demonstrate that the systematic codes outperform the non-systematic ones, in terms of both the buffer occupancy and computation overhead.","1938-1883;1550-3607;1550-3607","978-1-4577-2053-6978-1-4577-2052-9978-1-4577-2051","10.1109/ICC.2012.6364122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6364122","","Receivers;Systematics;Encoding;Complexity theory;Decoding;Maintenance engineering;Multimedia communication","buffer storage;mobile communication;multicast communication;multimedia communication;network coding;radio receivers;wireless channels","memory analysis;on-the-fly coding schemes;multimedia multicast communications;erasure codes;delay-constraint applications;multicast transmission;mobile receivers;computation complexity analysis;bursty erasure channels","","","","11","","","","","IEEE","IEEE Conferences"
"Power-aware complexity-scalable multiview video coding for mobile devices","M. Shafique; B. Zatt; S. Bampi; J. Henkel","Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Federal University of Rio Grande do Sul (UFRGS), Informatics Institute/PGMICRO, Porto Alegre, Brazil; Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany","28th Picture Coding Symposium","","2010","","","350","353","We propose a novel power-aware scheme for complexity-scalable multiview video coding on mobile devices. Our scheme exploits the asymmetric view quality which is based on the binocular suppression theory. Our scheme employs different quality-complexity classes (QCCs) and adapts at run time depending upon the current battery state. It thereby enables a run-time tradeoff between complexity and video quality. The experimental results show that our scheme is superior to state-of-the-art and it provides an up to 87% complexity reduction while keeping the PSNR close to the exhaustive mode decision. We have demonstrated the power-aware adaptivity between different QCCs using a laptop with battery charging and discharging scenarios.","","978-1-4244-7135-5978-1-4244-7134-8978-1-4244-7133","10.1109/PCS.2010.5702506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702506","","Complexity theory;PSNR;Encoding;Batteries;Mobile handsets;Video coding;Three dimensional displays","computational complexity;mobile computing;video coding","power aware complexity;mobile devices;power aware scheme;complexity scalable multiview video coding;asymmetric view quality;binocular suppression theory;quality complexity classes;current battery state;run-time tradeoff;complexity reduction;exhaustive mode decision;power aware adaptivity;battery charging;discharging scenarios","","3","","12","","","","","IEEE","IEEE Conferences"
"Computational complexity reduction algorithm for inter mode prediction in video coding","K. Bharanitharan; B. Somayehsadat; J. F. Wang; B. W. Chen","Department of Electrical Engineering, Korea University, Seoul, South Korea; Department of Electrical Engineering, Multimedia University, Malaysia; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","2010 2nd International Symposium on Aware Computing","","2010","","","24","30","H.264, MPEG-4 Part 10, is the latest digital video coding standard that achieves very high data compression by using several new coding features. One of the new feature is variable block sizes for inter frame coding to increase compression efficiency. However, to achieve this, the H.264 encoder employs a complex mode decision technique based on rate-distortion optimization (RDO) that requires high computational complexity, which significantly increases the encoder complexity. In this paper, we propose a classified region algorithm (CRA) that analyzes the spatial and temporal homogeneity of the block by using cross differences to reduce the number of modes that are required for RDO calculation in inter mode decision. The proposed low computational complexity algorithm significantly reduces the inter mode computations without affecting the video quality. The experimental results show that the proposed method is able to reduce the complexity by up to 67% on average with negligible degradation in subjective and objective quality.","","978-1-4244-8314-3978-1-4244-8313-6978-1-4244-8312","10.1109/ISAC.2010.5670508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670508","Complexity reduction;Edge detection;fast algorithm;gradient filter;H.264/AVC;inter prediction","Automatic voltage control","computational complexity;data compression;image classification;optimisation;video coding","computational complexity;inter mode prediction;video coding;H.264;MPEG-4;data compression;inter frame coding;compression efficiency;rate distortion optimization;encoder complexity;classified region algorithm;video quality","","","","24","","","","","IEEE","IEEE Conferences"
"Reduced complexity belief propagation decoders for polar codes","J. Lin; C. Xiong; Z. Yan","Department of Electrical and Computer Engineering, Lehigh University, PA, USA; Department of Electrical and Computer Engineering, Lehigh University, PA, USA; Department of Electrical and Computer Engineering, Lehigh University, PA, USA","2015 IEEE Workshop on Signal Processing Systems (SiPS)","","2015","","","1","6","Polar codes are newly discovered capacity-achieving codes, which have attracted lots of research efforts. Polar codes can be efficiently decoded by the low-complexity successive cancelation (SC) algorithm and the SC list (SCL) decoding algorithm. The belief propagation (BP) decoding algorithm not only is an alternative to the SC and SCL decoders, but also provides soft outputs that are necessary for joint detection and decoding. Both the BP decoder and the soft cancelation (SCAN) decoder were proposed for polar codes to output soft information about the coded bits. In this paper, first a belief propagation decoding algorithm, called reduced complexity soft cancelation (RCSC) decoding algorithm, is proposed. Let N denote the block length. Our RCSC decoding algorithm needs to store only 5N - 3 log-likelihood ratios (LLRs), significantly less than 4N - 2+ N log2N/2 and N(log2N +1) LLRs needed by the BP and SCAN decoders, respectively, when N ≥ 64. Besides, compared to the SCAN decoding algorithm, our RCSC decoding algorithm eliminates unnecessary additions over the real field. Then the simplified SC (SSC) principle is applied to our RCSC decoding algorithm, and the resulting SSC-aided RCSC (S-RCSC) decoding algorithm further reduces the computational complexity. Finally, based on the S-RCSC decoding algorithm, we propose a corresponding memory efficient decoder architecture, which has better error performance than existing architectures. Besides, our decoder architecture consumes less energy on updating LLRs.","","978-1-4673-9604-2978-1-4673-9603","10.1109/SiPS.2015.7344984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344984","","Decoding;Iterative decoding;Message passing;Schedules;Computer architecture;Complexity theory;Belief propagation","decoding","reduced complexity belief propagation decoders;polar codes;capacity-achieving codes;low-complexity successive cancelation algorithm;SC list decoding algorithm;BP decoding algorithm;soft cancelation;SCAN decoding algorithm;S-RCSC decoding algorithm","","10","","12","","","","","IEEE","IEEE Conferences"
"Low-complexity semi-parametric joint-stereo audio transform coding","C. R. Helmrich; A. Niedermeier; S. Bayer; B. Edler","International Audio Laboratories Erlangen, Am Wolfsmantel 33, 91058 Erlangen, Germany; Fraunhofer Institut f&#x00FC;r Integrierte Schaltungen (IIS), Am Wolfsmantel 33, 91058 Erlangen, Germany; International Audio Laboratories Erlangen, Am Wolfsmantel 33, 91058 Erlangen, Germany; International Audio Laboratories Erlangen, Am Wolfsmantel 33, 91058 Erlangen, Germany","2015 23rd European Signal Processing Conference (EUSIPCO)","","2015","","","794","798","Traditional audio codecs based on real-valued transforms utilize separate and largely independent algorithmic schemes for parametric coding of noise-like or high-frequency spectral components as well as channel pairs. It is shown that in the frequency-domain part of coders such as Extended HE-AAC, these schemes can be unified into a single algorithmic block located at the core of the modified discrete cosine transform path, enabling greater flexibility like semi-parametric coding and large savings in codec delay and complexity. This paper focuses on the stereo coding aspect of this block and demonstrates that, by using specially chosen spectral configurations when deriving the parametric side-information in the encoder, perceptual artifacts can be reduced and the spatial processing in the decoder can remain real-valued. Listening tests confirm the benefit of our proposal at intermediate bit-rates.","2076-1465","978-0-9928-6263","10.1109/EUSIPCO.2015.7362492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362492","Audio coding;decorrelation;MDCT;stereo","Encoding;Decoding;Transform coding;Filling;Codecs;Delays;Frequency-domain analysis","audio coding;codecs;discrete cosine transforms;frequency-domain analysis","low-complexity semiparametric audio transform coding;joint-stereo audio transform coding;audio codecs;real-valued transforms;channel pairs;frequency domain;extended HE-AAC;single algorithmic block;discrete cosine transform path;codec delay;parametric side-information;spatial processing","","4","","23","","","","","IEEE","IEEE Conferences"
"On the Tradeoff Between Accuracy and Complexity in Blind Detection of Polar Codes","P. Giard; A. Balatsoukas-Stimming; A. Burg","Telecommunications Circuits Laboratory, Ecole polytechnique fédérale de Lausanne (EPFL), Lausanne VD, 1015, Switzerland; Telecommunications Circuits Laboratory, Ecole polytechnique fédérale de Lausanne (EPFL), Lausanne VD, 1015, Switzerland; Telecommunications Circuits Laboratory, Ecole polytechnique fédérale de Lausanne (EPFL), Lausanne VD, 1015, Switzerland","2018 IEEE 10th International Symposium on Turbo Codes & Iterative Information Processing (ISTC)","","2018","","","1","5","Polar codes are a recent family of error-correcting codes with a number of desirable characteristics. Their disruptive nature is illustrated by their rapid adoption in the 5th-generation mobile-communication standard, where they are used to protect control messages. In this work, we describe a two-stage system tasked with identifying the location of control messages that consists of a detection and selection stage followed by a decoding one. The first stage spurs the need for polar-code detection algorithms with variable effort to balance complexity between the two stages. We illustrate this idea of variable effort for multiple detection algorithms aimed at the first stage. We propose three novel blind detection methods based on belief-propagation decoding inspired by early-stopping criteria. Then we show how their reliability improves with the number of decoding iterations to highlight the possible tradeoffs between accuracy and complexity. Additionally, we show similar tradeoffs for a detection method from previous work. In a setup where only one block encoded with the polar code of interest is present among many other blocks, our results notably show that, depending on the complexity budget, a variable number of undesirable blocks can be dismissed while achieving a missed-detection rate in line with the block-error rate of a complex decoding algorithm.","2165-4719;2165-4700","978-1-5386-7048-4978-1-5386-7047-7978-1-5386-7049","10.1109/ISTC.2018.8625366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625366","","Decoding;Iterative decoding;Complexity theory;Encoding;Measurement;Standards;Reliability","computational complexity;decoding;error correction codes;parity check codes","error-correcting codes;desirable characteristics;disruptive nature;rapid adoption;generation mobile-communication standard;control messages;two-stage system;polar-code detection algorithms;multiple detection algorithms;blind detection methods;belief-propagation decoding;decoding iterations;detection method;complexity budget;missed-detection rate;block-error rate;complex decoding algorithm","","1","","22","","","","","IEEE","IEEE Conferences"
"Simplified Multi-Level Quasi-Cyclic LDPC Codes for Low-Complexity Encoders","A. Mahdi; V. Paliouras","NA; NA","2012 IEEE Workshop on Signal Processing Systems","","2012","","","1","6","In this paper we propose a parity check matrix construction technique that simplifies the hardware encoders for Multi-Level-Quasi-Cyclic (ML-QC) LDPC codes. The proposed construction method is based on semi-random - ML-QC extension and appropriately selects shifting factors to reduce the density of the inverted matrix used in several encoding algorithms. The construction method derives low-complexity encoders with minimal degradation of error-correction performance, observable at low BER only. Furthermore a VLSI encoding architecture based on the suggested parity-check matrix (PCM) is also introduced. Experimental results show that the complexity of the proposed encoders depends on the density of the binary base matrix. A comparison with random QC codes reveals substantial complexity reduction without performance degradation for cases of practical interest. In fact a hardware complexity reduction by a factor of 7.5 is achieved, combined with the acceleration of the encoder, for certain cases.","2162-3570;2162-3562;2162-3562","978-1-4673-2987-3978-1-4673-2986-6978-0-7695-4856","10.1109/SiPS.2012.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363175","LDPC encoding;ML - quasi-cyclic LDPC;matrix inversion;hardware architecture","Encoding;Complexity theory;Parity check codes;Hardware;Decoding;Phase change materials;Bit error rate","binary codes;cyclic codes;matrix inversion;parity check codes;VLSI","multilevel quasi-cyclic LDPC code;low-complexity encoder;parity check matrix construction;hardware encoder;multilevel-quasi-cyclic LDPC code;construction method;shifting factor;inverted matrix;encoding algorithm;error-correction performance;VLSI encoding architecture;parity-check matrix;binary base matrix;hardware complexity reduction","","1","","28","","","","","IEEE","IEEE Conferences"
"A low complexity authentication protocol based on pseudorandomness, randomness and homophonic coding","M. Mihaljević; H. Watanabe; H. Imai","Mathematical Institute, Serbian Academy of Sci. and Arts, Kneza Mihaila 36, Belgrade, Serbia & RCIS-AIST, Tokyo, 101-0021 Japan; Res. Cent. Inform. Sec. (RCIS), Nat. Inst. Adv. Ind. Sci.&Tech. (AIST), Akihabara Daibiru, 1-18-13 Sotokanda, Chiyoda-ku, Tokyo, 101-0021 Japan; Faculty of Sciences and Engineering, Chuo University 1-13-27 Kasuga, Bunkyo-ku, Tokyo, 112-8551, Japan & RCIS-AIST, Tokyo, 101-0021 Japan","2010 International Symposium On Information Theory & Its Applications","","2010","","","690","695","An authentication protocol is proposed which originates from the elements of the authentication protocols belonging to the HB-family and the protocols based on employment of pseudorandom number generators. Desired features of the proposed protocol have been achieved via combining the pseudorandomnes generated by a compact keystream generator, randomness and dedicated homophonic and error-correction coding. It is shown that implementation complexity of the protocol is low. Security of the proposed protocol is considered from an information-theoretic and a computational-complexity points of view assuming the passive attacking The performed security evaluation yields an indication for a conjecture on the security within certain active attacking scenarios as well.","","978-1-4244-6017-5978-1-4244-6016","10.1109/ISITA.2010.5649666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5649666","","Protocols;Authentication;Complexity theory;Encoding;Generators;Equations","authorisation;communication complexity;cryptographic protocols;error correction codes;random number generation","low complexity authentication protocol;homophonic coding;pseudorandomness;randomness;HB-family protocol;pseudorandom number generator;keystream generator;error-correction coding;implementation complexity;information-theory;computational-complexity","","1","","24","","","","","IEEE","IEEE Conferences"
"A complexity scalable entropy coding scheme for video compression","T. Nguyen; D. Marpe; B. Bross; V. George; H. Kirchhoffer; M. Preiss; M. Siekmann; J. Stegemann; T. Wiegand","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Image Processing Department, Einsteinufer 37, D-10587 Berlin, Germany","2012 Picture Coding Symposium","","2012","","","421","424","In hybrid video coding, an entropy coding scheme transmits the quantized transform coefficients, resulting from block-based transformation and quantization of the difference between the prediction signal and the original signal, and additional side information. The state-of-the-art hybrid video coding standard H.264/AVC defines two different entropy coding schemes with different complexity-performance tradeoff. As a result, the support for two different entropy coding schemes has to be maintained and introduces several problems. To overcome these issues, a unified solution is proposed, which is based on the PIPE/V2V coding concept. It achieves the same complexity-performance trade-offs as the existing entropy coding schemes by scalability. The advantage of the proposed scheme over the existing concept is the usage of the same set of tools for all configurations. Simulation results and complexity analysis on hardware show the efficiency of the proposed scheme.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213377","","Decoding;Entropy coding;Throughput;Context;Context modeling","data compression;entropy codes;quantisation (signal);video coding","complexity scalable entropy coding;video compression;hybrid video coding;quantized transform;block-based transformation;prediction signal;side information;H.264/AVC;PIPE/V2V coding","","","","10","","","","","IEEE","IEEE Conferences"
"Reduced-Complexity Decoders of Long Reed-Solomon Codes Based on Composite Cyclotomic Fourier Transforms","X. Wu; Z. Yan; J. Lin","Department of Electrical and Computer Engineering, LSI Corporation, Milpitas, CA, USA; Department of Electrical and Computer Engineering, Lehigh University, PA, USA; Department of Electrical and Computer Engineering, Lehigh University, PA, USA","IEEE Transactions on Signal Processing","","2012","60","7","3920","3925","Long Reed-Solomon (RS) codes are desirable for digital communication and storage systems due to their improved error performance, but the high computational complexity of their decoders is a key obstacle to their adoption in practice. As discrete Fourier transforms (DFTs) can evaluate a polynomial at multiple points, efficient DFT algorithms are promising in reducing the computational complexities of syndrome based decoders for long RS codes. In this correspondence, we first propose partial composite cyclotomic Fourier transforms (CCFTs) and then devise syndrome based decoders for long RS codes over large finite fields based on partial CCFTs. The new decoders based on partial CCFTs achieve a significant saving of computational complexities for long RS codes. In comparison to previous results based on Horner's rule, our hardware implementation for a (2720, 2550) shortened RS code over GF(212) achieves much higher throughputs and better area-time complexity.","1053-587X;1941-0476","","10.1109/TSP.2012.2192435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176255","Complexity;composite cyclotomic Fourier transforms;Reed–Solomon codes","Decoding;Computational complexity;Polynomials;Frequency domain analysis;Hardware;Discrete Fourier transforms","computational complexity;decoding;Fourier transforms;Reed-Solomon codes","reduced-complexity decoders;long Reed-Solomon codes;composite cyclotomic Fourier transforms;computational complexity;Horner's rule;hardware implementation;area-time complexity","","2","","15","","","","","IEEE","IEEE Journals & Magazines"
"The MalSource Dataset: Quantifying Complexity and Code Reuse in Malware Development","A. Calleja; J. Tapiador; J. Caballero","Department of Computer Science, Universidad Carlos III de Madrid, 28911 Leganes, Madrid, Spain.; Department of Computer Science, Universidad Carlos III de Madrid, 28911 Leganes, Madrid, Spain.; IMDEA Software Institute, Madrid, Spain.","IEEE Transactions on Information Forensics and Security","","2018","PP","99","1","1","During the last decades, the problem of malicious and unwanted software (malware) has surged in numbers and sophistication. Malware plays a key role in most of today’s cyber attacks and has consolidated as a commodity in the underground economy. In this work, we analyze the evolution of malware from 1975 to date from a software engineering perspective. We analyze the source code of 456 samples from 428 unique families and obtain measures of their size, code quality, and estimates of the development costs (effort, time, and number of people). Our results suggest an exponential increment of nearly one order of magnitude per decade in aspects such as size and estimated effort, with code quality metrics similar to those of benign software. We also study the extent to which code reuse is present in our dataset. We detect a significant number of code clones across malware families and report which features and functionalities are more commonly shared. Overall, our results support claims about the increasing complexity of malware and its production progressively becoming an industry.","1556-6013;1556-6021","","10.1109/TIFS.2018.2885512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8568018","","Malware;Cloning;Complexity theory;Size measurement;Grippers","","","","","","","","","","","IEEE","IEEE Early Access Articles"
"High-Throughput Interpolator Architecture for Low-Complexity Chase Decoding of RS Codes","F. Garcia-Herrero; M. J. Canet; J. Valls; P. K. Meher","Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universidad Politécnica de Valencia, Spain; Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universidad Politécnica de Valencia, Spain; Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universidad Politécnica de Valencia, Spain; Department of Embedded Systems, Institute for Infocomm Research, Singapore","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2012","20","3","568","573","In this paper, a high-throughput interpolator architecture for soft-decision decoding of Reed-Solomon (RS) codes based on low-complexity chase (LCC) decoding is presented. We have formulated a modified form of the Nielson's interpolation algorithm, using some typical features of LCC decoding. The proposed algorithm works with a different scheduling, takes care of the limited growth of the polynomials, and shares the common interpolation points, for reducing the latency of interpolation. Based on the proposed modified Nielson's algorithm we have derived a low-latency architecture to reduce the overall latency of the whole LCC decoder. An efficiency of at least 39%, in terms of area-delay product, has been achieved by an LCC decoder, by using the proposed interpolator architecture, over the best of the previously reported architectures for an RS(255,239) code with eight test vectors. We have implemented the proposed interpolator in a Virtex-II FPGA device, which provides 914 Mb/s of throughput using 806 slices.","1063-8210;1557-9999","","10.1109/TVLSI.2010.2103961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5706404","Algebraic soft-decision decoding;interpolation;low-complexity chase (LCC);low latency;Nielson's algorithm;Reed–Solomon (R-S) codes","Decoding;Interpolation;Computer architecture;Polynomials;Very large scale integration;Complexity theory;Random access memory","decoding;delays;field programmable gate arrays;interpolation;polynomials;Reed-Solomon codes;scheduling;vectors","high-throughput interpolator architecture;low-complexity chase decoding;soft-decision decoding;Reed-Solomon code;RS code;LCC decoding;Nielson interpolation algorithm;polynomial;area-delay product;eight test vector;Virtex-II FPGA device;bit rate 914 Mbit/s","","6","","16","","","","","IEEE","IEEE Journals & Magazines"
"Design of Protograph-Based LDPC Codes with Limited Decoding Complexity","C. Tang; M. Jiang; C. Zhao; H. Shen","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","IEEE Communications Letters","","2017","21","12","2570","2573","In this letter, we present a design method to find good protograph-based low-density parity-check codes with the numbers of nonzero elements in the parity-check matrix and decoding iterations both strictly limited. We utilize the protograph-based extrinsic information transfer (PEXIT) analysis to evaluate the error performance and employ the differential evolution algorithm to devise a well-performed coding scheme. In particular, the number of iterations in the PEXIT analysis is set to a small value due to the limited iterations in the decoding process. Both numerical analyses and simulations verify that the resulting codes exhibit better performance than conventional code designs under the limitation of decoding complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2017.2757941","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053758","EXIT;protograph-based LDPC codes;limited iterations;decoding complexity","Iterative decoding;Complexity theory;Algorithm design and analysis;Convergence;Decoding;Linear programming","evolutionary computation;iterative decoding;matrix algebra;parity check codes","design method;good protograph;nonzero elements;parity-check matrix;decoding iterations;differential evolution algorithm;PEXIT analysis;decoding process;resulting codes;protograph-based low-density parity-check codes;protograph-based LDPC codes;limited decoding complexity;protograph-based extrinsic information transfer analysis;error performance evaluation","","4","","19","","","","","IEEE","IEEE Journals & Magazines"
"Complexity-aware adaptive spatial pre-processing for ROI scalable video coding with dynamic transition region","D. Grois; O. Hadar","Comm. Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Comm. Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel","2011 18th IEEE International Conference on Image Processing","","2011","","","741","744","We present a complexity-aware adaptive spatial pre- processing (pre-filtering) scheme for the efficient Region-of-Interest (ROI) Scalable Video Coding (SVC). According to the proposed approach, we adaptively vary various parameters of the SVC pre-filters, such as standard deviations, a number of filters for the dynamic pre-processing of a transition region between the ROI and background, etc., thereby enabling to dynamically adjust the desired SVC settings. In addition, our adaptive spatial pre-filtering system is based on an SVC computational complexity-rate-distortion (C-R-D) analysis, thereby adding a complexity dimension to the conventional Region-of-Interest SVC R-D analysis. As a result, the ROI SVC visual presentation quality is significantly improved, which can be especially useful for various resource-limited devices, such as mobile devices. The performance of the presented adaptive spatial ROI SVC pre-processing scheme is evaluated and tested in detail, further comparing it to the Joint Scalable Video Model reference software (JSVM 9.19) and demonstrating significant improvements.","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116661","Scalable Video Coding (SVC);Regions-of-interest (ROI) video coding;pre-processing/pre-filtering;image/video coding;ROI scalability;high-quality visual presentation;H.264/AVC","Static VAr compensators;Video coding;Computational complexity;Image coding;Scalability;Video sequences;Mobile communication","adaptive signal processing;computational complexity;filtering theory;rate distortion theory;video coding","complexity-aware adaptive spatial preprocessing;dynamic transition region;region-of-interest scalable video coding;dynamic preprocessing;adaptive spatial prefiltering system;computational complexity-rate-distortion analysis","","7","","11","","","","","IEEE","IEEE Conferences"
"Low Complexity HEVC INTRA Coding for High-Quality Mobile Video Communication","Y. Zhang; S. Kwong; G. Zhang; Z. Pan; H. Yuan; G. Jiang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Jiangsu Engineering Center of Network Monitoring, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Engineering, Shandong University, Ji’nan, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China","IEEE Transactions on Industrial Informatics","","2015","11","6","1492","1504","INTRA video coding is essential for high quality mobile video communication and industrial video applications since it enhances video quality, prevents error propagation, and facilitates random access. The latest high-efficiency video coding (HEVC) standard has adopted flexible quad-tree-based block structure and complex angular INTRA prediction to improve the coding efficiency. However, these technologies increase the coding complexity significantly, which consumes large hardware resources, computing time and power cost, and is an obstacle for real-time video applications. To reduce the coding complexity and save power cost, we propose a fast INTRA coding unit (CU) depth decision method based on statistical modeling and correlation analyses. First, we analyze the spatial CU depth correlation with different textures and present effective strategies to predict the most probable depth range based on the spatial correlation among CUs. Since the spatial correlation may fail for image boundary and transitional areas between textural and smooth areas, we then present a statistical model-based CU decision approach in which adaptive early termination thresholds are determined and updated based on the rate-distortion (RD) cost distribution, video content, and quantization parameters (QPs). Experimental results show that the proposed method can reduce the complexity by about 56.76% and 55.61% on average for various sequences and configurations; meanwhile, the RD degradation is negligible.","1551-3203;1941-0050","","10.1109/TII.2015.2491646","National Natural Science Foundation of China; Shenzhen Overseas High-Caliber Personnel Innovation and Entrepreneurship; Guangdong Special Support Program for Youth Science and Technology Innovation Talents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299678","High efficiency video coding;power efficient;low complexity;spatial correlation;coding unit;Coding unit (CU);high-efficiency video coding (HEVC);low complexity;power efficient;spatial correlation","Encoding;Complexity theory;Correlation;Video coding;Informatics;Mobile communication;Transforms","computational complexity;mobile computing;quadtrees;statistical analysis;video coding","low complexity HEVC INTRA coding;high-quality mobile video communication;INTRA video coding;high-efficiency video coding;coding complexity;statistical modeling;correlation analysis;rate-distortion cost distribution;video content;quantization parameters;flexible quad-tree-based block structure;complex angular INTRA prediction","","16","","33","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Decoding of Block Turbo Codes Based on the Chase Algorithm","J. Son; K. Cheun; K. Yang","Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Korea; Communications Research Group, Mobile Communication Business, Samsung Electronics Company, Ltd., Suwon, Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, Korea","IEEE Communications Letters","","2017","21","4","706","709","Block turbo codes (BTCs) are constructed by serially concatenating linear block codes and iteratively decoded by letting each component code be decoded in two stages. The Chase algorithm is employed in the first stage to make a list of candidate codewords by generating a fixed number of test sequences (TSs) and algebraically decoding them, regardless of the signal-to-noise ratio or the iteration number. In the second stage, the extrinsic information is generated for iterative decoding. In this letter, we propose a low-complexity decoding algorithm for BTCs. The proposed algorithm first checks whether an algebraic hard-decision decoder outputs a codeword for a given decoder input vector, and then adaptively applies one of the two estimation rules. Based on these two rules, the number of TSs in the proposed algorithm can be made monotonically decreasing with iterations. Numerical results demonstrate that the proposed algorithm has much lower computational complexity with a negligible performance loss, compared with the conventional decoding scheme based on the Chase algorithm.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2017.2650233","National Research Foundation (NRF) of Korea through the Ministry of Science, ICT and Future Planning (MSIP) of the Korea Government; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811190","Adaptive decoding algorithm;block turbo codes (BTCs);Chase algorithm;decoding complexity","Iterative decoding;Decoding;Signal to noise ratio;Complexity theory;Turbo codes;Block codes;Reliability","adaptive codes;algebraic codes;block codes;computational complexity;concatenated codes;iterative decoding;linear codes;turbo codes","block turbo code;serially concatenating linear block code;algebraic decoding;signal-to-noise ratio;iteration number;iterative decoding;low-complexity decoding algorithm;algebraic hard-decision decoder;lower computational complexity;Chase algorithm","","2","","10","","","","","IEEE","IEEE Journals & Magazines"
"Complexity reduction for 3D-HEVC depth map coding based on early Skip and early DIS scheme","R. Conceição; G. Avila; G. Corrêa; M. Porto; B. Zatt; L. Agostini","Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel); Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel); Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel); Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel); Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel); Group of Architectures and Integrated Circuits (GACI) - PPGC - Federal University of Pelotas (UFPel)","2016 IEEE International Conference on Image Processing (ICIP)","","2016","","","1116","1120","This paper presents a novel early Skip/DIS mode decision for 3D-HEVC depth encoding which aims at reducing the complexity effort of this process. The proposed solution is based on an adaptive threshold model, which takes into consideration the occurrence rate of both Skip and DIS modes. Occurrence analysis showed that the lower is the Skip and DIS Rate-Distortion cost, the higher is the probability of these modes being chosen. Furthermore, software evaluations showed that the proposed early Skip/DIS scheme is capable of reducing the depth coder complexity in 24.4% for a target hit rate of 99%, and in 33.7% for a target hit rate of 95%, leading to a negligible coding efficiency penalty in both scenarios.","2381-8549","978-1-4673-9961-6978-1-4673-9962","10.1109/ICIP.2016.7532531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532531","3D-HEVC;depth map;early skip;early DIS;complexity reduction","Decision support systems;Hafnium;Measurement;High definition video","computational complexity;decision making;probability;video coding","3D-high efficiency video coding;depth coder complexity;software evaluations;skip rate-distortion cost;DIS rate-distortion cost;occurrence analysis;adaptive threshold model;3D-HEVC depth encoding;early DIS mode decision;early skip mode decision;3D-HEVC depth map coding;complexity reduction","","4","","15","","","","","IEEE","IEEE Conferences"
"Low-Complexity MAP-Based Successive Data Detection for Coded OFDM Systems Over Highly Mobile Wireless Channels","E. Panayirci; H. Dogan; H. V. Poor","Department of Electronics Engineering, Kadir Has University, Istanbul, Turkey; Department of Electrical and Electronics Engineering, Istanbul University, Istanbul, Turkey; Department of Electrical and Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Vehicular Technology","","2011","60","6","2849","2857","This paper is concerned with the challenging and timely problem of data detection for coded orthogonal frequency-division multiplexing (OFDM) systems in the presence of frequency-selective and very rapidly time varying channels. New low-complexity maximum a posteriori probability (MAP) data detection algorithms are proposed based on sequential detection with optimal ordering (SDOO) and sequential detection with successive cancellation (SDSC). The received signal vector is optimally decomposed into reduced dimensional subobservations by exploiting the banded structure of the frequency-domain channel matrix whose bandwidth is a parameter to be adjusted according to the speed of the mobile terminal. The data symbols are then detected by the proposed algorithms in a computationally efficient way by means of the Markov chain Monte Carlo (MCMC) technique with Gibbs sampling. The impact of the imperfect channel state information (CSI) on the bit error rate (BER) performance of these algorithms is investigated analytically and by computer simulations. A detailed computational complexity investigation and simulation results indicate that, particularly, the algorithm based on SDSC has significant performance and complexity advantages and is very robust against channel estimation errors compared with existing suboptimal detection and equalization algorithms proposed earlier in the literature.","0018-9545;1939-9359","","10.1109/TVT.2011.2158564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5783526","Fast time-varying channels;Gibbs sampling;intercarrier interference (ICI);Markov chain;maximum a posteriori probability (MAP) detection;Monte Carlo technique;orthogonal frequency-division multiplexing (OFDM)","OFDM;Equalizers;Complexity theory;Frequency domain analysis;Mobile communication;Receivers;Signal processing algorithms","communication complexity;error statistics;Markov processes;maximum likelihood estimation;mobile radio;modulation coding;Monte Carlo methods;OFDM modulation;sampling methods;time-varying channels;wireless channels","low complexity MAP;successive data detection;coded OFDM system;mobile wireless channel;coded orthogonal frequencydivision multiplexing system;time varying channel;maximum a posteriori probability data detection algorithm;sequential detection with successive cancellation;sequential detection with optimal ordering;frequency domain channel matrix;Markov chain Monte Carlo technique;Gibbs sampling;channel state information;bit error rate;computer simulation;computational complexity;channel estimation error;equalization algorithm","","21","","25","","","","","IEEE","IEEE Journals & Magazines"
"Complexity of Decoding Positive-Rate Primitive Reed–Solomon Codes","Q. Cheng; D. Wan","School of Computer Science, University of Oklahoma, Norman; Department of Mathematics, University of California, Irvine","IEEE Transactions on Information Theory","","2010","56","10","5217","5222","It has been proved that the maximum likelihood decoding problem of Reed-Solomon codes is NP-hard. However, the length of the code in the proof is at most polylogarithmic in the size of the alphabet. For the complexity of maximum likelihood decoding of the primitive Reed-Solomon code, whose length is one less than the size of alphabet, the only known result states that it is at least as hard as the discrete logarithm in some cases where the information rate unfortunately goes to zero. In this paper, it is proved under a well known cryptography hardness assumption that: 1) There does not exist a randomized polynomial time maximum likelihood decoder for the Reed-Solomon code family [q, k(q)]q, where k(x) is any function in Z+→ Z+computable in time xO(1)satisfying √x ≤ k(x) ≤ x - √x. 2) There does not exist a randomized polynomial time bounded-distance decoder for primitive Reed-Solomon codes at distance 2/3 + ϵ of the minimum distance for any constant 0 <; ϵ <; 1/3. In particular, this rules out the possibility of a polynomial time algorithm for maximum likelihood decoding problem of primitive Reed-Solomon codes of any rate under the assumption.","0018-9448;1557-9654","","10.1109/TIT.2010.2060234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5571830","Computational complexity;maximum likelihood decoding;Reed–Solomon codes","Maximum likelihood decoding;Polynomials;Information rates;Complexity theory;Cryptography","maximum likelihood decoding;optimisation;Reed-Solomon codes","positive-rate primitive Reed-Solomon codes;maximum likelihood decoding;NP-hard problem;information rate;cryptography hardness assumption;randomized polynomial time bounded-distance decoder","","14","","9","","","","","IEEE","IEEE Journals & Magazines"
"Cooperative Versus Receiver Coded Diversity with Low-Complexity Encoding and Decoding","S. E. A. Alnawayseh; P. Loskot","NA; NA","2010 IEEE 71st Vehicular Technology Conference","","2010","","","1","5","The amplify-and-forward (AF) and the decode-and-forward (DF) cooperative coded diversity is compared with the conventional receiver coded diversity in terms of the bit error rate (BER). The comparison assumes channel coding with non-binary modulations, and fast fading channels with path-loss attenuation proportional to the distance between nodes. Numerical results obtained by extensive computer simulations suggest that, for our path-loss system model, the AF diversity outperforms the receiver diversity provided that the relay is closer to the source and to the destination than the distance between the source and the destination. This condition can be satisfied more easily provided that there is more than one relay about the source, and for higher order modulations. In addition, the BER performance of the DF relaying is found to be more sensitive to the relay location than the BER performance of the AF relaying.","1550-2252;1550-2252","978-1-4244-2518-1978-1-4244-2519","10.1109/VETECS.2010.5493780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493780","","Encoding;Decoding;Relays;Bit error rate;Channel coding;Modulation coding;Fading;Attenuation;Computer simulation;Protective relaying","communication complexity;decoding;diversity reception;encoding;error statistics;modulation","receiver coded diversity;low-complexity encoding;decoding;amplify-and-forward cooperative coded diversity;decode-and-forward cooperative coded diversity;bit error rate;BER;channel coding;nonbinary modulations;fast fading channels;path-loss attenuation;computer simulations;path-loss system model;AF diversity;higher order modulations;AF relaying","","1","","11","","","","","IEEE","IEEE Conferences"
"Ultra low complexity soft output detector for non-binary LDPC coded large MIMO systems","P. Suthisopapan; A. Meesomboon; K. Kasai; V. Imtawil","Dept. of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand 40002; Dept. of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand 40002; Dept. of Communications and Integrated Systems, Tokyo Institute of Technology, 152-8550 Tokyo, Japan; Dept. of Electrical Engineering, Faculty of Engineering, Khon Kaen University, Thailand 40002","2012 7th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2012","","","230","234","The theoretic results of MIMO capacity tell us that the higher the number of antennas are employed, the higher the transmission rate is. This makes MIMO systems with hundreds of antennas very attractive but one of the major problems that obstructs such large dimensional MIMO systems from the practical realization is a high complexity of the MIMO detector. We present in this paper the new soft output MIMO detector based on matched filtering that can be applied to the large MIMO systems which are coded by the powerful non-binary LDPC codes. The per-bit complexity of the proposed detector is just 0.28% to that of low complexity soft output MMSE detector and scales only linearly with a number of antennas. Furthermore, the coded performances with small information length 800 bits are within 4.2 dB from the associated MIMO capacity.","2165-4719;2165-4700;2165-4700","978-1-4577-2115-1978-1-4577-2114-4978-1-4577-2113","10.1109/ISTC.2012.6325233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325233","","MIMO;Detectors;Complexity theory;Parity check codes;Transmitting antennas;Receiving antennas;Signal to noise ratio","antennas;communication complexity;least mean squares methods;MIMO communication;parity check codes;signal detection","ultra low complexity soft output detector;nonbinary LDPC coded large MIMO systems;antennas;transmission rate;MIMO detector;MMSE detector;multiple input multiple output systems;minimum mean square error techniques","","3","","20","","","","","IEEE","IEEE Conferences"
"A low-complexity pre-processing system for restoring low-quality QR code images","D. Munoz-Mejias; I. Gonzalez-Diaz; F. Diaz-de-Maria","Department of Signal Theory and Communications at Universidad Carlos III de Madrid; Student Member; Department of Signal Theory and Communications at Universidad Carlos III de Madrid; Member; Department of Signal Theory and Communications at Universidad Carlos III de Madrid","IEEE Transactions on Consumer Electronics","","2011","57","3","1320","1328","In this paper we propose a complete preprocessing system for restoring low-quality Quick Response code images. The target application for the system is to restore and decode a code image in a cell phone when the image is shown in another cell phone display. Therefore, the system design focuses on low complexity solutions that are effective for a variety of capturing and displaying devices. The system has been designed to deal with low contrast, blurred, rotated, and deformed code images, while keeping complexity low enough to run in real-time. The proposed system takes advantage of intrinsic Quick Response code characteristics in order to restore the code properly and efficiently. The system has been assessed on a realistic database composed of 533 images captured by a cell a phone from several different cell phone displays. After the proposed preprocessing, 97.4% of the images in the database were correctly decoded.","0098-3063;1558-4127","","10.1109/TCE.2011.6018890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018890","QR codes;low complexity;bi-tonal imagedeblurring;blind image deconvolution;cell phone","Cellular phones;Cameras;Image restoration;Indexes;Algorithm design and analysis;Filtering algorithms;Image edge detection","decoding;display devices;image coding;image restoration;mobile handsets","low-complexity preprocessing system;low-quality QR code image restoration;low-quality quick response code image restoration;code image decoding;cell phone display device;low contrast code image;blurred code image;rotated code image;deformed code image;intrinsic quick response code characteristic;realistic database assessment","","7","","12","","","","","IEEE","IEEE Journals & Magazines"
"On the encoding complexity of systematic polar codes","L. Li; W. Zhang","Key Laboratory of Intelligent Computing and Signal Processing of the Ministry of Education of China Anhui University, China; Key Laboratory of Wireless-Optical Communications, Chinese Academy of Sciences and Department of Electronic Engineering and Information Sciences, University of Science and Technology of China","2015 28th IEEE International System-on-Chip Conference (SOCC)","","2015","","","415","420","Systematic polar codes are proposed by Arikan and are shown to have better BER performance than non-systematic polar codes. From a recursive decomposition of the generator matrix of polar codes, Arikan showed that the encoding complexity of systematic polar codes is also O(N log N) where N is the code block length. But the recursive process involves some additional calculations in transforming the problem instances back and forth. In this paper, by using the sparsity property of the generator matrix, we propose an encoding process which has the same complexity as non-systematic polar codes in the presence of an additional memory array. Without the additional memory elements, the number of additions of the proposed encoding process increases compared with non-systematic polar codes. We also provide an analysis to quantify this additional increase of the complexity.","2164-1706","978-1-4673-9094-1978-1-4673-9093","10.1109/SOCC.2015.7406996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406996","","Encoding;Systematics;Complexity theory;Decoding;Generators;Bit error rate;Electronic mail","decoding;error statistics;matrix decomposition;recursive estimation","systematic polar code encoding complexity;BER performance;polar code generator matrix recursive decomposition;generator matrix sparsity property;additional memory array;additional memory elements;decoding complexity","","3","","10","","","","","IEEE","IEEE Conferences"
"A common framework for ML detection of spatially multiplexed and space time coded MIMO signals and reducing its computational complexity","N. K. Chavali; S. Mounika; K. Kuchi","Uurmi Systems Pvt. Ltd., Plot no: 860/A, Jubilee Hills, Hyderabad, India; Uurmi Systems Pvt. Ltd., Plot no: 860/A, Jubilee Hills, Hyderabad, India; Department of Electrical Engineering, Indian Institute of Technology, Hyderabad, India","2016 International Conference on Signal Processing and Communications (SPCOM)","","2016","","","1","5","In this paper, a common framework for maximum likelihood (ML) detection of spatial multiplexed (SM) and space time coded multiple-input multiple-output (MIMO) signals is proposed. It facilitates a common field programmable gate array (FPGA) based hardware design for the detection of both SM and space time block coded (STBC) MIMO signals with reduced computational complexity. We reduce the number of multipliers required for the ML detection by utilizing a set of precomputed combinations of the constellation symbols from lookup tables (LUT) without any degradation in the packet error rate (PER) performance. The reduction in the computational complexity is achieved by using the additional memory that is available in the FPGA. To evaluate the performance of the proposed method, we perform simulations for the WLAN system based on IEEE 802.11 standard using SM-MIMO and STBC-MIMO.","","978-1-5090-1746-1978-1-5090-1747","10.1109/SPCOM.2016.7746625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746625","","MIMO;Detectors;Computational complexity;IEEE 802.11 Standard;Transmitting antennas;Receivers","computational complexity;field programmable gate arrays;maximum likelihood detection;MIMO communication;space division multiplexing;space-time block codes","LUT;lookup table;constellation symbol;IEEE 802.11 standard;WLAN system;STBC-MIMO signal;SM-MIMO signal;common FPGA-based hardware design;common field programmable gate array based hardware design;multiple-input multiple-output signal detection;space time block coded MIMO signal ML detection;spatially multiplexed coded MIMO signal ML detection;computational complexity reduction","","","","15","","","","","IEEE","IEEE Conferences"
"Low complexity belief propagation polar code decoder","S. M. Abbas; Y. Fan; J. Chen; C. Tsui","VLSI Research Laboratory, Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST), Hong Kong; VLSI Research Laboratory, Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST), Hong Kong; VLSI Research Laboratory, Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST), Hong Kong; VLSI Research Laboratory, Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology (HKUST), Hong Kong","2015 IEEE Workshop on Signal Processing Systems (SiPS)","","2015","","","1","6","Since their invention, polar codes have received a lot of attention because of their capacity-achieving performance and low encoding and decoding complexity. Successive cancellation decoding (SCD) and belief propagation decoding (BPD) are two approaches for decoding polar codes. SCD is able to achieve good error-correcting performance and is less computationally expensive as compared to BPD. However SCD suffers from long latency due to the serial nature of the successive cancellation algorithm. BPD is parallel in nature and hence is more attractive for low latency applications. However, since it is iterative, the required latency and energy dissipation increases linearly with the number of iterations. In this work, we borrow the idea of SCD and propose a novel scheme based on sub-factor-graph freezing to reduce the average number of computations as well as the average number of iterations required by BPD, which directly translates into lower latency and energy dissipation. Simulation results show that the proposed scheme has no performance degradation and achieves significant reduction in computation complexity over the existing methods.","","978-1-4673-9604-2978-1-4673-9603","10.1109/SiPS.2015.7344986","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344986","Belief propagation decoding (BPD);successive cancellation decoding (SCD);energy efficiency;iterative decoders;factor graph;polar codes","Decoding;Iterative decoding;Complexity theory;Convergence;Belief propagation;Reliability;Encoding","decoding;error correction codes","low complexity belief propagation polar code decoder;capacity achieving performance;low encoding;decoding complexity;successive cancellation decoding;SCD;belief propagation decoding;BPD;decoding polar codes;error correcting performance;energy dissipation;subfactor graph freezing;computation complexity","","10","","22","","","","","IEEE","IEEE Conferences"
"Construction of rate compatible length scalable LDPC codes with low implementation complexity","Z. Chen; K. Peng; Y. Liu","Tsinghua National Laboratory for Information Science and Technology, Beijing 100084, China; Electronic Engineering Department, Tsinghua University, Beijing 100084 China; Shenzhen City Key Laboratory of Digital TV System, Shenzhen 518057, China","2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2017","","","1","6","Due to the rapid development of communication systems, a highly flexible scheme of multi-rate multi-length channel code is urgently required. In this paper, we propose a design method of highly flexible rate-compatible (RC) length-scalable (LS) low density parity check (LDPC) codes, which is based on the progressive extension idea of both information bit/node and parity check bit/node. According to our method, a family of RC-LS-LDPC codes with one common template matrix can be generated, which facilitates hardware implementation. With the aid of multi-edge type density evolution (MET-DE) analysis, the method can achieve near-capacity performance for each code rate and code length. The simulation results show that the designed codes with arbitrary rate and variable code length are within 0.30dB to 0.48dB compared with Shannon limit at bit error rate (BER) =2*10<sup>-9</sup> under BI-AWGN channel.","2155-5052","978-1-5090-4937-0978-1-5090-4938","10.1109/BMSB.2017.7986228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986228","channel coding;density evolution;extrinsic information transfer (EXIT);low-density parity-check (LDPC);rate compatible","Design methodology;Hardware;Complexity theory;Decoding;Iterative decoding;Channel coding","AWGN channels;channel coding;error statistics;matrix algebra;parity check codes","rate compatible length scalable LDPC code construction;low implementation complexity;multirate multilength channel code;RC LS low density parity check codes;information bit-node;parity check bit-node;common template matrix;hardware implementation;multiedge type density evolution analysis;MET-DE analysis;bit error rate;BI-AWGN channel","","","","12","","","","","IEEE","IEEE Conferences"
"Algorithms and VLSI Architectures for Low-Density Parity-Check Codes: Part 1-Low-Complexity Iterative Decoding","K. Gunnam; J. M. Catala Perez; F. Garcia-Herrero","Western Digital Corporation, San Jose, CA 95035 USA; Instituto de Telecom y Aplicaciones Multimedia, Universitat Politecnica de Valencia, Gandia, 46022, Spain; Grupo de Invest Discapacidad Fisica Sensorial, European University Miguel de Cervantes, Valladolid, 47012, Spain","IEEE Solid-State Circuits Magazine","","2016","8","4","57","63","This article is the first of a two-part tutorial about low-density parity-check (LDPC) codes. These codes have been firmly established as error-correction coding techniques for communication and storage channels.","1943-0582;1943-0590","","10.1109/MSSC.2016.2573938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743072","","Decoding;Detectors;Tutorials;Iterative decoding;Error correction codes;Channel coding;Parity check codes;Algorithm design and analysis;Very large scale integration","communication complexity;iterative decoding;parity check codes;power aware computing;VLSI","VLSI architectures;low-density parity-check codes;low-complexity iterative decoding;LDPC codes;error correction coding;communication channels;storage channels;energy-efficient very-large-scale integration architectures;area-efficient very-large-scale integration architectures","","2","","12","","","","","IEEE","IEEE Journals & Magazines"
"Rate-distortion-complexity analysis for prediction unit modes in 3D-HEVC depth coding","R. Conceição; G. Avila; G. Corrêa; M. Porto; B. Zatt; L. Agostini","Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil; Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil; Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil; Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil; Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil; Federal University of Pelotas - UFPel, Group of Architectures and Integrated Circuits - GACI, Pelotas, Brazil","2016 IEEE 7th Latin American Symposium on Circuits & Systems (LASCAS)","","2016","","","287","290","3D-HEVC achieves significant compression efficiency thanks to the addition of several tools and features for multiview and 3D video content on top of those already available in HEVC. However, such gains come at the cost of large increases in the encoding computational complexity, which is a problem for computationally and battery-constrained devices. As the use of each tool or feature yields different compression efficiency, those that do not contribute significantly should be the first to be disabled in a complexity-constrained system with limited computational resources. Thus, this paper presents two major analyses on the use of Prediction Unit (PU) modes in 3D-HEVC: a preliminary, statistical study and a Rate-Distortion-Complexity (RDC) analysis. The first study observed the occurrence of asymmetric and rectangular PU modes in depth maps, while the RDC analysis evaluated the impact caused by the deactivation of the least used modes during the encoding of depth maps, based on the results found in the preliminary study. Experimental results have shown that a computational complexity reduction of 9% can be achieved with a negligible rate-distortion penalty (0.1%) if the least used modes are disabled.","","978-1-4673-7835-2978-1-4673-7834","10.1109/LASCAS.2016.7451066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451066","3D-HEVC;video coding;Rate-Distortion-Complexity analysis;mode decision;computational complexity","Encoding;Computational complexity;Video sequences;Three-dimensional displays;Integrated circuits;Standards","computational complexity;data compression;rate distortion theory;video coding","rate-distortion-complexity analysis;prediction unit modes;3D-HEVC depth coding;compression efficiency;multiview video content;3D video content;encoding computational complexity;complexity-constrained system;RDC analysis;asymmetric PU modes;rectangular PU modes;depth maps;high efficiency video coding","","","","14","","","","","IEEE","IEEE Conferences"
"A multi-level dynamic complexity reduction scheme for multiview video coding","B. Zatt; M. Shafique; S. Bampi; J. Henkel","Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Federal University of Rio Grande do Sul (UFRGS), Informatics Institute/PGMICRO, Porto Alegre, Brazil; Karlsruhe Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany","2011 18th IEEE International Conference on Image Processing","","2011","","","749","752","In this paper, we propose a novel scheme for dynamically reducing the computational complexity of MVC. Our scheme exploits the coding mode correlation available in the 3D-neighborhood (i.e., spatial, temporal, and view) along with the rate-distortion proper- ties of the neighboring Macroblocks. Our scheme incorporates a multi-level mode decision process based on a mode-ranking mechanism that categorizes more-probable and less-probable coding modes. In order to react to the changing bitrates, our scheme deploys Quantization Parameter based threshold equations which are formulated using an offline statistical analysis. Compared to the exhaustive Rate-Distortion-Optimized Mode Decision (RDO-MD), our scheme achieves a complexity reduction of up to 80% (68% on average) with an average PSNR loss of 0.075 dB. Compared to state-of-the-art fast RDO-MD, our scheme achieves a complexity reduction of up to 34% with an average PSNR gain of 0.007 dB.","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116663","Multiview Video coding;MVC;Fast Mode Decision;Complexity Reduction;Video Coding","Complexity theory;Image coding;Encoding;Correlation;Video coding;Bit rate;Probability density function","computational complexity;correlation methods;quantisation (signal);rate distortion theory;statistical analysis;video coding","multilevel dynamic complexity reduction scheme;multiview video coding;computational complexity reduction;MVC;coding mode correlation;rate distortion properties;macroblocks;multilevel mode decision process;mode-ranking mechanism;more-probable coding modes;less-probable coding modes;quantization parameter based threshold equations;offline statistical analysis;PSNR","","3","","12","","","","","IEEE","IEEE Conferences"
"Delay-Optimal Probabilistic Scheduling for Low-Complexity Wireless Links With Fixed Modulation and Coding: A Cross-Layer Design","X. Chen; W. Chen","Department of Electronic Engineering and Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing, China; Department of Electronic Engineering and Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing, China","IEEE Transactions on Vehicular Technology","","2016","65","10","8036","8051","A fundamental trade-off lies between the average queueing delay and the power consumption when transmitting stochastically arriving data over a fading channel. Providing real-time services on low-energy-supply and low-complexity devices has attracted much attention from both academia and industry, for its extensive use in transportation, manufacturing, living, and so on. In this paper, we investigate a joint channel-and-buffer-aware probabilistic scheduling policy, which is generalized from conventional deterministic scheduling, in order to optimize the delay-power trade-off without paying the cost of high complexity. In particular, we focus on communication systems with a given fixed modulation and coding scheme, where power adaptation is adopted to compensate the bad channel state. We formulate a Markov reward process (MRP) to analyze the average delay and power consumption, based on which we minimize the average delay, given a certain power constraint via linear programming (LP). By solving the LP problem, optimal delay-power trade-off curves are obtained under a threshold-based scheduling policy.","0018-9545;1939-9359","","10.1109/TVT.2015.2512232","National Basic Research Program of China (973 Program); NSFC Excellent Young Investigator; 973 Program; NSFC Innovative Team Project; National innovative talents promotion program; Beijing nova program; MoE new century talent program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365481","Average delay;cross-layer design;delay–power trade-off;energy efficiency;fading channels;joint channel-and-buffer-aware scheduling;linear programming (LP);Markov reward process (MRP);queueing;wireless networks","Delays;Optimal scheduling;Job shop scheduling;Markov processes;Modulation;Encoding;Probabilistic logic","channel coding;fading channels;linear programming;Markov processes;modulation;probability;radio links;telecommunication scheduling","delay-optimal probabilistic scheduling;low-complexity wireless links;average queueing delay;fading channel;joint channel-and-buffer-aware probabilistic scheduling policy;fixed modulation and coding scheme;bad channel state;Markov reward process;linear programming","","3","","29","","","","","IEEE","IEEE Journals & Magazines"
"Irregular code-aided low-complexity three-stage turbo space time processing","Y. Wang; P. Li; L. Li; A. G. Burr; R. C. de Lamare","Communications Research Group, Department of Electronics, University of York, UK; Communications Research Group, Department of Electronics, University of York, UK; Communications Research Group, Department of Electronics, University of York, UK; Communications Research Group, Department of Electronics, University of York, UK; Communications Research Group, Department of Electronics, University of York, UK","2011 8th International Symposium on Wireless Communication Systems","","2011","","","604","608","This paper proposes an irregular code-aided iteratively detected three-stage space-time scheme for a MIMO system. Benefiting from the serial concatenation at the transmitter of a space-time mapper, a low complexity unit rate code and an outer irregular code, and iterative processing among the three constituent detector/decoder at the receiver, the resultant system is capable of achieving error free transmission for a sufficiently long interleaver block, and the overall rate is close to the space-time detector constrained maximum achievable rate. The space-time detector of our system employs either multi-feedback serial interference cancelation (MFSIC) aided soft-output parallel interference cancelation (SOPIC) which can effectively reduce the required iterations to achieve full iteration gain or soft-output complex sphere decoding (SOCSD) which is capable of operating within 0.4dB away from the ergodic MIMO channel capacity.","2154-0225;2154-0217;2154-0217","978-1-61284-402-2978-1-61284-403-9978-1-61284-401","10.1109/ISWCS.2011.6125363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125363","","Decoding;Bit error rate;Iterative decoding;Detectors;MIMO;Encoding;Vectors","MIMO communication;space-time codes;turbo codes","irregular code-aided low-complexity three-stage turbo space time processing;MIMO system;space-time mapper;iterative processing;detector/decoder;receiver;resultant system;error free transmission;interleaver block;space-time detector constrained maximum achievable rate;multifeedback serial interference cancelation;soft-output parallel interference cancelation;soft-output complex sphere decoding;ergodic MIMO channel capacity","","","","19","","","","","IEEE","IEEE Conferences"
"Low-complexity distributed set-theoretic decoders for analog fountain codes","R. L. G. Cavalcante; S. Stanczak; D. Schupke; J. Klaue","Fraunhofer Heinrich Hertz Institute and Technical University of Berlin, Germany; Fraunhofer Heinrich Hertz Institute and Technical University of Berlin, Germany; Airbus, CTO, Wireless Communications, Munich, Germany; Airbus, DTO, Wireless Connectivity, Hamburg, Germany","2018 52nd Asilomar Conference on Signals, Systems, and Computers","","2018","","","1480","1484","Analog fountain codes (AFCs) have been recently proposed as a capacity-approaching coding scheme for Gaussian channels. In AFCs, coded symbols are real valued, and the number of generated codes can grow unboundedly. In this work, we exploit these characteristics of AFCs to pose the decoding process as an adaptive filtering problem. This formulation opens up the possibility of deriving a large family of online decoding algorithms that process coded symbols as they arrive, in a truly online fashion. In particular, we present two algorithms based on the adaptive projected subgradient method, and we further improve the performance of these algorithms by using heuristics inspired by recent results on superiorization. We also show that the proposed techniques can be trivially extended to distributed settings where receivers can exchange information locally. Simulations show that the proposed techniques can outperform state-of-the-art batch methods in some scenarios.","2576-2303;1058-6393","978-1-5386-9218-9978-1-5386-9216-5978-1-5386-9217-2978-1-5386-9219","10.1109/ACSSC.2018.8645310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8645310","","Receivers;Decoding;Encoding;Transmitters;Wireless communication;Heuristic algorithms;Generators","adaptive filters;channel coding;decoding;Gaussian channels;gradient methods;set theory","analog fountain codes;AFCs;capacity-approaching coding scheme;coded symbols;generated codes;decoding process;adaptive filtering problem;online decoding algorithms;adaptive projected subgradient method;distributed settings;Gaussian channels;low-complexity distributed set-theoretic decoders","","","","19","","","","","IEEE","IEEE Conferences"
"Rate-aware three phase analog network coding with low-complexity multi-antenna relay processing","J. S. Wang; S. R. Lee; Y. H. Kim","Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Gyeonggi, Korea; Department of Information and Electronics, Mokpo National University, Mokpo, Jeonnam, Korea; Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Gyeonggi, Korea","2014 IEEE Wireless Communications and Networking Conference (WCNC)","","2014","","","1065","1069","A low-complexity design of multi-antenna relay processing is proposed for two-way relay networks with asymmetric data flows. The proposed scheme adopts three-phase (3P) analog network coding (ANC) which allows orthogonal transmission of two communicating nodes to overcome some disadvantages of two-phase (2P) ANC protocols. For the 3P ANC, we propose antenna selection at the relay to exploit the diversity without channel estimation and rate-aware relay power allocation (RPA) to improve the outage performance under the asymmetric rate requirements. The results show that the proposed scheme without or with RPA outperforms the conventional two-phase ANC significantly in various relay locations. It is also shown that the suboptimal RPA proposed for practical implementation provides a performance close to that of the optimal RPA.","1525-3511;1558-2612","978-1-4799-3083","10.1109/WCNC.2014.6952276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952276","Two-way relay;Analog network coding;Rate awareness;Outage probability;Antenna selection;Relay power allocation","Relays;Antennas;Signal to noise ratio;Protocols;Resource management;Network coding;Channel estimation","antenna arrays;channel estimation;network coding;orthogonal codes;protocols;relay networks (telecommunication)","rate-aware three phase analog network coding;low-complexity multiantenna relay processing;two-way relay networks;asymmetric data flows;3P ANC;communicating nodes;orthogonal transmission;2P ANC protocols;two-phase ANC protocols;antenna selection;channel estimation;rate-aware relay power allocation;RPA;asymmetric rate requirements;outage performance;relay locations","","3","","8","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Maximum-Likelihood Decoder for Tail-Biting Convolutional Codes","Y. S. Han; T. Wu; P. Chen; P. K. Varshney","School of Electrical Engineering and Intelligentization, Dongguan University of Technology, Dongguan, China; Department of Electrical and Computer Engineering, University of Illinois at Urbana–Champaign, Champaign, IL, USA; Department of Electrical and Computer Engineering, Institute of Communications Engineering, National Chiao-Tung University, Hsinchu, Taiwan; Department of Electrical Engineering and Computer Science, Syracuse University, Syracuse, NY, USA","IEEE Transactions on Communications","","2018","66","5","1859","1870","Due to the growing interest in applying tail-biting convolutional coding techniques in real-time communication systems, fast decoding of tail-biting convolutional codes has become an important research direction. In this paper, a new maximum-likelihood decoder for tail-biting convolutional codes is proposed. It is named bidirectional priority-first search algorithm (BiPFSA) because priority-first search algorithm has been used both in forward and backward directions during decoding. Simulations involving the antipodal transmission of (2, 1, 6) and (2, 1, 12) tail-biting convolutional codes over additive white Gaussian noise channels shows that BiPFSA not only has the least average decoding complexity among the state-of-the-art decoding algorithms for tail-biting convolutional codes but can also provide a highly stable decoding complexity with respect to growing information length and code constraint length. More strikingly, at high SNR, its average decoding complexity can even approach the ideal benchmark complexity, obtained under a perfect noise-free scenario by any sequential-type decoding. This demonstrates the superiority of BiPFSA in terms of decoding efficiency.","0090-6778;1558-0857","","10.1109/TCOMM.2018.2790935","National Natural Science Foundation of China; Ministry of Science and Technology, R.O.C.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8249849","Tail-biting convolutional codes;maximum-likelihood decoding;convolutional codes;viterbi algorithm","Maximum likelihood decoding;Convolutional codes;Complexity theory;Measurement;Encoding;Shift registers","computational complexity;convolutional codes;maximum likelihood decoding;search problems","low-complexity maximum-likelihood decoder;tail-biting convolutional coding techniques;highly stable decoding complexity;bidirectional priority-first search algorithm","","3","","22","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Joint Channel Estimation and Decoding for LDPC Coded MIMO-OFDM Systems","X. Xu; R. Mathar","NA; NA","2011 IEEE 73rd Vehicular Technology Conference (VTC Spring)","","2011","","","1","5","In this paper, a joint iterative channel estimation and low-density parity-check (LDPC) decoding algorithm based on factor graphs and the sum-product algorithm is proposed for orthogonal frequency division multiplexing (OFDM) systems employing multiple transmit and receive antennas (MIMO). By modeling time-varying frequency-selective fading channels as autoregressive processes and approximating messages as Gaussian pdf, this receiver algorithm is able to maintain a low complexity. Moreover, with the help of strong channel coding and proper pilot allocation, the signal overhead can be significantly reduced.","1550-2252;1550-2252;1550-2252","978-1-4244-8331-0978-1-4244-8332-7978-1-4244-8329-7978-1-4244-8330","10.1109/VETECS.2011.5956229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956229","","Channel estimation;OFDM;Correlation;Iterative decoding;Time frequency analysis;Receivers","antenna arrays;channel coding;channel estimation;fading channels;graph theory;iterative methods;MIMO communication;OFDM modulation;parity check codes;receiving antennas;time-varying channels;transmitting antennas","LDPC coded MIMO-OFDM systems;joint iterative channel estimation;low-density parity-check decoding algorithm;factor graphs;sum-product algorithm;orthogonal frequency division multiplexing;multiple transmit antennas;multiple receive antennas;MIMO;time-varying frequency-selective fading channels;autoregressive processes;Gaussian pdf;channel coding;pilot allocation","","11","","15","","","","","IEEE","IEEE Conferences"
"Low-complexity LDPC-coded iterative MIMO receiver based on belief propagation algorithm for detection","A. Haroun; C. A. Nour; M. Arzel; C. Jego","Institut Mines-T&#x00E9;l&#x00E9;com / T&#x00E9;l&#x00E9;com Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Universit&#x00E9; Europ&#x00E9;enne de Bretagne, France; Institut Mines-T&#x00E9;l&#x00E9;com / T&#x00E9;l&#x00E9;com Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Universit&#x00E9; Europ&#x00E9;enne de Bretagne, France; Institut Mines-T&#x00E9;l&#x00E9;com / T&#x00E9;l&#x00E9;com Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Universit&#x00E9; Europ&#x00E9;enne de Bretagne, France; IPB / Enseirb-Matmeca, CNRS IMS, UMR 5218, Bordeaux. Universit&#x00E9; de Bordeaux, France","2014 8th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2014","","","213","217","A low-complexity Multiple Input Multiple Output receiver based on Belief Propagation (MIMO-BP) detector associated with a Non-Binary Low Density Parity Check (NB-LDPC) decoder is proposed in this paper. Such detection and decoding algorithms are represented thanks to a larger Joint Factor Graph. Shuffle schedule is also applied to efficiently exchange information between the detector and the decoder. Actions are undertaken at the detector, decoder and the iterative receiver levels in order to reduce overall computational complexity. An important reduction in terms of operations per iteration is obtained with a negligible performance penalty. Then, EXtrinsic Information Transfer (EXIT) charts are used to find a schedule in terms of number of iterations to be performed for which the proposed receiver achieves error correction performance similar to that of a full-complexity iterative MIMO receiver.","2165-4719;2165-4700","978-1-4799-5985","10.1109/ISTC.2014.6955116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955116","Belief Propagation (BP) principle;Non-Binary Low-Density Parity-Check (NB-LDPC);Multiple-Input Multiple-Output (MIMO);EXtrinsic Information Transfer (EXIT) chart","Detectors;Decoding;MIMO;Iterative decoding;Silicon;Receivers;Reliability","computational complexity;iterative decoding;MIMO communication;parity check codes;radio receivers;scheduling","multiple input multiple output receiver based on belief propagation detector;MIMO-BP detector;nonbinary low density parity check decoder;NB-LDPC decoder;error correction performance;EXIT;extrinsic information transfer charts;computational complexity;iterative receiver levels;information exchange;shuffle schedule;joint factor graph;decoding algorithms","","6","","22","","","","","IEEE","IEEE Conferences"
"Selective Gray-Coded Bit-Plane-Based Two-Bit Transform and Its Efficient Hardware Architecture for Low-Complexity Motion Estimation","A. T. Çelebi; S. Yavuz; A. Çelebi; O. Urhan","Electronics and Telecommunication Engineering Department, Integrated Systems Laboratory, Kocaeli University Umuttepe Campus, İzmit, Turkey; Electronics and Telecommunication Engineering Department, Integrated Systems Laboratory, Kocaeli University Umuttepe Campus, İzmit, Turkey; Electronics and Telecommunication Engineering Department, Integrated Systems Laboratory, Kocaeli University Umuttepe Campus, İzmit, Turkey; Electronics and Telecommunication Engineering Department, Laboratory of Embedded and Vision Systems, Kocaeli University Umuttepe Campus, İzmit, Turkey","IEEE Transactions on Consumer Electronics","","2018","64","3","259","266","In this paper, a novel low bit-depth representation-based motion estimation approach with its hardware architecture is presented. The low bit-depth representation of pixels in the proposed method is constructed by choosing them from the gray-coded bit planes in an efficient way. Additionally, a new matching creation is also presented to improve the motion estimation accuracy. Thanks to the proposed binarization approach, the computation load is considerably reduced compared to filtering-based low bit-depth representation motion estimation methods. A novel hardware architecture for the proposed method is also presented in this paper. Experimental results revealed that the proposed motion estimation method and its hardware architecture provides a good balance between motion estimation accuracy and hardware resources especially for consumer electronics applications.","0098-3063;1558-4127","","10.1109/TCE.2018.2851786","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400471","Low-complexity binarization;gray coding;motion estimation;one-bit transform","Hardware;Computer architecture;Videos;Transforms;Standards;Encoding;Motion estimation","Gray codes;image coding;image representation;motion estimation;transform coding;transforms","selective gray-coded bit-plane;low bit-depth representation-based motion estimation;pixel representation;binarization;consumer electronics applications;low-complexity motion estimation;efficient hardware architecture;two-bit transform","","","","25","","","","","IEEE","IEEE Journals & Magazines"
"Linear Precoding for MIMO With LDPC Coding and Reduced Complexity","T. Ketseoglou; E. Ayanoglu","NA; NA","IEEE Transactions on Wireless Communications","","2015","14","4","2192","2204","In this paper, the problem of designing a linear precoder for Multiple-Input Multiple-Output (MIMO) systems employing Low-Density Parity-Check (LDPC) codes is addressed under the constraint of minimizing the dependence between the system's receiving branches, thus reducing the relevant transmitter and receiver complexities. Our approach constitutes an interesting generalization of Bit-Interleaved Coded Modulation with Multiple Beamforming (BICMB) which has shown many benefits in MIMO systems. We start with a Pareto optimal surface modeling of the system and show the difficulty involved in the corresponding optimization problem. We then propose an alternative, practical technique, called Per-Group Precoding (PGP), which groups together multiple input symbol streams and corresponding receiving branches in the “virtual” channel domain (after singular value decomposition of the original MIMO channel), and thus results in independent transmitting/receiving streams between groups. We show with numerical results that PGP offers almost optimal performance, albeit with significant reduction both in the precoder optimization and LDPC EXIT chart based decoding complexities.","1536-1276;1558-2248","","10.1109/TWC.2014.2382125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998869","Per-groupprocessing;maximal diversity precoder;near-capacityachieving channel coding;pareto optimal precoder;receiver independence factor","MIMO;Complexity theory;Mutual information;Parity check codes;Receivers;Pareto optimization","array signal processing;interleaved codes;linear codes;MIMO communication;modulation coding;optimisation;parity check codes;precoding;singular value decomposition","linear precoding;MIMO;LDPC Coding;reduced complexity;multiple-input multiple-output systems;low density parity check codes;bit interleaved coded modulation;multiple beamforming;per-group precoding;multiple input symbol stream grouping;virtual channel domain;singular value decomposition","","10","","20","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity rate compatible puncturing patterns design for LDPC codes","F. Babich; M. Noschese; A. Soranzo; F. Vatta","University of Trieste, Trieste - Italy; University of Trieste, Trieste - Italy; University of Trieste, Trieste - Italy; University of Trieste, Trieste - Italy","2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","","2017","","","1","5","In contemporary digital communications design, two major challenges should be addressed: adaptability and flexibility. The system should be capable of flexible and efficient use of all available spectrums and should be adaptable to provide efficient support for the diverse set of service characteristics. These needs imply the necessity of limit-achieving and flexible channel coding techniques, to improve system reliability. Low Density Parity Check (LDPC) codes fit such requirements well, since they are capacity-achieving. Moreover, through puncturing, allowing the adaption of the coding rate to different channel conditions with a single encoder/decoder pair, adaptability and flexibility can be obtained at a low computational cost. In this paper, the design of rate-compatible puncturing patterns for LDPCs is addressed. We use a previously defined formal analysis of a class of punctured LDPC codes through their equivalent parity check matrices. We address a new design criterion for the puncturing patterns using a simplified analysis of the decoding belief propagation algorithm, i.e., considering a Gaussian approximation for message densities under density evolution, and a simple algorithmic method, recently defined by the Authors, to estimate the threshold for regular and irregular LDPC codes on memoryless binary-input continuous-output Additive White Gaussian Noise (AWGN) channels.","1847-358X","978-953-290-078-1978-953-290-074-3978-1-5386-3212","10.23919/SOFTCOM.2017.8115558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115558","","Parity check codes;Algorithm design and analysis;Approximation algorithms;Gaussian approximation;Decoding;AWGN channels;Mathematical model","approximation theory;AWGN channels;channel capacity;channel coding;decoding;digital communication;matrix algebra;parity check codes","low computational cost;rate-compatible puncturing patterns;punctured LDPC codes;equivalent parity check matrices;design criterion;decoding belief propagation algorithm;low complexity rate compatible puncturing patterns design;contemporary digital communications design;diverse set;service characteristics;limit-achieving;flexible channel coding techniques;system reliability;coding rate;channel conditions;formal analysis;low density parity check codes;single encoder-decoder pair;Gaussian approximation;memoryless binary-input continuous-output additive white Gaussian noise channels;AWGN channels","","2","","15","","","","","IEEE","IEEE Conferences"
"Reduced Complexity Decoder for Orthogonal Space-Time Codes When Using QAM Constellations and Multiple Receive Antennas","A. A. Enescu; S. Ciochina; C. Paleologu","NA; NA; NA","2010 Fifth International Conference on Digital Telecommunications","","2010","","","143","148","Space-time block coding represents a practical and pragmatic method to mitigate the fading effects and to increase the channel capacity, by using spatial diversity. The space-time coding techniques are still preferred in communication systems because of the very simple transmit coding rule and of the relative reduced complexity of the maximum likelihood (ML) receivers with respect to other coding methods (e.g., space-time trellis codes) or other MIMO (Multiple Input Multiple Output) algorithms (e.g., spatial multiplexing). One of the drawbacks when using space-time block codes is the increase in complexity at the ML receiver when increasing the number of transmit antennas and the number of constellation points. This paper deals with this issue and introduces a decoding algorithm applicable only for orthogonal space-time block codes. We show that this decoding algorithm reduces the search space dimension of a regular ML receiver, leading to significant complexity reduction for quadrature amplitude modulation (QAM) constellations. The algorithm is then extended to an arbitrary number of receive antennas, increasing the spatial diversity order.","","978-1-4244-7272-7978-1-4244-7271-0978-0-7695-4071","10.1109/ICDT.2010.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5532367","Alamouti code;maximum likelihood (ML) decoding;orthogonal space-time codes;spatial diversity","Space time codes;Quadrature amplitude modulation;Receiving antennas;Maximum likelihood decoding;Block codes;MIMO;Fading;Channel capacity;Convolutional codes;Transmitting antennas","block codes;maximum likelihood detection;MIMO communication;multifrequency antennas;orthogonal codes;quadrature amplitude modulation;receiving antennas;space-time codes;transmitting antennas","reduced complexity decoder;QAM constellation;multiple receive antenna;channel capacity;spatial diversity;communication system;maximum likelihood receivers;multiple input multiple output algorithm;MIMO algorithm;transmit antenna;orthogonal space-time block code;quadrature amplitude modulation","","","","11","","","","","IEEE","IEEE Conferences"
"Write-Once Memory codes for low-complexity decoding of Asymmetric Multiple Access Channel","R. Sekiya; E. C. G. Alvarez; B. M. Kurkoski; H. Yagi","Japan Advanced Institute of Science and Technology (JAIST), Ishikawa, Japan; Japan Advanced Institute of Science and Technology (JAIST), Ishikawa, Japan; Japan Advanced Institute of Science and Technology (JAIST), Ishikawa, Japan; The University of Electro-Communications, Tokyo, Japan","2014 International Symposium on Information Theory and its Applications","","2014","","","610","614","Write-Once Memory (WOM) codes are designed for data storage, which allow re-writing on n cells that can change their bit value from 0 to 1 but not vice versa. This paper focuses on applying “WOM codes” to cooperative wireless communications. Due to the characteristics of WOM codes, the Asymmetric Multiple Access Channel (AMAC) (also referred to as the MAC with degraded messages) is considered, which is a conventional Multiple Access Channel (MAC), where one user can observe the other user's message. We describe an AMAC system where WOM codes are used to deal with the interference between two users. For a specific AMAC model, WOM codes can achieve the AMAC capacity, and using WOM codes for the AMAC with no errors leads to a low-complexity decoder. Finally, we comment of how the AMAC model can be applied to the relay channel. While we consider primarily the AMAC with no errors, this study forms a foundation for future work on the AMAC with errors.","","978-4-8855-2292","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979916","","","binary codes;codecs;cooperative communication;decoding;flash memories;multi-access systems;relay networks (telecommunication)","relay channel;AMAC capacity;AMAC model;cooperative wireless communications;data storage;AMAC system;asymmetric multiple access channel;WOM codes;write-once memory codes","","","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity wire-tap codes with security and error-correction guarantees","Y. Cassuto; Z. Bandic","School of Computer and Communication Sciences, ALGO, EPFL, Lausanne, Switzerland; Hitachi Global Storage Technologies, San Jose Research Center, 3403 Yerba Buena Road, San Jose, CA 95135, U.S.A.","2010 IEEE Information Theory Workshop","","2010","","","1","5","New code constructions are proposed for the wiretap channel with security and error-correction guarantees. For the case of error-free main channels, two families of codes are constructed with optimal encoding and decoding complexities for their wire-tap security. For the case of main channels with errors, two concatenation types are studied for the wire-tap and error-correcting codes. For each of these concatenated schemes, code families are constructed that give optimal cooperation between the wire-tap and error-correction properties. The motivation to study low-complexity wire-tap codes with security and error-correction guarantees comes from data storage applications. Due to imperfect physical erasure processes, important secret information needs to be protected from adversarial access to residual, post erasure, information, and at the same time be protected from errors when read by the legitimate device user.","","978-1-4244-8264-1978-1-4244-8262-7978-1-4244-8263","10.1109/CIG.2010.5592824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5592824","","Security;Construction industry;Encoding;Error correction codes;Decoding;Complexity theory;Generators","channel coding;concatenated codes;error correction codes;telecommunication security","low-complexity wire-tap codes;error-correction guarantees;code constructions;wiretap channel;error-free main channels;optimal encoding;decoding complexity;wire-tap security;concatenation types;error-correcting codes;code family;data storage applications;physical erasure processes;secret information","","5","","9","","","","","IEEE","IEEE Conferences"
"Evaluation of a Reduced Complexity ML Decoding Algorithm for Tailbiting Codes on Wireless Systems","J. Ortin; P. Garcia; F. Gutierrez; A. Valdovinos","NA; NA; NA; NA","2010 IEEE 72nd Vehicular Technology Conference - Fall","","2010","","","1","5","Tailbiting convolutional codes will be used for several applications in new cellular mobile radio systems. This encoding method does not reset the encoder memory at the end of each data block, avoiding the overhead of the zero tail and improving the efficiency. Nevertheless, the absence of a known tail highly increases the complexity of the decoding process. Recently, the use of the A* algorithm has highly decreased the computational complexity of Maximum Likelihood (ML) decoding of tailbiting convolutional codes. The decoding process in this case requires two steps. In the first step, a typical Viterbi decoding is employed to collect information regarding the trellis. The A* algorithm is then applied in the second step, using the information obtained in the first step to calculate the heuristic function. The improvements proposed in this work decrease the computational complexity of the A* algorithm using further information from the first step of the algorithm. This information is used for finding early terminating conditions for the A* algorithm. Simulation results show that the proposed modification decreases the complexity of ML decoding with the A* algorithm in terms of the performed number of operations.","1090-3038;1090-3038","978-1-4244-3573-9978-1-4244-3574","10.1109/VETECF.2010.5594467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5594467","","Memory management;Measurement;Maximum likelihood decoding;Signal to noise ratio;Bit error rate;Encoding","cellular radio;encoding;maximum likelihood decoding;mobile radio;trellis codes;Viterbi decoding","tailbiting codes;wireless system;tailbiting convolutional codes;cellular mobile radio system;encoding method;decoding process;A* algorithm;computational complexity;maximum likelihood decoding;Viterbi decoding;trellis;heuristic function","","1","","7","","","","","IEEE","IEEE Conferences"
"An improved low-complexity multiple description coding for peer-to-peer video streaming","M. R. Ardestani; A. A. B. Shirazi; M. R. Hashemi","Image Processing Lab, School of Electrical Eng., Iran University of Science and Technology, Tehran, Iran; Image Processing Lab, School of Electrical Eng., Iran University of Science and Technology, Tehran, Iran; Multimedia Processing Lab, School of Electrical and Computer Eng., University of Tehran, Tehran, Iran","2010 IEEE International Conference on Multimedia and Expo","","2010","","","255","260","Multiple description scalable coding based on T+2D wavelet decomposition structure is highly flexible for peer-to-peer (P2P) video streaming. Finding the optimal truncation point of each code block (CB) within each description is an NP-hard problem. To implement an efficient low-complexity solution, we propose a simple clustering algorithm for partitioning the CBs into a limited number of clusters, such that one can find the optimal cluster-level redundancy-rate assignment matrix using a low-complexity full search. This approach improves the decoding quality compared to the co-echelon frameworks in which a non-optimal rate assignment matrix is used. In addition, the proposed clustering approach may be analytically represented by closed-form relations for low-complexity computation of optimal encoding parameters. The simulation results demonstrate that the adaptive proposed framework outperforms the approaches in by (0.25~1 dB), the scheme of by (1.3~1.6 dB), and the non-adaptive multiple description coding by (2.3~4.3 dB). Furthermore, the proposed clustering approach requires %52-%88 less computations compared to the framework in.","1945-788X;1945-7871;1945-7871","978-1-4244-7493-6978-1-4244-7491-2978-1-4244-7492","10.1109/ICME.2010.5583053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583053","Scalable video coding;multiple description coding;channel-aware redundancy-rate allocation;clustering","Encoding;Clustering algorithms;Streaming media;Resource management;Peer to peer computing;Video coding;Algorithm design and analysis","computational complexity;optimisation;pattern clustering;peer-to-peer computing;video coding;video streaming","peer to peer video streaming;wavelet decomposition structure;code block;NP hard problem;clustering algorithm;optimal cluster level redundancy rate assignment matrix;optimal encoding","","1","","11","","","","","IEEE","IEEE Conferences"
"Fast mode decision for combined scalable video coding based on the block complexity function","T. Kim; J. Hong; J. Suh","Department of electronics engineering, Chungbuk National University, Cheongju, Korea.; school of Electrical and computer engineering, Chungbuk National University, Korea.; Member; College of Electrical and Computer Engineering, Chungbuk National University, Cheongju, Korea.","IEEE Transactions on Consumer Electronics","","2011","57","1","247","252","Scalable Video Coding is an H.264/AVC scalable extension that is used to provide various network-friendly scalability using a single bit stream. In SVC, the newly adapted predictive coding techniques can achieve high encoding efficiency, but they increase the computational complexity. To reduce the computational complexity, we present a fast mode decision algorithm based on the block complexity function considering the degree of correlation between base layer and enhancement layer. The simulation results show that the encoding time of the proposed fast mode decision algorithm for the combined scalability is about 54.72% compared with normal method although the loss of visual quality is negligible.","0098-3063;1558-4127","","10.1109/TCE.2011.5735509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5735509","Scalable video coding;combined scalability;inter layer prediction;fast mode decision","Scalability;Complexity theory;Encoding;Static VAr compensators;Correlation;Prediction algorithms;Video coding","encoding;video coding","combined scalable video coding;block complexity function;H.264/AVC scalable extension;adapted predictive coding technique;fast mode decision algorithm","","6","","16","","","","","IEEE","IEEE Journals & Magazines"
"A Reduced-Complexity Successive Cancellation List Decoding of Polar Codes","K. Chen; K. Niu; J. Lin","NA; NA; NA","2013 IEEE 77th Vehicular Technology Conference (VTC Spring)","","2013","","","1","5","Polar codes are the first constructive and provable capacity-achieving codes. In finite code length cases, successive cancellation list (SCL) decoding algorithm is reported to have performance very close to maximum-likelihood (ML) decoding. In this paper, a reduced-complexity version of SCL decoding algorithm is proposed to boost the finite- length performance of polar codes. By regarding the SCL decoding algorithm as a path searching procedure in a code tree representation, a tree-pruning technique is used to avoid unnecessary path searching operations. With only a negligible loss of performance, the computational complexity of pruned SCL decoder can be very close to that of the successive cancellation (SC) decoder in the moderate and high signal-to- noise ratio (SNR) regime.","1550-2252","978-1-4673-6337","10.1109/VTCSpring.2013.6691844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691844","","Maximum likelihood decoding;Measurement;Computational complexity;Signal to noise ratio;Vectors","computational complexity;maximum likelihood decoding;trees (mathematics)","polar codes;finite code length;successive cancellation list decoding algorithm;maximum-likelihood decoding;SCL decoding algorithm;path searching procedure;code tree representation;tree-pruning technique;computational complexity;signal-to-noise ratio regime","","6","","14","","","","","IEEE","IEEE Conferences"
"Complexity reduction algorithm for prediction unit decision process in high efficiency video coding","J. Lee; B. Kim; D. Jun; S. Jung; J. S. Choi","SunMoon University, Korea; SunMoon University, Korea; ETRI, Korea; ETRI, Korea; ETRI, Korea","IET Image Processing","","2016","10","1","53","60","To provide very high quality and/or high resolution video content under limited bandwidth conditions for transmission or storage, the high efficiency video coding (HEVC) has been recently finalised by the joint collaborative team on video coding. In this study, the authors propose a fast prediction unit (PU) decision method to reduce the computational complexity of HEVC encoder. They use an early PU decision in each coding unit-level based on spatio-temporal analyses and depth correlations. They also consider a classification of motion activity. Experimental results show that the encoding complexity can be reduced by up to 38% on average in the random access main profile configuration with only a small bit-rate increment and a peak signal to noise ratio (PSNR) decrement, compared to high efficiency video coding test model (HM) 7.0 reference software.","1751-9659;1751-9667","","10.1049/iet-ipr.2013.0631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361047","","","computational complexity;image classification;image motion analysis;image resolution;prediction theory;spatiotemporal phenomena;video coding","PSNR decrement;motion activity classification;depth correlation;spatiotemporal analysis;computational complexity reduction algorithm;PU decision method;joint collaborative team;HEVC;high resolution video content;high efficiency video coding;prediction unit decision process","","2","","36","","","","","IET","IET Journals & Magazines"
"Low-Complexity Receiver for Multi-Level Polar Coded Modulation in Non-Orthogonal Multiple Access","B. Tomasi; F. Gabry; V. Bioglio; I. Land; J. Belfiore","NA; NA; NA; NA; NA","2017 IEEE Wireless Communications and Networking Conference Workshops (WCNCW)","","2017","","","1","6","Non-orthogonal multiple access (NOMA) schemes have been proved to increase the multiple-access achievable rate with respect to orthogonal multiple access (OMA). In this paper we propose a novel communication system that combines multi-level coded modulation and polar codes in a NOMA scenario. Computational complexity decreases with the proposed scheme with respect to state-of-the-art solutions. We also highlight the trade-off between error rate performance and computational complexity.","","978-1-5090-5908-9978-1-5090-5909","10.1109/WCNCW.2017.7919045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919045","","NOMA;Decoding;Receivers;Modulation;Encoding;Computational complexity;Transmitters","computational complexity;modulation coding;multi-access systems;radio receivers","low-complexity receiver;multilevel polar coded modulation;nonorthogonal multiple access;NOMA scheme;orthogonal multiple access;OMA;computational complexity;error rate performance","","1","","16","","","","","IEEE","IEEE Conferences"
"A study on low complexity models to predict flaws in the Linux source code","L. Kanashiro; A. Ribeiro; D. Silva; P. Meirelles; A. Terceiro","FLOSS Competence Center, University of S&#x00E3;o Paulo, S&#x00E3;o Paulo, Brazil; FLOSS Competence Center, University of S&#x00E3;o Paulo, S&#x00E3;o Paulo, Brazil; UnB Faculty in Gama, University of Bras&#x00ED;lia, Bras&#x00ED;lia, Brazil; UnB Faculty in Gama, University of Bras&#x00ED;lia, Bras&#x00ED;lia, Brazil; QA Services Team, Linaro Limited, Curitiba, Brazil","2017 12th Iberian Conference on Information Systems and Technologies (CISTI)","","2017","","","1","6","Due to the constant evolution of technology, each day brings new programming languages, development paradigms, and ways of evaluating processes. This is no different with source code metrics, where there is always new metric classes. To use a software metric to support decisions, it is necessary to understand how to perform the metric collection, calculation, interpretation, and analysis. The tasks of collecting and calculating source code metrics are most often automated, but how should we monitor them during the software development cycle? Our research aims to assist the software engineer to monitor metrics of vulnerability threats present in the source code through a reference prediction model, considering that real world software have non-functional security requirements, which implies the need to know how to monitor these requirements during the software development cycle. As a first result, this paper presents an empirical study on the evolution of the Linux project. Based on static analysis data, we propose low complexity models to study flaws in the Linux source code. About 391 versions of the project were analyzed by mining the official Linux repository using an approach that can be reproduced to perform similar studies. Our results show that it is possible to predict the number of warnings triggered by a static analyzer for a given software project revision as long as the software is continuously monitored.","","978-9-8998-4347-9978-1-5090-5047","10.23919/CISTI.2017.7975747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975747","Source Code Static Analysis;Source Code Metrics;Common Weakness Enumeration;Prediction;Linux","Software;Linux;Complexity theory;Predictive models;Data models;Measurement;Monitoring","Linux;program diagnostics;software metrics;software quality;source code (software)","low-complexity model;flaw prediction;Linux source code metrics;metric collection;metric calculation;metric interpretation;metric analysis;software development cycle;vulnerability threats;nonfunctional security requirements;static analysis data;Linux repository mining","","","","23","","","","","IEEE","IEEE Conferences"
"A Computation Control Motion Estimation Method for Complexity-Scalable Video Coding","W. Lin; K. Panusopone; D. M. Baylon; M. Sun","Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai, China; Department of Advanced Technology, CTO Office, Home and Networks Mobility, Motorola, Inc., San Diego, CA, USA; Department of Advanced Technology, CTO Office, Home and Networks Mobility, Motorola, Inc., San Diego, CA, USA; Department of Electrical Engineering, University of Washington, Seattle, WA, USA","IEEE Transactions on Circuits and Systems for Video Technology","","2010","20","11","1533","1543","In this paper, a new computation-control motion estimation (CCME) method is proposed which can perform motion estimation (ME) adaptively under different computation or power budgets while keeping high coding performance. We first propose a new class-based method to measure the macroblock (MB) importance where MBs are classified into different classes and their importance is measured by combining their class information as well as their initial matching cost information. Based on the new MB importance measure, a complete CCME framework is then proposed to allocate computation for ME. The proposed method performs ME in a one-pass flow. Experimental results demonstrate that the proposed method can allocate computation more accurately than previous methods and, thus, has better performance under the same computation budget.","1051-8215;1558-2205","","10.1109/TCSVT.2010.2077773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5585734","Computation-control video coding;macroblock (MB) classification;motion estimation (ME)","Resource management;Classification algorithms;Current measurement;Prediction algorithms;Computer integrated manufacturing;Encoding;Motion estimation","motion estimation;video coding","computation control motion estimation;complexity-scalable video coding;CCME method;class-based method;macroblock measurement","","26","","19","","","","","IEEE","IEEE Journals & Magazines"
"Performance improvement of short-length regular low-density parity-check codes with low-complexity post-processing","R. K. Bhattar; K. R. Ramakrishnan; K. S. Dasgupta","Department of Electrical Engineering, Indian Institute of Science (IISC); SATCOM and Navigation Applications Area, Space Applications Centre (ISRO); Indian Institute of Space Science and Technology (IIST), Department of Space","IET Communications","","2012","6","15","2487","2496","It is well known that extremely long low-density parity-check (LDPC) codes perform exceptionally well for error correction applications, short-length codes are preferable in practical applications. However, short-length LDPC codes suffer from performance degradation owing to graph-based impairments such as short cycles, trapping sets and stopping sets and so on in the bipartite graph of the LDPC matrix. In particular, performance degradation at moderate to high <i>Eb</i>/<i>N</i><sub>0</sub> is caused by the oscillations in bit node a posteriori probabilities induced by short cycles and trapping sets in bipartite graphs. In this study, a computationally efficient algorithm is proposed to improve the performance of short-length LDPC codes at moderate to high <i>Eb</i>/<i>N</i><sub>0</sub>. This algorithm makes use of the information generated by the belief propagation (BP) algorithm in previous iterations before a decoding failure occurs. Using this information, a reliability-based estimation is performed on each bit node to supplement the BP algorithm. The proposed algorithm gives an appreciable coding gain as compared with BP decoding for LDPC codes of a code rate equal to or less than 1/2 rate coding. The coding gains are modest to significant in the case of optimised (for bipartite graph conditioning) regular LDPC codes, whereas the coding gains are huge in the case of unoptimised codes. Hence, this algorithm is useful for relaxing some stringent constraints on the graphical structure of the LDPC code and for developing hardware-friendly designs.","1751-8628;1751-8636","","10.1049/iet-com.2011.0292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6410524","","","belief networks;decoding;error correction codes;oscillations;parity check codes;probability","short-length regular low-density parity-check code;low-complexity postprocessing;error correction;short-length LDPC code;performance degradation;graph-based impairment;bipartite graph;LDPC matrix;oscillation;bit node;a posteriori probabilities;trapping set;belief propagation algorithm;BP algorithm;decoding failure;reliability-based estimation;coding gain;BP decoding;code rate","","","","","","","","","IET","IET Journals & Magazines"
"A low complexity block matching algorithm for fast motion estimation in High Efficiency Video Coding","K. C. R. C. Varma; Venkata Phani Kumar M; S. Mahapatra","Electronics and Electrical Communication Engineering, IIT Kharagpur, West Bengal, India; Electronics and Electrical Communication Engineering, IIT Kharagpur, West Bengal, India; Electronics and Electrical Communication Engineering, IIT Kharagpur, West Bengal, India","2015 Fifth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)","","2015","","","1","4","This paper proposes a search mechanism, namely accelerated internal stop search (AISS), to reduce the complexity of the block matching process used for motion estimation in High Efficiency Video Coding (HEVC). The proposed mechanism reduces the number of operations required to find the best matching reference block. Moreover, it uses the correlation with the previous reference frames to further reduce the complexity of motion estimation for the current reference frame. Results demonstrate that the proposed mechanism is able to reduce the complexity of motion estimation in the fast search mode of the HM-14.0 reference software for HEVC, i.e., the TZ Search motion estimation. The AISS mechanism outperforms the dynamic internal stop search algorithm (DISS) proposed by Ismail et al. in the year 2012 in terms of motion estimation time saving. Moreover, when AISS is integrated with the dynamic model based fast motion estimation proposed by Ismail et al. [1], the complexity of motion estimation is further reduced.","","978-1-4673-8564","10.1109/NCVPRIPG.2015.7489951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489951","","Complexity theory;Motion estimation;Mathematical model;Heuristic algorithms;Bit rate;Video recording;Quality assessment","computational complexity;motion estimation;search problems;video coding","low complexity block matching algorithm;fast motion estimation;high efficiency video coding;search mechanism;accelerated internal stop search;AISS;complexity reduction;block matching process;HEVC;best matching reference block;HM-14.0 reference software;TZ Search motion estimation;dynamic internal stop search algorithm;DISS","","","","13","","","","","IEEE","IEEE Conferences"
"Low-complexity motion-based saliency map estimation for perceptual video coding","A. B. Mejía-Ocaña; M. de-Frutos-López; S. Sanz-Rodríguez; Ó. del-Ama-Esteban; C. Peláez-Moreno; F. Díaz-de-María","Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain; Department of Signal Theory and Communications, University Carlos III Madrid, Leganés, Spain","CONATEL 2011","","2011","","","1","6","In this paper, a low-complexity motion-based saliency map estimation method for perceptual video coding is proposed. The method employs a camera motion compensated vector map computed by means of a hierarchical motion estimation (HME) procedure and a Restricted Affine Transformation (RAT)-based modeling of the camera motion. To allow for a computationally efficient solution, the number of layers of the HME has been restricted and the potential unreliable motion vectors due to homogeneous regions have been detected and specially managed by means of a smooth block detector. Special care has been taken of the smoothness of the resulting compensated camera motion vector map to avoid unpleasant artifacts in the perceptually-coded sequence, by including a final post-processing based on morphological filtering. The proposed saliency map has been both visually and subjectively assessed showing quality improvements when used as a part of the H.264/AVC standard codec at medium-to-low bitrates.","","978-1-4577-1047-6978-1-4577-1046-9978-1-4577-1045","10.1109/CONATEL.2011.5958666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958666","region of interest;perceptual video coding;visual saliency;visual attention;hierarchical motion estimation;camera motion estimation;mathematical morphology","Cameras;Motion estimation;Filtering;Estimation;Pixel;Video coding;Optical filters","affine transforms;filtering theory;image sequences;motion compensation;motion estimation;object detection;video cameras;video coding","low-complexity motion-based saliency map estimation;perceptual video coding;camera motion compensated vector map;hierarchical motion estimation;restricted affine transformation;RAT-based modeling;motion vector;block detector;perceptually-coded sequence;morphological filtering;H.264/AVC standard codec;medium-to-low bitrate","","4","","10","","","","","IEEE","IEEE Conferences"
"Joint Timing and Channel Estimation for Bandlimited Long-Code-Based MC-DS-CDMA: A Low-Complexity Near-Optimal Algorithm and the CRLB","S. Wang; S. Chen; A. Wang; J. An; L. Hanzo","School of Information Science and Electronics, Beijing Institute of Technology, P. R. China; School of Electronics and Computer Science, University of Southampton, U.K; School of Information Science and Electronics, Beijing Institute of Technology, P. R. China; School of Information Science and Electronics, Beijing Institute of Technology, P. R. China; School of Electronics and Computer Science, University of Southampton, U.K","IEEE Transactions on Communications","","2013","61","5","1998","2011","Joint Timing and Channel Estimation (JTCE) for bandlimited long-code-aided Multi-Carrier Direct-Sequence Code Division Multiple Access (MC-DS-CDMA) systems is investigated. We establish the optimal multiuser timing and channel estimates for the uplink MC-DS-CDMA receiver by minimising a weighted least squares cost function with respect to K independent parameters, where K is the number of active users. A guided random search procedure known as Repeated Weighted Boosting Search (RWBS) is invoked for numerically solving this challenging multivariate optimisation problem, and thereby for producing near-optimal timing and channel estimates. The Cramer-Rao Lower Bound (CRLB) for the JTCE problem of interest is derived to benchmark the performance of the proposed RWBS based estimator. Quantitatively, for the scenario of K=10 users, E_b/N_0≥3 dB where E_b is the energy per bit and N_0 the single-sided noise power spectral density, and for a near-far ratio of 10 dB, the RWBS based estimator using an observation window of 20 symbols is shown to approach the CRLB at a complexity 10 orders of magnitude lower in comparison to its full maximum likelihood search based counterpart. The proposed algorithm does not require the transmission of known pilots, yet it is capable of handling time-variant channel states.","0090-6778;1558-0857","","10.1109/TCOMM.2013.021913.120858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6469000","Joint timing and channel estimation (JTCE);multi-carrier direct-sequence code division multiple access (MC-DS-CDMA);repeated weighted boosting search (RWBS);Cramer-Rao lower bound (CRLB)","Channel estimation;Complexity theory;Timing;Maximum likelihood estimation;Optimization;Vectors;Receivers","channel estimation;code division multiple access;least squares approximations;maximum likelihood estimation;radio receivers;search problems;timing","bandlimited long-code-based MC-DS-CDMA;low-complexity near-optimal algorithm;CRLB;JTCE;joint timing and channel estimation;bandlimited long-code-aided MC-DS-CDMA;MC-DS-CDMA receiver;MC-DS-CDMA systems;optimal multiuser timing;weighted least squares cost function;repeated weighted boosting search;guided random search procedure;multivariate optimisation problem;Cramέr-Rao Lower Bound;RWBS;single-sided noise power spectral density;maximum likelihood search;time-variant channel states;multicarrier direct-sequence code division multiple access","","4","","49","","","","","IEEE","IEEE Journals & Magazines"
"A low-complexity joint detection-decoding algorithm for nonbinary LDPC-coded modulation systems","X. Wang; B. Bai; X. Ma","State Key Lab. of ISN, Xidian University, Xi'an 710071, China; State Key Lab. of ISN, Xidian University, Xi'an 710071, China; Department of ECE, Sun Yat-sen University, Guangzhou, GD 510275, China","2010 IEEE International Symposium on Information Theory","","2010","","","794","798","In this paper, we present a low-complexity joint detection-decoding algorithm for nonbinary LDPC coded-modulation systems. The algorithm combines hard-decision decoding using the message-passing strategy with the signal detector in an iterative manner. It requires low computational complexity, offers good system performance and has a fast rate of decoding convergence. Compared to the q-ary sum-product algorithm (QSPA), it provides an attractive candidate for practical applications of q-ary LDPC codes.","2157-8095;2157-8117","978-1-4244-7892-7978-1-4244-7890-3978-1-4244-7891","10.1109/ISIT.2010.5513632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513632","","Parity check codes;Iterative decoding;Iterative algorithms;Modulation coding;Detectors;Signal detection;Computational complexity;System performance;Convergence;Sum product algorithm","computational complexity;decoding;message passing;modulation coding;parity check codes","low-complexity joint detection-decoding algorithm;nonbinary LDPC-coded modulation systems;hard-decision decoding;message-passing strategy;signal detector;computational complexity;q-ary sum-product algorithm","","13","","15","","","","","IEEE","IEEE Conferences"
"Error-trellis state complexity of LDPC convolutional codes based on circulant matrices","M. Tajima; K. Okino; T. Miyagoshi","Graduate School of Sci. and Eng., University of Toyama, 3190 Gofuku, Toyama 930-8555, Japan; Information Technology Center, University of Toyama, 3190 Gofuku, Toyama 930-8555, Japan; Graduate School of Sci. and Eng., University of Toyama, 3190 Gofuku, Toyama 930-8555, Japan","2010 International Symposium On Information Theory & Its Applications","","2010","","","19","24","Let H(D) be the parity-check matrix of an LDPC convolutional code corresponding to the parity-check matrix H of a QC code obtained using the method of Tanner et al. We see that the entries in H(D) are all monomials and several rows (columns) have monomial factors. Let us cyclically shift the rows of H. Then the parity-check matrix H'(D) corresponding to the modified matrix H' defines another convolutional code. However, its free distance is lower-bounded by the minimum distance of the original QC code. Also, each row (column) of H'(D) has a factor different from the one in H(D). We show that the statespace complexity of the error-trellis associated with H'(D) can be significantly reduced by controlling the row shifts applied to H with the error-correction capability being preserved.","","978-1-4244-6017-5978-1-4244-6016","10.1109/ISITA.2010.5648981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5648981","","Convolutional codes;Parity check codes;Polynomials;Complexity theory;Delay;Electronic mail;Block codes","computational complexity;convolutional codes;cyclic codes;matrix algebra;parity check codes;trellis codes","error-trellis state complexity;LDPC convolutional codes;circulant matrices;parity-check matrix;QC code;monomial factors;statespace complexity","","1","","10","","","","","IEEE","IEEE Conferences"
"A low complexity embedded image coding algorithm using Hierarchical Listless DTT","R. K. Senapati; U. C. Pati; K. K. Mahapatra","Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela., India, Orissa, 769008; Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela., India, Orissa, 769008; Department of Electronics and Communication Engineering, National Institute of Technology, Rourkela., India, Orissa, 769008","2011 8th International Conference on Information, Communications & Signal Processing","","2011","","","1","5","Listless SPECK (LSPECK) is a low complexity image coding algorithm compared to SPECK. The problem of LSPECK is that, it encode each insignificant subband by a zero. Therefore, these block based coders codes as many zeros as the number of insignificant subbands. This gives rise to many zeros at the encoder output on early bit plane passes. By looking at the statistics of transformed images, the number of significant coefficients at some of the higher bitplanes are likely to be very few. We propose a variant of LSPECK algorithm, called as Improved LSPECK (ILSPECK), that code a single zero to several insignificant subbands. This reduces the length of the output bit string as well as encoding/decoding time. Further, ILSPECK algorithm is coupled with discrete tchebichef transform (DTT). The propose new coder called as Hierarchical Listless DTT (HLDTT), preserves most of the properties of wavelet coders. Extensive simulations on various kind of images shows the effictiveness of our coder.","","978-1-4577-0031-6978-1-4577-0029-3978-1-4577-0030","10.1109/ICICS.2011.6174297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174297","Listless SPECK;Improved Listless SPECK;Hierarchical Listless DTT;Image Compression;Embedded Coder","Image coding;Wavelet transforms;Discrete cosine transforms;PSNR;Bit rate;Decoding","block codes;decoding;image coding;wavelet transforms","low complexity embedded image coding algorithm;hierarchical listless DTT;listless SPECK;block based coders codes;improved LSPECK;output bit string;encoding-decoding time;discrete tchebichef transform;wavelet coders","","3","","13","","","","","IEEE","IEEE Conferences"
"A message-passing decoding algorithm for q-ary LDPC codes with low-complexity","C. Chen; Q. Huang; C. Chao; S. Lin","Inst. of Communications Engineering, National Tsing Hua University, Hsinchu 30013, Taiwan, R.O.C.; Dept. of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA; Inst. of Communications Engineering, National Tsing Hua University, Hsinchu 30013, Taiwan, R.O.C.; Dept. of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA","2010 International Symposium On Information Theory & Its Applications","","2010","","","501","506","This paper presents a novel low-complexity iterative reliability-based decoding algorithm for LDPC codes over q-ary finite fields. This proposed algorithm has low complexity and hence provides an effective trade-off between error performance and decoding complexity compared to q-ary sum product algorithm. This decoding algorithm is devised based on simple orthogonal concept of one-step majority-logic decoding for q-ary linear block codes. It requires only integer and finite field operations and converges very fast in decoding. It is particularly effective for decoding LDPC codes constructed based on finite geometries and finite fields.","","978-1-4244-6017-5978-1-4244-6016","10.1109/ISITA.2010.5650154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650154","","Iterative decoding;Complexity theory;Reliability;Decoding;Null space;Variable speed drives","block codes;iterative decoding;linear codes;message passing;parity check codes;reliability","message-passing decoding algorithm;low-complexity iterative reliability-based decoding algorithm;decoding complexity;q-ary sum product algorithm;one-step majority-logic decoding;q-ary linear block codes;finite field operations","","","","26","","","","","IEEE","IEEE Conferences"
"Construction of girth-eight quasi-cyclic low-density parity-check codes with low encoding complexity","R. Wang; Y. Li; H. Zhao; L. Qin; H. Zhang","Chongqing University of Posts and Telecommunications, People's Republic of China; Chongqing University of Posts and Telecommunications, People's Republic of China; Chongqing University of Posts and Telecommunications, People's Republic of China; Chongqing University of Posts and Telecommunications, People's Republic of China; Chongqing University of Posts and Telecommunications, People's Republic of China","IET Communications","","2016","10","2","148","153","A construction method for girth-eight quasi-cyclic low-density parity-check codes (QC-LDPCs) with low encoding complexity is proposed in this study. To avoid short cycles and further improve the performance of the authors proposed codes, the greatest common divisor (GCD)-based method is utilised to construct the information part with girth-eight of the parity-check matrix with systematic form. Furthermore, the quasi-dual-diagonal structure is adopted as the parity part of the parity-check matrix for fast encoding, which not only maintains the parity-check matrix with girth-eight, but also efficiently decreases the encoding complexity of their proposed codes. Simulation results show that their proposed codes perform better than Mackay's codes and the QC-LDPCs constructed by the GCD-based method, and have comparable performance compared with progressive edge growth based QC-LDPCs. Besides, their proposed codes have lower encoding complexity due to the property of fast encoding.","1751-8628;1751-8636","","10.1049/iet-com.2015.0056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398231","","","cyclic codes;encoding;matrix algebra;parity check codes","Mackay codes;parity check matrix;GCD;greatest common divisor;QC-LDPC;low encoding complexity;girth eight quasi-cyclic low density parity check codes","","1","","21","","","","","IET","IET Journals & Magazines"
"Reduced complexity space-time coding in single-frequency networks","K. Pölönen; V. Koivunen","SMARAD CoE, Department of Signal Processing and Acoustics, Aalto University School of Electrical Engineering, P.O. Box 13000, FI-00076 Aalto, Finland; SMARAD CoE, Department of Signal Processing and Acoustics, Aalto University School of Electrical Engineering, P.O. Box 13000, FI-00076 Aalto, Finland","2011 IEEE Wireless Communications and Networking Conference","","2011","","","1523","1528","Mobile broadcasting MIMO systems differ in several essential ways from conventional MIMO applications, especially when they are operating in single-frequency networks. There are certain channel features, like the imbalance of received average powers, which should be taken into account in system design. In this paper, space-time coding that allows reduced complexity maximum likelihood decoding of the received signal in a single-frequency network is introduced. A novel optimization criterion for fast-decodable 4×2 structures is proposed and the performance in single-frequency networks is studied in different scenarios through simulations. It is noticed that the reduced complexity codes suffer only a fraction of a decibel performance loss compared to an earlier proposed alternative for digital video broadcasting while achieving significant savings in computational complexity when maximum likelihood decoding is applied.","1558-2612;1525-3511;1525-3511","978-1-61284-254-7978-1-61284-255-4978-1-61284-253","10.1109/WCNC.2011.5779356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779356","digital video broadcasting;MIMO;single-frequency networks;space-time coding","MIMO;Complexity theory;Maximum likelihood decoding;Matrix decomposition;Transmitters;Encoding;Optimization","channel coding;digital video broadcasting;maximum likelihood decoding;MIMO communication;optimisation;space-time block codes","space-time coding;single-frequency networks;mobile broadcasting;MIMO systems;maximum likelihood decoding;optimization;digital video broadcasting;computational complexity","","1","","8","","","","","IEEE","IEEE Conferences"
"A low-complexity soft-output decoder for polar codes","U. U. Fayyaz; J. R. Barry","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332-0250, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332-0250, USA","2013 IEEE Global Communications Conference (GLOBECOM)","","2013","","","2692","2697","A widely used soft-output decoder for polar codes is a message-passing algorithm based on belief propagation, which performs well at the cost of high processing and storage requirements. In this paper we propose a low-complexity alternative for soft-output decoding of polar codes that offers comparable performance but with significantly reduced processing and storage requirements. In particular we show that the complexity of the proposed decoder is about 5% of the total complexity of the belief propagation decoder for the dicode channel, while achieving comparable error-rate performance. Furthermore, we show that the proposed decoder requires about 39% of the memory required by the belief propagation decoder for a block length of 32768.","1930-529X","978-1-4799-1353","10.1109/GLOCOM.2013.6831481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831481","","Decoding;Memory management;Complexity theory;Belief propagation;Iterative decoding;AWGN channels","decoding;message passing","low-complexity soft-output decoder;polar codes;message-passing algorithm;storage requirements;belief propagation decoder;dicode channel;comparable error-rate performance;block length","","2","","14","","","","","IEEE","IEEE Conferences"
"Two Low-Complexity Reliability-Based Message-Passing Algorithms for Decoding Non-Binary LDPC Codes","C. Chen; Q. Huang; C. Chao; S. Lin","Institute of Communications Engineering, National Tsing Hua University, Hsinchu 30013, Taiwan, R.O.C; Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA; Institute of Communications Engineering, National Tsing Hua University, Hsinchu 30013, Taiwan, R.O.C; Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA","IEEE Transactions on Communications","","2010","58","11","3140","3147","This paper presents two low-complexity reliability-based message-passing algorithms for decoding LDPC codes over non-binary finite fields. These two decoding algorithms require only finite field and integer operations and they provide effective trade-off between error performance and decoding complexity compared to the non-binary sum product algorithm. They are particularly effective for decoding LDPC codes constructed based on finite geometries and finite fields.","0090-6778;1558-0857","","10.1109/TCOMM.2010.091310.090327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5582319","Reliability-based message-passing algorithms;non-binary LDPC codes;fast Fourier transform q-ary sum-product algorithm","Iterative decoding;Decoding;Reliability;Variable speed drives;Complexity theory;Geometry","decoding;message passing;parity check codes;telecommunication computing;telecommunication network reliability","low-complexity reliability-based message passing algorithms;decoding nonbinary LDPC codes;integer operations;nonbinary sum product algorithm;finite geometries;finite fields","","57","","32","","","","","IEEE","IEEE Journals & Magazines"
"Flexible Complexity Control Solution for Transform Domain Wyner-Ziv Video Coding","X. HoangVan; B. Jeon","Digital Media Lab, Sunkyunkwan University, Suwon, Korea; Digital Media Lab, Sunkyunkwan University, Suwon, Korea","IEEE Transactions on Broadcasting","","2012","58","2","209","220","Most Wyner-Ziv (WZ) video coding solutions in the literature focus on improving the coding efficiency. Recently, a few papers have addressed problems related to the complexity distribution of WZ video coding by sharing the motion estimation process between the encoder and decoder. However, these methods turn out to significantly increase the computational complexity of the encoding process, due to the presence of motion estimation at the encoder, which is less appealing in WZ coding applications, since their primary requirement is a low-encoding complexity. To address this problem, we propose a different approach to complexity control based on the adaptive selection between WZ and intra coding in the WZ frames. This solution is more flexible than the previous ones in that it can be implemented either at the encoder or decoder. The proposed intra mode selection algorithm exploits the spatial and temporal coherency in the transform domain Wyner-Ziv video coding (TDWZ), in order to achieve complexity control between the WZ encoder and decoder. The experimental results illustrate that the proposed algorithm not only effectively distributes the computational complexity over the encoder and decoder, but also retains the low-complexity feature at the encoder. This should make it attractive for a large number of real video applications of the WZ video coding paradigm. Moreover, the coding efficiency of the conventional TDWZ codec without intra mode decision is improved by up to 2 dB by the proposed intra mode selection algorithm.","0018-9316;1557-9611","","10.1109/TBC.2012.2187611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6172621","Complexity control;computational complexity;intra mode decision;video coding;video communication;Wyner-Ziv video coding","Encoding;Decoding;Video coding;Computational complexity;Transforms;Motion estimation","computational complexity;data compression;decoding;encoding;motion estimation;video coding","flexible complexity control solution;transform domain Wyner-Ziv video coding;coding efficiency;WZ video coding complexity distribution;motion estimation process;computational complexity;encoding process;low-encoding complexity;adaptive selection;WZ frames;intramode selection algorithm;spatial coherency;temporal coherency;WZ decoder;WZ encoder;TDWZ codec","","12","","40","","","","","IEEE","IEEE Journals & Magazines"
"Performance analysis and complexity study of LDPC and turbo coding schemes for inter vehicle communications","G. Kiokes; G. Economakos; A. Amditis; N. K. Uzunoglu","Researcher, Institute of Communication and Computer Systems (ICCS), National Technical University of Athens, I-SENSE Group; Lecturer, School of Electrical and Computer Engineering, National Technical University of Athens, Microprocessors and Digital Systems Lab; Institute of Communication and Computer Systems (ICCS), National Technical University of Athens, I-SENSE Group; Professor, School of Electrical and Computer Engineering, National Technical University of Athens, Microwaves and Optics Lab","2011 11th International Conference on ITS Telecommunications","","2011","","","559","564","This paper provides a comprehensive investigation of the performance and practical implementation issues of two coding schemes, employing Turbo Codes and low-density parity-check (LDPC) codes, over vehicular ad-hoc networks based on IEEE 802.11p specifications. Using simulation authors present the results of an evaluation of system performance for the two different coding schemes. We concentrate our evaluation on two different environments AWGN and Non Line of Sight propagation environment (Rayleigh Fading). BER (Bit Error Rate) and SNR (Signal to Noise Ratio) values for QPSK modulation are examined and tested. The Forward Error Correction (FEC) system model in the transceiver with the two schemes has been implemented in a Field Programmable Gate Array (FPGA) from Xilinx. At the end, a test-bed was developed using the Nallatech XtremeDsp Development Kit with a Xilinx Virtex-4 FPGA, for comparing simulation results with real time implementations.","","978-1-61284-671-2978-1-61284-668-2978-1-61284-670","10.1109/ITST.2011.6060119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6060119","LDPC;Turbo;FPGA;performance;coding;BER;implementation","Iterative decoding;Decoding;Turbo codes;Field programmable gate arrays","AWGN channels;channel coding;error statistics;field programmable gate arrays;forward error correction;parity check codes;quadrature phase shift keying;Rayleigh channels;telecommunication standards;turbo codes;vehicular ad hoc networks;wireless LAN","performance analysis;low density parity check codes;LDPC;turbo coding;inter vehicle communications;vehicular ad-hoc networks;IEEE 802.11p;AWGN;non line of sight propagation environment;Rayleigh fading;bit error rate;signal-to-noise ratio;QPSK modulation;forward error correction system;field programmable gate array;Xilinx Virtex-4 FPGA","","2","","14","","","","","IEEE","IEEE Conferences"
"Design of complexity-optimized raptor codes for BI-AWGN channel","C. Yao; Z. Zhang; K. Tu","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China","2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","","2013","","","824","829","This paper aims to design a class of Raptor codes with optimized complexity for the binary input additive white Gaussian noise (BI-AWGN) channel under the joint decoding framework in which soft information is exchanged between the pre-code and the LT code iteratively. Utilizing the belief propagation (BP) decoder, the decoding complexity is measured by the average number of arithmetic operations needed to correctly recover each information bit. Based on the analytical asymptotic convergence analysis which is built upon extrinsic information transfer (EXIT) charts, we develop a numerical approximation for the number of iterations needed for measuring the decoding complexity, and then formulate an optimization problem for the design of efficient output degree distributions. We further discuss the fundamental problem of complexity-rate tradeoff in Raptor code design. Simulations show that the optimized distribution indeed achieves lower complexity without much performance loss compared to other existing rate-optimized Raptor Codes.","2166-9570;2166-9589","978-1-4673-6235","10.1109/PIMRC.2013.6666250","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666250","","Complexity theory;Iterative decoding;Decoding;Optimization;Encoding;Approximation methods","approximation theory;AWGN channels;binary codes;channel coding;convergence;decoding;iterative methods;optimisation;precoding","complexity-optimized Raptor code design;bi-AWGN channel;binary input additive white Gaussian noise channel;joint decoding framework;LT code;precoding;belief propagation decoder;BP decoder;decoding complexity;arithmetic operations;analytical asymptotic convergence analysis;extrinsic information transfer chart;EXIT charts;numerical approximation;iteration method;optimization problem;output degree distributions;complexity-rate tradeoff;rate-optimized Raptor codes","","","","13","","","","","IEEE","IEEE Conferences"
"Improved low complexity hybrid turbo codes and union bound analysis","A. Bhise; P. D. Vyavahare","Department of Electronics & Telecommunication Engineering, K.J.S. Institute of Engineering and Information Technology, Sion, Mumbai, India; Department of Electronics & Telecommunication Engineering, S.G.S. Institute of Technology and Science, Indore, India","IET Communications","","2011","5","4","512","518","Turbo convolutional codes (TCC) are excellent error correcting codes for wireless channels. However, TCC decoders require large decoding complexity. Moreover, complexity of TCC decoder does not reduce even if puncturing is used to change the coding rate. Modified turbo codes require lower decoding complexity than TCC as they use multiple concatenations of simple block codes and convolutional codes. Recently, a class of modified turbo codes called low complexity hybrid turbo codes (LCHTC) and improved low complexity hybrid turbo codes (ILCHTC) have been proposed. It has been shown that LCHTC and ILCHTC achieve bit error rate (BER) which is comparable to TCC and have much lower decoding complexity. Simulation results show that BER performance of ILCHTC is better than that of LCHTC. Rate-1/3 ILCHTC achieve BER of 10<sup>-5</sup> at bit energy-to-noise ratio (<i>E</i><sub>b</sub>/<i>N</i><sub>0</sub>) of 1.9 dB, which is 0.4 dB higher than <i>E</i><sub>b</sub>/<i>N</i><sub>0</sub> for TCC adopted by third generation partnership project (3GPP). Moreover, ILCHTC and LCHTC decoders require half the number of computations as compared to those required for TCC decoder. In this study, union-bound analysis of ILCHTC is presented to investigate BER performance<;10<sup>-6</sup>. For large interleaver lengths, analysis of theoretical union bound requires numerous computations. Therefore approximate analysis of union bound is derived from theoretical union bound. It is shown that the analysis of approximate union bound achieves reasonable accuracy. Moreover, approximate union bound can be evaluated with significantly less computational complexity than the theoretical union bound.","1751-8628;1751-8636","","10.1049/iet-com.2009.0611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728959","","","convolutional codes;decoding;error correction codes;error statistics;turbo codes;wireless channels","low complexity hybrid turbo codes;union bound analysis;turbo convolutional codes;error correcting codes;wireless channel;decoder;decoding complexity;coding rate;modified turbo codes;bit error rate;BER performance;third generation partnership project;computational complexity","","1","","","","","","","IET","IET Journals & Magazines"
"Flexible and Low-Complexity Encoding and Decoding of Systematic Polar Codes","G. Sarkis; I. Tal; P. Giard; A. Vardy; C. Thibeault; W. J. Gross","Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Department of Electrical Engineering, Technion–Israel Institute of Technology, Haifa, Israel; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada; Department of Electrical and Computer Engineering and the Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA; Department of Electrical Engineering, École de Technologie Supérieure, Montreal, QC, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada","IEEE Transactions on Communications","","2016","64","7","2732","2745","In this paper, we present hardware and software implementations of flexible polar systematic encoders and decoders. The proposed implementations operate on polar codes of any length less than a maximum and of any rate. We describe the low-complexity, highly parallel, and flexible systematic-encoding algorithm that we use and prove its correctness. Our hardware implementation results show that the overhead of adding code rate and length flexibility is little, and the impact on operation latency minor compared with code-specific versions. Finally, the flexible software encoder and decoder implementations are also shown to be able to maintain high throughput and low latency.","0090-6778;1558-0857","","10.1109/TCOMM.2016.2574996","BSF; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482677","Polar codes;systematic encoding;multi-code encoders;multi-code decoders","Systematics;Encoding;Decoding;Software algorithms;Hardware;Software;Throughput","codes;decoding","software decoder;software encoder;length flexibility;code rate;systematic encoding algorithm;systematic polar code;decoding;low-complexity encoding","","26","","18","","","","","IEEE","IEEE Journals & Magazines"
"A reduced complexity decoder using compact genetic algorithm for linear block codes","A. Berkani; M. Belkasmi","SIME Laboratory, ENSIAS, Mohamed V University in Rabat, Morocco; SIME Laboratory, ENSIAS, Mohamed V University in Rabat, Morocco","2016 International Conference on Advanced Communication Systems and Information Security (ACOSIS)","","2016","","","1","6","The compact Genetic Algorithm decoder has been introduced in [9] as an efficient decoding method of linear block codes. It requires less storage memory than Genetic Algorithms based decoders. One of its major weakness is the big number of necessary iterations to reach convergence in comparison with Genetic Algorithms (GA) based decoders. We propose, in this work, new ideas allowing us to reduce the number of iterations from about 10<sup>5</sup> to just about 10<sup>3</sup> which reduces the complexity of decoding. This, without decreasing the decoding performance. We introduce a new stopping criterion based on the soft weight of the probability vector p, a new initialization method of p and we tried to combine both methods all together. Both performance study and the calculation of the average number of iterations ensure the effectiveness of the proposed decoder.","","978-1-5090-6227-0978-1-5090-6228","10.1109/ACOSIS.2016.7843925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843925","compact Genetic Algorithm;iterations;complexity;stopping criterion;probability vector;soft weight;initialization","","block codes;decoding;genetic algorithms;iterative methods;linear codes","reduced complexity decoder;linear block codes;compact genetic algorithm decoder;stopping criterion;probability vector","","","","12","","","","","IEEE","IEEE Conferences"
"Novel Code Filter-Bank Multicarrier Scheme with Low Complexity for OQAM Symbol Transmission and Reception","Z. He; L. Zhou; Y. Yang; Y. Chen; X. Ling","National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; National Key Laboratory of Science and Technology on Communications, University of Electronic Science and Technology of China, Chengdu, Sichuan, China","2018 IEEE 18th International Conference on Communication Technology (ICCT)","","2018","","","635","640","Filter bank multicarrier with offset quadrature amplitude modulation (FBMC-OQAM) system has low out-of-band emission but needs complex receiver for broadband channel. To address this problem, we propose a new cyclic prefix (CP)-based code FBMC-OQAM system (CC-FBMC-OQAM) with low complexity in this paper. CC-FBMC-OQAM system can be realized within the framework of the standard OFDM system, and perfect recovery (PR) condition is derived in the ideal channel. In order to reduce compute complexity, a low complexity transform of the Fast Walsh-Hadamard and Fourier transform (FWFT) is adopted and it uses a fast orthonormal unitary transform to calculate both the discrete Fourier transform (DFT) and the Walsh-Hadamard transform (WHT). Theoretical analysis and simulation results show that, compared to the C-FBMC-OQAM system and orthogonal frequency division multiplexing (OFDM) system, the computational complexity of proposed CC-FBMC-OQAM system is reduced substantially with almost same bit error rate (BER) performance in doubly-flat channel. Moreover, the BER performance of proposed CC-FBMC-OQAM system slightly outperform conventional C-FBMC-OQAM system in doubly-selective channel.","2576-7828;2576-7844","978-1-5386-7635-6978-1-5386-7633-2978-1-5386-7636","10.1109/ICCT.2018.8599886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8599886","Filter bank multicarrier;code FBMC-OQAM;cyclic prefix;low complexity;Walsh-Hadamard transform","Complexity theory;OFDM;Manganese;Interference;Filter banks;Prototypes;Quadrature amplitude modulation","channel bank filters;computational complexity;cyclic codes;discrete Fourier transforms;error statistics;Hadamard transforms;OFDM modulation;quadrature amplitude modulation;Walsh functions;wireless channels","novel code filter-bank multicarrier scheme;OQAM symbol transmission;filter bank multicarrier;offset quadrature amplitude modulation system;complex receiver;cyclic prefix-based code FBMC-OQAM system;standard OFDM system;computational complexity;Fast Walsh-Hadamard transform;CC-FBMC-OQAM system;perfect recovery condition;low complexity transform;Fourier transform;FWFT;fast orthonormal unitary transform;discrete Fourier transform;DFT;WHT;orthogonal frequency division multiplexing system;doubly-flat channel;bit error rate performance;BER performance;doubly-selective channel;low out-of-band emission;broadband channel;OQAM symbol reception","","","","20","","","","","IEEE","IEEE Conferences"
"Fast Intra Prediction Based on Content Property Analysis for Low Complexity HEVC-Based Screen Content Coding","J. Lei; D. Li; Z. Pan; Z. Sun; S. Kwong; C. Hou","School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; Department of Computer Science, City University of Hong Kong, Hong Kong; School of Electronic Information Engineering, Tianjin University, Tianjin, China","IEEE Transactions on Broadcasting","","2017","63","1","48","58","Screen content coding (SCC) has evolved into the extension of the High Efficiency Video Coding (HEVC). Low-latency, real-time transport between devices in the form of screen content video is becoming popular in many applications. However, the complexity of encoder is still very high for intra prediction in HEVC-based SCC. This paper proposes a fast intra prediction method based on content property analysis for HEVC-based SCC. First, coding units (CUs) are classified into natural content CUs (NCCUs) and screen content CUs (SCCUs), based on the statistic characteristics of the content. For NCCUs, the newly adopted prediction modes, including intra block copy mode and palette mode are skipped, if the DC or PLANR mode is the best mode, after testing the traditional intra prediction rough modes. In addition, the quadtree partition process is also terminated due to the homogeneous and smooth block usually chooses a large size CU. For SCCUs, a rank-based decision strategy is introduced to terminate the splitting process of current CU. For all CUs, the bit per pixel of current CU is used to make a CU size decision. Meanwhile, the depth information of neighboring CUs and co-located CU are utilized to further improve the performance. Experimental results show that the proposed algorithm can save 44.92% encoding time on average with negligible loss of video quality.","0018-9316;1557-9611","","10.1109/TBC.2016.2623241","Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province of China; Natural Science Foundation of Hebei Province of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7747507","Screen content coding (SCC);fast algorithm;intra prediction;screen content CU (SCCU);natural content CU (NCCU)","Encoding;Copper;Prediction algorithms;Video coding;Computational complexity;Computers;Partitioning algorithms","content management;quadtrees;video coding","content property analysis;high efficiency video coding;low complexity HEVC;screen content coding;screen content video;fast intraprediction method;coding units;natural content CU;NCCU;screen content CU;SCCU;intrablock copy mode;palette mode;quadtree partition process;video quality","","11","","40","","","","","IEEE","IEEE Journals & Magazines"
"Nonbinary LDPC-coded differential modulation with reduced-complexity decoding","M. Li; B. Bai; X. Ma","State Key Lab. of ISN, Xidian University, Xi'an 710071, China; State Key Lab. of ISN, Xidian University, Xi'an 710071, China; Department of ECE, Sun Yat-sen University, Guangzhou, GD 510275, China","2013 IEEE/CIC International Conference on Communications in China (ICCC)","","2013","","","44","49","In this paper, we apply q-ary LDPC codes to differential modulation systems, and study the design and performance of the resultant coded modulation systems. Two low-complexity joint detection-decoding methods for noncoherent demodulation are proposed, in which the hard-message-passing strategy is used for a joint factor graph. The first method is based on the joint detection-decoding algorithm introduced in [1]; and the second one is developed by combining trellis-based differential detection and the hard-decision-based decoding of nonbinary LDPC codes. The Max-Log-MAP algorithm with soft-in hard-out is used for the differential detection. Simulation results show that both methods offer acceptable performances with greatly reduced complexities.","2377-8644","978-1-4799-1033","10.1109/ICCChina.2013.6671087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671087","","Iterative decoding;Decoding;Demodulation;Complexity theory;Joints","computational complexity;demodulation;graph theory;maximum likelihood decoding;message passing;parity check codes","nonbinary LDPC-coded differential modulation;reduced-complexity decoding;q-ary LDPC codes;resultant coded modulation systems;low-complexity joint detection-decoding methods;noncoherent demodulation;hard-message-passing strategy;joint factor graph;trellis-based differential detection;hard-decision-based decoding;max-log-MAP algorithm","","1","","17","","","","","IEEE","IEEE Conferences"
"Flexible Complexity Fast Decoding of Multiplexed Alamouti Codes in Space-Time-Polarization Systems","L. M. Davis; S. Srinivasan; S. Sirianunpiboon","NA; NA; NA","2010 IEEE 71st Vehicular Technology Conference","","2010","","","1","5","Space-time codes built with multiplexed Alamouti components provide multiplexing as well as diversity gain. Their orthogonal structure also leads to simpler decoding algorithms. We consider a 4 × 2 system with multiplexed Alamouti codes. An exhaustive search maximum likelihood (ML) decoding for a 4×2 system is of order M4, for a modulation scheme with constellation size M. For multiplexed orthogonal designs, an exact fast ML decoding algorithm has recently been reported whose complexity order is M2for a 4 × 2 system with QAM constellations. Nevertheless, the quadratic complexity of this fast ML algorithm may still be infeasible in practice for large constellations (e.g. M ≥ 64 QAM). In this paper, we present a method for designing low complexity sub-optimal decoders based on a combination of search based ML decoding and linear decoding. Our formulation facilitates a direct investigation of the trade-off between performance and complexity. The complexity of our hybrid decoder is flexible and it can be fixed based on the desired performance for a hardware implementation. Although extendable to more general multiplexed Alamouti systems, we focus here on a 4 × 2 space-time-polarization system comprising of two dual-polarized transmit antennas and one dual-polarized receive antenna.","1550-2252;1550-2252","978-1-4244-2518-1978-1-4244-2519","10.1109/VETECS.2010.5493714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5493714","","Maximum likelihood decoding;Receiving antennas;Quadrature amplitude modulation;Transmitting antennas;Bayesian methods;Vectors;Detectors;Interference cancellation;Space technology;Diversity methods","code division multiplexing;maximum likelihood decoding;quadrature amplitude modulation;receiving antennas;space-time codes;transmitting antennas","decoding complexity;Alamouti codes multiplexing;space time polarization systems;space time codes;maximum likelihood decoding;ML decoding;QAM;quadratic complexity;suboptimal decoders;linear decoding;transmit antennas;receive antenna","","3","","8","","","","","IEEE","IEEE Conferences"
"X-Codes: A Low Complexity Full-Rate High-Diversity Achieving Precoder for TDD MIMO Systems","S. K. Mohammed; E. Viterbo; Y. Hong; A. Chockalingam","NA; NA; NA; NA","2010 IEEE International Conference on Communications","","2010","","","1","5","We consider a time division duplex multiple-input multiple-output (nt× nrMIMO). Using channel state information (CSI) at the transmitter, singular value decomposition (SVD) of the channel matrix is performed. This transforms the MIMO channel into parallel subchannels, but has a low overall diversity order. Hence, we propose X-Codes which achieve a higher diversity order by pairing the subchannels, prior to SVD preceding. In particular, each pair of information symbols is encoded by a fixed 2 × 2 real rotation matrix. X-Codes can be decoded using nrvery low complexity two-dimensional real sphere decoders. Error probability analysis for X-Codes enables us to choose the optimal pairing and the optimal rotation angle for each pair. Finally, we show that our new scheme outperforms other low complexity precoding schemes.","1938-1883;1550-3607;1550-3607","978-1-4244-6404-3978-1-4244-6402-9978-1-4244-6403","10.1109/ICC.2010.5502216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5502216","","MIMO;Maximum likelihood decoding;Channel state information;Transmitters;Singular value decomposition;Propagation losses;Transmitting antennas;Receiving antennas;Communications Society;Matrix decomposition","computational complexity;diversity reception;error statistics;matrix algebra;MIMO communication;precoding;singular value decomposition;time division multiplexing","X-codes;TDD MIMO systems;channel state information;CSI;singular value decomposition;SVD;channel matrix;rotation matrix;error probability analysis;optimal rotation angle;complexity precoding schemes;time division duplex multiple-input multiple-output systems","","","","13","","","","","IEEE","IEEE Conferences"
"A low complexity architecture for video coding with overlapped block motion compensation","Y. Liao; A. Leontaris; A. M. Tourapis","ECE Department, University of California, Santa Barbara, USA; Image Technology Research, Dolby Laboratories, Inc., USA; Image Technology Research, Dolby Laboratories, Inc., USA","2010 IEEE International Conference on Image Processing","","2010","","","2041","2044","Video coding standards, such as H.264/MPEG-4 AVC, have adopted the hybrid codec paradigm that consists of intra and inter prediction, followed by transform and quantization of the prediction residual. Inter prediction uses block-based motion compensation (BMC) to predict samples in the current picture. BMC, however, suffers from blocking artifacts and discontinuities. Along with deblocking, overlapped block motion compensation (OBMC) has been proposed in the past, primarily to suppress blocking artifacts. However, issues that include decoding complexity and implementation have been impediments to practical applications of OBMC. Here<sup>1</sup>, we propose a new decoder- and encoder-friendly architecture for inter prediction with OBMC. Furthermore, we show preliminary experimental results that point to coding efficiency gains over the BMC architecture of H.264/MPEG-4 AVC.","2381-8549;1522-4880;1522-4880","978-1-4244-7994-8978-1-4244-7992-4978-1-4244-7993","10.1109/ICIP.2010.5649822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5649822","Low complexity;overlapped block motion compensation;H.264/MPEG-4 Part 10 AVC;motion estimation","Decision support systems;Radio frequency;Image processing;Conferences","computational complexity;decoding;motion compensation;video coding","low complexity architecture;overlapped block motion compensation;video coding standards;H.264/MPEG-4 AVC;hybrid codec paradigm;intraprediction;interprediction;prediction residual;block-based motion compensation;blocking artifacts;decoding complexity","","","","16","","","","","IEEE","IEEE Conferences"
"Resource Optimized Distributed Source Coding for Complexity Constrained Data Gathering Wireless Sensor Networks","H. Arjmandi; F. Lahouti","Wireless Multimedia Communications Laboratory, Center of Excellence on Applied Electromagnetic Systems, School of Electrical & Computer Engineering, University of Tehran, Tehran, Iran; Wireless Multimedia Communications Laboratory, Center of Excellence on Applied Electromagnetic Systems, School of Electrical & Computer Engineering, University of Tehran, Tehran, Iran","IEEE Sensors Journal","","2011","11","9","2094","2101","This paper addresses the problem of efficient data gathering based on distributed source coding (DSC) in wireless sensor networks (WSNs) with a complexity constrained data gathering node (DGN). A particular scenario of interest is a cluster of low complexity sensor nodes among which, one node is selected as the cluster head (CH) or the DGN. Utilizing DSC allows for reducing the required rate of communications by exploiting the dependency between the nodes observations in a distributed manner. We consider a DSC-based rate allocation structure, which takes into account the CH (DGN) memory and computational constraints. Specifically, this is accomplished, respectively, by limiting the number of nodes whose data may be stored at the CH and exploited during decoding, and the number of nodes that can be jointly (de)compressed using DSC. Based on this structure, we investigate two WSN resource optimization problems aiming at: (i) minimizing the total network cost and (ii) maximizing the network lifetime. To these ends, optimal dynamic programming solutions based on a trellis structure are proposed that incur substantially smaller computational complexity in comparison to an exhaustive search. Also, a suboptimal yet high performance solution is presented whose complexity grows in polynomial order with the number of network nodes. Numerical results demonstrate that the proposed rate allocation structure and solutions, even with limited complexity, allow for exploiting most of the available dependency and hence the achievable compression gain.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2011.2109947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705538","Data gathering;distributed source coding;resource optimization;slepian-wolf (SW) coding;wireless sensor networks","Wireless sensor networks;Silicon;Memory management;Computational complexity;Optimization;Resource management","source coding;wireless sensor networks","data gathering;distributed source coding;wireless sensor networks;sensor nodes;cluster head;memory;computational constraints;network lifetime;computational complexity","","8","","22","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Receiver for Large-MIMO Space-Time Coded Systems","C. Knievel; M. Noemm; P. A. Hoeher","NA; NA; NA","2011 IEEE Vehicular Technology Conference (VTC Fall)","","2011","","","1","5","The substantial gains promised by multi-antenna systems require accurate channel knowledge, at least at the receiver side. Furthermore, the complexity of most receivers becomes unfeasible when the number of transmit antennas gets large (known as ""large-MIMO"" systems). In this paper, channel estimation by means of cooperative particle swarm optimization (CPSO) and factor-graph based data detection with belief propagation is presented. The overall complexity of the receiver grows linearly with respect to the number of transmit and receive antennas. Numerical results validate the performance of the CPSO channel estimation integrated within a graph-based iterative receiver, including a 24×16 MIMO system.","1090-3038;1090-3038;1090-3038","978-1-4244-8327-3978-1-4244-8328-0978-1-4244-8325-9978-1-4244-8326","10.1109/VETECF.2011.6093090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093090","","Channel estimation;Complexity theory;MIMO;Receiving antennas;Particle swarm optimization;Detectors","antenna arrays;belief networks;graph theory;MIMO communication;particle swarm optimisation;radio receivers;receiving antennas;space-time codes;transmitting antennas","low-complexity receiver;large-MIMO space-time coded systems;multi-antenna systems;transmit antennas;cooperative particle swarm optimization;factor-graph based data detection;belief propagation;receive antennas;channel estimation;graph-based iterative receiver","","19","","15","","","","","IEEE","IEEE Conferences"
"A Low Complexity Sparse Code Multiple Access Detector Based on Stochastic Computing","K. Han; J. Hu; J. Chen; H. Lu","National Key Laboratory on Communication, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communication, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communication, University of Electronic Science and Technology of China, Chengdu, China; National Key Laboratory on Communication, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Circuits and Systems I: Regular Papers","","2018","65","2","769","782","Sparse code multiple access (SCMA) is a promising multiple access technology candidate for the next-generation communication system, which can dramatically improve spectral efficiency. However, the major challenge of SCMA is the very high detection complexity. Stochastic computing is a new number representation, which can carry out complex computations with very simple logics. In this paper, we extend the application of stochastic computing to SCMA detection and propose a low complexity stochastic SCMA detector. We also design three novel stochastic logic architectures: a new low hardware cost bit stream generation architecture, a low hardware cost stochastic function node update architecture and a fast converging stochastic variable node update architecture. Analysis and simulation results show that the proposed stochastic SCMA detector saves 69% complexity compared with the traditional SCMA detectors with a comparable bit error rate performance. The synthesis results with SIMC 65-nm CMOS technology show that the proposed stochastic SCMA detector achieves 640 Mbps total system throughput with only 1.45-mm2cell area.","1549-8328;1558-0806","","10.1109/TCSI.2017.2722692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983434","Stochastic computing;sparse code multiple access","Detectors;Complexity theory;Hardware;Computer architecture;Message passing;Stochastic processes;Communication systems","CMOS integrated circuits;error statistics;multi-access systems;multiuser detection;next generation networks;stochastic processes","low complexity sparse code multiple access detector;stochastic computing;next-generation communication system;high detection complexity;complex computations;SCMA detection;low complexity stochastic SCMA detector;low hardware cost bit stream generation architecture;stochastic function node update architecture;stochastic variable node update architecture;traditional SCMA detectors;CMOS technology;stochastic logic architectures","","3","","19","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity layered BP-based detection and decoding for a NB-LDPC coded MIMO system","A. Haroun; C. A. Nour; M. Arzel; C. Jego","Institut Mines-Télécom / Télécom Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Université Européenne de Bretagne, France; Institut Mines-Télécom / Télécom Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Université Européenne de Bretagne, France; Institut Mines-Télécom / Télécom Bretagne, CNRS Lab-STICC, UMR 3192, Brest. Université Européenne de Bretagne, France; IPB / Enseirb-Matmeca, CNRS IMS, UMR 5218, Bordeaux. Université de Bordeaux, France","2014 IEEE International Conference on Communications (ICC)","","2014","","","5107","5112","In this paper, the combination of a low-complexity Multiple-input Multiple-output based on belief propagation (MIMO-BP) detector with a Non-Binary Low-Density Parity-Check (NB-LDPC) decoder is investigated. Such detection and decoding algorithms can enable an equivalent representation based on a larger Joint Factor Graph (JFG). Shuffle schedule can therefore be used jointly and simultaneously on the detector and the decoder. Actions are undertaken at the detector, decoder and the iterative receiver levels in order to reduce overall complexity. Indeed, applying the proposed low-complexity BP-based detection greatly reduces the number of operations per iteration (divided by ten), with a negligible performance penalty. EXtrinsic Information Transfer (EXIT) charts enable to analyze the convergence behaviour of the proposed iterative receiver. This analysis is used to find the best set of parameters enabling a detection-decoding process with a performance closest to the full-complexity system.","1550-3607;1938-1883","978-1-4799-2003","10.1109/ICC.2014.6884131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884131","","Detectors;Decoding;Iterative decoding;Silicon;MIMO;Reliability","decoding;graph theory;iterative methods;MIMO communication;parity check codes;signal detection","low-complexity layered BP-based detection;NB-LDPC coded MIMO system;low-complexity multiple-input multiple-output system;belief propagation detector;nonbinary low-density parity-check decoder;joint factor graph;JFG;iterative receiver levels;extrinsic information transfer charts;performance penalty;EXIT charts;convergence behaviour;detection-decoding process;full-complexity system","","3","","25","","","","","IEEE","IEEE Conferences"
"A Low Complexity Equalization Method for Cooperative Communication Systems Based on Distributed Frequency-Domain Linear Convolutive Space-Frequency Codes","J. Xiao; Y. Jiang; X. You","NA; NA; NA","2011 IEEE 73rd Vehicular Technology Conference (VTC Spring)","","2011","","","1","5","In this paper, we consider the cooperative communication system based on distributed frequency-domain linear convolutive space-frequency codes (FLC-SFC) with multiple carrier frequency offsets (CFOs) when the channels from relay nodes to destination node are flat fading. Through the mathematical derivation, the cooperative system model is simplified. Then, the equivalent banded system model is obtained and the related analysis shows the banded property of the equivalent channel matrix. Furthermore, a banded block minimum mean square error (MMSE) equalization method applying LDLH matrix decomposition is proposed for the banded system model. Besides, a special class of mask matrix is utilized to realize the banded operation. Compared with the traditional MMSE equalization method, the proposed equalization method has a relatively low computational complexity with satisfactory system performance.","1550-2252;1550-2252;1550-2252","978-1-4244-8331-0978-1-4244-8332-7978-1-4244-8329-7978-1-4244-8330","10.1109/VETECS.2011.5956742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5956742","","Matrix decomposition;Relays;Mathematical model;Communication systems;Bit error rate;Frequency domain analysis;Computational complexity","channel coding;computational complexity;convolutional codes;cooperative communication;equalisers;fading channels;frequency-domain analysis;least mean squares methods;linear codes;matrix decomposition","low complexity equalization method;cooperative communication systems;distributed frequency-domain linear convolutive space-frequency codes;FLC-SFC;multiple carrier frequency offsets;relay nodes;destination node;flat fading channel;equivalent banded system model;equivalent channel matrix;banded block minimum mean square error equalization method;LDLH matrix decomposition;mask matrix;MMSE equalization method;low computational complexity","","","","7","","","","","IEEE","IEEE Conferences"
"A low complexity pre-coding scheme in next generation mobile communication system","X. Wang; T. Jiang; J. Wu; Y. Rui","NCRL, Southeast University, China; NCRL, Southeast University, China; NCRL, Southeast University, China; SARI, Chinese Academy of Sciences","2014 19th International Conference on Digital Signal Processing","","2014","","","893","897","The distributed MIMO (D-MIMO) antenna technique is the most promising architecture for the next generation (5G) of mobile communication system. In this paper, a low complexity downlink group pre-coding scheme is proposed for multimedia broadcast multicast services in D-MIMO system with antenna selection in remote antenna units (RAUs). The ideal channel state information (CSI) is assumed to know at both transmitters and receivers. In order to evaluate the performance of the group pre-coding, the average symbol error rate (SER) of the BD-ZF (Block Diagonalization - Zero forcing) and BD-MMSE (Block Diagonalization - Minimum Mean Square Error) algorithm with group pre-coding and conventional pre-coding in D-MIMO are represented through Monte Carlo simulation. The simulation results and the complexity analysis show that the proposed pre-coding scheme achieves comparable SER performance as the conventional scheme with much lower complexity.","2165-3577;1546-1874","978-1-4799-4612","10.1109/ICDSP.2014.6900797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900797","Distributed MIMO antennas system;MBMS;group pre-coding;lower complexity","Transmitting antennas;MIMO;Complexity theory;Digital signal processing;Fading;Signal to noise ratio","mean square error methods;MIMO communication;mobile communication;multimedia communication;next generation networks;precoding;radio links;radio receivers;radio transmitters","low complexity precoding scheme;next generation mobile communication system;distributed MIMO;D-MIMO antenna technique;downlink group precoding scheme;multimedia broadcast multicast services;D-MIMO system;antenna selection;remote antenna units;RAU;channel state information;CSI;transmitters;receivers;symbol error rate;SER;block diagonalization - zero forcing;BD-ZF;BD-MMSE;block diagonalization - minimum mean square error algorithm","","","","13","","","","","IEEE","IEEE Conferences"
"Linear computational complexity decoding for semi orthogonal full rate space time codes","A. Laufer; Y. Bar-Ness","The Center of Wireless Communications and Signal Processing Research, ECE, New Jersey Institute of Technology, Newark, NJ 07102, USA; The Center of Wireless Communications and Signal Processing Research, ECE, New Jersey Institute of Technology, Newark, NJ 07102, USA","2011 IEEE Wireless Communications and Networking Conference","","2011","","","1534","1539","Communication with low complexity decoding is an important attribute for many applications. From low cost, distributed sensors arrays with desirably long operative life to ultra smart handheld battery operated devices, low complexity decoding results in better power consumption and simpler hardware implementation. In this paper a linear decoding complexity is applied and analyzed to the semi-orthogonal space time codes which are rate 1 codes originated from a regular orthogonal space time codes. The proposed transmission and decoding schemes are very appealing in the sense that they improve the code rate of the orthogonal codes family, yet, maintain its simple, symbol by symbol decoding with no filter calculation complexity overhead. In addition, a performance boosting modifications are suggested and investigated for the basic transmission and decoding schemes under different system settings such as the availability of limited feedback and multiple receiving antennas.","1558-2612;1525-3511;1525-3511","978-1-61284-254-7978-1-61284-255-4978-1-61284-253","10.1109/WCNC.2011.5779358","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5779358","Space time codes;Full rate;Diversity;Decoding Complexity","Signal to noise ratio;Complexity theory;Maximum likelihood decoding;Receiving antennas;Transmitters","antennas;computational complexity;orthogonal codes;space-time codes;wireless sensor networks","linear computational complexity decoding;semi orthogonal full rate space time code;smart handheld battery operated device;performance boosting modification;limited feedback;multiple receiving antenna;distributed sensors array","","","","5","","","","","IEEE","IEEE Conferences"
"Adaptive fast intra mode decision of depth map coding by Low Complexity RD-Cost in 3D-HEVC","H. Zhang; C. Fu; W. Su; S. Tsang; Y. Chan","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong","2015 IEEE International Conference on Digital Signal Processing (DSP)","","2015","","","487","491","The coding efficiency of 3D-HEVC can be improved significantly by adopting more intra modes in depth map coding, including up to 35 conventional HEVC intra prediction modes and 4 depth modelling modes (DMMs). However, traversing these modes dramatically results in unendurable high complexity. In this paper, we propose an efficient intra mode decision algorithm by taking advantage of the Low Complexity Rate-Distortion Cost information in the rough mode decision. Only one conventional intra mode and several most probable modes are then selected for most cases adaptively based on the threshold derived from the prior probability. Experimental results show that the proposed algorithm provides about 34% saving in terms of the total depth coding time with little drop of the coding performance compared with the state-of-the-art algorithm.","2165-3577;1546-1874","978-1-4799-8058-1978-1-4799-8057","10.1109/ICDSP.2015.7251920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7251920","multi-view videos plus depth;3D-HEVC;intra prediction;rough mode decision;depth map coding","Encoding;Complexity theory;Videos;Three-dimensional displays;Prediction algorithms;Video coding;Image coding","probability;rate distortion theory;video coding","intra mode decision;depth map coding;low complexity rate-distortion cost information;low complexity RD;3D HEVC;coding efficiency;intraprediction modes;depth modelling modes;DMM;intramode decision algorithm;rough mode decision;depth coding time","","7","","19","","","","","IEEE","IEEE Conferences"
"The Complexity of Network Coding With Two Unit-Rate Multicast Sessions","W. Song; K. Cai; R. Feng; C. Yuen","LMAM, School of Mathematical Sciences, Peking University, China; Institute of Network Coding, The Chinese University of Hong Kong, Shatin, Hong Kong; LMAM, School of Mathematical Sciences, Peking University, China; Singapore University of Technology and Design, Singapore","IEEE Transactions on Information Theory","","2013","59","9","5692","5707","The encoding complexity of network coding for single multicast networks has been intensively studied from several aspects: e.g., the time complexity, the required number of encoding links, and the required field size for a linear code solution. However, these issues as well as the solvability are less understood for networks with multiple multicast sessions. Recently, Wang and Shroff showed that the solvability of networks with two unit-rate multicast sessions (2-URMS) can be decided in polynomial time . In this paper, we prove that for the 2-URMS networks: 1) the solvability can be determined with time O(|E|); 2) a solution can be constructed with time O(|E|); 3) an optimal solution can be obtained in polynomial time; 4) the number of encoding links required to achieve a solution is upper-bounded by max{3,2N - 2}; and 5) the field size required to achieve a linear solution is upper-bounded by max{2, ⌊√{2N-7/4}+1/2⌋}, where |E| is the number of links and N is the number of sinks of the underlying network. Both bounds are shown to be tight.","0018-9448;1557-9654","","10.1109/TIT.2013.2262492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515150","Encoding complexity;network coding;region decomposition","Network coding;Vectors;Time complexity;Linear codes;Polynomials","communication complexity;linear codes;multicast communication;network coding","network coding;unit-rate multicast session;encoding complexity;single multicast network;time complexity;encoding links;linear code;multiple multicast session;2-URMS network","","3","","29","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity LP Decoding of Nonbinary Linear Codes","M. Punekar; P. O. Vontobel; M. F. Flanagan","Department of Electrical and Computer Engineering, Texas A&M University, PO Box 23874, Education City, Doha, Qatar; Hewlett—Packard Laboratories, 1501 Page Mill Road, Palo Alto, CA 94304, USA; School of Electrical, Electronic and Communications Engineering, University College Dublin, Ireland","IEEE Transactions on Communications","","2013","61","8","3073","3085","Linear Programming (LP) decoding of Low-Density Parity-Check (LDPC) codes has attracted much attention in the research community in the past few years. LP decoding has been derived for binary and nonbinary linear codes. However, the most important problem with LP decoding for both binary and nonbinary linear codes is that the complexity of standard LP solvers such as the simplex algorithm remains prohibitively large for codes of moderate to large block length. To address this problem, two low-complexity LP (LCLP) decoding algorithms for binary linear codes have been proposed by Vontobel and Koetter, henceforth called the basic LCLP decoding algorithm and the subgradient LCLP decoding algorithm. In this paper, we generalize these LCLP decoding algorithms to nonbinary linear codes. The computational complexity per iteration of the proposed nonbinary LCLP decoding algorithms scales linearly with the block length of the code. A modified BCJR algorithm for efficient check-node calculations in the nonbinary basic LCLP decoding algorithm is also proposed, which has complexity linear in the check node degree. Several simulation results are presented for nonbinary LDPC codes defined over Z4, GF(4), and GF(8) using quaternary phase-shift keying and 8-phase-shift keying, respectively, over the AWGN channel. It is shown that for some group-structured LDPC codes, the error-correcting performance of the nonbinary LCLP decoding algorithms is similar to or better than that of the min-sum decoding algorithm.","0090-6778;1558-0857","","10.1109/TCOMM.2013.070213.120801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559989","Linear programming decoding;nonbinary codes;LDPC codes;coordinate-ascent algorithm;subgradient algorithm","Iterative decoding;Decoding;Linear code;Complexity theory;Algorithm design and analysis;Vectors","AWGN channels;computational complexity;decoding;linear codes;linear programming;parity check codes;phase shift keying","low complexity LP decoding;nonbinary linear codes;linear programming decoding;basic LCLP decoding algorithm;subgradient LCLP decoding algorithm;computational complexity;modified BCJR algorithm;check node calculation;quaternary phase shift keying;AWGN channel;low density parity check code","","6","","31","","","","","IEEE","IEEE Journals & Magazines"
"A Low Complexity Decoder for Quasi-Orthogonal Space Time Block Codes","S. J. Alabed; J. M. Paredes; A. B. Gershman","Communication Systems Group, Darmstadt University of Technology, Merckstr. 25, D-64283 Darmstadt, Germany; Communication Systems Group, Darmstadt University of Technology, Merckstr. 25, D-64283 Darmstadt, Germany; Communication Systems Group, Darmstadt University of Technology, Merckstr. 25, D-64283 Darmstadt, Germany","IEEE Transactions on Wireless Communications","","2011","10","3","988","994","In this paper, a low-complexity suboptimal decoder for coherent and non-coherent quasi-orthogonal space time block codes with three and four transmit antennas is proposed. Our decoder enjoys a nearly linear complexity and approximately the same performance as the optimal maximum-likelihood (ML) decoder. Simulations show the advantages of the proposed decoder with respect to several other popular approaches to the coherent and non-coherent decoding.","1536-1276;1558-2248","","10.1109/TWC.2011.010411.101263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5692894","Multiple-input multiple-output (MIMO) systems;quasi-orthogonal space time block codes;differential space-time coding (DSTC);low decoding complexity","Complexity theory;Signal to noise ratio;Maximum likelihood decoding;Block codes;Transmitting antennas;Detectors","antenna arrays;computational complexity;maximum likelihood decoding;orthogonal codes;space-time block codes;transmitting antennas","low-complexity decoder;quasiorthogonal space time block codes;transmit antennas;linear complexity;optimal maximum-likelihood decoder;ML decoder;coherent decoding;noncoherent decoding","","11","","22","","","","","IEEE","IEEE Journals & Magazines"
"Efficient Motion and Disparity Estimation Optimization for Low Complexity Multiview Video Coding","Z. Pan; Y. Zhang; S. Kwong","Jiangsu Engineering Center of Network Monitoring, Nanjing University of Information Science and Technology, Nanjing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Broadcasting","","2015","61","2","166","176","The use of variable block-size motion estimation (ME), disparity estimation (DE), and multiple reference frames selection aims to improve the coding efficiency of multiview video coding (MVC), however, this is at the cost of high computational complexity of these advanced coding techniques, which are not suitable for real-time video broadcasting applications. In this paper, we propose an efficient ME and DE algorithm for reducing the computational complexity of MVC. Firstly, according to the characteristics of the coded block pattern and rate distortion (RD) cost, an early DIRECT mode decision algorithm is proposed. Then, based on the characteristics of the initial search point in the ME/DE process and the observation that the best point is center-biased, an early ME/DE termination strategy is proposed. If the ME/DE early termination is not satisfied, the ME/DE search window will be reduced by applying the optimal theory. At last, two block matching search strategies are proposed to predict the best point for the ME/DE. Experimental results show that the proposed algorithm can achieve 50.05% to 77.61%, 64.83% on average encoding time saving. Meanwhile, the RD performance degradation is negligible. Especially, the proposed algorithm can be applied to not only the odd views but also the even views.","0018-9316;1557-9611","","10.1109/TBC.2015.2419824","National Natural Science Foundation of China; Project through the Priority Academic Program Development of Jiangsu Higher Education Institutions; Startup Foundation for Introducing Talent of NUIST; Shenzhen Overseas High-Caliber Personnel Innovation and Entrepreneurship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095565","Early termination;optimal stopping theory;motion estimation (ME);disparity estimation (DE);multiview video coding (MVC);Early termination;optimal stopping theory;motion estimation (ME);disparity estimation (DE);multiview video coding (MVC)","Encoding;Computational complexity;Video coding;Probability;Prediction algorithms;Correlation","distortion;motion estimation;video coding","disparity estimation optimization;motion estimation optimization;low complexity multiview video coding;variable block-size motion estimation;disparity estimation;multiple reference frames selection;multiview video coding;computational complexity;advanced coding techniques;real-time video broadcasting;rate distortion cost;coded block pattern;DIRECT mode decision algorithm;ME/DE process;ME/DE termination;ME/DE search window;RD performance degradation;encoding time saving","","245","","28","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Prediction Model for Coding Remote-Sensing Data with Regression Wavelet Analysis","N. Amrani; J. Serra-Sagristà; M. Marcellin","NA; NA; NA","2017 Data Compression Conference (DCC)","","2017","","","112","121","Fast and efficient coding techniques are being increasingly required to meet the complexity restrictions of on-board satellite compression. The recently proposed Regression Wavelet Analysis (RWA) has proven to be highly effective as a spectral transform for coding remote sensing images. The algorithm is based on a pyramidal prediction, using multiple regression analysis, to tackle residual data dependencies in the wavelet domain. RWA combines low complexity and reversibility and has demonstrated competitive performance for lossless and progressive lossy-to-lossless compression superior to the state-of-the-art predictive-based CCSDS-123.0 and the widely used transform-based principal component analysis (PCA). In this paper we introduce a very low-complexity RWA approach, where prediction is based on only a few components, while the performance is maintained. When RWA computational complexity is taken to an extremely low level, careful model selection is necessary. Contrary to expensive selection procedures, we propose a simple and efficient strategy called neighbor selection for using small regression models. On a set of well-known and representative hyperspectral images, these small models maintain the excellent coding performance of RWA, while reducing the computational cost by about 90%.","2375-0359","978-1-5090-6721-3978-1-5090-6722","10.1109/DCC.2017.61","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921906","Compression;Remote Sensing;regression","Computational modeling;Discrete wavelet transforms;Wavelet analysis;Predictive models;Data models;Computational efficiency","computational complexity;geophysical image processing;hyperspectral imaging;image coding;principal component analysis;regression analysis;remote sensing;wavelet transforms","low complexity prediction model;regression wavelet analysis;remote-sensing data coding;complexity restrictions;on-board satellite compression;spectral transform;remote sensing image coding;pyramidal prediction;multiple regression analysis;residual data dependencies;wavelet domain;progressive lossy-to-lossless compression;predictive-based CCSDS-123.0;transform-based principal component analysis;low-complexity RWA approach;RWA computational complexity;model selection;computational cost reduction;neighbor selection;representative hyperspectral images","","2","","12","","","","","IEEE","IEEE Conferences"
"MDS codes with low repair complexity for distributed storage networks","H. Xie; Z. Yan","Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015, USA; Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015, USA","2013 22nd Wireless and Optical Communication Conference","","2013","","","384","387","In this paper, we propose two new constructions of maximum distance separable (MDS) codes with low repair complexity for distributed storage networks. For both constructions, the encoded symbols are obtained by first treating the message vector as a linearized polynomial and then evaluating it at carefully chosen points. Compared to traditional MDS codes, data repair for our codes does not have to decode the entire original message, but only forms linear combinations of available encoded symbols. This linear repair complexity is attractive for applications with a large amount of data repair.","2379-1268;2379-1276","978-1-4673-5699-2978-1-4673-5697","10.1109/WOCC.2013.6676397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676397","Distributed storage;MDS codes;repair complexity","Maintenance engineering;Polynomials;Complexity theory;Vectors;Block codes;Bandwidth","decoding;encoding;polynomials;storage area networks","linear repair complexity;available encoded symbols;linear combinations;linearized polynomial;message vector;distributed storage networks;low repair complexity;MDS codes;maximum distance separable codes","","1","","12","","","","","IEEE","IEEE Conferences"
"Low-complexity wavelet-based scalable image & video coding for home-use surveillance","M. J. H. Loomans; C. J. Koeleman; P. H. N. De With","VDG Security B.V., Electrical Engineering Department, Eindhoven University of Technology; VDG Security B.V.; CycloMedia Technology B.V., Electrical Engineering Department, Eindhoven University of Technology","IEEE Transactions on Consumer Electronics","","2011","57","2","507","515","We study scalable image and video coding for the surveillance of rooms and personal environments based on inexpensive cameras and portable devices. The scalability is achieved through a multi-level 2D dyadic wavelet decomposition featuring an accurate low-cost integer wavelet implementation with lifting. As our primary contribution, we present a modification to the SPECK wavelet coefficient encoding algorithm to significantly improve the efficiency of an embedded system implementation. The modification consists of storing the significance of all quadtree nodes in a buffer, where each node comprises several coefficients. This buffer is then used to efficiently construct the code with minimal and direct memory access. Our approach allows efficient parallel implementation on multi-core computer systems and gives a substantial reduction of memory access and thus power consumption. We report experimental results, showing an approximate gain factor of 1,000 in execution time compared to a straightforward SPECK implementation, when combined with code optimization on a common digital signal processor. This translates to 75 full color 4CIF 4:2:0 encoding cycles per second, clearly demonstrating the realtime capabilities of the proposed modification.","0098-3063;1558-4127","","10.1109/TCE.2011.5955186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5955186","Scalable;Image Compression;Wavelet Transforms;Embedded Systems;Image Coding","Encoding;Sorting;Wavelet coefficients;Image coding;Correlation;Finite impulse response filter","embedded systems;file organisation;image colour analysis;video coding;video surveillance;wavelet transforms","video coding;image coding;home-use surveillance;multilevel 2D dyadic wavelet decomposition;low-cost integer wavelet transform;SPECK wavelet coefficient encoding algorithm;embedded system;direct memory access;multicore computer systems;power consumption;digital signal processor","","7","","11","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-Complexity Min-Sum Algorithm for Decoding LDPC Codes With Low Error-Floor","F. Angarita; J. Valls; V. Almenar; V. Torres","Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universitat Politècnica de València, Gandia, Spain; Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universitat Politècnica de València, Gandia, Spain; Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universitat Politècnica de València, Gandia, Spain; Instituto de Telecomunicaciones y Aplicaciones Multimedia, Universitat Politècnica de València, Gandia, Spain","IEEE Transactions on Circuits and Systems I: Regular Papers","","2014","61","7","2150","2158","This paper proposes a low-complexity min-sum algorithm for decoding low-density parity-check codes. It is an improved version of the single-minimum algorithm where the two-minimum calculation is replaced by one minimum calculation and a second minimum emulation. In the proposed one, variable correction factors that depend on the iteration number are introduced and the second minimum emulation is simplified, reducing by this way the decoder complexity. This proposal improves the performance of the single-minimum algorithm, approaching to the normalized min-sum performance in the water-fall region. Also, the error-floor region is analyzed for the code of the IEEE 802.3an standard showing that the trapping sets are decoded due to a slow down of the convergence of the algorithm. An error-floor free operation below BER=10-15 is shown for this code by means of a field-programmable gate array (FPGA)-based hardware emulator. A layered decoder is implemented in a 90-nm CMOS technology achieving 12.8 Gbps with an area of 3.84 mm2.","1549-8328;1558-0806","","10.1109/TCSI.2014.2304660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742731","Error correction codes (ECC);error-floor;low-density parity-check (LDPC) codes;VLSI","Parity check codes;Decoding;Algorithm design and analysis;Complexity theory;IEEE 802.3 Standards;EPON;Quantization (signal)","computational complexity;error correction codes;error statistics;field programmable gate arrays;parity check codes","LDPC decoding;low error-floor;low-complexity min-sum algorithm;low-density parity-check codes decoding;decoder complexity;single-minimum algorithm;normalized min-sum performance;performance error-floor region;IEEE 802.3an standard;error-floor free operation;field-programmable gate array-based hardware emulator","","19","","34","","","","","IEEE","IEEE Journals & Magazines"
"Complexity scalability design in coding of the adaptive codebook for ITU-T G.729 speech coder","F. Chen; G. Chen; Y. Jou","Department of Computer Science and Information, Engineering, Southern Taiwan University, Tainan, Taiwan; Department of Computer Science and Information, Engineering, Southern Taiwan University, Tainan, Taiwan; Department of Electrical Engineering, R. O. C. Military Academy, Kaohsiung, Taiwan","2011 8th International Conference on Information, Communications & Signal Processing","","2011","","","1","4","Using the adaptive codebook and stochastic codebook, the code-excited linear prediction (CELP) speech coders can achieve a good speech quality at low bit rate. When the complexity reduction of the stochastic codebook search reaches a bottleneck, the only way to reduce the complexity of the speech codec would be simplifying the computation of the adaptive codebook. This paper introduces and describes the complexity scalability design for the adaptive codebook search of the G.729 speech coder. Two schemes, the search-range reduction and the dimension-reduced method, for complexity reduction will be discussed. Simulation results show that the proposed approaches can effectively reduce the computational complexity of the close-loop pitch search with imperceptible degradation of the speech quality.","","978-1-4577-0031-6978-1-4577-0029-3978-1-4577-0030","10.1109/ICICS.2011.6174270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6174270","speech coder;adaptive codebook;pitch;close-loop search;complexity reduction;complexity scalability","Speech;Speech coding;Complexity theory;Neodymium;Scalability;Speech processing","computational complexity;speech codecs;speech coding;vocoders","complexity scalability design;adaptive codebook coding;ITU-T G.729 speech coder;stochastic codebook;code-excited linear prediction speech coders;CELP speech coders;speech quality;speech codec complexity;search-range reduction;dimension-reduced method;computational complexity reduction;close-loop pitch search","","1","","18","","","","","IEEE","IEEE Conferences"
"Coded Caching with Low Per-Request Complexity","B. Farbman; Y. Cassut; A. Sprintson","Viterbi Dept. of Electrical Engineering, Technion – Israel Institute of Technology; Viterbi Dept. of Electrical Engineering, Technion – Israel Institute of Technology; Dept. of Electrical and Computer Engineering, Texas AM University","2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2018","","","723","732","The massive growth in high-rate streams (mostly video) over the Internet warranted the establishment of content distribution networks (CDNs), which bring the content closer to the consumer and hence reduce delays and congestion in the network. In this paper we address the setup of multiple Internet service providers (ISPs) sharing a federated cache. When multiple ISPs with different demands share a cache, storage cost can be reduced when the cache is coded. We address the problem of constructing the coded cache to fulfill all demands from all ISPs, where the constructions simultaneously optimize the cache's three main operational costs: storage, communication, and computation (to encode and decode requested objects). When only the storage needs to be minimized, the code-design problem can be formulated as an index coding problem, and solved by known tools in network coding. However, in practical setups the cache needs to respond efficiently (in communication and computation) to individual content-object requests, which renders known index/network-coding solutions non-applicable and non-scalable. We first show constructively that when the number of ISPs is less than or equal to 4, there is an efficient algorithm that achieves optimal storage with only bit-wise XOR operations, and with guaranteed low computation cost for serving requests. For larger numbers of ISPs, we present randomized algorithms that achieve optimal storage with XOR-only coding and low computation complexity per requested objects.","","978-1-5386-6596-1978-1-5386-6597","10.1109/ALLERTON.2018.8636083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636083","","Encoding;Complexity theory;Content distribution networks;Web and internet services;Network coding;Partitioning algorithms","communication complexity;decoding;Internet;network coding","XOR-only coding;only bit-wise XOR operations;CDN;multiple ISP;index-network-coding solutions;index coding design problem;computation complexity;multiple Internet service providers;content distribution networks;per-request complexity;coded caching;optimal storage;individual content-object requests","","","","18","","","","","IEEE","IEEE Conferences"
"A Class of Generalized Quasi-Cyclic LDPC Codes: High-Rate and Low-Complexity Encoder for Data Storage Devices","V. Tam Van; H. Matsui; S. Mita","NA; NA; NA","2010 IEEE Global Telecommunications Conference GLOBECOM 2010","","2010","","","1","6","In this paper, we study no 4-cycle, high-rate LDPC codes based on finite geometries for use in data storage devices and prove that these codes cannot be classified as quasi-cyclic (QC) codes but should be considered as broader generalized quasi-cyclic (GQC) codes. Because of the GQC structure of such codes, they can be systematically encoded using Groebner bases and their encoder can be implemented using simple feedback-shift registers. In order to demonstrate the efficiency of the encoder, we show that the hardware complexity of the serial-in serial-out encoder architecture of these codes is of linear order O(n). To encode a binary codeword of length n, less than 2n adders and 3n memory elements are required. Furthermore, we evaluated the error performances of these codes with sum product algorithm (SPA) decoding over additive white Gaussian noise (AWGN) channels. At a bit error rate (BER) of 10<sup>-5</sup>, they perform 1-dB away from the Shannon limit after 10 decoding iterations.","1930-529X;1930-529X","978-1-4244-5638-3978-1-4244-5636-9978-1-4244-5637","10.1109/GLOCOM.2010.5683369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5683369","","Parity check codes;Geometry;Null space;Orbits;Bit error rate;Matrix decomposition;Hardware","AWGN channels;cyclic codes;parity check codes","quasi cyclic LDPC codes;low complexity encoder;data storage device;quasi cyclic code;bit error rate;additive white Gaussian noise channel;AWGN channel","","","","19","","","","","IEEE","IEEE Conferences"
"Reduced-routing complexity decoder for high-rate QC-LDPC codes","Y. Niu; Z. Xiao; D. Jin; L. Su; L. Zeng","State Key Laboratory on Microwave and Digital Communications, Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; State Key Laboratory on Microwave and Digital Communications, Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; State Key Laboratory on Microwave and Digital Communications, Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; State Key Laboratory on Microwave and Digital Communications, Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China; State Key Laboratory on Microwave and Digital Communications, Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University, Beijing 100084, China","2011 International Conference on Computational Problem-Solving (ICCP)","","2011","","","703","707","This paper presents a high-throughput and routing complexity reduced decoder for Quasi-Cyclic Low-Density Parity-Check (QC-LDPC) codes in the high rate wireless personal area network applications (WPAN). Our selected code is the rate-1/2 QC-LDPC code in IEEE P802.11ad/D1.0. A new decoder architecture is proposed. The message paths between variable nodes and check nodes are constructed by a fixed wire network. Compared to the fully parallel architecture, routing complexity of the new architecture is reduced greatly. The decoder is implemented using FPGA, and a data rate of 1.21 Gbps can be achieved in the air.","","978-1-4577-0603-5978-1-4577-0602-8978-1-4577-0600-4978-1-4577-0601","10.1109/ICCPS.2011.6089946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089946","LDPC decoder;QC-LDPC;WPAN","Decoding;Iterative decoding;Wireless personal area networks;Wires;Random access memory;Complexity theory","cyclic codes;field programmable gate arrays;parity check codes;personal area networks;telecommunication network routing;telecommunication standards;wireless LAN","reduced-routing complexity decoder;quasi-cyclic low-density parity check codes;QC-LDPC codes;wireless personal area network;IEEE P802.11ad;message paths;variable nodes;check nodes;fixed wire network;field programmable gate arrays;FPGA","","2","","10","","","","","IEEE","IEEE Conferences"
"Low complexity SOVA for Turbo codes","R. Pei; Z. Wang; Q. Huang; J. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, 430079, China; School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; State Key Laboratory of Wireless Mobile Communications (CATT), Beijing, 100191, China","China Communications","","2017","14","8","33","40","Recently, trimming Soft-output Viterbi algorithm (T-SOVA) has been proposed to reduce the complexity of SOVA for Turbo codes. In its first stage, a dynamic algorithm, lazy Viterbi algorithm, is used to indicate the minimal metric differences which brings obstacle on hardware implementation. This paper proposes a Viterbi algorithm (VA) based T-SOVA to facilitate hardware implementation. In the first stage of our scheme, a modified VA with regular structure is used to find the maximum likelihood (ML) path and calculate the metric differences. Further, local sorting is introduced to trim the metric differences, which reduces the complexity of trimming operation. Simulation results and complexity analysis show that VA based T-SOVA performs as well as lazy VA based T-SOVA and is easier to be applied to hardware implementation.","1673-5447","","10.1109/CC.2017.8014345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014345","VA;SOVA;local sorting;hardware implementation","Measurement;Sorting;Complexity theory;Turbo codes;Viterbi algorithm;Hardware;Simulation","maximum likelihood estimation;turbo codes","low complexity SOVA;Turbo codes;trimming soft-output Viterbi algorithm;lazy Viterbi algorithm;maximum likelihood path","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity distributed multiple description coding for wireless video sensor networks","W. Liu; K. R. Vijayanagar; J. Kim","Department of Electrical and Computer Engineering, Illinois Institute of Technology, South Dearborn Street, Chicago, IL 60616-3793, USA; Department of Electrical and Computer Engineering, Illinois Institute of Technology, South Dearborn Street, Chicago, IL 60616-3793, USA; Department of Electrical and Computer Engineering, Illinois Institute of Technology, South Dearborn Street, Chicago, IL 60616-3793, USA","IET Wireless Sensor Systems","","2013","3","3","205","215","Quality-of-service-guaranteed video communication in wireless video sensor networks (WVSNs) is extremely challenging because of the unique constraints of WVSN (e.g. limited resources of sensors and high error rates in wireless sensor networks) and the characteristics of video traffic (e.g. huge data rate and tight delay bounds). Distributed video coding (DVC) has emerged as a new video coding paradigm for video applications with resource-limited encoders. However, current DVC architectures still have several technical limitations that prevent their widespread use in real-world applications. A low-complexity distributed multiple description coding method that combines the principles of multiple description coding and DVC has been proposed to further improve the error resilience of DVC, while maintaining low encoder complexity and good rate-distortion performance in WVSN. Simulation results show that the proposed method is robust against transmission errors, while maintaining low encoder complexity and low system latency for resource-limited applications in WVSNs.","2043-6386;2043-6394","","10.1049/iet-wss.2012.0115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6585110","","","network coding;quality of service;rate distortion theory;telecommunication traffic;video coding;video communication;wireless sensor networks","low-complexity distributed multiple description coding method;wireless video sensor network;quality-of-service-guaranteed video communication;WVSN;video traffic characteristics;tight delay bound;DVC;distributed video coding;resource-limited encoder application;error resilience;rate-distortion performance","","1","","49","","","","","IET","IET Journals & Magazines"
"Prediction of the Complexity of Code Changes Based on Number of Open bugs, New Feature and Feature Improvement","V. B. Singh; M. Sharma","NA; NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","478","483","During the last decade, a paradigm shift has been taken place in the software development process. Advancement in the internet technology has eased the software development under distributed environment irrespective of geographical locations. Result of this, Open Source Software systems which serve as key components of critical infrastructures in the society are still ever-expanding now. Open source software is evolved through an active participation of the users in terms of reporting of bugs, request for new features and feature improvements. These active users distributed across different geographical locations and are working towards the evolution of open source software. The code-changes due to bug fixes, new features and feature improvements for a given time period are used to predict the possible code changes in the software over a long run (potential complexity of code changes). It is evident that the open source software are evolved through these modification but an empirical understanding among the bug fix, new features, feature improvements and modifications in the files are unexplored till now. In this paper, we have predicted the potential of bugs that can be detected/fixed and new features, improvements that can be diffused in the software over a period of time. We have quantified the complexity of code changes (entropy) and after that predicted the complexity of code changes by applying Cobb-Douglas and extended Cobb-Douglas (two dimensions and three dimensions) based diffusion models. The developed models can be used to determine the quantitative value of complexity of code changes for reported bugs, new features and feature improvements in addition to their potential values. This empirical study mathematically models the interaction of a system (the debugging and code change system) with the external open world which will assist support managers in software maintenance activities and software evolution.","","978-1-4799-7377","10.1109/ISSREW.2014.95","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983889","Cobb-Douglas;Software Repositories;Open Source Software;Github repository","Entropy;Complexity theory;Computer bugs;Predictive models;Open source software;Mathematical model","critical infrastructures;Internet;program debugging;public domain software;software maintenance","code change complexity prediction;open bugs;feature improvement;software development process;Internet technology;distributed environment;open source software systems;critical infrastructures;bug reporting;feature request;extended Cobb-Douglas based diffusion model;debugging;software maintenance activities;software evolution","","3","","13","","","","","IEEE","IEEE Conferences"
"Low Complexity Iterative Receiver Design for Sparse Code Multiple Access","F. Wei; W. Chen","Shanghai Key Laboratory of Navigation and location based services, Shanghai Jiao Tong University, Minhang, China; Shanghai Key Laboratory of Navigation and location based services, Shanghai Jiao Tong University, Minhang, China","IEEE Transactions on Communications","","2017","65","2","621","634","Sparse code multiple access (SCMA) is one of the most promising methods among all the non-orthogonal multiple access techniques in the future 5G communication. Compared with some other non-orthogonal multiple access techniques, such as low density signature, SCMA can achieve better performance due to the shaping gain of the SCMA code words. However, despite the sparsity of the code words, the decoding complexity of the current message passing algorithm utilized by SCMA is still prohibitively high. In this paper, by exploring the lattice structure of SCMA code words, we propose a low-complexity decoding algorithm based on list sphere decoding (LSD). The LSD avoids the exhaustive search for all possible hypotheses and only considers signal within a hypersphere. As LSD can be viewed a depth-first tree search algorithm, we further propose several methods to prune the redundancy-visited nodes in order to reduce the size of the search tree. Simulation results show that the proposed algorithm can reduce the decoding complexity substantially while the performance loss compared with the existing algorithm is negligible.","0090-6778;1558-0857","","10.1109/TCOMM.2016.2631468","NSFC; National 863 Project; STCSM Key Fundamental Project; GXNSF; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752784","Non-orthogonal multiple access;SCMA;message passing algorithm;list sphere decoding;node pruning","Complexity theory;Maximum likelihood decoding;NOMA;Lattices;Message passing;Iterative decoding","5G mobile communication;iterative decoding;message passing;tree searching","low complexity iterative receiver design;sparse code multiple access;non-orthogonal multiple access techniques;5G communication;low density signature;shaping gain;SCMA code words;decoding complexity;current message passing algorithm;lattice structure;list sphere decoding;LSD;depth-first tree search algorithm;redundancy-visited nodes","","40","","37","","","","","IEEE","IEEE Journals & Magazines"
"Study of the Concatenation of Irregular LDPC Code with Equalizer of Low Complexity for 4G Mobile Wireless Network","W. Fernandez; R. Cifuentes; F. Rivera","Univ. del Bio Bio, Conception, Chile; NA; NA","IEEE Latin America Transactions","","2011","9","7","1018","1024","The performance of a system of communication for 4G mobile wireless networks, using the irregular low density parity check code and equalizer of low complexity is studied. The transmitter uses the codification of irregular LDPC encoder, with low error floor. The receiver is composes of a sum product decoder and equalizer with a maximum algorithm probability estimation sequence which a big decrease of the probability of error of the system is achieved. The channel of transmission is an additive, white, Gaussian noise and fast fading with Rayleigh distribution. A speed of 150 Km/Hr to the mobile is considered.","1548-0992","","10.1109/TLA.2011.6129697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6129697","Equalizer;fast fading;Irregular Low Density Parity Check Code","Parity check codes;Binary phase shift keying;Vectors;Irrigation;Signal to noise ratio;Mobile communication;Equalizers","4G mobile communication;AWGN channels;equalisers;estimation theory;parity check codes;probability;radio receivers;radio transmitters;Rayleigh channels","irregular LDPC code concatenation;equalizer;4G mobile wireless network;low density parity check code;transmitter;receiver;sum product decoder;probability estimation sequence;error probability;additive white Gaussian noise;fast fading;Rayleigh distribution","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity and implementation analysis of synthesized view distortion estimation architecture in 3D High Efficiency Video Coding","W. Ahmad; M. Martina; G. Masera","Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Corso Duca degli Abruzzi 24, Italy","2015 International Conference on 3D Imaging (IC3D)","","2015","","","1","8","Depth-based 3D formats for state of the art High Efficiency Video Coding standard facilitate the synthesis of virtual views with a brief binary representation. Depth maps coding error results in synthesis artefacts for virtual views rendered during Depth Image Based Rendering (DIBR) process. Based on depth maps distortion, synthesized view distortion estimation models are integrated in 3D-High Efficiency Video Coding (3D-HEVC) to cater the synthesis artefacts. This paper presents complexity and implementation analysis of synthesized view distortion estimation model integrated in Emerging 3D-HEVC standard. Profiling of the reference software of 3D-HEVC is performed and computational hotspots are identified based on the profiling information. Profiling results show that the Renderer model integrated in 3D-HEVC, for Rate Distortion Optimization (RDO) of depth maps coding by estimation of synthesized view distortion, takes about (18-26%) of total encoding time and is identified as one of the computational complex part of the 3D-HEVC standard. Algorithmic and implementation complexity analysis of the renderer model is presented in this paper.","","978-1-5090-1265-7978-1-5090-1264","10.1109/IC3D.2015.7391818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391818","3D-High Efficiency Video Coding (3D-HEVC);Depth-Image Based Rendering (DIBR);Rate Distortion Optimization (RDO)","Encoding;Three-dimensional displays;Standards;Distortion;Complexity theory;Software;Decoding","image representation;optimisation;rate distortion theory;rendering (computer graphics);video coding","view distortion estimation architecture synthesis;3D high efficiency video coding;depth-based 3D formats;brief binary representation;depth maps coding error;synthesis artefacts;virtual views;depth image based rendering process;DIBR process;depth map distortion;synthesized view distortion estimation models;3D-HEVC;renderer model;rate distortion optimization;RDO;depth map coding","","3","","12","","","","","IEEE","IEEE Conferences"
"Reduced-complexity search for video coding geometry partitions using texture and depth data","Q. Wang; J. Li; G. J. Sullivan; M. Sun","TNList and Dept. of Automation, Tsinghua University, Beijing, China, 100084; Microsoft Corporation, One Microsoft Way, Redmond, WA, USA, 98052; Microsoft Corporation, One Microsoft Way, Redmond, WA, USA, 98052; Dept. of Electrical Engineering, University of Washington, Seattle, USA, 98195","2011 Visual Communications and Image Processing (VCIP)","","2011","","","1","4","In this paper, a texture-space geometry partitioning approach is proposed to reduce the computational complexity of searching for geometry partitions for video coding. Additionally, for systems that capture both video and depth data, an enhanced geometry partitioning approach using both texture and depth information is proposed to further improve the partitioning accuracy and reduce the search complexity. Compared with a full-search approach, the proposed geometry partition search approaches achieve about 94% reduction of the encoding time while retaining similar rate-distortion performance.","","978-1-4577-1322-4978-1-4577-1321-7978-1-4577-1320","10.1109/VCIP.2011.6115980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115980","geometry partitioning;multi-view video;depth;computational complexity","Geometry;Encoding;Computational complexity;PSNR;Video coding;Conferences;Gain","computational complexity;geometry;image texture;rate distortion theory;video coding","reduced-complexity search;video coding geometry partitions;depth data;texture-space geometry partitioning approach;computational complexity;geometry partition searching;search complexity;rate-distortion performance","","3","","11","","","","","IEEE","IEEE Conferences"
"Computational Complexity of Decoding Orthogonal Space-Time Block Codes","E. Ayanoglu; E. G. Larsson; E. Karipidis","NA; NA; NA","2010 IEEE International Conference on Communications","","2010","","","1","6","The computational complexity of optimum decoding for an orthogonal space-time block code is quantified. Four equivalent techniques of optimum decoding which have the same computational complexity are specified. Modifications to the basic formulation in special cases are calculated and illustrated by means of examples.","1938-1883;1550-3607;1550-3607","978-1-4244-6404-3978-1-4244-6402-9978-1-4244-6403","10.1109/ICC.2010.5501884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501884","","Computational complexity;Block codes;Maximum likelihood decoding;Receiving antennas;Symmetric matrices;Transmitting antennas;Matrix decomposition;Communications Society;Peer to peer computing;Pervasive computing","block codes;computational complexity;maximum likelihood decoding;orthogonal codes;space-time codes","computational complexity;optimum decoding;orthogonal space-time block code;maximum likelihood metric","","1","","8","","","","","IEEE","IEEE Conferences"
"Low-complexity probability generation algorithm for stochastic decoding of non-binary LDPC codes","Guanghua He; B. Bai","State Key Lab. of ISN, Xidian University, Xi'an 710071, China; State Key Lab. of ISN, Xidian University, Xi'an 710071, China","2011 6th International ICST Conference on Communications and Networking in China (CHINACOM)","","2011","","","65","70","Non-binary LDPC codes offer higher performances than their binary counterpart but suffer from highest decoding complexity. A solution to reduce the decoding complexity is the use of stochastic decoding algorithm, but the computational complexity of probability generation in the first step is very high. In this paper, we propose a simple method to fast generate the probability especially in high-order modulation schemes. Simulation results show that the proposed algorithm causes negligible performance degradation with reduction in computational complexity for stochastic decoding of non-binary LDPC codes, which gives some advantages in hardware implementation of the decoders.","","978-1-4577-0101-6978-1-4577-0100-9978-1-4577-0099","10.1109/ChinaCom.2011.6158121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6158121","Non-binary LDPC codes;stochastic decoding;demodulations;probability;approximations;decoder","Decoding;Parity check codes;Modulation;Probability;Algorithm design and analysis;Computational complexity","computational complexity;decoding;modulation;parity check codes;probability;stochastic processes","low-complexity probability generation algorithm;stochastic decoding algorithm;nonbinary LDPC codes;decoding complexity;computational complexity;high-order modulation schemes;low density parity check codes","","","","15","","","","","IEEE","IEEE Conferences"
"Low complexity ADMM-LP based decoding strategy for LDPC convolutional codes","H. Ben Thameur; B. Le Gal; N. Khouja; F. Tlili; C. Jego","SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; Bordeaux INP, CNRS IMS, UMR 5218, Bordeaux University, France; SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; Bordeaux INP, CNRS IMS, UMR 5218, Bordeaux University, France","2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","","2017","","","1","5","A node-wise (NS) schedule has been recently proposed for decoding LDPC codes with the linear programming (LP) decoding approach, based on the alternate direction method of multipliers (ADMM). It improves the error correction performances as well as the convergence speed of the ADMM-LP decoder. However, it suffers from a high computational complexity resulted from residuals calculation. In this paper, a new formulation of the ADMM-LP algorithm with a modified computational scheduling is proposed for decoding LDPC convolutional codes. Simulations show that it reduces the computational complexity while improving the error correction performances and speeding up the decoding process. The decoding complexity is further reduced when considering the NS-ADMM approximation which allows savings up to 70% of the residuals calculation.","1847-358X","978-953-290-078-1978-953-290-074-3978-1-5386-3212","10.23919/SOFTCOM.2017.8115500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115500","","Decoding;Schedules;Computational complexity;Processor scheduling;Parity check codes;Error correction","approximation theory;computational complexity;convolutional codes;decoding;error correction codes;linear programming;parity check codes;telecommunication scheduling","computational complexity reduction;NS scheduling;node-wise scheduling;alternate direction method of multipliers;LDPC convolutional code decoding;ADMM-LP decoder;convergence speed;linear programming decoding approach;low complexity ADMM-LP based decoding strategy;NS-ADMM approximation;decoding complexity;decoding process;error correction performances;modified computational scheduling","","","","17","","","","","IEEE","IEEE Conferences"
"Interactive low-complexity codes for synchronization from deletions and insertions","R. Venkataramanan; H. Zhang; K. Ramchandran","Dept. of Electrical Engineering, Yale University, USA; Dept. of Electrical Engineering &amp; Computer Sciences, University of California, Berkeley, USA; Dept. of Electrical Engineering &amp; Computer Sciences, University of California, Berkeley, USA","2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2010","","","1412","1419","We study the problem of synchronization of two remotely located data sources, which are mis-synchronized due to deletions and insertions. This is an important problem since a small number of synchronization errors can induce a large Hamming distance between the two sources. The goal is to effect synchronization with the rate-efficient use of lossless bidirectional links between the two sources. In this work, we focus on the following model. A binary sequence X of length n is edited to generate the sequence at the remote end, say Y, where the editing involves random deletions and insertions, possibly in small bursts. The problem is to synchronize Y with X with minimal exchange of information (in terms of both the average communication rate and the average number of interactive rounds of communication). We focus here on the case where the number of edits is much smaller than n, and propose an interactive algorithm which is computationally simple and has near-optimal communication complexity. Our algorithm works by efficiently splitting the source sequence into pieces containing either just a single deletion/insertion or a single burst deletion/insertion. Each of these pieces is then synchronized using an optimal one-way synchronization code, based on the single-deletion correcting channel codes of Varshamov and Tenengolts (VT codes).","","978-1-4244-8216-0978-1-4244-8215-3978-1-4244-8214","10.1109/ALLERTON.2010.5707079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5707079","","Synchronization;Decoding;Protocols;Approximation algorithms;Channel coding;Source coding","computational complexity;error correction codes;sequences;synchronisation","interactive low complexity codes;remotely located data source;synchronization error;Hamming distance;sequence generation;near optimal communication complexity;single burst deletion;single burst insertion;single deletion correcting channel code","","19","","16","","","","","IEEE","IEEE Conferences"
"Simple near-maximum-likelihood low-complexity detection scheme for Alamouti space-time block coded spatial modulation","H. Xu; N. Pillay","School of Engineering, University of KwaZulu-Natal, Republic of South Africa; School of Engineering, University of KwaZulu-Natal, Republic of South Africa","IET Communications","","2014","8","15","2611","2618","Space-time block coded spatial modulation (STBC-SM) has been proposed to exploit the advantages of both spatial modulation and space-time block codes. The first objective of this study is to introduce a very simple near-maximum-likelihood (ML) low-complexity detection scheme forNt×NrM-ary quadrature amplitude modulation (M-QAM) STBC-SM. Simulation results validate that the proposed simple detection scheme achieves near-ML detection error performance. Furthermore, in comparison to existing schemes, the computational complexity of the proposed detector was demonstrated to be independent of the modulation size, hence exhibiting a very low computational complexity. The second objective of this study is to present an asymptotic bound to quantify the average bit-error rate performance of M-QAM STBC-SM over independent and identically distributed Rayleigh flat fading channels. The analytical frameworks are validated by Monte Carlo simulations.","1751-8628;1751-8636","","10.1049/iet-com.2013.1086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6917109","","","computational complexity;error statistics;maximum likelihood detection;Monte Carlo methods;quadrature amplitude modulation;Rayleigh channels;space-time block codes","simple-near-maximum-likelihood low-complexity detection scheme;Alamouti space-time block coded spatial modulation;M-ary quadrature amplitude modulation;M-QAM STBC-SM;near-ML detection error performance;computational complexity;modulation size;asymptotic bound;average bit-error rate performance;independent-identically distributed Rayleigh flat fading channel;Monte Carlo simulation","","3","","16","","","","","IET","IET Journals & Magazines"
"Complexity-constrained spatially coupled LDPC codes based on protographs","M. Battaglioni; M. Baldi; E. Paolini","Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, Ancona, Italy; Department of Electrical, Electronic, and Information Engineering “G. Marconi”, University of Bologna, Cesena (FC), Italy","2017 International Symposium on Wireless Communication Systems (ISWCS)","","2017","","","49","53","Spatially coupled low-density parity-check (SC-LDPC) codes are often thought as codes with very long block lengths. However, they can be decoded through sliding window (SW) decoders achieving near-optimal performance when the window size is a few times larger than the code syndrome former constraint length. This makes SC-LDPC codes with short constraint length suitable for low-latency transmissions, and they can even outperform their block code counterparts. Complexity of SW decoders increases linearly with the window size and with the number of decoding iterations. When complexity is constrained, an optimal trade-off between the window size and the maximum number of decoding iterations has to be found. In this paper, we propose a PEXIT-based method to find the best trade-off for codes with short syndrome former constraint length. We consider codes based on protographs, and validate the results of the PEXIT-based method through Monte Carlo simulations.","2154-0225","978-1-5386-2913-0978-1-5386-2914","10.1109/ISWCS.2017.8108160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8108160","LDPC codes;protograph-based codes;spatial coupling;sliding window decoding","Decoding;Iterative decoding;Complexity theory;Block codes;Matrices","computational complexity;decoding;parity check codes","code syndrome former constraint length;SC-LDPC codes;low-latency transmissions;decoding iterations;short syndrome former constraint length;low-density parity-check;window decoders;spatially coupled LDPC code;low density parity check codes;PEXIT-based method;protograph extrinsic information transfer analysis;SC","","1","","17","","","","","IEEE","IEEE Conferences"
"Improved Low-Complexity Soliton-Like Network Coding for a Resource-Limited Relay","A. Liau; I. Kim; S. Yousefi","Queen's University, Kingston, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON K7L 3N6, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON K7L 3N6, Canada","IEEE Transactions on Communications","","2013","61","8","3327","3335","In this paper, we examine the marriage of Fountain coding and network coding (NC). Fountain codes are capacity achieving erasure codes designed for point-to-point transmissions. NC is a throughput-optimal data dissemination technique, but its high-complexity decoding makes it unattractive for applications where limited resources are available. In this paper, we consider Fountain network coding to take advantage of efficient fountain decoders. Protocols such as Soliton-like rateless coding (SLRC) have previously addressed this issue, yet the re-encoding at the relay is expensive while there is still room for improving the performance. Extending SLRC, we propose the Improved Soliton-like Rateless Coding (ISLRC) protocol, where the relay is designed to perform distribution shaping given limited resources. ISLRC preserves the same properties as SLRC, but also makes the aggregate degree distribution more efficient for Fountain decoding. We analyze ISLRC's degree distribution and perform an asymptotic error analysis for the case where resources are most scarce. The ISLRC scheme is compared against other existing schemes. Simulation results show that even under the worst-case scenario of ISLRC, better performance can be achieved compared to SLRC and other existing schemes.","0090-6778;1558-0857","","10.1109/TCOMM.2013.061913.110585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549237","Distributed LT codes;fountain codes;linear network coding;LT codes","Decoding;Relays;Aggregates;Encoding;Network coding","network coding;relay networks (telecommunication);solitons","ISLRC scheme;asymptotic error analysis;distribution shaping;ISLRC protocol;Soliton-like rateless coding;SLRC scheme;Fountain network coding;high-complexity decoding;throughput-optimal data dissemination technique;point-to-point transmissions;resource-limited relay;improved low-complexity soliton-like network coding","","18","","21","","","","","IEEE","IEEE Journals & Magazines"
"Flexible complexity control based on intra frame mode decision in Distributed Video Coding","X. HoangVan; J. Park; B. Jeon","School of Information and Computer Engineering, Sungkyunkwan University, 300 Chunchun-dong, Jangan-gu, Suwon, 440-746, Korea; School of Information and Computer Engineering, Sungkyunkwan University, 300 Chunchun-dong, Jangan-gu, Suwon, 440-746, Korea; School of Information and Computer Engineering, Sungkyunkwan University, 300 Chunchun-dong, Jangan-gu, Suwon, 440-746, Korea","2011 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2011","","","1","5","Distributed Video Coding (DVC) is a coding paradigm that can shift a computationally expensive motion estimation module from encoder to decoder. Although this feature eases encoder complexity, its increased decoder complexity poses another problem. To solve these dilemmas, we propose a DVC scheme in which user can control complexity balance between encoder and decoder by controlling the number of frames which should be intra coded. Because intra mode decision is made to control the computational complexity balance, our proposed method still retains the low encoding complexity which is the main key feature of DVC architecture. Experimental results show that the proposed coding scheme not only increases coding efficiency but also makes itself easy to distribute the computational complexity over encoder and decoder for variety of applications.","2155-5052;2155-5044;2155-5044","978-1-61284-122-9978-1-61284-121-2978-1-61284-120","10.1109/BMSB.2011.5954957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954957","Complexity Control;Distributed Video Coding;Intra Mode Decision;Wyner-Ziv Video Coding","Complexity theory;Decoding;Encoding;Video coding;Pixel;Motion estimation;Correlation","computational complexity;decoding;motion estimation;video coding","flexible complexity control;intra frame mode decision;distributed video coding;coding paradigm;computationally expensive motion estimation module;encoder complexity;decoder complexity;intracoded;intra mode decision;computational complexity balance;encoding complexity;DVC architecture;coding scheme;coding efficiency","","3","","14","","","","","IEEE","IEEE Conferences"
"Odd type DCT/DST for video coding: Relationships and low-complexity implementations","M. Masera; M. Martina; G. Masera","Department of Electronics and Telecommunications, Politecnico di Torino, Turin 10129, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin 10129, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin 10129, Italy","2017 IEEE International Workshop on Signal Processing Systems (SiPS)","","2017","","","1","6","In this paper, we show a class of relationships which link Discrete Cosine Transforms (DCT) and Discrete Sine Transforms (DST) of types V, VI, VII and VIII, which have been recently considered for inclusion in the future video coding technology. In particular, the proposed relationships allow to compute the DCT-V and the DCT-VIII as functions of the DCT-VI and the DST-VII respectively, plus simple reordering and sign-inversion. Moreover, this paper exploits the proposed relationships and the Winograd factorization of the Discrete Fourier Transform to construct low-complexity factorizations for computing the DCT-V and the DCT-VIII of length 4 and 8. Finally, the proposed signal-flow-graphs have been implemented using an FPGA technology, thus showing reduced hardware utilization with respect to the direct implementation of the matrix-vector multiplication algorithm.","2374-7390","978-1-5386-0446-5978-1-5386-0445-8978-1-5386-0447","10.1109/SiPS.2017.8110009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8110009","","Discrete Fourier transforms;Discrete cosine transforms;Video coding;Transform coding;Hardware;Prediction algorithms","discrete cosine transforms;discrete Fourier transforms;field programmable gate arrays;matrix decomposition;matrix multiplication;video coding","DST-VII;matrix-vector multiplication algorithm;FPGA technology;signal-flow-graphs;Winograd factorization;video coding technology;odd type DCT;odd type DST;discrete sine transforms;discrete cosine transforms;discrete Fourier transform;DCT-VI;low-complexity implementations;DCT-VIII;DCT-V;low-complexity factorizations;sign-inversion","","1","","27","","","","","IEEE","IEEE Conferences"
"Design of complexity-optimized bilayer lengthened LDPC codes for relay channels","O. Vahabzadeh; M. Salehi","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115","2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2011","","","1019","1024","We propose a technique to design complexity-optimized bilayer lengthened LDPC codes by generalizing EXIT charts to bilayer codes and applying a method to estimate the number of decoding iterations. Using this approach, we have designed bilayer lengthened LDPC codes with a relay decoding complexity reduced up to 60% compared to the highest rate codes designed in the literature.","","978-1-4577-1818-2978-1-4577-1817-5978-1-4577-1816","10.1109/Allerton.2011.6120279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120279","The relay channel;bilayer low-density parity-check (LDPC) codes;complexity-optimization;extrinsic-information transfer (EXIT) charts;bilayer density evolution","Relays;Iterative decoding;Complexity theory;Error probability;Decoding;Optimization","channel coding;cooperative communication;decoding;parity check codes","complexity-optimized bilayer lengthened LDPC code design;relay channels;EXIT charts;decoding iteration approach;relay decoding complexity;extrinsic-information transfer charts;cooperative communication","","1","","10","","","","","IEEE","IEEE Conferences"
"Low-Complexity Array Codes for Random and Clustered 4-Erasures","Y. Cassuto; J. Bruck","Department of Electrical Engineering, California Institute of Technology; Department of Electrical Engineering, California Institute of Technology, Pasadena, U.S.A.","IEEE Transactions on Information Theory","","2012","58","1","146","158","A new family of low-complexity array codes is proposed for correcting 4 column erasures. The new codes are tailored for the new error model of clustered column erasures that captures the properties of high-order failure combinations in storage arrays. The model of clustered column erasures considers the number of erased columns, together with the number of clusters into which they fall, without pre-defining the sizes of the clusters. This model addresses the problem of correlated device failures in storage arrays, whereby each failure event may affect multiple devices in a single cluster. The new codes correct essentially all combinations of clustered 4 erasures, i.e., those combinations that fall into three or less clusters. The new codes are significantly more efficient, in all relevant complexity measures, than the best known 4-erasure correcting codes. These measures include encoding complexity, decoding complexity and update complexity.","0018-9448;1557-9654","","10.1109/TIT.2011.2171518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6121982","Array codes;clustered erasures;correlated failures;storage arrays","Arrays;Complexity theory;Encoding;Decoding;Polynomials;Redundancy;Shape","error correction codes;telecommunication network reliability","low-complexity array codes;random 4-erasures;clustered 4-erasures;high-order failure combinations;storage arrays;clustered column erasures;erased columns;failure event;complexity measures;4-erasure correcting codes;encoding complexity;decoding complexity;update complexity","","3","","13","","","","","IEEE","IEEE Journals & Magazines"
"Digital Fountain Codes with Reduced Latency, Complexity and Buffer Requirements for Wireless Communications","R. Razavi; H. Claussen","NA; NA","2014 IEEE 79th Vehicular Technology Conference (VTC Spring)","","2014","","","1","5","Digital Fountain Codes (DCFs) are proven to be very beneficial in providing reliable, yet flexible solutions in data communications over erasure wireless channels. Their randomness can additionally provide a strong protection against security and privacy risks. However, existing solutions based on DCFs suffer from increased packets, latency, increased decoding complexity, and require very large buffers both at the transmitter and receiver sides. This paper presents a practical solution to overcome most of these issues by exploiting the feedback path. Simulation results suggest significant up to 77% improvement in latency and up to 94% reduction in buffer requirements compared to the existing solutions.","1550-2252","978-1-4799-4482","10.1109/VTCSpring.2014.7022880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022880","","Decoding;Transmitters;Receivers;Encoding;Jitter;Complexity theory;Wireless communication","codes;computational complexity;telecommunication security;wireless channels","digital fountain codes;latency reduction;buffer requirements;wireless communications;data communications;erasure wireless channels;privacy risks;decoding complexity;feedback path;DCF-based data delivery scheme","","1","","10","","","","","IEEE","IEEE Conferences"
"Effective complexity control by inter-layer motion analysis for spatial scalability video coding","J. Wu; G. Li; M. Chen","Dept. of Electrical Engineering, National Dong Hwa University, Hualien, Taiwan; Industrial Technology Research Institute, Hsinchu, Taiwan; Dept. of Electrical Engineering, National Dong Hwa University, Hualien, Taiwan","2012 IEEE International Conference on Signal Processing, Communication and Computing (ICSPCC 2012)","","2012","","","78","83","Scalable video coding incorporated with computation-aware concept is an interesting issue in video coding due to its ability to achieve quality as well as computation scalable. This paper presents a computation-aware algorithm for scalable video coding with spatial scalability to aim at best trade-off between rate distortion performance and computational consumption. We first observe and analyze the motion vector difference relationship between scalable base and enhancement layers to find out whether the motion vector difference of the base layer can be used for showing some information about the motion behavior of the enhancement layer. Afterwards, a linear model is proposed to establish the motion vector difference relationship between base and enhancement layers. By using the modeling results, a linear algorithm for computation distribution is thus proposed to allocate the computation of the search process for each macroblock in the enhancement layer. In addition, the rate distortion costs of the base layer are also taken into account for the computation allocation process to further improve the coding performance. Simulation results demonstrate that our proposed computation-aware algorithm not only achieves better rate distortion performance than other works under the same computational constraints but also achieves less computation necessities.","","978-1-4673-2193-8978-1-4673-2192-1978-1-4673-2191","10.1109/ICSPCC.2012.6335683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6335683","Computation-aware;Scalable video coding;Motion vector difference;Rate distortion cost","Rate-distortion;Vectors;Video coding;Complexity theory;Encoding;Static VAr compensators;Algorithm design and analysis","image enhancement;image motion analysis;rate distortion theory;video coding","effective complexity control;interlayer motion analysis;spatial scalability video coding;computation-aware algorithm;rate distortion cost performance;motion vector difference analysis;image enhancement layers;base layer;linear algorithm;computation allocation process","","2","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity successive interference cancelation and one/two-bit feedback transmit antenna shuffling for 4 × 4 quasi-orthogonal space-time block codes","Y. Lee","Department of Electrical Engineering, National Chi Nan University, Puli, Taiwan","The 15th International Symposium on Wireless Personal Multimedia Communications","","2012","","","404","408","In this paper, we introduce a new way of successive interference cancelation (SIC) and transmit antenna shuffling (TAS) for quasi-orthogonal space-time block codes (QO-STBC) with 4×4 block structure. To be more specific, with the proposed SIC, single-symbol decoding becomes possible. Importantly, we show that only simple scalar operations are required, which keeps the computation load for decoding at a very low level. An efficient one/two-bit feedback TAS scheme is also derived for QO-STBC employing the SIC. The resultant error-rate performance can then be further improved with limited increase in feedback redundancy. All these are attractive features for actual QO-STBC usage. Simulation results show that the bit-error rate (BER) of our proposed SIC can be very close to that of the conventional pair-wise maximum likelihood (ML) decoding. Besides, for the considered QO-STBC system, applying our SIC together with TAS can let the BER performance approach that of ideal four-path diversity with reasonable decoding complexity and feedback amount.","1882-5621;1347-6890;1347-6890","978-986-03-3407-4978-1-4673-4533","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398726","Multiple-input multiple-output (MIMO);quasiorthogonal space-time block codes (QO-STBC);successive interference cancelation (SIC);transmit antenna shuffling (TAS)","Silicon carbide;Maximum likelihood decoding;Complexity theory;Bit error rate;Block codes;Transmitting antennas","error statistics;interference suppression;maximum likelihood decoding;orthogonal codes;space-time block codes;transmitting antennas","low-complexity successive interference cancellation;one-two-bit feedback transmit antenna shuffling;quasiorthogonal space-time block codes;SIC;QO-STBC;single-symbol decoding;one-two-bit feedback TAS scheme;resultant error-rate performance;bit-error rate;maximum likelihood decoding;ML decoding;BER performance approach;ideal four-path diversity","","","","15","","","","","IEEE","IEEE Conferences"
"Low Complexity Turbo Detection of Coded Under-Determined MIMO Systems","M. L. Walker; J. Tao; J. Wu; Y. R. Zheng","NA; NA; NA; NA","2011 IEEE International Conference on Communications (ICC)","","2011","","","1","5","The turbo detection of a coded under-determined multiple-input multiple-output (UD-MIMO) system is studied in this paper. A UD-MIMO system with N transmit antennas and M <; N receive antennas has more unknowns than the observations. However, most existing low complexity detectors, such as the vertical Bell Laboratories Layered Space-Time (V BLAST) or the block decision feedback equalizer (BDFE), work only when M ≥ N. We propose a new turbo detector structure that combines the existing generalized parallel interference cancellation (GPIC) technique and a new generalized serial interference cancellation (GSIC) technique, with a soft-input soft output (SISO) BDFE. In the first iteration, the GPIC-BDFE equalization is adopted. From the second iteration and beyond, due to the availability of the a priori soft information, the UD MIMO system can be partitioned into multiple sub-systems by using a reliability-based partition scheme. Then, the GSIC-BDFE equalization, which has a much lower complexity than the GPIC BDFE equalization, can be used. Simulation results show that the new detection scheme achieves significant performance gain as the iteration progresses with a reasonable complexity.","1938-1883;1550-3607;1550-3607","978-1-61284-233-2978-1-61284-232-5978-1-61284-231","10.1109/icc.2011.5963231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5963231","","Complexity theory;MIMO;Equalizers;Transmitting antennas;Receiving antennas;Interference cancellation;Bit error rate","antenna arrays;interference suppression;MIMO communication;receiving antennas;telecommunication network reliability;transmitting antennas;turbo codes","low complexity turbo detection;coded under-determined MIMO systems;multiple-input multiple-output system;transmit antennas;receive antennas;vertical Bell Laboratories Layered Space-Time;V-BLAST;block decision feedback equalizer;generalized parallel interference cancellation technique;generalized serial interference cancellation;soft-input soft-output system;a priori soft information;reliability-based partition scheme","","6","","12","","","","","IEEE","IEEE Conferences"
"A reduced-complexity successive-cancellation decoding algorithm for polar codes","C. Xing; B. Wang; S. Zhao","Institute of Signal Processing & Transmission, Nanjing University of Posts & Telecommunications, China; Institute of Signal Processing & Transmission, Nanjing University of Posts & Telecommunications, China; Institute of Signal Processing & Transmission, Nanjing University of Posts & Telecommunications, China","2013 6th International Congress on Image and Signal Processing (CISP)","","2013","03","","1221","1225","Polar codes are codes which provably achieve the capacity of arbitrary symmetric binary-input channels with the complexity of encoders and decoders O(N log N), where N is the code block-length. In the paper, we will focus on a lower complexity implementation of decoding algorithm in the log-likelihood ratio domain. We use the update rule proposed by Gallager in the decoding algorithm of low density parity check (LDPC) codes to replace the node update rules used in successive cancellation (SC) algorithm for polar codes. To simplify the logarithmic and the exponential operations in the Gallager's approach node updates rule for polar codes, we further utilize a piece-wise linear algorithm to approximate the involution transform function, where the piece-wise linear algorithm only uses multiplication and addition operation. It has resulted in a reduced complexity SC decoding algorithm for polar codes. The numerical simulations show that our proposed SC algorithm (Piece-wise approx.) has a lower implementation complexity for polar code decoding, but at the cost about 0.7dB degradation in the bit-error-rate (BER) performance in comparison with the SC algorithm proposed by Arikan when the BER is 10-5. The proposed SC algorithm (Piece-wise approx.) is a tradeoff between the error performance and the decoding complexity.","","978-1-4799-2764-7978-1-4799-2763","10.1109/CISP.2013.6743858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743858","Polar codes;Successive-Cancelation Algorithm;Reduced-Complexity Decoding;Piecewise-Linear Functions","Decoding;Complexity theory;Approximation algorithms;Algorithm design and analysis;Signal processing algorithms;Parity check codes;Bit error rate","computational complexity;decoding;error statistics;parity check codes;transforms","reduced-complexity successive-cancellation decoding algorithm;arbitrary symmetric binary-input channels;log-likelihood ratio domain;low density parity check codes;LDPC codes;piece-wise linear algorithm;involution transform function;reduced complexity SC decoding algorithm;numerical simulations;polar code decoding;bit-error-rate;BER","","2","","22","","","","","IEEE","IEEE Conferences"
"Design of Low-Complexity Convolutional Codes over GF(q)","R. Klaimi; C. Abdel Nour; C. Douillard; J. Farah","NA; NA; NA; NA","2018 IEEE Global Communications Conference (GLOBECOM)","","2018","","","1","7","This paper proposes a new family of recursive systematic convolutional codes, defined in the non-binary domain over different Galois fields GF(q) and intended to be used as component codes for the design of non-binary turbo codes. A general framework for the design of the best codes over different GF(q) is described. The designed codes offer better performance than the non-binary convolutional codes found in the literature. They also outperform their binary counterparts when combined with their corresponding QAM modulation or with lower order modulations.","2576-6813;1930-529X","978-1-5386-4727-1978-1-5386-6976-1978-1-5386-4728","10.1109/GLOCOM.2018.8647824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8647824","","Convolutional codes;Modulation;Systematics;Turbo codes;Euclidean distance;Parity check codes","binary codes;convolutional codes;Galois fields;turbo codes","recursive systematic convolutional codes;nonbinary domain;component codes;nonbinary turbo codes;designed codes;nonbinary convolutional codes;convolutional codes;Galois fields","","","","22","","","","","IEEE","IEEE Conferences"
"Low-Complexity Decoding for RaptorQ Codes Using a Recursive Matrix Inversion Formula","Y. Lu; I. Lai; C. Lee; T. Chiueh","Graduate Institute of Electronics Engineering, National Taiwan University, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taiwan","IEEE Wireless Communications Letters","","2014","3","2","217","220","In this letter, we propose a low-complexity recursive decoding algorithm for rateless RaptorQ codes, the next generation error correction codes adopted by several standards such as DVB-H. Conventionally, the decoding of RaptorQ codes requires inverting a huge receive code generator matrix (RCGM). Instead of such costly matrix inversion, we propose to calculate the inverse of the transmit code generator matrix (TCGM) beforehand. Then, based on this pre-calculated inverse, the Sherman-Morrison formula is applied to recursively compute the inverse of the RCGM at run time. Most computations are thus shifted offline. Moreover, this recursive decoding distributes the computations among different time slots, thereby significantly shortening the decoding latency and improving the hardware utilization. Numerical simulations demonstrate that the computational complexity of the proposed recursive decoding is as low as 6.5% of that required by the conventional method using direct matrix inversion.","2162-2337;2162-2345","","10.1109/WCL.2014.020314.130872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733534","Error correction code;fountain code;Raptor code;RaptorQ code;Sherman-Morrison formula","Decoding;Vectors;Generators;Digital video broadcasting;Multimedia communication;Computational complexity","communication complexity;error correction codes;matrix inversion;next generation networks;recursive estimation","TCGM;direct matrix inversion;computational complexity;hardware utilization;decoding latency;RCGM;Sherman-Morrison formula;transmit code generator matrix;receive code generator matrix;next generation error correction code;rateless RaptorQ codes;recursive matrix inversion formula;complexity recursive decoding algorithm","","4","","15","","","","","IEEE","IEEE Journals & Magazines"
"BFrWF: Block-based FrWF for coding of high-resolution images with memory-complexity constrained -devices","M. Tausif; E. Khan; M. Hasan","Department of Electronics Engineering, ZHCET, AMU, Aligarh; Department of Electronics Engineering, ZHCET, AMU, Aligarh; Department of Electronics Engineering, ZHCET, AMU, Aligarh","2018 5th IEEE Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)","","2018","","","1","5","A typical image coder generally consists of a transform stage followed by quantization and coding stages. The memory requirement of an image coder would be the maximum of both the stages and complexity would be the sum of both stages. Due to large memory requirements, most of the existing image coders are unsuitable for their implementation on memory-constrained-platforms especially for high-resolution images. In this paper, we propose a low memory approach, Block-based Fractional Wavelet Filter (BFrWF), to compute wavelet transform coefficients of high-resolution images. Furthermore, BFrWF can be combined with low memory wavelet-based image coding algorithms to design low-memory image codec. Evaluation results show that the BFrWF requires less than 10 kB of RAM (available over most of the low-cost sensor nodes) even for high-resolution (HR) images, thus making it suitable for visual sensor networks. Moreover, the proposed BFrWF implemented with 8 blocks has 25.64% less complexity than FrWF and 80.45% less complexity than segmented FrWF (SFrWF) implemented with partitioning of an image line into 8 segments (SFrWF - a low memory variant of FrWF) for HR-image of dimension 2048×2048.","","978-1-5386-5002-8978-1-5386-5003","10.1109/UPCON.2018.8597104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8597104","Image coder;low memory;Discrete Wavelet Transform;Fractional wavelet filter and sensor nodes","Discrete wavelet transforms;Memory management;Random access memory;Buffer storage;Complexity theory","codecs;data compression;image coding;wavelet transforms","high-resolution images;Block-based Fractional Wavelet Filter;low memory wavelet-based image coding algorithms;image codec;image coders;image coder;memory size 10.0 KByte","","","","20","","","","","IEEE","IEEE Conferences"
"Complexity-Constrained H.264 Video Coding in Wireless Environment","J. Zhang; Y. Zhang; S. Chen","NA; NA; NA","2012 8th International Conference on Wireless Communications, Networking and Mobile Computing","","2012","","","1","4","Nowadays, real-time video applications on portable devices became more and more popular. In this paper, we propose a novel joint computation-rate-distortion optimization algorithm for H.264/AVC-based wireless video communications through handheld devices under power constraint. Firstly, we propose a computation resources (CR) allocation model, which is based on virtual computation resources buffer (VCB) and employs the previous method for computation consumption quantitative measurement to facilitate the optimal allocation of constrained CR for each frame. Secondly, in order to meet the allocated CR, we develop an encoder based on complexity adjustable motion estimation (ME) and inter mode decision (MD). The proposed algorithm takes end-to-end distortion into consideration to address the distortion caused by packet loss. The experimental results demonstrate that the proposed algorithm can minimize the end-to-end distortion in the environments with both packet loss and power constraint.","2161-9654;2161-9646;2161-9646","978-1-61284-683-5978-1-61284-684-2978-1-61284-682","10.1109/WiCOM.2012.6478458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478458","","Complexity theory;Packet loss;Computational modeling;Wireless communication;Resource management;Streaming media","motion estimation;optimisation;rate distortion theory;resource allocation;video coding;video communication","H.264 video coding;wireless environment;portable devices;joint computation-rate-distortion optimization algorithm;wireless video communications;handheld devices;computation resources allocation model;virtual computation resources buffer;VCB;motion estimation;intermode decision;packet loss;power constraint","","","","8","","","","","IEEE","IEEE Conferences"
"Toward a Preferred 4 x 4 Space-Time Block Code: A Performance-Versus-Complexity Sweet Spot with Linear-Filter Decoding","S. Kundu; D. A. Pados; W. Su; R. Grover","COMSENS Research Center, Dept. of Electrical Engineering, State University of New York at Buffalo, Buffalo, NY, 14260, USA; COMSENS Research Center, Dept. of Electrical Engineering, State University of New York at Buffalo, Buffalo, NY, 14260, USA; COMSENS Research Center, Dept. of Electrical Engineering, State University of New York at Buffalo, Buffalo, NY, 14260, USA; Mirics, Needham, MA, 02494, USA","IEEE Transactions on Communications","","2013","61","5","1847","1855","We develop a new 4 × 4 Hadamard-precoded quasi-orthogonal space-time block code (QO-STBC) that enables highly effective near-maximum-likelihood (near-ML) reliability-based prioritized symbol detection using linear filters. Approximate block-error-rate minimization is being used to optimize the code rotation angle. Detailed computational complexity evaluation of the decoder in terms of real multiplications and additions shows significant complexity reduction for symbol alphabet sizes of interest. Numerical and simulation studies demonstrate negligible bit-error-rate degradation compared to the state-of-the-art in bit-error-rate by ML decoded 4 × 4 codewords.","0090-6778;1558-0857","","10.1109/TCOMM.2013.021913.120338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468994","Hadamard precoding;matched-filter (MF);maximum-likelihood (ML) detection;minimum-mean-square-error (MMSE) filter;multi-input multi-output (MIMO) communications;quasi-orthogonal space-time block codes (QO-STBC)","Decoding;Signal to noise ratio;Interference;Error probability;Joints;Block codes;Error analysis","error statistics;Hadamard codes;linear codes;maximum likelihood decoding;orthogonal codes;precoding;reliability;space-time block codes","performance-versus-complexity sweet spot;linear-filter decoding;Hadamard-precoded quasiorthogonal space-time block code;QO-STBC;near-maximum-likelihood reliability-based prioritized symbol detection;linear filters;block-error-rate minimization;computational complexity;code rotation angle;decoder;complexity reduction;symbol alphabet sizes;bit-error-rate;ML decoded 4 × 4 codewords;4 × 4 space-time block code","","8","","25","","","","","IEEE","IEEE Journals & Magazines"
"High-Rate and Full-Diversity Space-Time Block Codes with Low Complexity Partial Interference Cancellation Group Decoding","L. Shi; W. Zhang; X. Xia","School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, Australia; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, Australia; Department of Electrical and Computer Engineering, University of Delaware, DE 19716, USA","IEEE Transactions on Communications","","2011","59","5","1201","1207","In this letter, we propose a systematic design of space-time block codes (STBC) which can achieve high rate and full diversity when the partial interference cancellation (PIC) group decoding is used at receivers. The proposed codes can be applied to any number of transmit antennas and admit a low decoding complexity while achieving full diversity. For M transmit antennas, in each codeword real and imaginary parts of PM complex information symbols are parsed into P diagonal layers and then encoded, respectively. With PIC group decoding, it is shown that the decoding complexity can be reduced to a joint decoding of M/2 real symbols. In particular, for 4 transmit antennas, the code has real symbol pairwise (i.e., single complex symbol) decoding that achieves full diversity and the code rate is 4/3. Simulation results demonstrate that the full diversity is offered by the newly proposed STBC with the PIC group decoding.","0090-6778;1558-0857","","10.1109/TCOMM.2011.030411.100176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5733449","MIMO systems;Space-time block codes;partial interference cancellation;decoding complexity","Decoding;Complexity theory;Transmitting antennas;Receiving antennas;Quality of service;Interference cancellation;Bit error rate","antenna arrays;decoding;interference suppression;space-time block codes;transmitting antennas","high-rate space-time block codes;full-diversity space-time block codes;low-complexity partial interference cancellation group decoding;STBC systematic design;PIC group decoding;transmit antennas;low-decoding complexity;PM complex information symbols;diagonal layers;joint decoding;real-symbol pairwise decoding","","14","","12","","","","","IEEE","IEEE Journals & Magazines"
"Reduced complexity joint decoding for turbo-coded wireless sensor networks","J. Haghighat; F. Labeau; D. V. Plant; S. Naderi","Department of Electrical and Electronics Engineering, Shiraz University of Technology, Shiraz, Iran; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada; Department of Electrical and Electronics Engineering, Shiraz University of Technology, Shiraz, Iran","2015 23rd Iranian Conference on Electrical Engineering","","2015","","","156","161","We consider a data-gathering wireless sensor network, modeled by a Chief Executive Officer (CEO) problem. The Fusion Centre (FC) decodes data that are separately encoded at each sensor node by turbo codes. The CEO model introduces a correlation model between sensors' data. As shown in the literature, this correlation can be employed at the FC to perform a joint graph-based decoding. The optimal decoding is provided by the well-known sum-product algorithm; however, the sum-product summations impose a computational complexity that exponentially grows by increasing the number of sensors. In this paper, we propose a suboptimal joint decoding algorithm in which we first perform a reliability sorting and then we use a set of most-reliable nodes to update extrinsic information for other nodes. This algorithm exponentially reduces the decoding complexity compared to the sum-product algorithm and achieves BERs impressively close to the ones achieved by the sum-product decoding. We show by simulations that applying reliability sorting substantially improves the performance of the proposed algorithm. We also show that, by fixing the number of most-reliable nodes and increasing the total number of sensors, we could further reduce the average BER of the system, while keeping the same decoding complexity for joint decoding.","2164-7054","978-1-4799-1972-7978-1-4799-1971","10.1109/IranianCEE.2015.7146201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146201","Wireless sensor networks;sum-product algorithm;iterative decoding;turbo codes","Decision support systems;Electrical engineering;Conferences","correlation methods;decoding;graph theory;sensor fusion;sorting;turbo codes;wireless sensor networks","reduced complexity joint decoding;turbo codes;wireless sensor networks;data gathering;chief executive officer problem;fusion centre;correlation model;joint graph-based decoding;suboptimal joint decoding algorithm;reliability sorting;most reliable node set","","","","23","","","","","IEEE","IEEE Conferences"
"A New High-Performance and Low-Complexity Turbo-LDPC Code","J. Hung; J. Shyu; S. Chen","NA; NA; NA","2011 International Conference on Multimedia and Signal Processing","","2011","1","","68","72","In this paper, a new coding scheme, named Turbo-LDPC code, is proposed. It applies LDPC codes to the product coding scheme, and performs the decoding operation iteratively between the two component codes. Besides, to boost the decoding performance, a linear extrinsic information normalization (EIN) method is also devised, which greatlyreduces the problem of belief propagation distortion ofextrinsic information after message interleaving. Altogether,the decoding performances of Turbo-LDPC codes are muchhigher than those of the original LDPC codes undercomparable computational complexities. Compared to blockturbo code combined with BCH codes, the proposed Turbo-LDPC code also has much better decoding performance as wellas lower computational complexity.","","978-1-61284-314","10.1109/CMSP.2011.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5957374","channel coding;block turbo codes;LDPC codes;product codes","Decoding;Iterative decoding;Computational complexity;Turbo codes;Product codes","BCH codes;computational complexity;decoding;parity check codes;product codes;turbo codes","low-complexity turbo-LDPC code;product coding scheme;decoding operation;linear extrinsic information normalization method;belief propagation distortion;message interleaving;computational complexities;BCH codes","","1","","11","","","","","IEEE","IEEE Conferences"
"Selective gray-coded bit-plane based low-complexity motion estimation and its hardware architecture","S. Yavuz; A. Celebi; M. Aslam; O. Urhan","Kocaeli University Integrated Systems Laboratory (KUTSAL), Electronics and Telecom. Eng. Dept., Umuttepe Campus, 41380, Izmit/Kocaeli, Turkey; Kocaeli University Integrated Systems Laboratory (KUTSAL), Electronics and Telecom. Eng. Dept., Umuttepe Campus, 41380, Izmit/Kocaeli, Turkey; Kocaeli University Integrated Systems Laboratory (KUTSAL), Electronics and Telecom. Eng. Dept., Umuttepe Campus, 41380, Izmit/Kocaeli, Turkey; Kocaeli University Laboratory of Embedded and Vision Systems (KULE), Electronics and Telecom. Eng. Dept., Umuttepe Campus, 41380, Izmit/Kocaeli, Turkey","IEEE Transactions on Consumer Electronics","","2016","62","1","76","84","Today, many consumer electronics devices have video capturing capability which is one of the most time, power and memory consuming application. Motion estimation (ME) is the key part of the video coding process in terms of computational load. Thus, it is important to implement this process in a resource efficient way without degrading the encoding quality and real-time operation performance. Low bitdepth representation based ME methods draw a lot of attention in consumer electronics area mainly thanks to its highly efficient hardware and software implementations. However, these low bit-depth representation based methods generally assume that the low bit-depth images are already available. Furthermore, these methods simply neglect the binarization cost which is not a proper approach when whole encoding architecture is of concern. This paper presents a novel selective Gray-coding based ME method and its hardware architecture with an embedded system integration by making use of one of the most common interconnect architecture in consumer electronics devices. Experimental results show that it is possible to reduce computational load of binarization stage significantly while improving the ME accuracy by the proposed approach compared to methods at the same category.","0098-3063;1558-4127","","10.1109/TCE.2016.7448566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448566","Motion estimation;Gray-coding;One-bittransform;Low-complexity ME","Motion estimation;Hardware;Complexity theory;Computer architecture;Encoding;Video coding;Consumer electronics","consumer electronics;embedded systems;Gray codes;image capture;image representation;motion estimation;power consumption;video coding","selective Gray-coded bit-plane based low-complexity motion estimation;hardware architecture;consumer electronics device;video capturing capability;power consuming application;memory consuming application;video coding process;low bit-depth representation based ME method;low bit-depth image;embedded system integration;interconnect architecture;binarization stage computational load reduction","","8","","45","","","","","IEEE","IEEE Journals & Magazines"
"A content adaptive complexity reduction scheme for HEVC-based 3D video coding","H. R. Tohidypour; M. T. Pourazad; P. Nasiopoulos; V. Leung","Dept. of Electrical &amp; Computer Eng., University of British Columbia, Canada; University of British Columbia, TELUS Communications Inc., Canada; Dept. of Electrical and Computer Eng., University of British Columbia, Canada; Dept. of Electrical and Computer Eng., University of British Columbia, Canada","2013 18th International Conference on Digital Signal Processing (DSP)","","2013","","","1","5","3D-HEVC is aiming at utilizing the advanced tools present in HEVC to efficiently code multiview video content. The computational complexity of this codec is significantly increased and is a challenge for real-time applications. In our study, we propose an adaptive early-termination inter and intra prediction mode search that reduces the 3D-HEVC coding complexity by utilizing the correlations between views, while maintaining the overall video quality.","2165-3577;1546-1874","978-1-4673-5807","10.1109/ICDSP.2013.6622774","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622774","3D HEVC;video compression;low complexity compression","Vectors","computational complexity;real-time systems;video codecs;video coding","HEVC-based 3D video coding;content adaptive complexity reduction;multiview video content;computational complexity;codec;real-time applications;intra prediction mode;video quality;high efficiency video coding","","10","","7","","","","","IEEE","IEEE Conferences"
"Network coded two-way relaying with reduced relay complexity","O. Iscan; M. Heindlmaier; C. Hausl","Institute for Communications Engineering, Technische Universit&#x00E4;t M&#x00FC;nchen, 80290 Munich, Germany; Institute for Communications Engineering, Technische Universit&#x00E4;t M&#x00FC;nchen, 80290 Munich, Germany; Institute for Communications Engineering, Technische Universit&#x00E4;t M&#x00FC;nchen, 80290 Munich, Germany","2011 8th International Symposium on Wireless Communication Systems","","2011","","","497","501","We consider a two-way relay communication with two time-phases. During the first phase the two source nodes transmit simultaneously to the relay. During the second phase the relay broadcasts to the two source nodes whereas it is only allowed to perform scalar symbol-to-symbol operations. We consider a strategy where the relay forwards an estimate of the multiplication of the two transmitted BPSK-symbols at the source nodes. For this strategy, we design several receiver schemes which support soft-decision channel decoding at the source nodes. Our simulation results show a significantly better performance of the proposed strategy compared to a linear amplification at the relay.","2154-0225;2154-0217;2154-0217","978-1-61284-402-2978-1-61284-403-9978-1-61284-401","10.1109/ISWCS.2011.6125409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125409","","Relays;Approximation methods;Decoding;Network coding;Signal to noise ratio;Bit error rate","channel coding;decoding;network coding;phase shift keying;radio receivers","two-way relay communication;network coding;time phases;source nodes;relay broadcasts;scalar symbol-to-symbol operations;BPSK-symbols;receiver schemes;soft-decision channel decoding;linear amplification;relay complexity reduction","","1","","18","","","","","IEEE","IEEE Conferences"
"Flexible Convolutional Codes: Variable Rate and Complexity","A. Katsiotis; P. Rizomiliotis; N. Kalouptsidis","National and Kapodistrian University of Athens, Department of Informatics and Telecommunications, Division of Communications and Signal Processing, Panepistimiopolis, 15784 Athens, Greece; University of the Aegean, Department of Information and Communication Systems Engineering, Karlovassi, Samos, GR-83200, Greece; National and Kapodistrian University of Athens, Department of Informatics and Telecommunications, Division of Communications and Signal Processing, Panepistimiopolis, 15784 Athens, Greece","IEEE Transactions on Communications","","2012","60","3","608","613","In this study, a method is presented for constructing convolutional codes of variable rate and decoding complexity. Starting with an (n,1,m) mother code, the techniques of puncturing and path pruning are utilized in order to construct large families of convolutional codes of various code rates and complexity. Decoding is performed using the trellis of the mother code.","0090-6778;1558-0857","","10.1109/TCOMM.2011.121211.110124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6109373","Convolutional codes;trellis complexity;path pruning;puncturing;maximum likelihood decoding","Convolutional codes;Computational complexity;Maximum likelihood decoding;Polynomials;Transient analysis","computational complexity;convolutional codes;decoding;trellis codes","flexible convolutional codes;variable rate;decoding complexity;mother code;puncturing;path pruning;code rates;trellis code","","10","","20","","","","","IEEE","IEEE Journals & Magazines"
"Interpolation filter design in HEVC and its coding efficiency - complexity analysis","K. Ugur; A. Alshin; E. Alshina; F. Bossen; W. Han; J. Park; J. Lainema","Nokia Corporation, Finland; Samsung Electronics, Korea; Samsung Electronics, Korea; DOCOMO Innovations, Japan; Gachon University, Korea; Samsung Electronics, Korea; Nokia Corporation, Finland","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","","2013","","","1704","1708","Coding efficiency gains in the High Efficiency Video Coding (H.265/HEVC) standard are achieved by improving many aspects of the traditional hybrid coding framework. Motion compensated prediction, and in particular the interpolation filter, is one of the areas that was improved significantly over H.264/AVC. This paper presents the details of the motion compensation interpolation filter design of the H.265/HEVC standard and its improvements over the interpolation filter design of H.264/AVC. These improvements include discrete cosine transform based filter coefficient design, utilizing longer filter taps for luma and chroma interpolation and using higher precision operations in the intermediate computations. The computational complexity of HEVC interpolation filter is also analyzed both from theoretical and practical perspectives. Experimental results show that a 4.5% average bitrate reduction for the luma component and 13.0% average bitrate reduction for the chroma components are achieved compared to interpolation filter of H.264/AVC. The coding efficiency gains are significant for some video sequences and can reach up to 21.7%.","1520-6149;2379-190X","978-1-4799-0356","10.1109/ICASSP.2013.6637943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637943","Video coding;standards;HEVC;interpolation filter","Video coding;Interpolation;ISO;Complexity theory;Abstracts;Indexes","computational complexity;discrete cosine transforms;motion compensation;video coding","HEVC;coding efficiency;complexity analysis;high efficiency video coding standard;H.265;hybrid coding framework;motion compensation interpolation filter design;discrete cosine transform based filter coefficient design;filter taps;luma interpolation;chroma interpolation;computational complexity;chroma components;video sequences","","4","","11","","","","","IEEE","IEEE Conferences"
"Improved tone mapping operator for HDR coding optimizing the distortion/spatial complexity trade-off","P. Lauga; G. Valenzise; G. Chierchia; F. Dufaux","Institut Mines-Télécom; Télécom ParisTech; CNRS LTCI, France; Institut Mines-Télécom; Télécom ParisTech; CNRS LTCI, France; Institut Mines-Télécom; Télécom ParisTech; CNRS LTCI, France; Institut Mines-Télécom; Télécom ParisTech; CNRS LTCI, France","2014 22nd European Signal Processing Conference (EUSIPCO)","","2014","","","1607","1611","A common paradigm to code high dynamic range (HDR) image/video content is based on tone-mapping HDR pictures to low dynamic range (LDR), in order to obtain backward compatibility and use existing coding tools, and then use inverse tone mapping at the decoder to predict the original HDR signal. Clearly, the choice of a proper tone mapping is essential in order to achieve good coding performance. The state-of-the-art to design the optimal tone mapping operator (TMO) minimizes the mean-square-error distortion between the original and the predicted HDR image. In this paper, we argue that this is suboptimal in rate-distortion sense, and we propose a more effective TMO design strategy that takes into account also the spatial complexity (which is a proxy for the bitrate) of the coded LDR image. Our results show that the proposed optimization approach enables to obtain substantial coding gain with respect to the minimum-MSE TMO.","2076-1465;2219-5491","978-0-9928-6261","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952581","High dynamic range;coding;convex optimization;spatial complexity","Transform coding;Image coding;Image reconstruction;Encoding;Dynamic range;Complexity theory;Rate-distortion","image coding;mean square error methods;optimisation","improved tone mapping operator for;HDR image coding;high dynamic range image coding;mean-square-error distortion;TMO design strategy;spatial complexity;optimization approach;substantial coding gain","","","","20","","","","","IEEE","IEEE Conferences"
"View-Adaptive Motion Estimation and Disparity Estimation for Low Complexity Multiview Video Coding","L. Shen; Z. Liu; T. Yan; Z. Zhang; P. An","Key Laboratory of Ministry of Education for Advanced Display and System Application, Shanghai University, Shanghai, China; Key Laboratory of Ministry of Education for Advanced Display and System Application, Shanghai University, Shanghai, China; Key Laboratory of Ministry of Education for Advanced Display and System Application, Shanghai University, Shanghai, China; Key Laboratory of Ministry of Education for Advanced Display and System Application, Shanghai University, Shanghai, China; Key Laboratory of Ministry of Education for Advanced Display and System Application, Shanghai University, Shanghai, China","IEEE Transactions on Circuits and Systems for Video Technology","","2010","20","6","925","930","The emerging international standard for multiview video coding (MVC) is an extension of H.264/advanced video coding. In the joint mode of MVC, both motion estimation (ME) and disparity estimation (DE) are included in the encoding process. This achieves the highest coding efficiency but requires a very high computational complexity. In this letter, we propose a fast ME and DE algorithm that adaptively utilizes the inter-view correlation. The coding mode complexity and the motion homogeneity of a macroblock (MB) are first analyzed according to the coding modes and motion vectors from the corresponding MBs in the neighbor views, which are located by means of global disparity vector. According to the coding mode complexity and the motion homogeneity, the proposed algorithm adjusts the search strategies for different types of MBs in order to perform a precise search according to video content. Experimental results demonstrate that the proposed algorithm can save 85% computational complexity on average, with negligible loss of coding efficiency.","1051-8215;1558-2205","","10.1109/TCSVT.2010.2045910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432965","Disparity estimation;motion estimation;multiview video coding (MVC)","Motion estimation;Video coding;Computational complexity;Motion analysis;Cameras;Educational programs;Costs;Encoding;Video compression;Streaming media","computational complexity;motion estimation;video coding","view-adaptive motion estimation;disparity estimation;low complexity multiview video coding;H.264/advanced video coding;computational complexity;macroblock","","58","","15","","","","","IEEE","IEEE Journals & Magazines"
"An adaptive frame complexity based rate quantization model for intra-frame rate control of High Efficiency Video Coding (HEVC)","L. Sun; O. C. Au; W. Dai; Y. Guo; R. Zou","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","","2012","","","1","6","An efficient and accurate R-Q model is greatly important for intra-frame rate control of the latest High Efficiency Video Coding (HEVC) standard. However, previous methods pay more attention to the gradient based rate quantization (R-Q) model for the intra-frame rate control. In this paper, we analyze the drawbacks of the gradient based frame complexity measure when applied different Quantization Parameters (QPs). Then we propose a novel edge based frame complexity measure using the Gaussian Gradient operator with properly selected parameters. In order to tackle the problems that the gradient based rate quantization model fails when using the different QPs, we propose an adaptive frame complexity based R-Q model for intra-frame rate control based on these two complexity measures. Simulations have been conducted based on HM6.2 which is the latest reference software of HEVC. Note that we may be the first to do this work, so we do not have the classical methods which have been implemented in HEVC to compare with. In this paper we implement the traditional gradient based and the Cauchy distribution based rate quantization model in HEVC and compare the performance with each other. Here we use bit rate mismatch ratio as the evaluation method. The simulation results show that by using our proposed scheme, up to 33.1% mismatch ratio reduction compared with the Cauchy distribution based model and up to 13% mismatch ratio reduction compared with the gradient based model for intra frames can be achieved.","","978-0-6157-0050-2978-1-4673-4863","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411913","","Complexity theory;Image edge detection;Bit rate;Adaptation models;Video coding;Quantization;Encoding","Gaussian processes;quantisation (signal);video coding","adaptive frame complexity based rate quantization model;intraframe rate control;high efficiency video coding;HEVC;R-Q model;gradient based frame complexity;quantization parameter;QP;Gaussian Gradient operator;reference software;Cauchy distribution based rate quantization model;HM6.2","","1","","12","","","","","IEEE","IEEE Conferences"
"Exploiting the shift property of structured LDPC codes for reduced-complexity sliced message passing based decoder design","X. Chen; C. Wang","Microsoft Corporation, 1 Microsoft Way Redmond, WA 98052; LSI Corporation, 1621 Barber Lane, Milpitas, CA 95035","2012 International Conference on Computing, Networking and Communications (ICNC)","","2012","","","471","475","The sliced message passing (SMP) scheme breaks the sequential tie of CN update and VN update and presents an efficient way of elevating the hardware utilization ratio and increasing throughput. However, the forward and backward permutation networks complicate the message routing and make the SMP-based decoder not amenable for codes with large CN degrees. By utilizing the shift property in some structured codes, we reduced the permutation network by almost 59% when compared with the state-of-art designs. Overall, this design technique will make SMP-based decoder more cost-efficient and applicable for the future communications and storage systems.","","978-1-4673-0009-4978-1-4673-0008-7978-1-4673-0007-0978-1-4673-0723","10.1109/ICCNC.2012.6167467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6167467","","Decoding;Parity check codes;Multiplexing;Registers;Message passing;Throughput;Random access memory","message passing;parity check codes;telecommunication network routing","structured LDPC codes;sliced message passing;forward permutation networks;backward permutation networks;message routing;SMP-based decoder;check node;variable node","","","","12","","","","","IEEE","IEEE Conferences"
"A Family of Erasure Correcting Codes with Low Repair Bandwidth and Low Repair Complexity","S. Kumar; A. Graell i Amat; I. Andriyanova; F. Brannstrom","NA; NA; NA; NA","2015 IEEE Global Communications Conference (GLOBECOM)","","2015","","","1","6","We present the construction of a new family of erasure correcting codes for distributed storage that yield low repair bandwidth and low repair complexity. The construction is based on two classes of parity symbols. The primary goal of the first class of symbols is to provide good fault tolerance, while the second class facilitates node repair, reducing the repair bandwidth and the repair complexity. We compare the proposed codes with other codes proposed in the literature.","","978-1-4799-5952-5978-1-4799-5951","10.1109/GLOCOM.2015.7417790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417790","","Maintenance engineering;Bandwidth;Complexity theory;Arrays;Fault tolerance;Fault tolerant systems","error correction codes;fault tolerance;reliability","repair complexity reduction;repair bandwidth reduction;node repair;fault tolerance;parity symbol;distributed storage;low repair bandwidth;low repair complexity;erasure correcting code family","","3","","8","","","","","IEEE","IEEE Conferences"
"Sphere Decoding Complexity Exponent for Decoding Full-Rate Codes Over the Quasi-Static MIMO Channel","J. Jalden; P. Elia","ACCESS Linnaeus Center, Signal Processing Lab, KTH Royal Institute of Technology, Stockholm, Sweden; Mobile Communications Department, EURECOM, Sophia Antipolis, France","IEEE Transactions on Information Theory","","2012","58","9","5785","5803","In the setting of quasi-static multiple-input multiple-output channels, we consider the high signal-to-noise ratio (SNR) asymptotic complexity required by the sphere decoding (SD) algorithm for decoding a large class of full-rate linear space-time codes. With SD complexity having random fluctuations induced by the random channel, noise, and codeword realizations, the introduced SD complexity exponent manages to concisely describe the computational reserves required by the SD algorithm to achieve arbitrarily close to optimal decoding performance. Bounds and exact expressions for the SD complexity exponent are obtained for the decoding of large families of codes with arbitrary performance characteristics. For the particular example of decoding the recently introduced threaded cyclic-division-algebra-based codes—the only currently known explicit designs that are uniformly optimal with respect to the diversity multiplexing tradeoff—the SD complexity exponent is shown to take a particularly concise form as a non-monotonic function of the multiplexing gain. To date, the SD complexity exponent also describes the minimum known complexity of any decoder that can provably achieve a gap to maximum likelihood performance that vanishes in the high SNR limit.","0018-9448;1557-9654","","10.1109/TIT.2012.2203581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216420","Complexity;diversity multiplexing tradeoff (DMT);large deviations;space-time codes;sphere decoding (SD)","Complexity theory;Signal to noise ratio;Maximum likelihood decoding;Multiplexing;Vectors;MIMO","","","","26","","52","","","","","IEEE","IEEE Journals & Magazines"
"Understanding Variable Code: Reducing the Complexity by Integrating Variability Information","D. Lüdemann; N. Asad; K. Schmid; C. Voges","NA; NA; NA; NA","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2016","","","312","322","Software product lines often use preprocessor statements as a basis for representing variability, which makes understanding the artifacts rather complex. An approach that has been proposed in the past to improve the understanding of code with preprocessor statements is formal concept analysis. This approach has been applied to a number of causes in reengineering. However, the lattices constructed by this approach can become rather large and complex. Hence, any approach that helps to reduce them can be beneficial to understanding the preprocessor-dependencies contained in the code. Here, we show how consistency analysis both within code variability and between code and a variability model can be used to reduce the complexity of a lattice, supporting the analysis of product-line code. We apply our approach to Linux, one of the largest open-source product lines, and analyze both multiple versions and different architectures. We show that our approach typically leads to reductions of the concept lattice and identify situations in which the savings can be rather significant. This leads to a reduction of any efforts for followup analysis or reverse engineering.","","978-1-5090-3806-0978-1-5090-3807","10.1109/ICSME.2016.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816477","software product line;linux;variability","Lattices;Data preprocessing;Complexity theory;Context;Formal concept analysis;Linux;Analytical models","formal concept analysis;Linux;public domain software;software architecture;software product lines","variable code;complexity reduction;software product lines;preprocessor statements;code understanding;formal concept analysis;reengineering;preprocessor-dependencies;consistency analysis;code variability;product-line code analysis;Linux;open-source product lines;architectures;concept lattice;followup analysis;reverse engineering","","","","41","","","","","IEEE","IEEE Conferences"
"Low Complexity Blind Detection Scheme for Polar Codes: A Segmented CRC Approach","X. Wang; K. Qin; Z. Zhu; Z. Zhang","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China","2018 10th International Conference on Wireless Communications and Signal Processing (WCSP)","","2018","","","1","5","Recently, polar codes are selected by 3GPP as the channel coding method for control information in the 5-th generation mobile communications (5G). In 5G, to reduce the signaling overhead and avoid unnecessary post-processing, each user equipment (UE) is required to use blind detection method to identify which control messages are transmitted to it. Therefore, how to implement blind detection of polar coded frames with limited computational complexity, latency, and power consumption has attracted a lot of research interests. To address this issue, we propose a low-complexity blind detection scheme based on segmented cyclic redundancy check (CRC). Besides using the radio network temporary identifier (RNTI) as the frozen bit pattern, it places several CRC segments at some breaking points of every coding frame. Since now the UE can perform CRC check much earlier, early termination occurs with high probability when other UEs' control messages are received, thus reducing the decoding complexity and power consumption. Numerical results show that the proposed scheme provides good blind detection performance with much lower decoding complexity.","2472-7628;2325-3746","978-1-5386-6119-2978-1-5386-6118-5978-1-5386-6120","10.1109/WCSP.2018.8555729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8555729","","Decoding;Complexity theory;Measurement;Long Term Evolution;Detectors;Sorting;3GPP","3G mobile communication;5G mobile communication;block codes;channel coding;computational complexity;cyclic redundancy check codes;decoding;error detection codes;linear codes;radio networks","control information;5G;UE;blind detection method;polar coded frames;power consumption;low-complexity blind detection scheme;segmented cyclic redundancy check;radio network temporary identifier;CRC segments;coding frame;good blind detection performance;low complexity blind detection scheme;polar codes;segmented CRC approach;channel coding method;limited computational complexity;decoding complexity reduction;3GPP;signaling overhead reduction;user equipment;UE control messages","","","","12","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Multiple Error Correcting Architecture Using Novel Cross Parity Codes Over GF$(2^{m})$","M. Poolakkaparambil; J. Mathew; A. M. Jabir; D. K. Pradhan","Department of Computing and Communication Technologies, Oxford Brookes University, Oxford, U.K.; Department of Computer Science, University of Bristol, Bristol, U.K.; Department of Computing and Communication Technologies, Oxford Brookes University, Oxford, U.K.; Department of Computer Science, University of Bristol, Bristol, U.K.","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2015","23","8","1448","1458","This paper presents a novel low-complexity cross parity code, with a wide range of multiple bit error correction capability at a lower overhead, for improving the reliability in circuits over GF(2m). For an m input circuit, the proposed scheme can correct m ≤ Dw≤ 3m/2-1 multiple error combinations out of all the possible 2m- 1 errors, which is superior to many existing approaches. From the mathematical and practical evaluations, the best case error correction is m/2 bit errors. Tests on 80-bit parallel and, for the first time, on 163-bit Federal Information Processing Standard/National Institute of Standards and Technology (FIPS/NIST) standard word-level Galois field (GF) multipliers, suggest that it requires only 106% and 170% area overheads, respectively, which is lower than the existing approaches, while error injection-based behavioral analysis demonstrates its wider error correction capability.","1063-8210;1557-9999","","10.1109/TVLSI.2014.2341631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876030","Application specific integrated circuits (ASICs);Bose–Choudhury–Hocquenghem (BCH) code;error correction circuit (ECC);Galois field (GF);multiple event upsets (MEUs);radiation hardening;simple parity;single event upsets (SEUs);VLSI;Application specific integrated circuits (ASICs);Bose–Choudhury–Hocquenghem (BCH) code;error correction circuit (ECC);Galois field (GF);multiple event upsets (MEUs);radiation hardening;simple parity;single event upsets (SEUs);VLSI","Error correction;Decoding;Error correction codes;Standards;Cryptography;Computer architecture;Encoding","circuit reliability;error correction;parity check codes","low-complexity multiple error correcting architecture;cross parity codes;circuit reliability;FIPS/NIST standard;Federal Information Processing Standard;National Institute of Standards and Technology;word-level Galois field multiplier;GF multiplier;error correction capability","","1","","21","","","","","IEEE","IEEE Journals & Magazines"
"Design of irregular LDPC codes with optimized performance-complexity tradeoff","B. Smith; M. Ardakani; W. Yu; F. R. Kschischang","University of Toronto; University of Alberta; University of Toronto; University of Toronto","IEEE Transactions on Communications","","2010","58","2","489","499","The optimal performance-complexity tradeoff for error-correcting codes at rates strictly below the Shannon limit is a central question in coding theory. This paper proposes a numerical approach for the minimization of decoding complexity for long-block-length irregular low-density parity-check (LDPC) codes. The proposed design methodology is applicable to any binary-input memoryless symmetric channel and any iterative message-passing decoding algorithm with a parallel-update schedule. A key feature of the proposed optimization method is a new complexity measure that incorporates both the number of operations required to carry out a single decoding iteration and the number of iterations required for convergence. This paper shows that the proposed complexity measure can be accurately estimated from a density-evolution and extrinsic-information transfer chart analysis of the code. A sufficient condition is presented for convexity of the complexity measure in the variable edge-degree distribution; when it is not satisfied, numerical experiments nevertheless suggest that the local minimum is unique. The results presented herein show that when the decoding complexity is constrained, the complexity-optimized codes significantly outperform threshold-optimized codes at long block lengths, within the ensemble of irregular codes.","0090-6778;1558-0857","","10.1109/TCOMM.2010.02.080193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5407615","Convex optimization, extrinsic-information transfer (EXIT) charts;decoding complexity, low-density parity-check (LDPC) codes","Parity check codes;Design optimization;Iterative decoding;Error correction codes;Design methodology;Iterative methods;Iterative algorithms;Scheduling algorithm;Optimization methods;Convergence","block codes;computational complexity;decoding;error correction codes;information theory;message passing;parity check codes","irregular LDPC codes;performance-complexity tradeoff optimization;error-correcting codes;Shannon limit;decoding complexity;long-block-length irregular low-density parity-check;parallel-update schedule;single decoding iteration;threshold-optimized codes","","28","","32","","","","","IEEE","IEEE Journals & Magazines"
"Localization-Based Polar Code Construction with Sublinear Complexity","R. Zhang; Y. Ge; H. Saber; W. Shi; X. Shen","NA; NA; NA; NA; NA","2018 IEEE International Conference on Communications (ICC)","","2018","","","1","6","In this paper, a localization-based polar construction method is proposed to directly find the set of synthetic channels for information bits given a code configuration. Taking advantage of the partial order of polar codes, only a small number of synthetic channels need to be ordered, which scales as O(N/ log23/2 N), resulting in a sublinear complexity to construct a polar code. Specifically, a practical method is put forward first to fast construct a group-based partial order diagram. A local area in the diagram with adaptive boundaries is then identified. By ordering the synthetic channels within the local area and combining selected ones with all the synthetic channels beyond the local area, the final set of synthetic channels for information bits are determined. Simulation results demonstrate how to adapt the boundary settings to different rate matching schemes and code configurations, and validate the effectiveness of the proposed method compared with the density evolution based methods.","1938-1883","978-1-5386-3180-5978-1-5386-3181","10.1109/ICC.2018.8422461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8422461","","Reliability;Indexes;Decoding;Complexity theory;Encoding;Signal to noise ratio;Market research","channel coding;computational complexity","group-based partial order diagram;localization-based polar construction method;localization-based polar code construction;density evolution based methods;synthetic channels","","","","12","","","","","IEEE","IEEE Conferences"
"Low complexity decoding of variable length source-channel codes","H. Mercier; A. Wasae; F. Labeau","Université de Neuchâtel Institute of Computer Science 2000 Neuchâtel, Switzerland; McGill University Electrical and Computer Engineering Montréal (Québec) Canada H3A 0G4; Université de Neuchâtel Institute of Computer Science 2000 Neuchâtel, Switzerland","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2014","","","1911","1915","Soft source decoders, in conjunction with error correcting channel codes, can be used to improve the error resilience of digital communication systems based on variable length codes. In this paper, we present a novel approach to reduce the complexity of maximum a posteriori variable length decoders implemented on a bit-symbol trellis. The decoding algorithm is implemented in a narrow corridor along the trellis diagonal to reduce the decoder complexity. Furthermore, by periodically adjusting the corridor boundaries, a significant reduction in complexity is achieved at the price of a small degradation in decoding performance.","1520-6149;2379-190X","978-1-4799-2893","10.1109/ICASSP.2014.6853931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853931","Exponential Golomb Code;variable length joint source-channel decoder;maximum a posteriori algorithm;bit-symbol trellis;corridor decoding","Decoding;Complexity theory;Standards;Signal to noise ratio;Joints;Redundancy;Synchronization","combined source-channel coding;decoding;error correction codes;trellis codes;variable length codes","variable length source-channel codes;low complexity decoding;soft source decoders;error correcting channel codes;error resilience;digital communication;maximum a posteriori decoders;bit-symbol trellis;narrow corridor;trellis diagonal;corridor boundaries","","","","17","","","","","IEEE","IEEE Conferences"
"Low-Complexity Soft-Decoding Algorithms for Reed–Solomon Codes—Part II: Soft-Input Soft-Output Iterative Decoding","J. Bellorado; A. Kavcic; M. Marrow; L. Ping","Link-A-Media Devices, Santa Clara; The Department of Electrical Engineering, University of Hawaii, Honolulu; Link-A-Media Devices, Santa Clara; The Department of Electronic Engineering, City University of Hong Kong","IEEE Transactions on Information Theory","","2010","56","3","960","967","In this paper, we present a practical approach to the iterative decoding of Reed-Solomon (RS) codes. The presented methodology utilizes an architecture in which the output produced by steps of belief-propagation (BP) is successively applied to a legacy decoding algorithm. Due to the suboptimal performance of BP conducted on the inherently dense RS parity-check matrix, a method is first provided for the construction of reduced-density, binary, parity-check equations. Iterative decoding is then conducted utilizing a subset of a redundant set of parity-check equations to minimize the number of connections into the least-reliable bits. Simulation results show that performance comparable to (and exceeding) the best known practical RS decoding techniques is achievable with the presented methodology. The complexity of the proposed algorithm is significantly lower than these existing procedures and permits a practical implementation in hardware.","0018-9448;1557-9654","","10.1109/TIT.2009.2039091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5429125","Reed-Solomon (RS) codes;reduced-density parity-check matrices;belief propagation","Iterative algorithms;Iterative decoding;Parity check codes;Maximum likelihood decoding;Equations;Sparse matrices;Reed-Solomon codes;Hardware;Iterative methods;Belief propagation","iterative decoding;parity check codes;Reed-Solomon codes","low complexity soft decoding algorithms;Reed-Solomon codes;belief propagation;soft-input soft-output iterative decoding;legacy decoding algorithm;RS parity check matrix;reduced-density parity check equations;RS codes","","11","","16","","","","","IEEE","IEEE Journals & Magazines"
"LDPC codes for low-complexity analog decoders","Hao Zheng; Xuhui Ding; Z. Wang; Yimin Li; Haikun Luo","School of Information and Electronics, Beijing Institute of Technology, 100081, China; School of Information and Electronics, Beijing Institute of Technology, 100081, China; School of Information and Electronics, Beijing Institute of Technology, 100081, China; Beijing Institute of Remote Sensing Equipment, China; Science and Technology on Electronic Information Control Lab, 610036, Chengdu, China","2015 10th International Conference on Communications and Networking in China (ChinaCom)","","2015","","","167","171","Compared with their digital counterparts, LDPC analog decoders own higher power efficiency which is suitable for low-power-consumptions applications. However, the high designing complexity of analog decoders with long block length has impeded their real-world applications. This paper presents a method to construct LDPC codes that can lower the designing complexity significantly by using identical models to form the whole decoder, as well as rules that codes should satisfy. Simulation results show that the proposed LDPC codes could lower the designing complexity with little performance lose and have better error-correcting performance than codes in CCSDS standard.","","978-1-4799-8795","10.1109/CHINACOM.2015.7497931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497931","LDPC codes;Low-complexity analog decoders;Factor graphs;Identical building block","Decoding;Parity check codes;Complexity theory;High definition video;Optimization;Logic gates;Wires","decoding;error correction codes;parity check codes","LDPC analog decoders;designing complexity;block length;error-correcting performance;low-complexity analog decoders","","","","16","","","","","IEEE","IEEE Conferences"
"Low-Complexity List Successive-Cancellation Decoding of Polar Codes Using List Pruning","J. Chen; Y. Fan; C. Xia; C. Tsui; J. Jin; K. Chen; B. Li","NA; NA; NA; NA; NA; NA; NA","2016 IEEE Global Communications Conference (GLOBECOM)","","2016","","","1","6","The performance of List Successive-Cancellation Decoding (LSCD) of Polar Codes with large list size have exceeded that of Turbo codes and Low-Density Parity-Check codes. However, large list size results in huge computation complexity and this limits the applicability of LSCD in high-throughput and power- sensitive applications. In this work, a low complexity design for LSCD with large list size based on list pruning is proposed. In particular, the property of the relative path metric (RPM) of each list candidate with respect to that of the most-likely candidate is investigated. It is found that the correct candidate has a low possibility of having a large value of RPM and based on this property, a list pruning method and the corresponding low-complexity LSCDs are proposed. From the simulation results, as compared to the conventional LSCD, the proposed LSCDs have negligible performance loss while the computation complexity is reduced by more than 80%. In addition, the proposed design is hardware-friendly and easily adaptable to the existing LSCDs hardware architectures.","","978-1-5090-1328-9978-1-5090-1329","10.1109/GLOCOM.2016.7841969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841969","","Measurement;Complexity theory;Standards;Hardware;Maximum likelihood decoding;Vegetation","codes;computational complexity;decoding","low-complexity list successive-cancellation decoding;polar code;list successive-cancellation decoding;LSCD;turbo code;low-density parity-check code;computational complexity;relative path metric;RPM;list pruning method","","3","","23","","","","","IEEE","IEEE Conferences"
"A low complexity video coding for combining RFID and video surveillance with padding based DVC","T. C. Lei; S. Chern","Center for General Education, National Kaohsiung First University of Science and Technology, Taiwan; Department of Electrical Engineering, Tamkang University, Taipei County, Taiwan","2010 International Symposium on Next Generation Electronics","","2010","","","215","218","Radio frequency identification (RFID) tag has been extensively used in diverse applications, recently. For instance, we may use it to track freights on a warehouse, to catch a thief for lost luggage and to rapidly locate lost children or passengers who fail to arrive on time at the departure gates of major airport. It can also be employed for transmitting multimedia content (especially for video). Presently, many research works focus on designing the RFID tag with high transmission data rate. In this paper we present a new practical scheme, it integrates the video surveillance into RFID tag at range of 250 kbps data rate for video data transmission. We believe that has not been addressed before in the literature. Here the Distributed Video Coding (DVC) scheme is integrated into RFID tag, where the so-called padding-based DVC scheme, proposed by us recently, is deployed. It is known that the padding-based DVC scheme has the advantages of requiring very low complexity and achieving low bit rate. Via computer simulations we show that the proposed scheme can easily work at data rate less than 250 kbps, and achieve better performance (PSNR) than the conventional approaches.","2378-8593;2378-8607","978-1-4244-6694-8978-1-4244-6693-1978-1-4244-6692","10.1109/ISNE.2010.5669158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5669158","Distributed Video Coding;RFID and Video Surveillance;Padding Based Distributed Video Coding","Automatic voltage control;Complexity theory;Indexes;Performance evaluation;Radiofrequency identification;PSNR","multimedia communication;radiofrequency identification;video coding;video surveillance","low complexity video coding;RFID;video surveillance;padding;DVC;radio frequency identification;multimedia content","","1","","13","","","","","IEEE","IEEE Conferences"
"Generation of Binary Sequences Having Better Odd Correlation and Linear Complexity Property Compared to Gold Codes for use in Global Navigation Satellites Systems (GNSS) Applications","D. Dharmappa; M. V. Mandi; S. Ramesh","Department of Electronics and Communication Engineering (ECE), Sri Siddhartha Academy of Higher Education (SSAHE), Tumkur, Karnataka, India; Department of Electronics and Communication Engineering (ECE), Dr. Ambedkar Institute of Technology, Bengaluru, India; Department of Electronics and Communication Engineering (ECE), Dr. Ambedkar Institute of Technology, Bengaluru, India","2017 International Conference on Current Trends in Computer, Electrical, Electronics and Communication (CTCEEC)","","2017","","","155","160","Often in GNSS systems due to environmental issues the desired signal to be received may be degraded and attenuated due to multipath or some other reasons. Also the binary sequences used in GNSS systems are having low linear complexity and hence are easy to attack. To overcome above said problems it is essential to propose a method to generate binary sequences derived from chaotic real sequences which are found to have better odd correlation property than the GPS Ll C/A Signal and hence are more resistant to environment variations such as multipath or interference. The binary sequences generated from the proposed method are found to have very large linear complexity as compared with GPS Ll C/A Signals. This paper also presents the odd correlation properties of 32 GPS Ll C/A sequences, and compared with the odd correlation properties of the generated sequences in detail.","","978-1-5386-3243-7978-1-5386-3242-0978-1-5386-3240-6978-1-5386-3241-3978-1-5386-3244","10.1109/CTCEEC.2017.8455026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455026","Odd Correlation;Linear Complexity;Chaotic Map Binary Sequences;CDMA;GNSS","Correlation;Complexity theory;Global navigation satellite system;Global Positioning System;Chaotic communication;Multiaccess communication;Histograms","binary sequences;computational complexity;correlation methods;electromagnetic wave attenuation;Global Positioning System","GNSS systems applications;environmental issues;chaotic real sequences;GPS Ll C-A sequences;global navigation satellites systems applications;linear complexity property;odd correlation property;binary sequences;low linear complexity","","","","22","","","","","IEEE","IEEE Conferences"
"Low-Complexity Shorten MSR Code for Limited Bandwidth Systems","K. Li; S. Gu; Y. Wang; Q. Zhang","Communication Engineering Research Center, Harbin Institute of Technology, Shenzhen, Guangdong, China; Communication Engineering Research Center, Harbin Institute of Technology, Shenzhen, Guangdong, China; Communication Engineering Research Center, Harbin Institute of Technology, Shenzhen, Guangdong, China; Communication Engineering Research Center, Harbin Institute of Technology, Shenzhen, Guangdong, China","2018 IEEE/CIC International Conference on Communications in China (ICCC)","","2018","","","882","887","Regenerating code is a valid approach to improve the reliability and availability in distributed storage system (DSS), considered as the optimal tradeoff between repair bandwidth and storage amount. Nevertheless, in some realistic practical network system, due to the limited nodes and bandwidth resources, redundant bandwidth consumption and assisted nodes connections will be essential indeed. Aim to the problem of regenerating code applying to limited resource systems, we propose a novel distributed coding scheme, termed shorten minimum storage regenerating (sMSR) code with two critical targets, unit storage cost and unit repair bandwidth. The construction of sMSR is also given by removing some information bits whose mother code is generated by product matrix in encoding process. This code could cut down the number of nodes occupied and minimize unit bandwidth cost for repairing failed nodes. Additionally, we implement binary addition and shift implementable convolution (BASIC) on our code to decrease the computational complexity. It demonstrates that our coding method outperforms existing conventional RGCs with fewer bandwidth overhead and higher repair efficiency.","2377-8644","978-1-5386-7005-7978-1-5386-7004-0978-1-5386-7006","10.1109/ICCChina.2018.8641182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641182","Distributed storage system;Limited bandwidth resources;Regenerating codes;Shorten codes;BASIC","Bandwidth;Maintenance engineering;Complexity theory;Encoding;Mathematical model;Convolution;Indexes","binary codes;computational complexity;convolutional codes;matrix algebra;reliability","realistic practical network system;bandwidth resources;redundant bandwidth consumption;resource systems;unit storage cost;unit repair bandwidth;unit bandwidth cost;binary addition;shift implementable convolution;coding method;low-complexity shorten MSR code;limited bandwidth systems;reliability;distributed storage system;optimal tradeoff;node connections;information bits;bandwidth overhead;repair efficiency;regenerating code problem;shorten minimum storage regenerating code;product matrix;encoding process;computational complexity","","","","13","","","","","IEEE","IEEE Conferences"
"Reduced complexity multi-view video coding scheme for 2D camera arrays","A. Avci1; J. De Cock; R. Beernaert; J. De Smet; L. Bogaert; Y. Meuret; H. Thienpont; P. Lambert; H. De Smet","Ghent University, Department of Electronics and Information Systems, Centre for Microsystems Technology, Technologiepark 914, 9052, Belgium; Ghent University-IBBT, Department of Electronics and Information Systems, Multimedia Lab, Gaston Crommenlaan 8 bus 201, B-9050 Ledeberg, Belgium; Ghent University, Department of Electronics and Information Systems, Centre for Microsystems Technology, Technologiepark 914, 9052, Belgium; Ghent University, Department of Electronics and Information Systems, Centre for Microsystems Technology, Technologiepark 914, 9052, Belgium; Vrije Universiteit Brussel, Department of Applied Physics and Photonics, Brussels Photonics Team, Pleinlaan 2, 1050, Belgium; Vrije Universiteit Brussel, Department of Applied Physics and Photonics, Brussels Photonics Team, Pleinlaan 2, 1050, Belgium; Vrije Universiteit Brussel, Department of Applied Physics and Photonics, Brussels Photonics Team, Pleinlaan 2, 1050, Belgium; Ghent University-IBBT, Department of Electronics and Information Systems, Multimedia Lab, Gaston Crommenlaan 8 bus 201, B-9050 Ledeberg, Belgium; Ghent University, Department of Electronics and Information Systems, Centre for Microsystems Technology, Technologiepark 914, 9052, Belgium","2011 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)","","2011","","","1","4","As the number of views comprised in multi-view videos increases, some challenging problems emerge. Besides the bandwidth problems caused by the huge data flow, the calculation power needed by the multi-view encoder is an even higher burden than that of a single view encoder. In this paper, a complexity efficient way to encode a single-time instant of 5×3 view frames is presented. Some of the P frames known from the traditional encoding schemes have been replaced by a new type of frame called the D frame, in which the disparity vector of a block in a view can be derived from the other views due to the strong geometrical correspondence existing between adjacent views. Experimental results show that 20.2% complexity gain is achieved without compromising quality and bit-rate by wisely selecting threshold values at different QPs.","2161-203X;2161-2021;2161-2021","978-1-61284-162-5978-1-61284-161-8978-1-61284-160","10.1109/3DTV.2011.5877178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5877178","Multi-view video coding;camera array;disparity vector;encoder;complexity","Complexity theory;Voltage control;Three dimensional displays;Video coding;Cameras;Encoding","video cameras;video coding","reduced complexity multiview video coding scheme;2D camera arrays;multiview encoder;disparity vector","","","","13","","","","","IEEE","IEEE Conferences"
"An Encoding Algorithm of Triply Extended Reed–Solomon Codes With Asymptotically Optimal Complexities","S. Lin","CAS Key Laboratory of Electro-magnetic Space Information, School of Information Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Communications","","2018","66","8","3235","3244","In this paper, we devise a fast encoding algorithm for triply extended Reed-Solomon codes. The proposed approach requires approximately two XORs per bit, which improves the prior result of three XORs per bit established by certain maximum distance separable (MDS) array codes. We also prove that, for MDS codes with two and three parities, the scheduling algorithms require at least two XORs per bit. To the best of our knowledge, this is the first provable scheduling algorithm for the triple-parity MDS codes to approach the theoretical lower bounds. The implementation with SIMD instructions is provided. The simulations show that the proposed approach is competitive, as compared with other cutting edge implementations.","0090-6778;1558-0857","","10.1109/TCOMM.2017.2737441","CAS Pioneer Hundred Talents Program; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004526","Reed-Solomon codes;algorithm design and anlaysis;parallel processing","Encoding;Complexity theory;Scheduling algorithms;Algorithm design and analysis;Decoding;Generators;Approximation algorithms","linear codes;Reed-Solomon codes;telecommunication scheduling","provable scheduling algorithm;triple-parity MDS codes;asymptotically optimal complexities;fast encoding algorithm;triply extended Reed-Solomon codes;maximum distance separable array codes;scheduling algorithms;XOR","","","","42","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-Complexity Iterative Receiver for Improving the IEEE 802.15.7 Convolutional-Coded Color Shift Keying Mode","Z. Babar; C. Zhu; H. V. Nguyen; P. Botsinis; D. Alanis; D. Chandra; S. X. Ng; L. Hanzo","School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.; School of Electronics and Computer Science, University of Southampton, Southampton, U K.","IEEE Communications Letters","","2017","21","9","2005","2008","In this letter, we conceive novel symbol-based color-shift keying (CSK)-aided concatenated coding schemes, which provide attractive performance gains over the comparable bit-based systems. Quantitatively, our 4 CSK-aided and 16 CSK-aided symbol-based concatenated systems require 0.8 and 0.45 dB lower SNR than the equivalent bit-based schemes. In terms of decoding complexity, the 4 CSK-aided and 16 CSK-aided systems reduce the decoding complexity by 67% and 33%, respectively. We have also analyzed the convergence behavior of our system with the aid of non-binary extrinsic information transfer charts adapted for symbol-based iterative CSK-assisted systems.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2017.2705707","European Research Council under the Advanced Fellow Grant; Royal Society’s Wolfson Research Merit Award; Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932976","Visible light communication;color-shift keying;iterative decoding;exit charts","Decoding;Iterative decoding;Convergence;Signal to noise ratio;Color;Detectors;Optical transmitters","concatenated codes;convolutional codes;free-space optical communication;iterative decoding;optical receivers","iterative receiver;IEEE 802.15.7;convolutional-coded color shift keying mode;symbol-based color-shift keying-aided concatenated coding schemes;4 CSK-aided symbol-based concatenated systems;16 CSK-aided symbol-based concatenated systems;bit-based schemes;symbol-based iterative CSK-assisted systems;visible light communication","","2","","10","","","","","IEEE","IEEE Journals & Magazines"
"A robust R-FFAST framework for computing a k-sparse n-length DFT in O(k log n) sample complexity using sparse-graph codes","S. Pawar; K. Ramchandran","Dept. of EECS, University of California, Berkeley, USA; Dept. of EECS, University of California, Berkeley, USA","2014 IEEE International Symposium on Information Theory","","2014","","","1852","1856","The Fast Fourier Transform (FFT) is the most efficiently known way to compute the Discrete Fourier Transform (DFT) of an arbitrary n-length signal, and has a computational complexity of O(n log n). If the DFT X⃗ of the signal x⃗ has only k non-zero coefficients (where k <; n), can we do better? In [1], we presented a novel FFAST (Fast Fourier Aliasing-based Sparse Transform) algorithm that cleverly induces sparse graph codes in the DFT domain, via a Chinese-Remainder-Theorem (CRT)-guided sub-sampling operation of the time-domain samples. The resulting sparse graph code is then exploited to devise a simple and fast iterative onion-peeling style decoder that computes an n length DFT of a signal using only O(k) time-domain samples and O(k log k) computations, in the absence of any noise. In this paper, we extend the FFAST framework of [1] to the case where the time-domain samples are corrupted by white Gaussian noise. In particular, we show that the extended noise robust algorithm R-FFAST computes an n-length k-sparse DFT X⃗ using O(k log n)1 noise-corrupted time-domain samples, in O(n log n) computations2. While our theoretical results are for signals with a uniformly random support of the non-zero DFT coefficients and additive white Gaussian noise, we provide simulation results which demonstrates that the R-FFAST algorithm performs well even for signals like MR images, that have an approximately sparse Fourier spectrum with a non-uniform support for the dominant DFT coefficients.","2157-8117;2157-8095","978-1-4799-5186","10.1109/ISIT.2014.6875154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875154","","Discrete Fourier transforms;Time-domain analysis;Noise;Computer architecture;Signal processing algorithms;Vectors;Information theory","AWGN;computational complexity;discrete Fourier transforms;fast Fourier transforms;graph theory;sampling methods;signal processing","robust R-FFAST framework;k-sparse n-length DFT;O(k log n) sample complexity;sparse-graph codes;discrete Fourier transforms;arbitrary n-length signal;non-zero coefficients;fast Fourier aliasing-based sparse transform;chinese-remainder-theorem;CRT-guided sub-sampling operation;time-domain samples;iterative onion-peeling style decoder;O(k log k) computations;additive white Gaussian noise;Fourier spectrum","","4","","17","","","","","IEEE","IEEE Conferences"
"Performance complexity of raptor codes in TCP/IP-based wireless networks","F. Mokesioluwa; M. Mzyece; G. Noel","French South African Institute of Technology (F'SATI), Department of Electrical Engineering, Tshwane University of Technology, Private Bag. Box X680, Pretoria 0001, South Africa; French South African Institute of Technology (F'SATI), Department of Electrical Engineering, Tshwane University of Technology, Private Bag. Box X680, Pretoria 0001, South Africa; French South African Institute of Technology (F'SATI), Department of Electrical Engineering, Tshwane University of Technology, Private Bag. Box X680, Pretoria 0001, South Africa","2013 IEEE International Conference on Industrial Technology (ICIT)","","2013","","","1365","1370","This paper assesses the enhancement of TCP-SACK using a new error correction mechanism. Performance metrics like packet recovery rate and throughput are applied as a function of packet loss rate. The raptor standalone module is built and implemented in ns2. While our results show higher throughputs and recovery rates, we demonstrate that this comes at the expense of increased decoding complexity. Three additional performance metrics are used to quantify the decoding complexity: row swap operations, delay and redundancy rate. Each was measured as a function of symbol packet length. The results show an increase in redundancy rate as the block length increases. When compared with Luby Transform (LT) codes, we see a significant reduction in the redundancy introduced by raptor codes. The same can be said about the delay involved in the process. It also increased as the packet symbol length increased. Likewise, with larger packet length, a considerably larger number of row swap operations will be required.","","978-1-4673-4569-9978-1-4673-4567-5978-1-4673-4568","10.1109/ICIT.2013.6505871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505871","","Decoding;Forward error correction;Redundancy;Encoding;Throughput;Receivers;Delays","decoding;delays;error correction codes;network coding;radio networks;transport protocols","performance complexity;raptor code;TCP-IP-based wireless network;TCP-SACK enhancement;error correction mechanism;packet recovery rate;packet loss rate;raptor standalone module;ns2 implementation;decoding complexity;row swap operation;delay;redundancy rate;block length;Luby transform code;LT;packet symbol length","","1","","14","","","","","IEEE","IEEE Conferences"
"Block-orthogonal space-time codes with decoding complexity reduction","T. P. Ren; Y. L. Guan; C. Yuen; E. Y. Zhang","National University of Defense Technology, Changsha 410073, China; Nanyang Technological University, Singapore 639798; Institute for Information Research, Singapore 119613; National University of Defense Technology, Changsha 410073, China","2010 IEEE 11th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2010","","","1","5","Conventional approaches to high-rate space-time codes (STC) design focus on achieving maximum diversity and spatial multiplexing gains, hence they always have high decoding complexity. In this paper, we propose and construct a new class of STC, called block-orthogonal STC (BOSTC), which has a simplified ML (maximum likelihood) or near-ML decoding structure. We demonstrate the tradeoff between decoding complexity and decoding performance. Results show that due to the decoding complexity reduction, our proposed BOSTC can outperform the previously known codes when keeping the decoding complexity to be the same.","1948-3252;1948-3244;1948-3244","978-1-4244-6991-8978-1-4244-6990-1978-1-4244-6989","10.1109/SPAWC.2010.5671091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5671091","","Complexity theory;Signal to noise ratio","maximum likelihood decoding;orthogonal codes;space-time block codes","block-orthogonal space-time codes;decoding complexity reduction;maximum likelihood;decoding performance","","2","","14","","","","","IEEE","IEEE Conferences"
"Low-Complexity Hyperspectral Image Coding Using Exogenous Orthogonal Optimal Spectral Transform (OrthOST) and Degree-2 Zerotrees","M. Barret; J. Gutzwiller; M. Hariti","École Supérieure d'Électricité (SUPELEC), Information, Multimodality, and Signal Team, Metz, France; École Supérieure d'Électricité (SUPELEC), Information, Multimodality, and Signal Team, Metz, France; École Supérieure d'Électricité (SUPELEC), Information, Multimodality, and Signal Team, Metz, France","IEEE Transactions on Geoscience and Remote Sensing","","2011","49","5","1557","1566","We introduce a low-complexity codec for lossy compression of hyperspectral images. These images have two kinds of redundancies: 1) spatial; and 2) spectral. Our coder is based on a compression scheme consisting in applying a 2-D discrete wavelet transform (DWT) to each component and a linear transform between components to reduce, respectively, spatial and spectral redundancies. The DWT used is the Daubechies 9/7. However, the spectral transform depends on the spectrometer sensor and the kind of images to be encoded. It is calculated once and for all on a set of images (the learning basis) from (only) one sensor, thanks to Akam Bita et al. 's OrthOST algorithm that returns an orthogonal spectral transform, whose optimality in high-rate coding has been recently proved under mild conditions. The spectral transform obtained in this way is applied to encode other images from the same sensor. Quantization and entropy coding are then achieved with a well-suited extension to hyperspectral images of the Said and Pearlman's SPIHT algorithm. Comparisons with a JPEG2000 codec using the Karhunen-Loève transform (KLT) to reduce spectral redundancy show good performance for our codec.","0196-2892;1558-0644","","10.1109/TGRS.2010.2083671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5641608","Compression;hyperspectral images;remote sensing;SPIHT;transform coding;zerotree coder","Codecs;Image coding;Hyperspectral imaging;Discrete wavelet transforms;Bit rate;Redundancy","discrete wavelet transforms;geophysical image processing;image coding;trees (mathematics)","low-complexity hyperspectral image coding;exogenous orthogonal optimal spectral transform;degree-2 zerotrees;spatial redundancy;spectral redundancy;2D discrete wavelet transform;linear transform;spectrometer sensor;OrthOST algorithm;entropy coding;SPIHT algorithm;Karhunen-Loève transform","","5","","27","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity LDPC Coding Scheme for Channels With Phase Slips","L. Schmalen","Alcatel-Lucent Bell Labs, Stuttgart, Germany","Journal of Lightwave Technology","","2015","33","7","1319","1325","We propose low-complexity coding schemes that are resilient against sporadic phase slip events. These schemes are based on a new phase-slip transparent construction of LDPC codes, called block symmetric codes and on outer differential coding. The slip transparent LDPC codes correct single bit error events but are transparent to phase slips. The phase slips, which correspond to runs of bit errors, are corrected by the outer coding part of the receiver. We present practical binary and nonbinary code constructions for multiple modulation formats, and evaluate the proposed scheme by means of simulation examples.","0733-8724;1558-2213","","10.1109/JLT.2014.2377493","Bundesministerium für Bildung und Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974985","Error correction codes;Iterative decoding;Lowdensity parity-check codes;Phase Slips;Error correction codes;iterative decoding;low-density parity-check codes;phase slips","Parity check codes;Decoding;Encoding;Phase shift keying;Forward error correction;Receivers","binary codes;block codes;modulation;parity check codes;telecommunication channels","low-complexity LDPC coding scheme;sporadic phase slip;block symmetric code;differential coding;slip transparent LDPC code construction;bit error event;multiple modulation format;channel model;binary code construction","","7","","19","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity encoding of binary quasi-cyclic codes based on Galois Fourier transform","L. Tang; Q. Huang; Z. Wang; Z. Xiong","School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; School of Electronic and Information Engineering, Beihang University, Beijing, China, 100191; Dept of ECE, Texas A&amp;M University, College Station, TX, USA, 77843","2013 IEEE International Symposium on Information Theory","","2013","","","131","135","This paper presents a novel low-complexity encoding algorithm for binary quasi-cyclic (QC) codes based on matrix transformation. First, a message vector is encoded into a transformed codeword in the transform domain. Then, the transmitted codeword is obtained from the transformed codeword by the inverse Galois Fourier transform. Moreover, a simple and fast mapping is devised to post-process the transformed codeword such that the transmitted codeword is binary as well. The complexity of our proposed encoding algorithm is less than ek(n-k)log<sub>2</sub> e+ne(log<sup>2</sup><sub>2</sub> e+log<sub>2</sub> e)+ n/2 elog<sup>3</sup><sub>2</sub> e bit operations for binary codes. This complexity is much lower than its traditional complexity 2e<sup>2</sup>(n - k)k. In the examples of encoding the binary (4095, 2016) and (15500, 10850) QC codes, the complexities are 12.09% and 9.49% of those of traditional encoding, respectively.","2157-8117;2157-8095","978-1-4799-0446","10.1109/ISIT.2013.6620202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620202","","Encoding;Vectors;Generators;Parity check codes;Fourier transforms;Computational complexity","binary codes;cyclic codes;encoding;Fourier transforms","binary quasicyclic codes;low-complexity encoding algorithm;matrix transformation;message vector;transformed codeword;transform domain;transmitted codeword;inverse Galois Fourier transform","","1","","19","","","","","IEEE","IEEE Conferences"
"Asynchronous Space Time Block Codes: Low complexity decoding methods","M. Nahas; A. Saadani; R. Hatoum","Orange Labs, 38-40 rue du General Leclerc, 92794 Issy les Moulineaux Cedex 9, France; Orange Labs, 38-40 rue du General Leclerc, 92794 Issy les Moulineaux Cedex 9, France; Orange Labs, 38-40 rue du General Leclerc, 92794 Issy les Moulineaux Cedex 9, France","2011 IEEE 12th International Workshop on Signal Processing Advances in Wireless Communications","","2011","","","401","405","In previous works, delay tolerant codes were proposed with variable length ensuring a full transmit diversity for a certain interval of delay profiles. The code performances are enhanced when the code length is incremented; however, this causes an increasing complexity at the receiver which motivates our research for low complexity decoding methods. First, it is shown that optimal decoding methods cannot be used for large code lengths. Moreover, new decoding schemes with reasonable complexity and good performance are proposed for any code length and any tolerated delay.","1948-3252;1948-3244;1948-3244","978-1-4244-9332-6978-1-4244-9333-3978-1-4244-9331","10.1109/SPAWC.2011.5990439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5990439","","Delay;Complexity theory;Maximum likelihood decoding;Bit error rate;Transmitters;Lattices","decoding;space-time block codes;variable length codes","asynchronous space time block code;low complexity decoding method;delay tolerant code;code length","","","","10","","","","","IEEE","IEEE Conferences"
"A low-complexity joint noncoherent demodulation/decoding algorithm for nonbinary LDPC-coded differential modulation systems","M. Li; R. Chen; B. Bai; X. Ma","State Key Lab. of ISN, Xidian University, Xi'an 710071, China; State Key Lab. of ISN, Xidian University, Xi'an 710071, China; State Key Lab. of ISN, Xidian University, Xi'an 710071, China; Department of ECE, Sun Yat-sen University, Guangzhou, GD 510275, China","2013 IEEE China Summit and International Conference on Signal and Information Processing","","2013","","","403","407","In this paper, we apply q-ary LDPC codes to differential modulation systems, and study the design and performance of the resultant coded modulation systems. A low-complexity joint detection/decoding method for noncoherent demodulation is proposed, in which the hard-message-passing strategy is used for a joint factor graph. It combines trellis-based differential detection aided with channel prediction and the reliability-based decoding of nonbinary LDPC codes introduced in [1]. The Max-Log-MAP algorithm with soft-in hard-out is used for the differential detection. Simulation results show that the proposed method can offer good performances with a greatly reduced complexity.","","978-1-4799-1043","10.1109/ChinaSIP.2013.6625370","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625370","","Decoding;Iterative decoding;Demodulation;Joints;Complexity theory","decoding;demodulation;maximum likelihood estimation;message passing;parity check codes;trellis coded modulation","low-complexity joint noncoherent demodulation-decoding algorithm;nonbinary LDPC-coded differential modulation systems;q-ary LDPC codes;hard-message-passing strategy;trellis-based differential detection;channel prediction;reliability-based decoding;max-log-MAP algorithm;soft-in hard-out","","1","","18","","","","","IEEE","IEEE Conferences"
"Low Complexity Embedded Quantization Scheme Compatible with Bitplane Image Coding","F. Aulí-Llinàs","NA","2013 Data Compression Conference","","2013","","","271","280","Embedded quantization is a mechanism through which image coding systems provide quality progressivity. Although the most common embedded quantization approach is to use uniform scalar dead zone quantization (USDQ) together with bit plane coding (BPC), recent work suggested that similar coding performance as that achieved with USDQ+BPC can be obtained with a general embedded quantization (GEQ) scheme than performs fewer quantization stages. Unfortunately, practical approaches of GEQ can not be implemented in bit plane coding engines without substantially modifying their structure. This work overcomes this drawback introducing a 2-step scalar dead zone quantization (2SDQ) scheme compatible with bit plane image coding that provides the same advantages of practical GEQ approaches. Herein, 2SDQ is introduced in the framework of JPEG2000 to demonstrate its viability and efficiency.","1068-0314","978-0-7695-4965-1978-1-4673-6037","10.1109/DCC.2013.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6543063","Embedded quantization;2-step scalar deadzone quantization;general embedded quantization;JPEG2000","Quantization (signal);Encoding;Image reconstruction;Codecs;Engines;Transform coding;Wavelet transforms","image coding","low complexity embedded quantization scheme;bitplane image coding;uniform scalar deadzone quantization;USDQ;bitplane coding;BPC;general embedded quantization;GEQ scheme;2-step scalar deadzone quantization;2SDQ scheme;JPEG2000","","1","","26","","","","","IEEE","IEEE Conferences"
"Reduced-complexity synchronization for high-order coded modulations","M. Martalò; G. Ferrari; M. Asim; J. Gambini; C. Mazzucco; G. Cannalire; S. Bianchi; R. Raheli","Department of Information Engineering, University of Parma, Italy; Department of Information Engineering, University of Parma, Italy; Department of Information Engineering, University of Parma, Italy; Huawei Technologies, European Research Center, Milan, Italy; Huawei Technologies, European Research Center, Milan, Italy; Huawei Technologies, European Research Center, Milan, Italy; Huawei Technologies, European Research Center, Milan, Italy; Department of Information Engineering, University of Parma, Italy","2015 IEEE International Conference on Communications (ICC)","","2015","","","4721","4726","This paper focuses on phase noise-impaired communications. An efficient Maximum A-posteriori Probability (MAP) iterative synchronization algorithm, where detection and decoding are performed separately from phase estimation, is proposed. This approach has the following key advantages: (i) its computational complexity is relatively low and its performance is near optimal; (ii) it requires very limited statistical knowledge of the phase noise process; and (iii) it enables the direct use of “off-the-shelf” demodulation and decoding blocks. These features are particularly attractive from the implementation viewpoint, as they lead to the design of effective pragmatic high-order coded modulated schemes. The proposed iterative synchronization and decoding algorithm, evaluated for Low-Density Parity-Check (LDPC)-coded pilot symbol-assisted Quadrature Amplitude Modulation (QAM) schemes, entails a negligible energy efficiency loss with respect to optimized joint decoding and phase estimation approaches, with significantly lower computational complexity.","1550-3607;1938-1883","978-1-4673-6432-4978-1-4673-6431","10.1109/ICC.2015.7249069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7249069","Phase noise suppression;iterative decoding;synchronization;Maximum A Posteriori (MAP) receiver","Iterative decoding;Decoding;Phase estimation;Channel estimation;Delays;Encoding","computational complexity;decoding;demodulation;iterative methods;maximum likelihood estimation;parity check codes;phase estimation;quadrature amplitude modulation","reduced-complexity synchronization;high-order coded modulations;phase noise-impaired communications;maximum a-posteriori probability;iterative synchronization algorithm;computational complexity;demodulation blocks;decoding blocks;low-density parity-check-coded pilot symbol-assisted quadrature amplitude modulation;LDPC;QAM;phase estimation","","3","","21","","","","","IEEE","IEEE Conferences"
"Irregular Polar Coding for Complexity-Constrained Lightwave Systems","T. Koike-Akino; C. Cao; Y. Wang; S. C. Draper; D. S. Millar; K. Kojima; K. Parsons; L. Galdino; D. J. Elson; D. Lavery; P. Bayvel","Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Department of Electrical & Computer Engineering, University of Alberta, Edmonton, AB, Canada; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Department of Electrical & Computer Engineering, University of Toronto, Toronto, ON, Canada; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Optical Networks Group, University College London, Torrington Place, London, U.K.; Optical Networks Group, University College London, Torrington Place, London, U.K.; Optical Networks Group, University College London, Torrington Place, London, U.K.; Optical Networks Group, University College London, Torrington Place, London, U.K.","Journal of Lightwave Technology","","2018","36","11","2248","2258","Next-generation fiber-optic communications call for ultra-reliable forward error correction codes that are capable of low-power and low-latency decoding. In this paper, we propose a new class of polar codes, whose polarization units are irregularly pruned to reduce computational complexity and decoding latency without sacrificing error correction performance. We then experimentally demonstrate that the proposed irregular polar codes can outperform state-of-the-art low-density parity-check (LDPC) codes, while decoding complexity and latency can be reduced by at least 30% and 70%, respectively, versus regular polar codes, while also obtaining a marginal performance improvement.","0733-8724;1558-2213","","10.1109/JLT.2018.2802539","UK EPSRC Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281453","Coherent fiber-optic communications;complexity/latency reduction;FEC;irregular degree;polar coding","Encoding;Complexity theory;Forward error correction;Iterative decoding;Maximum likelihood decoding","block codes;computational complexity;decoding;error correction codes;forward error correction;linear codes;next generation networks;optical fibre networks;parity check codes","complexity-constrained lightwave systems;next-generation fiber-optic communications;ultra-reliable forward error correction codes;polarization units;computational complexity;error correction performance;irregular polar codes;regular polar codes;low-latency decoding;low-density parity-check codes;LDPC codes","","5","","46","","","","","IEEE","IEEE Journals & Magazines"
"Reduced Complexity Window Decoding of Spatially Coupled LDPC Codes for Magnetic Recording Systems","S. Khittiwitchayakul; W. Phakphisut; P. Supnithi","Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand; Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand; Faculty of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand","IEEE Transactions on Magnetics","","2018","54","11","1","5","Spatially coupled low-density parity-check (SC-LDPC) codes have emerged to possess capacity-approaching performance. The SC-LDPC codes can be decoded by a sliding window, therefore, the decoding latency and complexity of SC-LDPC codes are lower than those of underlying LDPC codes when the codeword length is very large. In this paper, the SC-LDPC decoder with the sliding window is employed in turbo equalization of magnetic recording systems. We examine the bit error rates (BERs) of the output of the sliding window during the iterative decoding, and then observe that the consecutive code blocks have approximately the same BERs. Herein, to reduce decoding complexity of SC-LDPC codes, the consecutive code blocks can be considered as the output of SC-LDPC codes. In addition, the non-uniform schedule update is adopted in the window decoding to avoid unnecessary updates within a window. The simulation results show that the proposed decoding algorithms applied in bit-patterned media magnetic recording systems can achieve a significant reduction in complexity compared to the traditional decoding without any loss in BER performance.","0018-9464;1941-0069","","10.1109/TMAG.2018.2832252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8401538","Low-density parity-check (LDPC) code;non-uniform schedules;spatially coupled codes;turbo equalization;window decoding","Decoding;Iterative decoding;Complexity theory;Microsoft Windows;Schedules;Magnetic recording","error statistics;iterative decoding;magnetic recording;parity check codes","spatially coupled LDPC codes;SC-LDPC codes;sliding window;SC-LDPC decoder;iterative decoding;consecutive code blocks;decoding complexity;bit-patterned media magnetic recording systems;reduced complexity window decoding;spatially coupled low-density parity-check code;bit error rates","","","","14","","","","","IEEE","IEEE Journals & Magazines"
"A loose-strategy-based Complexity Scalable Motion Estimation method for video coding","W. Lin; K. Panusopone; D. M. Baylon; M. Sun; H. Li","Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, Shanghai 200240, China; Advanced Technology Department, CTO Office, Home &amp; Networks Mobility, Motorola, Inc., San Diego, CA 92121, USA; Advanced Technology Department, CTO Office, Home &amp; Networks Mobility, Motorola, Inc., San Diego, CA 92121, USA; Department of Electrical Engineering University of Washington Seattle, WA 98105, USA; Department of Electrical and Computer Engineering, North Dakota State University Fargo, ND 58108, USA","2010 3rd International Congress on Image and Signal Processing","","2010","1","","11","15","In this paper, a new Complexity Scalable Motion Estimation (CSME) method is proposed which can perform Motion Estimation adaptively under different computation or power budgets while keeping high coding performance. We first propose to use a new class-based method to measure the Macroblock (MB) importance. Based on the new MB importance measure, a complete CSME framework is then proposed. The proposed method allocates computation in a one-pass flow and is based on a loose strategy where frames can borrow computation from other frames to achieve better performance under the same computation budget. Experimental results demonstrate that the proposed method can allocate computation more accurately than previous methods.","","978-1-4244-6516-3978-1-4244-6513-2978-1-4244-6515","10.1109/CISP.2010.5646350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646350","","Encoding;Resource management;Computer integrated manufacturing;Motion estimation;Video coding;Algorithm design and analysis;Classification algorithms","motion estimation;video coding","loose strategy;complexity scalable motion estimation;video coding;power budgets;macroblock importance;frame level computation allocation","","","","13","","","","","IEEE","IEEE Conferences"
"Low-complexity channel resolvability codes for the symmetric multiple-access channel","R. A. Chou; M. R. Bloch; J. Kliewer","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332-0250, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332-0250, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, 07102-1982, USA","2014 IEEE Information Theory Workshop (ITW 2014)","","2014","","","466","470","We investigate channel resolvability for the l-user multiple-access channel (MAC) with two different families of encoders. The first family consists of invertible extractors, while the second one consists of injective group homomorphisms, and was introduced by Hayashi for the point-to-point channel resolvability. The main benefit of these two families is to provide explicit low-complexity channel resolvability codes in the case of symmetric MACs. Specifically, we provide two examples of families of invertible extractors suitable for MAC resolvability with uniform input distributions, one based on finite-field multiplication, which can be implemented in O(n log n) for a limited range of values of the encoding blocklength n, and a second based on modified Toeplitz matrices, which can be implemented in O(n log n) for a wider range of values of n. We also provide an example of family of injective group homomorphisms based on finite-field multiplication suitable for MAC resolvability with uniform input distributions, which can be implemented in O(n log n) for some values of n.","1662-9019","978-1-4799-5999","10.1109/ITW.2014.6970875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970875","","Zinc;Encoding;Vectors;Random variables;Computers;Electronic mail;Silicon","channel coding;multi-access systems;Toeplitz matrices","low-complexity channel resolvability codes;symmetric multiple access channel;MAC resolvability;injective group homomorphisms;point-to-point channel resolvability;invertible extractors;finite-field multiplication;Toeplitz matrices","","5","","14","","","","","IEEE","IEEE Conferences"
"Low complexity depth intra coding in 3D-HEVC based on depth classification","H. Zheng; J. Zhu; H. Zeng; J. Chen; C. Cai; K. Ma","School of Information Science and Engineering, Huaqiao University, Xiamen, China 361021; School of Engineering, Huaqiao University, Quanzhou, China 362021; School of Information Science and Engineering, Huaqiao University, Xiamen, China 361021; School of Information Science and Engineering, Huaqiao University, Xiamen, China 361021; School of Engineering, Huaqiao University, Quanzhou, China 362021; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798","2016 Visual Communications and Image Processing (VCIP)","","2016","","","1","4","The latest high efficiency video coding-based three dimensional video coding (3D-HEVC) exploits sophisticated intra prediction scheme to improve the coding performance of the depth video, but incurring heavy computational complexity. To address this problem, a low complexity depth intra coding method is presented for 3D-HEVC based on depth classification. Firstly, a database of depth prediction units (PUs) with three kinds of complexities is collected based on their optimal intra prediction mode. Then, the histogram of oriented gradient (HOG) features are extracted on these established database to train the classifier using support vector machine (SVM). For the current depth PU, the trained classifier is applied to determine its most possible complexity class so as to select the corresponding modes for involving the mode decision process. Experimental results show that the proposed method is able to significantly reduce the computational complexity while keeping almost the same coding performance of depth video and video quality of the synthesized view, compared with the exhaustive mode decision in 3D-HEVC.","","978-1-5090-5316-2978-1-5090-5317","10.1109/VCIP.2016.7805527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805527","3D-HEVC;depth video;depth classification;mode decision","Encoding;Feature extraction;Support vector machines;Three-dimensional displays;Databases;Computational complexity","computational complexity;feature extraction;support vector machines;video coding","3D-HEVC;depth classification;high efficiency video coding;three dimensional video coding;computational complexity;feature extraction;support vector machine;SVM","","","","14","","","","","IEEE","IEEE Conferences"
"A Pragmatic View on Code Complexity Management","V. Antinyan; A. B. Sandberg; M. Staron","Volvo Car Group; Volvo Car Group; Software Engineering, University of Gothenburg","Computer","","2019","52","2","14","22","This article endeavors to underpin complexity understanding by scrutinizing how developers experience code complexity and how certain code characteristics impact complexity. The results provide a distinction between essential and accidental code characteristics and help in evaluating the influence of these characteristics on complexity increase.","0018-9162;1558-0814","","10.1109/MC.2018.2888761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672426","","Complexity theory;Task analysis;Micromechanical devices;Memory management;Couplings;Maintenance engineering","software metrics","code complexity management;code characteristics impact complexity;essential code characteristics;accidental code characteristics","","","","12","","","","","IEEE","IEEE Journals & Magazines"
"Performance, Complexity, and Receiver Design for Code-Aided Frame Synchronization in Multipath Channels","D. J. Jakubisin; R. M. Buehrer","NA; NA","IEEE Transactions on Communications","","2015","63","9","3363","3376","Next generation wireless communications systems are pushing the limits of both energy efficiency and spectral efficiency. This presents a challenge at the receiver when it comes to accomplishing tasks such as synchronization, channel estimation, and equalization and has motivated the development of code-aided iterative receiver algorithms in the technical literature. In this paper, we focus on the task of frame synchronization. While previous work has predominately assumed an additive white Gaussian noise channel, we develop code-aided frame synchronization algorithms for multipath channels. An iterative receiver is presented which integrates frame synchronization with iterative channel estimation, equalization, demodulation, and decoding. The receiver design includes a novel frame pre-processing stage to reduce the complexity of the proposed receiver. The complexity and performance of the proposed receiver is compared with that of a receiver based on conventional synchronization. The results demonstrate that the proposed receiver is capable of achieving a gain of up to 3 dB while increasing complexity by only 20%.","0090-6778;1558-0857","","10.1109/TCOMM.2015.2460246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165624","Frame synchronization;code-aided;iterative receivers;multipath channels;sum product algorithm;expectation-maximization algorithm","Synchronization;Receivers;Complexity theory;Multipath channels;Channel estimation;Iterative decoding;Signal to noise ratio","channel estimation;demodulation;equalisers;expectation-maximisation algorithm;iterative decoding;multipath channels;radio receivers;radio spectrum management;wireless channels","performance design;receiver design;complexity design;code-aided frame synchronization;multipath channels;next generation wireless communications systems;energy efficiency;spectral efficiency;iterative channel estimation;equalization;code-aided iterative receiver algorithms;demodulation;decoding;frame preprocessing stage;complexity reduction;expectation-maximization algorithm","","3","","39","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity in-loop prediction perceptual video coding for HEVC","Y. G. Joshi; J. Loo; P. Shah; S. Rahman; A. Tasiran; J. Cosmas","School of Science and Technology, Middlesex University, The Burroughs, Hendon, London, NW4 4BT, UK; School of Science and Technology, Middlesex University, The Burroughs, Hendon, London, NW4 4BT, UK; School of Science and Technology, Middlesex University, The Burroughs, Hendon, London, NW4 4BT, UK; School of Science and Technology, Middlesex University, The Burroughs, Hendon, London, NW4 4BT, UK; School of Science and Technology, Middlesex University, The Burroughs, Hendon, London, NW4 4BT, UK; Multimedia and Broadcast Networks Group, Department of Electronic & Computer Engineering, Brunel University, Uxbridge, Middlesex. UB8 3PH, United Kingdom","2016 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2016","","","1","7","This paper applies the concept of hybrid framework for perceptual video coding (PVC) during the `in-loop' stages by extending it to the prediction stage. As low complexity environments of mobile phones and tablets are increasingly used to capture video, PVC is not occurring here due to the high complexity of perceptual algorithms. Being able to encode using PVC will enable distortion to be merited by non-linear perceptual means than by uniform cost. While ideally, existing perceptual assessments of Structural Similarity (SSIM) is used, it is not processor friendly. The hybrid framework involves applying an additional low complexity perceptual assessment on top of existing Sum of Absolute Differences (SAD) and Sum of Absolute Transform Differences (SATD) only where distortion is perceptually significant. Consequently, the results show an increase in timing of <; +4% and <; +6% for video encoded with low delay P and random access profiles respectively, which is complexity competitively to other PVC solutions. This also affects bit redistribution with large reductions in bits allocated to signalling, -5 to -25%, with increases in small, medium and large block sizes. Visually, the proposed encoder encourages larger blocks on perceptually homogeneous regions and more dynamic smaller block where boundaries for textures or activity is occurring. This work can be extended to allow for perceptual quantisation to enable bandwidth reduction while maintaining perceptual quality.","2155-5052","978-1-4673-9044-6978-1-4673-9045","10.1109/BMSB.2016.7521919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7521919","","Integrated circuits","image capture;mobile handsets;notebook computers;video coding","low compiexity in-Ioop prediction perceptuaI video coding;HEVC;PVC;mobiIe phones;tabIets;video, capture;structural similarity perceptual assessment;SSIM perceptual assessment;sum of absolute difference;SAD;sum of absoiute transform difference;SATD;perceptuaI quantisation;bandwidth reduction;perceptuaI quaIity","","","","28","","","","","IEEE","IEEE Conferences"
"Performance and Computational Complexity of the Future Video Coding","N. Sidaty; P. Cabarat; W. Hamidouche; D. Menard; O. Deforges","IETR Lab, INSA of Rennes, Rennes, France; IETR Lab, INSA of Rennes, Rennes, France; IETR Lab, INSA of Rennes, Rennes, France; IETR Lab, INSA of Rennes, Rennes, France; IETR Lab, INSA of Rennes, Rennes, France","2018 IEEE International Workshop on Signal Processing Systems (SiPS)","","2018","","","31","36","The drastic increasing of multimedia applications, such as IPTV, Virtual Reality (VR, 360°) and Light Field videos has led to a high computing complexity in video compression and content quality assessment. In the last five years, HEVC standard has been widely used in the industrial community due to its bit-rate gain compared to its predecessor H.264/AVC. Recently, a new coding tools have been developed under the Joint Exploration Model (JEM) software, with the main goal to provide high bit rate saving compared to the HEVC standard. In this paper we investigate the performance and the associated computational complexity of these emerging video coding tools, from both encoding and decoding sides. Two spatial resolutions (HD & 4K) and several video contents have been used in this study. Results have shown that despite the bit-rate saving, a considerable computational complexity can be noticed. A bit-rate saving up to 37% and quality enhancements up to 30% can be achieved by JEM. However, these emerging tools are time consuming between x5 and x12 times compared to the HM reference software, depending on video sequence and encoding/decoding processes.","2374-7390;1520-6130","978-1-5386-6318-9978-1-5386-6317-2978-1-5386-6319","10.1109/SiPS.2018.8598306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598306","Future Video Coding;HM;JEM;Complexity;Video Quality;Subjective Assessment","Tools;Encoding;Bit rate;Software;Complexity theory;Decoding;Video coding","computational complexity;data compression;video coding","future video coding;multimedia applications;IPTV;VR;video compression;content quality assessment;HEVC standard;industrial community;bit-rate gain;JEM;spatial resolutions;video contents;HM reference software;video sequence;virtual reality;light field videos;computing complexity;predecessor H.264-AVC;joint exploration model software;encoding-decoding processes","","","","20","","","","","IEEE","IEEE Conferences"
"Low Complexity Adaptive View Synthesis Optimization in HEVC Based 3D Video Coding","S. Ma; S. Wang; W. Gao","Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University, Beijing, China; Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University, Beijing, China; Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Multimedia","","2014","16","1","266","271","In this correspondence, we explore a low-complexity adaptive view synthesis optimization (VSO) scheme in the upcoming high-efficiency video coding (HEVC)-based 3-D video coding standard. We first devise a novel zero-synthesized view difference (ZSVD) model which jointly accounts for the distortion of the synthesized view induced by the compound impact of depth-disparity mapping, texture adaptation, and occlusion in the view synthesis process. This model can efficiently estimate the maximum allowable depth distortion in synthesizing a virtual view without introducing any geometry distortion. Then, an adaptive ZSVD-aware VSO scheme is proposed by incorporating the ZSVD model into the rate-distortion optimization process, which is developed by pruning the conventional view synthesis algorithm. Extensive experimental results confirm that the proposed model is capable of accurately predicting the zero distortion of the synthesized view and exhibit that the proposed ZSVD-aware VSO scheme can remarkably reduce the coding computational complexity with negligible performance loss.","1520-9210;1941-0077","","10.1109/TMM.2013.2284751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623153","3D video coding;computational complexity;view synthesis optimization;zero synthesized view difference","Rendering (computer graphics);Optimization;Video coding;Adaptation models;Encoding;Three-dimensional displays;Standards","image texture;optimisation;video coding","low complexity adaptive view synthesis optimization;HEVC based 3D video coding;high-efficiency video coding;zero-synthesized view difference;depth-disparity mapping;texture adaptation;occlusion;geometry distortion;adaptive ZSVD-aware VSO scheme;rate-distortion optimization process","","30","","32","","","","","IEEE","IEEE Journals & Magazines"
"Non-orthogonal STBC for four transmit antennas with high coding gain and low decoding complexity","N. Sharma; M. R. Bhatnagar; M. Agrawal","Department of Electrical Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India; Centre for Applied Research in Electronics, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India","2011 5th International Conference on Signal Processing and Communication Systems (ICSPCS)","","2011","","","1","5","Multiple antenna based technology enhances performance of communication system by increasing capacity and reliable detection of the transmitted data. Space-time block codes (STBCs) for multiple-input multiple-output (MIMO) systems are designed to achieve full diversity when transmitter does not have channel state information. Orthogonal space-time block codes (OSTBCs) provide symbol-wise decoding, while quasi orthogonal STBCs are used to achieve high data rates with pair-wise decoding complexity. However, most of the existing OSTBCs and QOSTBCs utilize repeated transmission of symbols which reduces capacity of the wireless communication system. In this paper, we propose a non-orthogonal STBC for four transmit antennas that avoids repeated transmission of symbols. The proposed code transmits optimally mapped symbols in place of repeated transmissions of a symbol. The proposed STBC provides improved capacity and significant coding gain as compared to the existing STBCs for four transmit antennas. We also derive a low-complexity decoder of the proposed STBC which performs close to the maximum likelihood decoder.","","978-1-4577-1180-0978-1-4577-1179-4978-1-4577-1178","10.1109/ICSPCS.2011.6140834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140834","","Transmitting antennas;Signal to noise ratio;Complexity theory;Block codes;Maximum likelihood decoding","maximum likelihood decoding;MIMO communication;radiocommunication;space-time block codes;transmitting antennas","nonorthogonal STBC;transmit antennas;coding gain;decoding complexity;multiple antenna;space-time block codes;multiple-input multiple-output systems;MIMO systems;wireless communication system;maximum likelihood decoder","","","","13","","","","","IEEE","IEEE Conferences"
"Low-complexity low-delay distributed video coding","Gengfeng Qiu; Shaoshuai Gao; Guofang Tu","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","2013 18th International Conference on Digital Signal Processing (DSP)","","2013","","","1","6","Most state-of-the-art distributed video coding schemes are based on Stanford Wyner-Ziv video coding architecture which takes advantages of efficient channel codes such as LDPC or Turbo codes. The drawbacks that prevent it from practical applications are high decoding complexity and especially latency, since it needs to decode channel codes iteratively and require feedback information many times when decoding a WZ frame. In this paper, we proposed a novel distributed video coding scheme which needs no iterative decoding and only requires feedback information once per WZ frame. It uses a block classification coding strategy combined with a residual computing method based on modular arithmetic. Our scheme replaces complex channel coding with simple entropy coding, which greatly reduce decoding complexity and latency. Experimental results show that the proposed scheme can improve the rate-distortion performance over some state-of-the-art distributed video coding schemes for low and medium motion video sequences, while reducing decoding complexity to 10%~1%.","2165-3577;1546-1874","978-1-4673-5807","10.1109/ICDSP.2013.6622739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622739","Distributed video coding;differential data representation;block classification;Wyner-Ziv video coding","Decoding;Video coding;Encoding;Complexity theory;Parity check codes;Rate-distortion;Computer architecture","decoding;entropy codes;video coding","low complexity video coding;low delay video coding;distributed video coding;Wyner-Ziv video coding;feedback information;WZ frame;block classification coding strategy;residual computing method;modular arithmetic;channel coding replacement;entropy coding;decoding complexity reduction;latency reduction","","2","","15","","","","","IEEE","IEEE Conferences"
"A systematic design of space-time block codes with reduced-complexity partial interference cancellation group decoding","W. Zhang; L. Shi; X. Xia","School of Electrical Engineering &amp; Telecommunications, University of New South Wales, Sydney, 2052, Australia; School of Electrical Engineering &amp; Telecommunications, University of New South Wales, Sydney, 2052, Australia; Department of Electrical &amp; Computer Engineering, University of Delaware, Newark, 19716, USA","2010 IEEE International Symposium on Information Theory","","2010","","","1066","1070","Recently, space-time block codes (STBC) with a partial interference cancellation (PIC) group decoding was proposed to deal with the tradeoff among rate, diversity and complexity by Guo and Xia. In this paper, we propose a systematic design of STBC with reduced-complexity PIC group decoding. The proposed STBC is featured as Alamouti-type block code in which each entry of Alamouti code matrix is replaced by a block of multi-layer coded symbols. With the PIC group decoding and a particular grouping scheme, the proposed STBC can achieve full diversity, rate- 2M/M+2 and a low-complexity decoding for M transmit antennas. Simulation results show that the proposed codes can achieve the full diversity with PIC group decoding while requiring half decoding complexity of the existing codes.","2157-8095;2157-8117","978-1-4244-7892-7978-1-4244-7890-3978-1-4244-7891","10.1109/ISIT.2010.5513717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513717","","Block codes;Interference cancellation;Maximum likelihood decoding;Transmitting antennas;MIMO;Receiving antennas;Diversity methods;Australia;Computational modeling;Nonhomogeneous media","block codes;diversity reception;interference suppression;space-time codes;transmitting antennas","space time block codes;reduced complexity partial interference cancellation group decoding;reduced complexity PIC group decoding;STBC;Alamouti type block code matrix;multilayer coded symbol block;low complexity decoding;M transmit antennas;half decoding complexity","","3","","21","","","","","IEEE","IEEE Conferences"
"Complexity of dependencies in bounded domains, Armstrong Codes, and generalizations","Y. M. Chee; H. Zhang; X. Zhang","School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore; School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore; School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","2013 IEEE International Symposium on Information Theory","","2013","","","499","503","The study of Armstrong codes is motivated by the problem of understanding complexities of dependencies in relational database systems, where attributes have bounded domains. A (q, k, n)-Armstrong code is a q-ary code of length n with minimum Hamming distance n - k + 1, and for any set of k - 1 coordinates there exist two codewords that agree exactly there. Let f(q, k) be the maximum n for which such a code exists. In this paper, f(q, 3) = 3q - 1 is determined for all q ≥ 5 with three possible exceptions. This disproves a conjecture of Sali. Further, we introduce generalized Armstrong codes for branching, or (s, t)-dependencies and construct several classes of optimal Armstrong codes in this more general setting.","2157-8117;2157-8095","978-1-4799-0446","10.1109/ISIT.2013.6620276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620276","","Relational databases;Information theory;Arrays;Knowledge based systems;Complexity theory;Upper bound","codes","dependency complexity;bounded domains;Armstrong codes;relational database system;q-ary code;minimum Hamming distance;Sali conjecture","","1","","24","","","","","IEEE","IEEE Conferences"
"Low-complexity block size decision for HEVC intra coding using binary image feature descriptors","W. Geuder; P. Amon; E. Steinbach","Imaging and Computer Vision, Siemens Corporate Technology, Munich, Germany; Imaging and Computer Vision, Siemens Corporate Technology, Munich, Germany; Chair of Media Technology, Technische Universität München, Munich, Germany","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","242","246","We present a novel low-complexity algorithm for feature-based block size decision in HEVC intra coding. Our approach evaluates a set of point pairs within a coding unit (CU) in order to determine whether a CU should be further split into smaller sub CUs for coding. We apply a modified version of the Binary Robust Independent Elementary Feature (BRIEF) descriptor, which in its original version is usually applied to describe local image properties in the context of image/video analysis. While the elements of the original BRIEF descriptor describe which pixel of a pixel pair has the higher value, the modified descriptor evaluates whether the difference between the pixel values is above a pre-defined threshold, which is determined by training. If a certain number of pixel pair differences exceed their corresponding threshold the CU is split into its four sub CUs. Furthermore, we restrict the feature point pairs to be located within different potential sub CUs. For an adaptive training approach, we achieved in our experiments an average encoding time reduction of 58% compared to the HEVC reference software HM12.0 with an average rate increase of 3.8% at equal quality and an average encoding time reduction of 65% with an average rate increase of 5.98% at equal quality for an offline training approach.","","978-1-4799-8339-1978-1-4799-8338","10.1109/ICIP.2015.7350796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350796","HEVC;H.265;High Efficiency Video Coding;coding unit;BRIEF;binary feature descriptors;image descriptors;low-complexity;block size decision;intra coding","Encoding;Training;Bit rate;Complexity theory;Image coding;Software;Standards","computational complexity;video codecs;video coding","low-complexity block size decision;HEVC intracoding;binary image feature descriptors;feature-based block size decision;coding unit;binary robust independent elementary feature descriptor;BRIEF descriptor;pixel values;adaptive training approach;offline training approach","","4","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity soft-output detectors for LDPC coded spatial modulation systems","C. Li; Y. Cheng; Y. Zhang; Y. Huang","College of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China; College of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China; College of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China; College of Communications Engineering, PLA University of Science and Technology, Nanjing 210007, China","2015 International Conference on Wireless Communications & Signal Processing (WCSP)","","2015","","","1","6","In this paper, we present an efficient transmission scheme for multiple-input multiple-output (MIMO) systems, i.e., low density parity check (LDPC) coded spatial modulation (SM) systems. To exploit the powerful error correction of LDPC code, the key challenge of LDPC coded SM systems is on designing a reliable but low complexity soft-output detector. To tackle this problem, we propose two soft-output detection algorithms based on hard-decision detectors by exploiting the features of M-PSK and M-QAM constellations, namely, PSK-based soft-output detector (PBSOD) and QAM-based soft-output detector (QBSOD). The analytical results show that the number of the searched signal candidates is reduced from N<sub>t</sub>M to N<sub>t</sub>, where N<sub>t</sub> is the number of transmit antennas, M is the modulation order. Furthermore, we propose a more general simpler detector, that is, using the absolute value instead of the Frobenius norm to calculate the soft bit metrics. The findings of this paper suggest that the proposed three soft-output detectors for LDPC coded SM systems exhibit lower complexity compared to optimal maximum a posteriori (MAP) log likelihood ratio (LLR) detector and Max-Log detector while imposing only marginal performance degradation. In addition, a comprehensive performance and computational complexity comparison between the proposed detectors and the state-of-the-art detectors is provided to validate the proposed low-complexity detectors. The achieved performance and complexity of the proposed detectors make them suitable candidates for practical coded SM-MIMO systems.","","978-1-4673-7687-7978-1-4673-7686","10.1109/WCSP.2015.7341061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341061","","","computational complexity;MIMO communication;modulation coding;parity check codes;phase shift keying;quadrature amplitude modulation;signal detection;transmitting antennas","low-complexity soft-output detector;multiple input multiple output system;low density parity check coded spatial modulation system;hard-decision detector;M-PSK-based soft-output detector;PBSOD;M-QAM-based soft-output detector;QBSOD;transmit antenna;LDPC coded SM system;computational complexity","","3","","13","","","","","IEEE","IEEE Conferences"
"Low complexity perceptual image coding by just-noticeable difference model based adaptive downsampling","Z. Wang; Y. Baroud; S. M. Najmabadi; S. Simon","Institute of Parallel and Distributed Systems, University of Stuttgart, 70569 Stuttgart, Germany; Institute of Parallel and Distributed Systems, University of Stuttgart, 70569 Stuttgart, Germany; Institute of Parallel and Distributed Systems, University of Stuttgart, 70569 Stuttgart, Germany; Institute of Parallel and Distributed Systems, University of Stuttgart, 70569 Stuttgart, Germany","2016 Picture Coding Symposium (PCS)","","2016","","","1","5","A pixel domain algorithm for low complexity perceptual image coding is proposed. The algorithm exploits a combination of downsampling, predictive coding and just-noticeable difference (JND) model. Downsampling is performed adaptively on the input image based on regions-of-interest (ROI) identified by measuring the downsampling distortions against the visibility thresholds given by the JND model. The downsampled pixel is encoded if the differences are within the JND thresholds, and otherwise the original pixels are encoded with a quantization parameter selected based on the JND model. Noise shaping is employed to suppress potential visual artifacts due to quantization error propagation. The coding error at any pixel location can be guaranteed to be within the corresponding JND threshold. Experimental results show improved rate-distortion performance and visual quality over JPEG-LS as well as reduced rates compared with other standard codecs like JPEG 2000 at the same PSPNR.","2472-7822","978-1-5090-5966-9978-1-5090-5967","10.1109/PCS.2016.7906359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906359","","Image coding;Encoding;Quantization (signal);Prediction algorithms;Visualization;Context;Complexity theory","image coding","low complexity perceptual image coding;just-noticeable difference model;adaptive downsampling;pixel domain algorithm;predictive coding;JND thresholds;downsampling distortions;regions-of-interest;ROI;quantization parameter;noise shaping;rate-distortion performance;JPEG-LS;PSPNR;JPEG 2000","","","","18","","","","","IEEE","IEEE Conferences"
"Reduced complexity Chase-Pyndiah decoding algorithm for turbo product codes","J. Cho; W. Sung","Department of Electrical Engineering, Seoul National University, 599 Gwanangno, Gwanak-gu, Seoul, 151-744, Korea; Department of Electrical Engineering, Seoul National University, 599 Gwanangno, Gwanak-gu, Seoul, 151-744, Korea","2011 IEEE Workshop on Signal Processing Systems (SiPS)","","2011","","","210","215","Turbo product codes (TPC) are very suitable for applications requiring a large code length, a high code-rate, and good error performance. In the Chase decoding algorithm, normally a few least reliable positions are selected and the test sequences are generated from these positions. This paper proposes two methods to lower the complexity of the Chase-Pyndiah decoding algorithm. The first scheme reduces the number of least reliable positions by excluding those having relatively low error probabilities. The other one minimizes computations on unnecessary positions in an algebraic decoder. With these methods, we can significantly reduce the number of test sequences and lower the number of utilized positions for constructing an extended candidate codeword set. We show the simulation results with a squared (64, 57, 4) extended Hamming code-based TPC.","2162-3570;2162-3562;2162-3562","978-1-4577-1921-9978-1-4577-1920-2978-1-4577-1919","10.1109/SiPS.2011.6088976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6088976","Block codes;linear codes;turbo codes;product codes;iterative decoding","Decoding;Iterative decoding;Reliability;Complexity theory;Product codes;Bit error rate","algebraic codes;decoding;error statistics;Hamming codes;product codes;turbo codes","complexity reduction;Chase-Pyndiah decoding algorithm;turbo product codes;code length;code rate;test sequences;error probabilities;algebraic decoder;extended Hamming code-based TPC","","2","","13","","","","","IEEE","IEEE Conferences"
"Reduced-complexity binary-weight-coded associative memories","H. Jarollahi; N. Onizawa; V. Gripon; W. J. Gross","Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, H3A 0E9, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, H3A 0E9, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, H3A 0E9, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, H3A 0E9, Canada","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","","2013","","","2523","2527","Associative memories retrieve stored information given partial or erroneous input patterns. Recently, a new family of associative memories based on Clustered-Neural-Networks (CNNs) was introduced that can store many more messages than classical Hopfield-Neural Networks (HNNs). In this paper, we propose hardware architectures of such memories for partial or erroneous inputs. The proposed architectures eliminate winner-take-all modules and thus reduce the hardware complexity by consuming 65% fewer FPGA lookup tables and increase the operating frequency by approximately 1.9 times compared to that of previous work.","1520-6149;2379-190X","978-1-4799-0356","10.1109/ICASSP.2013.6638110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638110","Associative Memories;Hopfield;Clustered Neural Networks;Hardware Implementation","Neurons;Computer architecture;Hardware;Field programmable gate arrays;Decoding;Associative memory;Training","content-addressable storage;field programmable gate arrays;neural nets","reduced-complexity binary-weight-coded associative memories;stored information retrieval;clustered-neural-networks;CNNs;hardware architectures;hardware complexity reduction;FPGA lookup tables","","12","","9","","","","","IEEE","IEEE Conferences"
"Distributed Source–Channel Coding Using Reduced-Complexity Syndrome-Based TTCM","A. Aljohani; Z. Babar; S. X. Ng; L. Hanzo","University of Southampton, Southampton, U.K.; University of Southampton, Southampton, U.K.; University of Southampton, Southampton, U.K.; University of Southampton, Southampton, U.K.","IEEE Communications Letters","","2016","20","10","2095","2098","In the context of distributed joint source-channel coding, we conceive reduced-complexity turbo trellis coded modulation (TTCM)-aided syndrome-based block decoding for estimating the cross-over probability pe of the binary symmetric channel, which models the correlation between a pair of sources. Our joint decoder achieves an accurate correlation estimation for varying correlation coefficients at 3 dB lower SNR, than conventional TTCM decoder, despite its considerable complexity reduction.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2584598","Engineering and Physical Sciences Research Council; European Research Council’s Advanced Fellow Grant under the Beam-Me-Up project; Royal Society’s Wolfson Research Merit Award; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498628","Distributed joint source-channel coding;Slepian-Wolf coding;distributed source coding;turbo trellis coded modulation;syndrome decoding","Decoding;Encoding;Correlation;Multiuser detection;Modulation;Complexity theory;Fading channels","block codes;combined source-channel coding;probability;trellis coded modulation;turbo codes","distributed source-channel coding;reduced-complexity syndrome-based TTCM;reduced-complexity turbo trellis coded modulation;syndrome-based block decoding;cross-over probability;binary symmetric channel","","1","","12","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Soft Decoding of Huffman Codes and Iterative Joint Source Channel Decoding","A. Zribi; R. Pyndiah; S. Zaibi; F. Guilloud; A. Bouallegue","Communications Systems (SysCom) Laboratory, National Engineering School of Tunis, BP 37, Le Belvedere, 1002, Tunis, Tunisia; Signal and Communications Department, Institut Telecom, Telecom Bretagne, UMR CNRS 6285 Lab-STICC, Technopole Brest Iroise, CS 83818, 29238 Brest Cedex 3, Universite Europeenne de Bretagne, France; Communications Systems (SysCom) Laboratory, National Engineering School of Tunis, BP 37, Le Belvedere, 1002, Tunis, Tunisia; Signal and Communications Department, Institut Telecom, Telecom Bretagne, UMR CNRS 6285 Lab-STICC, Technopole Brest Iroise, CS 83818, 29238 Brest Cedex 3, Universite Europeenne de Bretagne, France; Communications Systems (SysCom) Laboratory, National Engineering School of Tunis, BP 37, Le Belvedere, 1002, Tunis, Tunisia","IEEE Transactions on Communications","","2012","60","6","1669","1679","Most source coding standards (voice, audio, image and video) use Variable-Length Codes (VLCs) for compression. However, the VLC decoder is very sensitive to transmission errors in the compressed bit-stream. Previous contributions, using a trellis description of the VLC codewords to perform soft decoding, have been proposed. Significant improvements are achieved by this approach when compared with prefix decoding. Nevertheless, for realistic VLCs, the complexity of the trellis technique becomes intractable. In this paper, we propose a soft-input VLC decoding method using an a priori knowledge of the lengths of the source-symbol sequence and the compressed bit-stream with Maximum A Posteriori (MAP) sequence estimation. Performance in the case of transmission over an Additive White Gaussian Noise (AWGN) channel is evaluated. Simulation results show that the proposed decoding algorithm leads to significant performance gain in comparison with the prefix VLC decoding besides exhibiting very low complexity. A new VLC decoding method generating additional information regarding the reliability of the bits of the compressed bit-stream is also proposed. We consider the serial concatenation of a VLC with two types of channel code and perform iterative decoding. Results show that, when concatenated with a recursive systematic convolutional code (RSCC), iterative decoding provides remarkable error correction performance. In fact, a gain of about 2.3 dB is achieved, in the case of transmission over an AWGN channel, with respect to tandem decoding. Second, we consider a concatenation with a low-density parity-check (LDPC) code and it is shown that iterative joint source/channel decoding outperforms tandem decoding and an additional coding gain of 0.25 dB is achieved.","0090-6778;1558-0857","","10.1109/TCOMM.2012.041212.100330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189017","Communication system performance;source coding;variable-length codes;error correction coding;iterative methods","Iterative decoding;Maximum likelihood decoding;Complexity theory;Measurement;Reliability;AWGN channels","AWGN channels;combined source-channel coding;concatenated codes;Huffman codes;iterative decoding;maximum likelihood estimation;parity check codes;variable length codes","low complexity soft decoding;Huffman codes;iterative joint source channel decoding;variable length codes;prefix decoding;source symbol sequence;compressed bit-stream;maximum a posteriori sequence estimation;additive white Gaussian noise channel;serial concatenation;recursive systematic convolutional code;error correction performance;low density parity check code;tandem decoding;coding gain","","5","","40","","","","","IEEE","IEEE Journals & Magazines"
"On the windowed encoding complexity of SC-LDGM codes for lossy source compression","A. Golmohammadi; J. Kliewer; D. J. Costello; D. G. M. Mitchell","Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM, USA; Dept. of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Dept. of Electrical Engineering, University of Notre Dame, Notre Dame, IN, USA; Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, NM, USA","2016 International Symposium on Information Theory and Its Applications (ISITA)","","2016","","","596","600","It has been shown that spatially coupled low-density generator-matrix (SC-LDGM) code ensembles display distortion saturation for the lossy binary symmetric source coding problem with belief propagation guided decimation algorithms, in the sense that the distortion of the SC-LDGM code ensemble approaches the optimal distortion of the underlying (uncoupled) LDGM block code ensemble. This has also been demonstrated for the class of protograph-based SC-LDGM code ensembles with windowed encoding (WE), where distortion close to the rate distortion limit was obtained with low-latency encoding. In this paper, we propose and compare two decimation techniques for lowering the WE complexity of SC-LDGM codes that maintain distortion performance close to the rate-distortion bound.","","978-4-88552-309-0978-1-5090-1917","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840494","","Distortion;Generators;Complexity theory;Couplings;Source coding;Belief propagation","binary codes;block codes;matrix algebra;rate distortion theory;source coding","windowed encoding complexity;SC-LDGM code;lossy source compression;spatially coupled low-density generator-matrix code;display distortion saturation;lossy binary symmetric source coding problem;belief propagation guided decimation algorithm;LDGM block code ensemble;protograph;low-latency encoding;WE complexity;rate-distortion bound","","","","8","","","","","IEEE","IEEE Conferences"
"Construction of Polar Codes With Sublinear Complexity","M. Mondelli; S. H. Hassani; R. L. Urbanke","Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland","IEEE Transactions on Information Theory","","2019","65","5","2782","2791","Consider the problem of constructing a polar code of block length N for a given transmission channel W. Previous approaches require one to compute the reliability of the N synthetic channels and then use only those that are sufficiently reliable. However, we know from two independent works by Schürch and by Bardet et al. that the synthetic channels are partially ordered with respect to degradation. Hence, it is natural to ask whether the partial order can be exploited to reduce the computational burden of the construction problem. We show that, if we take advantage of the partial order, we can construct a polar code by computing the reliability of roughly a fraction 1/ log3/2 N of the synthetic channels. In particular, we prove that N/ log3/2 N is a lower bound on the number of synthetic channels to be considered and such a bound is tight up to a multiplicative factor log log N. This set of roughly N/ log3/2 N synthetic channels is universal, in the sense that it allows one to construct polar codes for any W, and it can be identified by solving a maximum matching problem on a bipartite graph. Our proof technique consists of reducing the construction problem to the problem of computing the maximum cardinality of an antichain for a suitable partially ordered set. As such, this method is general, and it can be used to further improve the complexity of the construction problem, in case a refined partial order on the synthetic channels of polar codes is discovered.","0018-9448;1557-9654","","10.1109/TIT.2018.2889667","Dan David Foundation; Swiss National Science Foundation; Early Postdoc.Mobility fellowship from the Swiss National Science Foundation; Simons Institute for the Theory of Computing; NSF, NSF-CCF; Intel Science and Technology Center for Wireless Autonomous Systems (ISTC-WAS); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8589003","Polar codes;partial order;construction problem;antichain;chain","Complexity theory;Decoding;Degradation;Error probability;Reliability theory;Kernel","computational complexity;graph theory;set theory","construction problem;polar code;set theory;bipartite graph;maximum matching problem;computational burden;sublinear complexity;synthetic channels;transmission channel","","","","31","","","","","IEEE","IEEE Journals & Magazines"
"SCM-SM: Superposition Coded Modulation-Aided Spatial Modulation With a Low-Complexity Detector","X. Zhou; L. Yang; C. Wang; D. Yuan","NA; NA; NA; NA","IEEE Transactions on Vehicular Technology","","2014","63","5","2488","2493","Spatial modulation (SM) is a spatial multiplexing scheme that utilizes both the signal constellation and antenna index to convey information. In the original SM, traditional modulation schemes such as quadrature amplitude modulation (QAM) are employed for constellation mapping. In this paper, we propose a novel scheme where superposition coded modulation (SCM) is employed to modulate the information onto the constellation points. We also develop a low-complexity iterative detector for our proposed SCM-SM system. Analysis and simulations demonstrate that SCM-SM significantly outperforms the original SM system with the same data rate while maintaining the relatively low complexity, particularly in the high data rate scenario.","0018-9545;1939-9359","","10.1109/TVT.2013.2287806","National Science Foundation under; National Natural Science Foundation of China; Research Councils U.K. through the U.K.-China Science Bridges Project: R&D on (B)4G Wireless Mobile Communications; Opening Project of Key Laboratory of Cognitive Radio and Information Processing (Guilin University of Electronic Technology), Ministry of Education,; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650080","Multiple-Input Multiple-Output (MIMO);Spatial Modulation (SM);Superposition Coded Modulation (SCM);iterative detection;Iterative detection;multiple-input¿multiple-output (MIMO);spatial modulation (SM);superposition coded modulation (SCM)","Detectors;Complexity theory;Modulation;Transmitting antennas;Indexes;Receiving antennas","quadrature amplitude modulation;space division multiplexing","SCM-SM;superposition coded modulation-aided spatial modulation;low-complexity detector;spatial multiplexing scheme;signal constellation;antenna index;quadrature amplitude modulation;QAM;constellation mapping;SCM-SM system;constellation points;low-complexity iterative detector","","9","","20","","","","","IEEE","IEEE Journals & Magazines"
"A complexity reduction scheme with adaptive search direction and mode elimination for multiview video coding","M. Shafique; B. Zatt; J. H. Karlsruhe","Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany; Institute of Technology (KIT), Chair for Embedded Systems, Karlsruhe, Germany","2012 Picture Coding Symposium","","2012","","","105","108","A novel complexity reduction scheme for Multiview Video Coding (MVC) is presented that adaptively eliminates the less-probable Motion or Disparity Estimation (ME, DE) search directions and less-probable block coding modes. Based on their texture difference w.r.t. the current Macroblock, matching neighbors in the 3D-neighborhood (spatial, temporal, view domains) are identified. Our scheme employs a multi-level decision process to predict the more-probable ME/DE search direction based on the texture and (motion/disparity) activity classification of the matching neighbors. For a predicted search direction, more-probable block coding modes are predicted depending upon the texture classification and RD-Cost of the current Macroblock. Quantization Parameter based thresholds are formulated using an offline statistical analysis of texture, motion/disparity, and RD-Cost properties. Our scheme achieves a complexity reduction of up to 81% and 40% compared to the exhaustive Rate-Distortion-Optimized Mode Decision and state-of-the-art, respectively, at the cost of an average BD-PSNR loss of 0.03 dB.","","978-1-4577-2049-9978-1-4577-2047-5978-1-4577-2048","10.1109/PCS.2012.6213297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213297","Multiview Video Coding;MVC;H.264;AVC;Motion Estimation;Disparity Estimation;Complexity Reduction","Block codes;Complexity theory;Video coding;Correlation;Video sequences;Probability density function;Estimation","block codes;image classification;image texture;motion estimation;quantisation (signal);search problems;statistical analysis;video coding","complexity reduction scheme;adaptive search direction;mode elimination;multiview video coding;MVC;motion estimation;disparity estimation;ME search direction;DE search direction;block coding mode;texture difference;macroblock;matching neighbors;3D-neighborhood;spatial domain;temporal domain;view domain;multilevel decision process;motion activity classification;disparity activity classification;texture classification;quantization parameter based threshold;statistical analysis;RD-cost properties;BD-PSNR loss;loss 0.03 dB","","4","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity LDPC decoder for physical layer network coded multi-way wireless relay systems","N. Balasuriya; C. B. Wavegedara","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","2015 IEEE 10th International Conference on Industrial and Information Systems (ICIIS)","","2015","","","226","231","Recently, in multi-way relay systems employing physical-layer network coding (PNC), channel condition-based pair selection for pairwise information exchange has drawn increased attention as it provides improved performance over the alternative approaches such as round-robin scheduling of node pairs. With pairwise information exchange, there is additional time diversity available due to transmission of the same coded information block of most of the nodes twice in two adjacent time slots. This additional time diversity can be exploited to further enhance the system performance by means of joint channel decoding-network coding. To this end, a joint belief propagation (BP)-based channel decoding-network coding algorithm was proposed by us in [9] for the relay node of low density parity check (LDPC)-coded multi-way relay systems. Even though the algorithm of [9] offers much superior performance over separate channel decoding, it is highly computational expensive and may not be suitable for applications in wireless sensor networks. Alternatively, bit-flipping (BF)-based LDPC decoding can be adopted at much lower computational complexity. In this paper, we develop a low complexity joint bit-flipping (BF)-based channel decoding and network coding algorithm for LDPC-based multi-way relay systems. The BER performance of the proposed algorithm is investigated using computer simulation for two commonly used LDPC codes. The simulation results demonstrate that superior BER performance can be achieved using the proposed joint decoding algorithm over separate and independent bit-flipping-based LDPC channel decoding at the relay. The simulation results also verify that our proposed algorithm is capable of harnessing the aforesaid additional time diversity available. Though the performance of the proposed joint decoding algorithm is inferior to the BP-based algorithm of [9], due to its low computationally complexity, this algorithm may be appealing for applications in relay nodes with low computational capabilities and memory constraints.","","978-1-4799-1876-8978-1-5090-1741-6978-1-5090-1740","10.1109/ICIINFS.2015.7399015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399015","Multi-way relay channel;physical layer network coding;low density parity check codes (LDPC);bit-flipping decoder","Decoding;Binary phase shift keying;Relays;Reliability","channel coding;codecs;computational complexity;error statistics;network coding;parity check codes;relay networks (telecommunication);wireless sensor networks","LDPC decoder;physical layer network coded multi-way wireless relay systems;physical-layer network coding;channel condition-based pair selection;pairwise information exchange;round-robin scheduling;belief propagation-based channel decoding-network coding algorithm;low density parity check-coded multi-way relay systems;wireless sensor networks;computational complexity;bit-flipping-based channel decoding;BER;computer simulation","","","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity detection for space-time block coded spatial modulation","J. Cal-Braz; R. Sampaio-Neto","Center for Telecommunications Studies, PUC-Rio, Rio de Janeiro, Brazil; Center for Telecommunications Studies, PUC-Rio, Rio de Janeiro, Brazil","2015 International Symposium on Wireless Communication Systems (ISWCS)","","2015","","","671","675","Space-time block coding (STBC) can increase the diversity order of spatial modulation MIMO systems. Current deployments consider the use of the optimal detector, which benefits from the decoupled ML detection typical of STBC systems. However, its adaptation to spatial modulation system presents computational complexity increase as more transmit antennas or larger modulation orders are employed, which may turn infeasible its implementation in systems with higher bit rates. This works presents a suboptimal detection strategy for STBC-SM systems based on the generation of a list of candidates, sorted after matched-filtering. A list-length management scheme performs the balance between performance and complexity. Results show that it can achieve near-optimal detection performance with significant computational cost reduction.","2154-0225","978-1-4673-6540-6978-1-4673-6539","10.1109/ISWCS.2015.7454433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454433","MIMO systems;spatial modulation;space-time block coding;list detection","Detectors;Transmitting antennas;Modulation;Measurement;MIMO;Receiving antennas;Complexity theory","antenna arrays;computational complexity;matched filters;maximum likelihood detection;MIMO communication;modulation coding;space-time block codes;transmitting antennas","low-complexity detection;space-time block coded spatial modulation;spatial modulation MIMO systems;diversity order;decoupled ML detection;computational complexity;transmit antennas;suboptimal detection strategy;STBC-SM systems;matched-filtering;list-length management scheme;computational cost reduction","","","","9","","","","","IEEE","IEEE Conferences"
"Improving the Update Complexity of Locally Repairable Codes","M. Mehrabi; M. Shahabinejad; M. Ardakani; M. Khabbazian","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada","IEEE Transactions on Communications","","2018","66","9","3711","3720","Locally repairable codes (LRCs) have been recently proposed and used in real-world distributed storage systems (DSSs) such as Microsoft Azure Storage and Facebook HDFS-RAID (Hadoop Distributed File System-Redundant Array of Independent Disks). Since information in DSSs is changed frequently, reducing update complexity (UC) of LRCs is of great interest. In this paper, we propose code design algorithms that can reduce UC of existing LRCs without sacrificing their important code parameters such as minimum distance, code rate, or locality. We establish bounds on UC, and use them to show that our algorithms can achieve optimal or near optimal UC for a large class of LRCs.","0090-6778;1558-0857","","10.1109/TCOMM.2018.2827053","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338125","Update complexity;erasure coding;distributed storage system;locally repairable codes","Complexity theory;Systematics;Spread spectrum communication;Block codes;Facebook;Maintenance engineering","codes;computational complexity;data handling;distributed databases;RAID;storage management","LRCs;code design algorithms;locally repairable codes;real-world distributed storage systems;DSSs;Microsoft Azure Storage;Facebook HDFS-RAID;Hadoop Distributed File System-Redundant Array of Independent Disks;update complexity reduction","","","","22","","","","","IEEE","IEEE Journals & Magazines"
"Complexity-Aware Adaptive Preprocessing Scheme for Region-of-Interest Spatial Scalable Video Coding","D. Grois; O. Hadar","Communication Systems Engineering Department, Ben-Gurion University of the Negev, Be’er Sheva, Israel; Communication Systems Engineering Department, Ben-Gurion University of the Negev, Be’er Sheva, Israel","IEEE Transactions on Circuits and Systems for Video Technology","","2014","24","6","1025","1039","This paper presents a complexity-aware adaptive spatial preprocessing scheme for the efficient scalable video coding (SVC) by employing an adaptive prefilter for each SVC layer. According to the presented scheme, a dynamic transition region is defined between the region of interest (ROI) and background within each video frame, and then various parameters of each prefilter (such as the standard deviation, kernel matrix size, and also a number of filters for the dynamic preprocessing of a transition region between the ROI and the background) are adaptively varied. The presented scheme has proved to be very efficient because it is based on an SVC computational complexity-rate-distortion analysis, thereby adding a complexity dimension to the conventional SVC rate-distortion analysis. As a result, the encoding computational complexity resources are significantly reduced, which is especially useful for portable encoders with limited power resources. The performance of the presented adaptive spatial preprocessing scheme is evaluated and tested in detail from both computational complexity and visual presentation quality points of view, further comparing it with the joint scalable video model reference software (JSVM 9.19) and demonstrating significant improvements.","1051-8215;1558-2205","","10.1109/TCSVT.2014.2302557","NEGEV Consortium; MAGNET Program of the Israeli Chief Scientist; Israeli Ministry of Trade and Industry; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727577","Scalable Video Coding (SVC);Regions-of-interest (ROI) video coding;pre-processing/pre-filtering;image/video coding;ROI scalability;high-quality visual presentation;High-quality visual presentation;image or video coding;preprocessing or prefiltering;regions of interest (ROI) video coding;ROI scalability;scalable video coding (SVC)","Static VAr compensators;Video coding;Encoding;Standards;Scalability;Computational complexity;Visualization","adaptive filters;computational complexity;rate distortion theory;video codecs;video coding","complexity-aware adaptive preprocessing scheme;region-of-interest;scalable video coding;adaptive prefilter;SVC layer;dynamic transition region;video frame;SVC computational complexity;SVC rate-distortion analysis;computational complexity resources;portable encoders;adaptive spatial preprocessing scheme;visual presentation quality;joint scalable video model reference software;JSVM 9.19","","4","","35","","","","","IEEE","IEEE Journals & Magazines"
"Parallel Low-Complexity Lossless Coding of Three-Dimensional Medical Images","R. Pizzolante; A. Castiglione; B. Carpentieri; A. D. Santis","NA; NA; NA; NA","2014 17th International Conference on Network-Based Information Systems","","2014","","","91","98","Digital medical images are becoming even more widespread and used in a large variety of applications. Such images are often stored in local repositories and need to be transmitted/ received over the network. Therefore, data compression is an essential mechanism to adopt for improving the transmission time, as well as to optimize the required storage space. However, due to the clinical relevance of such data, lossless techniques are almost ever preferred, given that in such context it is not tolerated any loss of data. In this work, starting from the Medical Images Lossless Compression (MILC) algorithm, which enables the efficient coding of three-dimensional medical images achieving results comparable with the other state-of-art algorithms, we propose Parallel MILC, a fully parallelized version of that algorithm, which provides some attractive features with respect to MILC, especially in terms of execution speedup and scalability. In detail, we propose a novel design and implementation for the MILC algorithm, which is able to exploit the power and the capabilities of the parallel computing paradigm. It is important to point out that Parallel MILC can be executed on several heterogeneous device types supporting OpenCL (i.e. CPU, GPU, FPGA, etc.). The preliminary test results show a significant performance speedup of Parallel MILC compared to MILC. Consequently, the novel algorithm we propose provides MILC with strong scalability properties and complete transparency with respect the underlying hardware.","2157-0418;2157-0426","978-1-4799-4224-4978-1-4799-4226","10.1109/NBiS.2014.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7023939","Parallel compression; 3D medical image; lowcomplexity; scalable; heterogeneous devices; OpenCL","Biomedical imaging;Kernel;Prediction algorithms;Image coding;Context;Algorithm design and analysis;Equations","data compression;image coding;medical image processing","parallel low-complexity lossless coding;three-dimensional digital medical images;data compression;medical images lossless compression algorithm;MILC algorithm;parallel computing paradigm;OpenCL","","2","","9","","","","","IEEE","IEEE Conferences"
"A Low Complexity Mode Decision Method for Spatial Scalability Coding","B. Lee; M. Kim","Department of Information and Communications Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2011","21","1","88","95","In this paper, a fast mode decision method for the spatial higher layers (SHLs) in scalable video coding (SVC) is proposed based on coding dependency between two adjacent spatial lower and higher layers. The proposed fast mode decision method detects zero motion and zero transform coefficient blocks in the current spatial layer using the already encoded information of the corresponding blocks from the lower layer. The information for zero motion vectors and zero transform coefficients is used to induce a reduced set of candidate modes in SHLs of SVC, which reduces the computation complexity up to about 75% of the total encoding time while maintaining the coding performance with negligible amounts of degradation in peak signal-to-noise ratio values and bitrates.","1051-8215;1558-2205","","10.1109/TCSVT.2011.2106273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688301","Fast mode decision;scalable video coding;spatial scalability","Encoding;Scalability;Static VAr compensators;Transforms;Complexity theory;Materials;Electronic mail","computational complexity;video coding","low complexity mode decision method;spatial scalability coding;spatial higher layers;scalable video coding;fast mode decision method;computation complexity;zero motion vectors;zero transform coefficients","","10","","11","","","","","IEEE","IEEE Journals & Magazines"
"Analysis of performance and implementation complexity of simplified algorithms for decoding Low-Density Parity-Check codes","V. A. Chandrasetty; S. M. Aziz","School of Electrical and Information Engineering University of South Australia, Mawson Lakes Campus, SA 5095, Australia; School of Electrical and Information Engineering University of South Australia, Mawson Lakes Campus, SA 5095, Australia","2010 IEEE Globecom Workshops","","2010","","","430","435","This paper presents a novel technique to significantly reduce the implementation complexity of Low-Density Parity-Check (LDPC) decoders. The proposed technique uses high precision soft messages at the variable nodes but scales down the extrinsic message length, which reduces the number of interconnections between variable and check nodes. It also simplifies the check node operation. The effect on performance and complexity of the decoders due to such simplification is analyzed. A prototype model of the proposed decoders compliant with the WiMax application standard has been implemented and tested on Xilinx Virtex 5 FPGA. The implementation results show that the proposed decoders can achieve significant reduction in hardware complexity with comparable decoding performance to that of Min-Sum algorithm based decoders. The proposed decoders are estimated to achieve an average throughput in the range of 6-11 Gbps, even with short code lengths.","2166-0077","978-1-4244-8865-0978-1-4244-8863-6978-1-4244-8864","10.1109/GLOCOMW.2010.5700356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5700356","","Decoding;Complexity theory;Parity check codes;Bit error rate;Hardware;Field programmable gate arrays;Throughput","decoding;field programmable gate arrays;parity check codes;WiMax","performance analysis;implementation complexity;simplified algorithms;low density parity check codes;decoding;LDPC decoder;check node operation;WiMax application;FPGA;min-sum algorithm","","5","","36","","","","","IEEE","IEEE Conferences"
"The complexity of object reconciliation, and open problems related to set difference and coding","M. Mitzenmacher; G. Varghese","School of Engineering and Applied Sciences, Harvard University. Supported in part by the NSF under grants CNS-1011840, IIS-0964473, and CCF-0915922; Microsoft Research, on leave from UC San Diego","2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","","2012","","","1126","1132","We explore the connections between the classical problems of set difference and error correction codes, motivated by some recent results on Invertible Bloom Filters (communication-efficient set difference) and Biff Codes (fast error correction coding based on set difference). In particular, we seek to understand how these results generalize to settings where many parties communicate over a network represented by a graph, and the goal is for the parties to reconcile the objects owned by each, for some suitable definition of reconcile. Our general framework encompasses standard problems such as rumor spreading and network coding. We suggest that generalizing to other objects such as sequences with other measures such as edit distance may lead to a theory of reconciling objects over graphs. Such a theory may have practical consequences for modern cloud-based deployments.","","978-1-4673-4539-2978-1-4673-4537-8978-1-4673-4538","10.1109/Allerton.2012.6483345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6483345","","Synchronization;Protocols;Complexity theory;Network coding;Polynomials;Approximation algorithms;Standards","cloud computing;computational complexity;data structures;error correction codes;graph theory","object reconciliation complexity;open problems;invertible bloom filters;Biff codes;fast error correction coding;set difference;graph network representation;network coding;cloud based deployments","","7","","27","","","","","IEEE","IEEE Conferences"
"Low-complexity feedback-channel-free distributed video coding using local rank transform","P. Raj Bhagath; K. Mallick; J. Mukherjee; S. Mukopadhayay","Indian Institute of Technology, India; Azure Software Systems Ltd., India; Indian Institute of Technology, India; Indian Institute of Technology, India","IET Image Processing","","2017","11","2","126","134","In this study, the authors propose a new feedback-channel-free distributed video coding algorithm using local rank transform (LRT). The encoder computes LRT by considering selected neighbourhood pixels of Wyner–Ziv (WZ) frame. These LRT values are merged, and their positions are entropy coded and sent to the decoder. In addition, means of each block of WZ frame are also transmitted to assist motion estimation (ME). Using these measurements, the decoder generates side information (SI) by implementing ME and compensation in LRT domain. An iterative algorithm is executed on SI using LRT to reconstruct the WZ frame. Experimental results show that the coding efficiency of the authors’ codec is close to the efficiency of pixel domain distributed video coders based on low-density parity check and accumulate or turbo codes, with less encoder and decoder complexity.","1751-9659;1751-9667","","10.1049/iet-ipr.2016.0714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815537","","","video coding;entropy codes;turbo codes;transforms;motion estimation;image resolution;image reconstruction;parity check codes","turbo code;accumulate code;low-density parity check code;pixel domain distributed video coders;coding efficiency;side information generation;motion estimation;WZ frame reconstruction;Wyner-Ziv frame;neighbourhood pixels;LRT computation;local rank transform;low-complexity feedback-channel-free distributed video coding algorithm","","","","","","","","","IET","IET Journals & Magazines"
"Low-Complexity Low-Latency Architecture for Matching of Data Encoded With Hard Systematic Error-Correcting Codes","B. Y. Kong; J. Jo; H. Jeong; M. Hwang; S. Cha; B. Kim; I. Park","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2014","22","7","1648","1652","A new architecture for matching the data protected with an error-correcting code (ECC) is presented in this brief to reduce latency and complexity. Based on the fact that the codeword of an ECC is usually represented in a systematic form consisting of the raw data and the parity information generated by encoding, the proposed architecture parallelizes the comparison of the data and that of the parity information. To further reduce the latency and complexity, in addition, a new butterfly-formed weight accumulator (BWA) is proposed for the efficient computation of the Hamming distance. Grounded on the BWA, the proposed architecture examines whether the incoming data matches the stored data if a certain number of erroneous bits are corrected. For a (40, 33) code, the proposed architecture reduces the latency and the hardware complexity by ~32% and 9%, respectively, compared with the most recent implementation.","1063-8210;1557-9999","","10.1109/TVLSI.2013.2276076","IT Research and Development program of MOTIE/KEIT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583335","Data comparison;error-correcting codes (ECCs);Hamming distance;systematic codes;tag matching.;Data comparison;error-correcting codes (ECCs);Hamming distance;systematic codes;tag matching","Computer architecture;Complexity theory;Systematics;Encoding;Hamming distance;Decoding;Error correction codes","error correction codes;Hamming codes","hardware complexity;erroneous bits number;stored data;hamming distance efficient computation;BWA;butterfly-formed weight accumulator;encoding;parity information;raw data;complexity reduction;latency reduction;ECC codeword;data protected;hard systematic error-correcting codes;data encoded matching;low-complexity low-latency architecture","","5","","8","","","","","IEEE","IEEE Journals & Magazines"
"Analyzing the Resilience-Complexity Tradeoff of Network Coding in Dynamic P2P Networks","D. Niu; B. Li","University of Toronto, Toronto; University of Toronto, Toronto","IEEE Transactions on Parallel and Distributed Systems","","2011","22","11","1842","1850","Most current-generation P2P content distribution protocols use fine-granularity blocks to distribute content to all the peers in a decentralized fashion. Such protocols often suffer from a significant degree of imbalance in block distributions, especially when the users are highly dynamic. As certain blocks become rare or even unavailable, content availability and download efficiency are adversely affected. Randomized network coding may improve block diversity and availability in P2P networks, as coded blocks are equally innovative and useful to peers. However, the computational complexity of network coding mandates that, in reality, network coding needs to be performed within segments, each containing a subset of blocks. In this paper, we quantitatively evaluate how network coding may improve content availability, block diversity, and download performance in the presence of churn, as the number of blocks in each segment for coding varies. Based on stochastic models and a differential equation approach, we explore the fundamental tradeoff between the resilience gain of network coding to peer dynamics and its inherent coding complexity. We conclude that a small number of blocks in each segment is sufficient to realize the major benefits of network coding, with acceptable coding cost.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2011.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5703093","Peer-to-peer content distribution;generation-based network coding;peer dynamics;content availability;resilience.","Network coding;Availability;Encoding;Steady-state;Protocols;Peer to peer computing;Resilience","computational complexity;differential equations;network coding;peer-to-peer computing;protocols;stochastic processes","resilience complexity tradeoff analysis;network coding;dynamic P2P networks;P2P content distribution protocols;fine granularity blocks;block distributions;block coding;computational complexity;block diversity;stochastic models;differential equation","","5","","15","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity video coding and the emerging HEVC standard","K. Ugur; K. Andersson; A. Fuldseth; G. Bjøntegaard; L. P. Endresen; J. Lainema; A. Hallapuro; J. Ridge; D. Rusanovskyy; C. Zhang; A. Norkin; C. Priddle; T. Rusert; J. Samuelsson; R. Sjöberg; Z. Wu","Nokia Research Center; Ericsson Research; Tandberg Telecom (Cisco); Tandberg Telecom (Cisco); Tandberg Telecom (Cisco); Nokia Research Center; Nokia Research Center; Nokia Research Center; Nokia Research Center; Nokia Research Center; Ericsson Research; Ericsson Research; Ericsson Research; Ericsson Research; Ericsson Research; Ericsson Research","28th Picture Coding Symposium","","2010","","","474","477","This paper describes a low complexity video codec with high coding efficiency. It was proposed to the High Efficiency Video Coding (HEVC) standardization effort of MPEG and VCEG, and has been partially adopted into the initial HEVC Test Model under Consideration design. The proposal utilizes a quad-tree structure with a support of large macroblocks of size 64×64 and 32×32, in addition to macroblocks of size 16×16. The entropy coding is done using a low complexity variable length coding based scheme with improved context adaptation over the H.264/AVC design. In addition, the proposal includes improved interpolation and deblocking filters, giving better coding efficiency while having low complexity. Finally, an improved intra coding method is presented. The subjective quality of the proposal is evaluated extensively and the results show that the proposed method achieves similar visual quality as H.264/AVC High Profile anchors with around 50% and 35% bit rate reduction for low delay and random-access experiments respectively at high definition sequences. This is achieved with less complexity than H.264/AVC Baseline Profile, making the proposal especially suitable for resource constrained environments.","","978-1-4244-7135-5978-1-4244-7134-8978-1-4244-7133","10.1109/PCS.2010.5702540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702540","HEVC;standardization;video coding;H.264/AVC","Proposals;Complexity theory;Automatic voltage control;Encoding;Bit rate;Video coding;Interpolation","block codes;entropy codes;variable length codes;video codecs;video coding","low complexity video coding;HEVC standard;low complexity video codec;high efficiency video coding;standardization;MPEG;VCEG;entropy coding;complexity variable length coding;interpolation;deblocking filters","","6","","8","","","","","IEEE","IEEE Conferences"
"Low-Complexity Intra Coding for Scalable Extension of HEVC Based on Content Statistics","S. Lasserre; F. Le Léannec; J. Taquet; E. Nassor","Technicolor, Cesson-Sévigné, France; Technicolor, Cesson-Sévigné, France; Canon Research Centre France, Cesson-Sévigné, France; Canon Research Centre France, Cesson-Sévigné, France","IEEE Transactions on Circuits and Systems for Video Technology","","2014","24","8","1375","1389","This paper presents a new approach for a scalable extension of High Efficiency Video Coding (HEVC) intra pictures. The proposed scalable intra codec targets very low complexity together with coding efficiency, and thus employs only one prediction mode, known as inter layer intra prediction. The main novelty of the codec is a new fast rate-distortion optimization approach, which is founded on the rate-distortion theory. It exploits precomputed offline information for the reduction of the computational complexity, and a new allocation process for the balancing of the coded information. The precomputed information is a set of optimal quantifiers and associated rate-distortion curves for predetermined statistical models that are used for efficiently coding the Discrete Cosine Transform channels. This low-complexity intra coding tool has been proposed to the scalable HEVC call for proposals. In the scalable standard testing conditions, the proposed intra codec shows compression performance with a bit-rate increase of around 10% only compared with the HEVC single-layer encoder.","1051-8215;1558-2205","","10.1109/TCSVT.2014.2305513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737259","Generalized Gaussian distribution (GGD);High Efficiency Video Coding (HEVC);rate distortion optimal quantization (RDOQ);statistical modeling","Encoding;Discrete cosine transforms;Rate-distortion;Codecs;Optimization;Complexity theory;Decoding","codecs;discrete cosine transforms;rate distortion theory;video coding","low-complexity intra coding;scalable extension;high efficiency video coding intra pictures;content statistics;scalable intra codec targets;coding efficiency;interlayer intraprediction;fast rate-distortion optimization approach;ratedistortion theory;coded information;precomputed information;rate-distortion curves;discrete cosine transform channels;intracoding tool;scalable standard testing conditions;HEVC single-layer encoder","","4","","30","","","","","IEEE","IEEE Journals & Magazines"
"Reduced complexity decoding for Bit-Interleaved Coded Multiple Beamforming with Constellation Precoding","B. Li; H. J. Park; E. Ayanoglu","Center for Pervasive Communications and Computing, Department of Electrical Engineering and Computer Science, The Henry Samueli School of Engineering, University of California, Irvine, 92697-2625, USA; Samsung Electronics, Suwon, Korea; Center for Pervasive Communications and Computing, Department of Electrical Engineering and Computer Science, The Henry Samueli School of Engineering, University of California, Irvine, 92697-2625, USA","2011 7th International Wireless Communications and Mobile Computing Conference","","2011","","","152","156","Multiple beamforming is realized by singular value decomposition of the channel matrix which is assumed to be known to both the transmitter and the receiver. Bit-Interleaved Coded Multiple Beamforming (BICMB) can achieve full diversity as long as the code rate Rcand the number of employed subchannels S satisfy the condition RcS ≤ 1. Bit-Interleaved Coded Multiple Beamforming with Constellation Precoding (BICMB-CP), on the other hand, can achieve full diversity without the condition RcS ≤ 1. However, the decoding complexity of BICMB-CP is much higher than BICMB. In this paper, a reduced complexity decoding technique, which is based on Sphere Decoding (SD), is proposed to reduce the complexity of Maximum Likelihood (ML) decoding for BICMB-CP. The decreased complexity decoding achieves several orders of magnitude reduction, in terms of the average number of real multiplications needed to acquire one precoded bit metric, not only with respect to conventional ML decoding, but also, with respect to conventional SD.","2376-6492;2376-6506","978-1-4244-9538-2978-1-4244-9539-9978-1-4244-9537","10.1109/IWCMC.2011.5982407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982407","MIMO;Beamforming;Constellation Precoding;Bit-Interleaved Coded Modulation;SD;Decoding Complexity","Complexity theory;Measurement;Array signal processing;Signal to noise ratio;Interleaved codes;Maximum likelihood decoding","array signal processing;interleaved codes;maximum likelihood decoding;MIMO communication;precoding;radio receivers;radio transmitters;singular value decomposition","singular value decomposition;channel matrix;transmitter;receiver;bit-interleaved coded multiple beamforming;code rate;constellation precoding;BICMB-CP;decoding complexity;sphere decoding;maximum likelihood decoding;magnitude reduction;MIMO","","1","","18","","","","","IEEE","IEEE Conferences"
"Low ML Decoding Complexity STBCs via Codes Over the Klein Group","L. P. Natarajan; B. S. Rajan","Signal Processing Building, Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India; Signal Processing Building, Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India","IEEE Transactions on Information Theory","","2011","57","12","7950","7971","In this paper, we give a new framework for constructing low ML decoding complexity space-time block codes (STBCs) using codes over the Klein groupK. Almost all known low ML decoding complexity STBCs can be obtained via this approach. New full-diversity STBCs with low ML decoding complexity and cubic shaping property are constructed, via codes overK, for number of transmit antennasN=2m,m≥ 1, and ratesR>; 1 complex symbols per channel use. WhenR=N, the new STBCs are information-lossless as well. The new class of STBCs have the least known ML decoding complexity among all the codes available in the literature for a large set of (N,R) pairs.","0018-9448;1557-9654","","10.1109/TIT.2011.2170113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6094265","Clifford algebra;cubic shaping;full diversity;information-losslessness;Klein group;low maximum-likelihood (ML) decoding complexity;Pauli matrices;space-time codes","Complexity theory;Encoding;Maximum likelihood decoding;Quadrature amplitude modulation","communication complexity;diversity reception;space-time block codes;transmitting antennas","low ML decoding complexity;Klein group;space-time block codes;full-diversity STBC;cubic shaping property;transmit antennas;information-lossless","","3","","43","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity rate control based on ρ-domain model for Scalable Video Coding","M. Liu; Y. Guo; H. Li; C. W. Chen","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","2010 IEEE International Conference on Image Processing","","2010","","","1277","1280","In this paper, we present a novel low complexity yet very efficient rate control algorithm for Scalable Video Coding (SVC) based on the ρ-domain model. Compared with the conventional ρ-domain model in determining quantization parameter, the proposed algorithm adopts a new linear model to obtain the quantization parameter at frame-level. This linear model is able to characterize the relationship between the percentage of zero among quantized coefficients and the quantization step. Since this model considers the Mean Absolute Difference (MAD) of each frame, individual frame only needs to be encoded once to control the bit-rate. In particular, the model parameter in the ρ-domain model can be adaptively estimated by utilizing either temporal or inter-layer information. This leads to significant improvement in estimation accuracy. Experimental results show that the proposed algorithm can achieve accurate bit-rate and maintain the maximum mismatch between actual bit-rate and target bit-rate within 0.5%. More importantly, the proposed low complexity algorithm is able to obtain PSNR improvement up to 0.6dB comparing with the algorithm adopted in the Joint Scalable Video Model (JSVM).","2381-8549;1522-4880;1522-4880","978-1-4244-7994-8978-1-4244-7992-4978-1-4244-7993","10.1109/ICIP.2010.5653340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5653340","Rate control;low complexity;ρ-domain model;Scalable Video Coding","Adaptation model;Video coding;Encoding;Static VAr compensators;Estimation;Prediction algorithms;Joints","encoding;estimation theory;quantisation (signal);video coding","scalable video coding;low complexity rate control;ρ-domain model;quantization parameter;encoding;temporal information;inter-layer information;estimation accuracy;joint scalable video model","","6","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity finite alphabet iterative decoders for LDPC codes","F. Cai; X. Zhang; D. Declercq; B. Vasic; D. V. Nguyen; S. Planjery","Case Western Reserve University; Case Western Reserve University; ETIS Laboratory; University of Arizona; University of Arizona; University of Arizona","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","1332","1335","Low-density parity-check (LDPC) codes are adopted in many applications due to their Shannon-limit approaching error-correcting performance. Nevertheless, belief-propagation (BP) based decoding of these codes suffers from the error-floor problem. Recently, a new type of decoders termed finite alphabet iterative decoders (FAIDs) were introduced. The FAIDs use simple Boolean maps for variable node processing. With very short word length, they can surpass the BP-based decoders in the error floor region. This paper develops a low-complexity implementation architecture for FAIDs by making use of their properties. Particularly, an innovative bit-serial check node unit is designed for FAIDs, and the symmetric Boolean maps for variable node processing lead to small silicon area. An optimized data scheduling scheme is also proposed to increase the hardware utilization efficiency. From synthesis results, the proposed FAID implementation needs only 52% area to reach the same throughput as one of the most efficient Min-sum decoders for an example (7807, 7177) LDPC code, while achieving better error-correcting performance in the error-floor region.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6572100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572100","","Decoding;Iterative decoding;Clocks;Hardware;Computer architecture;Throughput","error correction;iterative decoding;parity check codes","low-complexity finite alphabet iterative decoders;LDPC codes;low-density parity-check codes;Shannon limit;error-correcting performance;belief propagation;BP-based decoding;error-floor problem;Boolean map;variable node processing;low-complexity implementation architecture;min-sum decoder;FAID implementation;hardware utilization efficiency;optimized data scheduling scheme;silicon area;innovative bit-serial check node unit","","1","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity decoding for non-binary LDPC codes in high order fields","A. Voicila; D. Declercq; F. Verdier; M. Fossorier; P. Urard","ETIS, ENSEA/univ. Cergy-Pontoise/CNRS; ETIS, ENSEA/univ. Cergy-Pontoise/CNRS; ETIS, ENSEA/univ. Cergy-Pontoise/CNRS; ETIS, ENSEA/univ. Cergy-Pontoise/CNRS; STMicroelectronics","IEEE Transactions on Communications","","2010","58","5","1365","1375","In this paper, we propose a new implementation of the Extended Min-Sum (EMS) decoder for non-binary LDPC codes. A particularity of the new algorithm is that it takes into accounts the memory problem of the non-binary LDPC decoders, together with a significant complexity reduction per decoding iteration. The key feature of our decoder is to truncate the vector messages of the decoder to a limited number n<sub>m</sub> of values in order to reduce the memory requirements. Using the truncated messages, we propose an efficient implementation of the EMS decoder which reduces the order of complexity to ¿(n<sub>m</sub> log<sub>2</sub> n<sub>m</sub>). This complexity starts to be reasonable enough to compete with binary decoders. The performance of the low complexity algorithm with proper compensation is quite good with respect to the important complexity reduction, which is shown both with a simulated density evolution approach and actual simulations.","0090-6778;1558-0857","","10.1109/TCOMM.2010.05.070096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464236","Iterative decoding, non-binary LDPC codes, low complexity algorithm","Parity check codes;Iterative decoding;Medical services;Channel capacity;Code standards;Digital video broadcasting;WiMAX;Modulation coding;Galois fields;Performance gain","iterative decoding;parity check codes","low-complexity decoding;nonbinary LDPC codes;extended min-sum decoder;complexity reduction;decoding iteration;truncated messages;binary decoders;simulated density evolution approach","","129","","18","","","","","IEEE","IEEE Journals & Magazines"
"VLSI Architecture for Low-Complexity Motion Estimation in H.264 Multiview Video Coding","A. Ahmed; M. U. Shahid; M. Martina; E. Magli; G. Masera","NA; NA; NA; NA; NA","2013 Euromicro Conference on Digital System Design","","2013","","","288","292","This paper presents a VLSI architecture for a low complexity motion estimation algorithm, referred to as Slim264, for multiview video coding extension of H.264. Algorithmic modifications are introduced to obtain a fully parallel computational structure able to meet the throughput requirements of high resolution and high frame rate videos. High parallelism is achieved by predicting small blocks, i.e. 4x4 pixel blocks, in parallel and then adding them up in order to get Sum of Absolute Differences (SADs) of large block sizes. The predictor is able to support high resolution videos i.e. 1080p. The modified algorithm shows promising PSNR results with respect to full search algorithm. The predictor is synthesized with a clock frequency of 200 MHz, occupying an area of 0.49 mm2, on 90-nm Standard Cell ASIC technology.","","978-1-4799-2978","10.1109/DSD.2013.145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628290","H.264/MVC;video coding;motion estimation;VLSI architecture","Prediction algorithms;Computer architecture;Video coding;Clocks;Throughput;Hardware;PSNR","application specific integrated circuits;motion estimation;parallel architectures;search problems;video coding;VLSI","standard cell ASIC technology;search algorithm;video resolution;SAD;size 90 nm;frequency 200 MHz;VLSI architecture;motion estimation;Slim264;multiview video coding extension;H.264 algorithm;parallel computational structure;throughput requirement;pixel block prediction;sum of absolute difference","","1","","17","","","","","IEEE","IEEE Conferences"
"Complexity comparison of the use of Vandermonde versus Hankel matrices to build systematic MDS Reed-Solomon codes","F. Mattoussi; V. Roca; B. Sayadi","Inria, France; Inria, France; Alcatel-Lucent Bell Labs, France","2012 IEEE 13th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2012","","","344","348","Reed Solomon RS(n, k) codes are Maximum Distance Separable (MDS) ideal codes that can be put into a systematic form, which makes them well suited to many situations. In this work we consider use-cases that rely on a software RS codec and for which the code is not fixed. This means that the application potentially uses a different RS(n, k) code each time, and this code needs to be built dynamically. A lightweight code creation scheme is therefore highly desirable, otherwise this stage would negatively impact the encoding and decoding times. Constructing such an RS code is equivalent to constructing its systematic generator matrix. Using the classic Vandermonde matrix approach to that purpose is feasible but adds significant complexity. In this paper we propose an alternative solution, based on Hankel matrices as the base matrix. We prove theoretically and experimentally that the code construction time and the number of operations performed to build the target RS code are largely in favor of the Hankel approach, which can be between 3.5 to 157 times faster than the Vandermonde approach, depending on the (n, k) parameters.","1948-3252;1948-3244;1948-3244","978-1-4673-0971-4978-1-4673-0970-7978-1-4673-0969","10.1109/SPAWC.2012.6292924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292924","AL-FEC;Reed-Solomon;Vandermonde matrix;Hankel matrix","Systematics;Generators;Reed-Solomon codes;Codecs;Complexity theory;Encoding;Decoding","decoding;encoding;Hankel matrices;Reed-Solomon codes","complexity comparison;Vandermonde matrices;Hankel matrices;systematic MDS Reed-Solomon codes;maximum distance separable ideal codes;software RS codec;lightweight code creation scheme;encoding time;decoding time;systematic generator matrix;base matrix;code construction time","","8","","10","","","","","IEEE","IEEE Conferences"
"New Constructions of High-Performance Low-Complexity Convolutional Codes","A. Katsiotis; P. Rizomiliotis; N. Kalouptsidis","National, Kapodistrian University of Athens, Department of Informatics, Telecommunications, Division of Communications, Signal Processing, Panepistimiopolis, 15784 Athens, Greece; University of the Aegean, Department of Information and Communication Systems Engineering, Karlovassi, Samos, GR-83200, Greece; National, Kapodistrian University of Athens, Department of Informatics, Telecommunications, Division of Communications, Signal Processing, Panepistimiopolis, 15784 Athens, Greece","IEEE Transactions on Communications","","2010","58","7","1950","1961","In this paper, new constructions of low trellis complexity convolutional codes are presented. New codes are found by searching into a specific class of time varying convolutional codes, which is shaped by some basic properties and search restrictions. An efficient technique for obtaining minimal trellis modules for the proposed codes is provided. Finally, new low complexity convolutional codes of various code rates and memory sizes are tabulated.","0090-6778;1558-0857","","10.1109/TCOMM.2010.07.090149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5504596","Convolutional codes;trellis complexity;minimal trellis;maximum likelihood decoding","Convolutional codes;Maximum likelihood decoding;Concatenated codes;Communications Society;Informatics;Signal processing;Communication systems;Systems engineering and theory","communication complexity;convolutional codes;maximum likelihood decoding;trellis codes","high-performance low-complexity convolutional codes;low trellis complexity convolutional codes;maximum likelihood decoding","","10","","25","","","","","IEEE","IEEE Journals & Magazines"
"Enabling complexity-performance trade-offs for successive cancellation decoding of polar codes","A. Balatsoukas-Stimming; G. Karakonstantis; A. Burg","Telecommunications Circuits Laboratory, EPFL, Lausanne, Switzerland; Telecommunications Circuits Laboratory, EPFL, Lausanne, Switzerland; Telecommunications Circuits Laboratory, EPFL, Lausanne, Switzerland","2014 IEEE International Symposium on Information Theory","","2014","","","2977","2981","Polar codes are one of the most recent advancements in coding theory and they have attracted significant interest. While they are provably capacity achieving over various channels, they have seen limited practical applications. Unfortunately, the successive nature of successive cancellation based decoders hinders fine-grained adaptation of the decoding complexity to design constraints and operating conditions. In this paper, we propose a systematic method for enabling complexity-performance tradeoffs by constructing polar codes based on an optimization problem which minimizes the complexity under a suitably defined mutual information based performance constraint. Moreover, a low-complexity greedy algorithm is proposed in order to solve the optimization problem efficiently for very large code lengths.","2157-8117;2157-8095","978-1-4799-5186","10.1109/ISIT.2014.6875380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875380","","Complexity theory;Decoding;Greedy algorithms;Optimization;Mutual information;Measurement","channel coding;computational complexity;decoding;greedy algorithms;minimisation","complexity-performance trade-offs;successive cancellation decoding;polar codes;coding theory;decoding complexity;design constraints;optimization problem;complexity minimization;mutual information based performance constraint;low-complexity greedy algorithm;code lengths","","5","","9","","","","","IEEE","IEEE Conferences"
"Design and Analysis of a Low-Complexity Decoding Algorithm for Spinal Codes","Y. Hu; R. Liu; H. Bian; D. Lyu","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Multimedia and Communication Laboratory, Beihang University, Beijing, China","IEEE Transactions on Vehicular Technology","","2019","68","5","4667","4679","In order to facilitate reliable and efficient data transmission in time-varying environments, a block dynamic decoding algorithm for spinal codes is proposed in this paper. First, the code tree is divided into several decoding units to scatter the decoding complexity. According to the results of the last decoding, the decoder will modify the scope of nodes accessed with dynamic parameters, which improves the execution efficiency of the algorithm. Furthermore, the complexity and related parameters of the algorithm are analyzed and verified by some simulations. The results show that the proposed algorithm has the ability to enhance the bandwidth efficiency (or rate) performance, and to reduce both complexity and frame error rate.","0018-9545;1939-9359","","10.1109/TVT.2019.2901495","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653979","Time-varying channels;rateless coding;spinal codes;capacity-achieving;low decoding complexity","Complexity theory;Heuristic algorithms;Encoding;Maximum likelihood decoding;Signal to noise ratio;Vehicle dynamics","","","","","","22","","","","","IEEE","IEEE Journals & Magazines"
"A high coding gain and low decoding complexity STBC for four transmit antennas","N. Sharma; M. R. Bhatnagar; M. Agrawal","Department of Electrical Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India; Centre for Applied Research in Electronics, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India","2012 National Conference on Communications (NCC)","","2012","","","1","5","Space-time block codes (STBCs) have attracted considerable attention as they provide reliable communication in multi-antenna systems. STBCs work on the principle of multiple transmissions of a data stream from different transmit antennas in multiple time slots. This redundancy reduces the data rate of the STBCs. In this paper, we propose a mapping function for PAM and square QAM constellation. Based on this mapping function, we propose an STBC for four transmit antennas which achieves high coding gain. We derive an expression of coding gain of the proposed STBC. It is further shown that the proposed STBC achieves full diversity and has a low peak-to-average power ratio (PAPR). A low-complexity decoder for the proposed STBC is also derived. We show that the proposed STBC outperforms the existing STBCs for four transmit antennas in terms of coding gain, PAPR, and decoding complexity.","","978-1-4673-0816-8978-1-4673-0815-1978-1-4673-0814","10.1109/NCC.2012.6176786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6176786","Coding gain;full-diversity;full-rate;multiple input-multiple output (MIMO);space-time block code (STBC)","Decoding;Peak to average power ratio;Complexity theory;Transmitting antennas;Quadrature amplitude modulation;Block codes","decoding;pulse amplitude modulation;quadrature amplitude modulation;slot antenna arrays;space-time block codes;transmitting antennas","high coding gain;low decoding complexity STBC;four transmit antennas;space-time block codes;multiantenna systems;data stream transmission;multiple time slot antenna;mapping function;square QAM constellation;PAM constellation;low peak-to-average power ratio;low-complexity decoder;pulse amplitude modulation","","","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity decoders for non-binary turbo codes","R. Klaimi; C. A. Nour; C. Douillard; J. Farah","IMT Atlantique, CNRS UMR 6285 Lab-STICC, Brest, France; IMT Atlantique, CNRS UMR 6285 Lab-STICC, Brest, France; IMT Atlantique, CNRS UMR 6285 Lab-STICC, Brest, France; Faculty of Engineering, Lebanese University, Roumieh, Lebanon","2018 IEEE 10th International Symposium on Turbo Codes & Iterative Information Processing (ISTC)","","2018","","","1","5","Following the increasing interest in non-binary coding schemes, turbo codes over different Galois fields have started to be considered recently. While showing improved performance when compared to their binary counterparts, the decoding complexity of this family of codes remains a main obstacle to their adoption in practical applications. In this work, a new low-complexity variant of the Min-Log-MAP algorithm is proposed. Thanks to the introduction of a bubble sorter for the different metrics used in the Min-Log-MAP decoder, the number of required computations is significantly reduced. A reduction by a factor of 6 in the number of additions and compare-select operations can be achieved with only a minor impact on error rate performance. With the use of an appropriate quantization, the resulting decoder paves the way for a future hardware implementation.","2165-4719;2165-4700","978-1-5386-7048-4978-1-5386-7047-7978-1-5386-7049","10.1109/ISTC.2018.8625359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625359","Non-binary turbo codes;decoding algorithm;bubble check;complexity reduction","Measurement;Decoding;Complexity theory;Sorting;Convolutional codes;Systematics;Turbo codes","binary codes;decoding;error statistics;Galois fields;maximum likelihood decoding;turbo codes","low-complexity decoders;decoding complexity;compare-select operations;error rate performance;Galois fields;minlog-MAP decoder algorithm;nonbinary turbo coding schemes;bubble sorter","","","","12","","","","","IEEE","IEEE Conferences"
"Low complexity minimum mean square error channel estimation for adaptive coding and modulation systems","G. Shuxia; S. Yang; G. Ying; H. Qianjin","Science and Technology on UAV Laboratory, Northwestern Polytechnical University, Xi'an 710065, China; AVIC Xi'an Flight Automatic Control Research Institute, Xi'an 710065, China; College of Marine Engineering, Northwestern Polytechnical University, Xi'an 710072, China; Science and Technology on UAV Laboratory, Northwestern Polytechnical University, Xi'an 710065, China","China Communications","","2014","11","1","126","137","Performance of the Adaptive Coding and Modulation (ACM) strongly depends on the retrieved Channel State Information (CSI), which can be obtained using the channel estimation techniques relying on pilot symbol transmission. Earlier analysis of methods of pilot-aided channel estimation for ACM systems were relatively little. In this paper, we investigate the performance of CSI prediction using the Minimum Mean Square Error (MMSE) channel estimator for an ACM system. To solve the two problems of MMSE: high computational operations and oversimplified assumption, we then propose the Low-Complexity schemes (LC-MMSE and Recursion LC-MMSE (R-LC-MMSE)). Computational complexity and Mean Square Error (MSE) are presented to evaluate the efficiency of the proposed algorithm. Both analysis and numerical results show that LC-MMSE performs close to the well-known MMSE estimator with much lower complexity and R-LC-MMSE improves the application of MMSE estimation to specific circumstances.","1673-5447","","10.1109/CC.2014.6821315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821315","adaptive coding and modulation;channel estimation;minimum mean square error;low-complexity minimum mean square error","Channel estimation;Radio frequency;Complexity theory;Estimation;OFDM;Mean square error methods;Noise measurement","adaptive codes;adaptive modulation;channel coding;channel estimation;computational complexity;least mean squares methods;time-varying channels","computational complexity;R-LC-MMSE;recursion LC-MMSE;low-complexity schemes;MMSE channel estimator;minimum mean square error channel estimator;CSI prediction;ACM systems;pilot-aided channel estimation;pilot symbol transmission;channel estimation techniques;channel state information;adaptive coding and modulation","","1","","","","","","","IEEE","IEEE Journals & Magazines"
"Agent based tool for topologically sorting badsmells and refactoring by analyzing complexities in source code","S. AyshwaryaLakshmi; S. A. S. A. Mary; S. S. Vadivu","M.E(Computer Science) University college of Engineering, Panruti; Dept of Computer Science and Engg, Jayaram College of Engineering and Tech, Pagalavadi, Tiruchirapalli-621014; M.E(Computer Science) University college of engineering","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","","2013","","","1","7","Code smells are smells found in source code. As the source code becomes larger and larger we find bad smells in the source code. These bad smells are removed using Refactoring. Hence experts say the method of removing bad smells without changing the quality of code is called as Refactoring[1]. But this Refactoring if not done properly is risky, and can take time (i.e.) might be days or weeks. Hence here we provide a technique to arrange these Bad smells analyze the complexities found in the source code and then Refactor them. These Bad smell detection and scheduling has been done manually or semi automatically. This paper provides a method of automatically detecting theses Bad smells. This Automatic detection of Bad smells are done with the help of Java Agent DEvelopment.","","978-1-4799-3926-8978-1-4799-3925","10.1109/ICCCNT.2013.6726851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726851","Software Engineering;Refactoring;Scheduling Badsmells","Complexity theory;Redundancy;Sorting;Educational institutions;Java;Software","Java;scheduling;software metrics;software quality;sorting;source code (software)","badsmells sorting;refactoring;source code;code smells;bad smell detection;scheduling;automatic detection;Java agent development","","","","15","","","","","IEEE","IEEE Conferences"
"Reduced Complexity Node-Wise Scheduling of ADMM Decoding for LDPC Codes","X. Jiao; J. Mu; H. Wei","School of Computer Science and Technology, Xidian University, Xi’an, 710071; School of Computer Science and Technology, Xidian University, Xi’an, 710071; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada","IEEE Communications Letters","","2017","21","3","472","475","Similar to the belief propagation decoder, linear programming decoding based on the alternating direction method of multipliers (ADMM) can also be seen as an iterative message-passing decoding algorithm. How to schedule messages efficiently is an important aspect since it will influence the convergence rate of iterative decoders. In this letter, we investigate the node-wise scheduling for ADMM decoders, named NS-ADMM. In particular, we propose a reduced-complexity method for the NS-ADMM decoder by avoiding Euclidean projections involved in the calculation of message residuals. Simulation results show that the proposed method converges much faster than the flooding and layered scheduling while keeping a lower complexity when compared with the NS-ADMM decoder.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2643629","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795194","Alternating direction method of multipliers (ADMM);message scheduling;low-density parity-check (LDPC) codes;convergence rate","Decoding;Iterative decoding;Complexity theory;Convergence;Signal to noise ratio;Indexes","communication complexity;iterative decoding;linear programming;message passing;parity check codes;telecommunication scheduling","reduced complexity node-wise scheduling;ADMM decoding;LDPC codes;linear programming decoding;alternating direction method of multipliers;iterative message-passing decoding algorithm;message schedule;NS-ADMM decoder;message residual calculation;low-density parity-check codes","","7","","18","","","","","IEEE","IEEE Journals & Magazines"
"Security and complexity of the McEliece cryptosystem based on quasi-cyclic low-density parity-check codes","M. Baldi; M. Bianchi; F. Chiaraluce","Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, Ancona, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica delle Marche, Ancona, Italy","IET Information Security","","2013","7","3","212","220","In the context of public key cryptography, the McEliece cryptosystem represents a very smart solution based on the hardness of the decoding problem, which is believed to be able to resist the advent of quantum computers. Despite this, the original McEliece cryptosystem based on Goppa codes, has encountered limited interest in practical applications, partly because of some constraints imposed by this very special class of codes. The authors have recently introduced a variant of the McEliece cryptosystem including low-density parity-check codes, that are state-of-the-art codes, now used in many telecommunication standards and applications. In this study, the authors discuss the possible use of a bit-flipping decoder in this context, which gives a significant advantage in terms of complexity. The authors also provide theoretical arguments and practical tools for estimating the trade-off between security and complexity, in such a way to give a simple procedure for the system design.","1751-8709;1751-8717","","10.1049/iet-ifs.2012.0127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6587877","","","cyclic codes;decoding;Goppa codes;parity check codes;public key cryptography","McEliece cryptosystem complexity;quasicyclic low-density parity-check codes;public key cryptography;decoding problem;quantum computers;Goppa codes;telecommunication standards;bit-flipping decoder;system design","","10","","49","","","","","IET","IET Journals & Magazines"
"Low complexity network error correction based on nonbinary LDPC codes over matrix channels","Yang Yu; W. Chen","Network Coding and Transmission Laboratory, Shanghai Jiao Tong University, 200240, China; Network Coding and Transmission Laboratory, Shanghai Jiao Tong University, 200240, China","2013 IEEE Wireless Communications and Networking Conference (WCNC)","","2013","","","2801","2806","This paper presents a low-complexity network error correction (NEC) code for wireless relay networks (WRN), where the non-binary low density parity check (LDPC) code is jointly designed with a random network code. We give different transmission schemes under which the decoding complexity of the network code can be much reduced by dividing the transfer matrix into smaller decodable sub-matrices. We also propose a complexity optimization algorithm to the non-binary LDPC code based on an upper bound of message error probability. Simulations show that the complexity optimized codes can outperform the threshold optimized codes in higher SNR regime.","1525-3511;1525-3511;1558-2612","978-1-4673-5939-9978-1-4673-5938-2978-1-4673-5937","10.1109/WCNC.2013.6555004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6555004","Network error correction;non-binary LDPC;ADT networks;performance-complexity tradeoff","Complexity theory;Decoding;Relays;Error probability;Optimization;Iterative decoding","channel coding;error correction codes;network coding;optimisation;parity check codes;radio networks;random codes","SNR regime;complexity optimized codes;message error probability;complexity optimization algorithm;decodable submatrices;network code;transmission schemes;random network code;nonbinary low density parity check code;wireless relay networks;NEC code;matrix channels;nonbinary LDPC codes;low complexity network error correction","","","","20","","","","","IEEE","IEEE Conferences"
"Coding schemes with constant sphere-decoding complexity and high DMT performance for MIMO multiple access channels with low-rate feedback","T. Tang; H. Tien; H. Lu","Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","2014 IEEE International Symposium on Information Theory","","2014","","","1902","1906","In a MIMO MAC where the base station has fewer receive antennas than the transmit antennas of all users, sphere (lattice) decoding for the existing MIMO-MAC codes requires an exhaustive search of exponentially large size before processing the root of a sphere-decoding tree. In this paper, two coding schemes are proposed and are shown to yield a constant sphere-decoding complexity, independent of the numbers of users and transmit antennas. The schemes require a channel feedback, but only at an extremely low rate. The first scheme is based on user selection, and the second scheme selects jointly users and transmit antennas, using a fast antenna selection algorithm recently proposed by Jiang and Varanasi. It also involves a design of rate-assignments that maximizes the overall DMT performance. It is shown that both schemes yield DMT performances far superior to the optimal MIMO-MAC DMT without channel feedback in certain multiplexing gain regime. Simulation results confirm that in some cases the second proposed scheme can provide an astonishing SNR gain of 14:5 dB at outage probability 10-6 compared to the optimal coding schemes without feedback.","2157-8117;2157-8095","978-1-4799-5186","10.1109/ISIT.2014.6875164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875164","","MIMO;Signal to noise ratio;Multiplexing;Transmitting antennas;Complexity theory;Decoding;Algorithm design and analysis","channel coding;communication complexity;diversity reception;error statistics;feedback;MIMO communication;multiplexing;receiving antennas;transmitting antennas","base station;receive antenna;MIMO-MAC codes;sphere decoding tree;coding scheme;constant sphere decoding complexity;transmit antennas;channel feedback;user selection;antenna selection algorithm;rate assignment;optimal MIMO-MAC DMT;multiplexing gain regime;SNR gain;outage probability;multiple input multiple output;multiple access channel;low rate feedback;diversity multiplexing gain tradeoff","","2","","6","","","","","IEEE","IEEE Conferences"
"A Classification of Video Games based on Game Characteristics linked to Video Coding Complexity","S. Zadtootaghaj; S. Schmidt; N. Barman; S. Möller; M. G. Martini","Telekom Innovation Labs, Deutsche Telekom AG, Germany; Quality and Usability Lab, TU Berlin, Germany; WMN Research Group, Kingston University, London, UK; Quality and Usability Lab, TU Berlin, Germany; WMN Research Group, Kingston University, London, UK","2018 16th Annual Workshop on Network and Systems Support for Games (NetGames)","","2018","","","1","6","Applications used for video streaming of gaming content have seen tremendous growth over the recent years as evident with the increasing popularity of services such as Twitch.tv and YouTubeGaming. Gaming video streaming encoding needs to be performed in real-time and thus has a strict set of encoding constraints. Therefore, many traditional encoding optimization methods such as multiple-pass encoding are not suitable for live gaming video streaming applications. The video quality of streaming services is highly content dependent. While this holds true also for conventional contents, there exist many characteristics of games that do not vary much over time. Therefore, such game-specific information can be exploited to optimize the encoding process. In this paper, we present a classification of games using characteristics such as the type of camera movement, texture details, and static areas of a scene. Using a database of gaming videos from different genres and complexity, we obtain clusters corresponding to the calculated quality values (VMAF). The derived gaming characteristics are then mapped to the quality classes to obtain a decision tree based game classification. We illustrate how the classification can be used for encoding bitrate selection and quality prediction.","2156-8146","978-1-5386-6098-0978-1-5386-6097-3978-1-5386-6099","10.1109/NetGames.2018.8463434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8463434","Gaming;Classification;Video encoding","Games;Streaming media;Complexity theory;Encoding;Cameras;Bit rate;Quality of experience","computer games;decision trees;image classification;image texture;optimisation;video coding;video streaming","game characteristics;video coding complexity;gaming content;encoding constraints;multiple-pass encoding;live gaming video streaming applications;video quality;game-specific information;encoding process;decision tree based game classification;Twitch.tv;YouTubeGaming;encoding optimization methods;VMAF;encoding bitrate selection;gaming video streaming encoding;camera movement","","1","","18","","","","","IEEE","IEEE Conferences"
"Computational complexity control for HEVC based on coding tree spatio-temporal correlation","G. Correa; P. Assuncao; L. A. da Silva Cruz; L. Agostini","Instituto de Telecomunicações, University of Coimbra, Coimbra, Portugal; Instituto de Telecomunicações, Polytechnic Institute of Leiria, Leiria, Portugal; Instituto de Telecomunicações, University of Coimbra, Coimbra, Portugal; Group of Architectures and Integrated Circuits, Federal University of Pelotas, Pelotas, Brasil","2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)","","2013","","","937","940","The High Efficiency Video Coding standard shows improved compression efficiency in comparison to previous standards at the cost of higher computational complexity. In this paper, a complexity control method for HEVC encoders based on the dynamic adjustment of the number of constrained coding treeblocks is proposed. The method limits the maximum tree depth used in the coding structures based on spatio-temporal correlation in order to decrease the number of evaluations performed in the Rate-Distortion Optimization process. Experimental results show that the proposed method is capable of maintaining the encoding time per frame under a pre-defined target, reaching computational complexity decreases of up to 50% at the cost of an average BD-PSNR loss of 0.26 dB in the worst case scenario.","","978-1-4799-2452","10.1109/ICECS.2013.6815566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815566","","Encoding;Computational complexity;Standards;Video coding;Video sequences;Rate-distortion","computational complexity;correlation methods;video coding","computational complexity control;HEVC;coding tree spatio-temporal correlation;high efficiency video coding standard;dynamic adjustment;constrained coding treeblock;rate-distortion optimization process","","1","","8","","","","","IEEE","IEEE Conferences"
"Low-Complexity Soft-Output Decoding of Polar Codes","U. U. Fayyaz; J. R. Barry","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332—0250; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia 30332—0250","IEEE Journal on Selected Areas in Communications","","2014","32","5","958","966","The state-of-the-art soft-output decoder for polar codes is a message-passing algorithm based on belief propagation, which performs well at the cost of high processing and storage requirements. In this paper, we propose a low-complexity alternative for soft-output decoding of polar codes that offers better performance but with significantly reduced processing and storage requirements. In particular we show that the complexity of the proposed decoder is only 4% of the total complexity of the belief propagation decoder for a rate one-half polar code of dimension 4096 in the dicode channel, while achieving comparable error-rate performance. Furthermore, we show that the proposed decoder requires about 39% of the memory required by the belief propagation decoder for a block length of 32768.","0733-8716;1558-0008","","10.1109/JSAC.2014.140515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804940","Polar codes;soft-output decoding;turbo equalization","Decoding;Memory management;Iterative decoding;Complexity theory;Belief propagation;Receivers","decoding;error statistics;message passing","low-complexity soft-output decoding;polar codes;message passing algorithm;belief propagation decoder;dicode channel;error rate performance","","44","","24","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity transform coding for depth maps in 3D video","F. Jäger; K. Naser","Institut für Nachrichtentechnik, RWTH Aachen University, GERMANY; Institut für Nachrichtentechnik, RWTH Aachen University, GERMANY","2013 Visual Communications and Image Processing (VCIP)","","2013","","","1","6","3D video is a new technology, which requires transmission of depth data alongside conventional 2D video. The additional depth information allows to synthesize arbitrary viewpoints at the receiver for adaptation of perceived depth impression and for driving of multi-view auto-stereoscopic displays. Depth maps typically show different signal characteristics compared to textured video data. Piecewise smooth regions are bounded by sharp edges resembling depth discontinuities. These edges lead to strong ringing artifacts when depth maps are coded with DCT-based transform codecs, such as AVC or its successor HEVC. In this paper alternative transforms are proposed to be used for coding depth maps for 3D video. By replacing the DCT with these transforms, ringing artifacts in the reconstructed depth maps are reduced and at the same time the complexity of the transform stage is lowered significantly. For high quality depth map coding the proposed alternative transforms can even increase coding efficiency.","","978-1-4799-0290-3978-1-4799-0288","10.1109/VCIP.2013.6706338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706338","depth map;video coding;transforms;complexity;3d video","Encoding;Discrete cosine transforms;Complexity theory;Bit rate;Three-dimensional displays;PSNR","discrete cosine transforms;image reconstruction;transform coding;video coding","depth map reconstruction;ringing artifacts;depth map coding;DCT-based transform codecs;depth discontinuity;piecewise smooth region;textured video data;signal characteristic;multiview auto-stereoscopic display;depth information;3D video technology;low complexity transform coding","","1","","16","","","","","IEEE","IEEE Conferences"
"Bit-Reliability Based Low-Complexity Decoding Algorithms for Non-Binary LDPC Codes","Q. Huang; M. Zhang; Z. Wang; L. Wang","NA; NA; NA; NA","IEEE Transactions on Communications","","2014","62","12","4230","4240","This paper presents bit-reliability based majority-logic decoding (MLgD) algorithms for non-binary LDPC codes. The proposed algorithms pass only one Galois field element and its reliability along each edge of the Tanner graph of a non-binary LDPC code. Since their reliability updates are in terms of bits rather than symbols, they are more efficient than traditional MLgD based decoding algorithms. By weighting the soft reliability of the extrinsic information-sums based on their hard reliability, the proposed algorithms can achieve good error performance for non-binary LDPC codes with various column weights. Moreover, their computational complexity and memory consumption are remarkably reduced compared with existing MLgD based decoding algorithms. As a result, they provide effective tradeoffs between error performance and complexity for decoding of non-binary LDPC codes.","0090-6778;1558-0857","","10.1109/TCOMM.2014.2370032","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6954447","Non-binary LDPC codes;majority-logic decoding;bit-reliability;binary representation;Hamming distance;Non-binary LDPC codes;majority-logic decoding;bit-reliability;binary representation;hamming distance","Hamming distance;Decoding;Computational complexity;Iterative decoding;Parity check codes","computational complexity;decoding;Galois fields;graph theory;parity check codes;reliability","bit reliability;low complexity decoding algorithm;nonbinary LDPC codes;majority logic decoding algorithms;Galois field element;Tanner graph;soft reliability;computational complexity;memory consumption","","15","","22","","","","","IEEE","IEEE Journals & Magazines"
"Global motion compensation and spectral entropy bit allocation for low complexity video coding","M. Bhaskaranand; J. D. Gibson","Department of Electrical and Computer Engineering University of California, Santa Barbara, CA - 93106; Department of Electrical and Computer Engineering University of California, Santa Barbara, CA - 93106","2012 IEEE International Conference on Communications (ICC)","","2012","","","2043","2047","Most standard video compression schemes such as H.264/AVC involve a high complexity encoder with block motion estimation (ME) engine. However, applications such as video reconnaissance and surveillance using unmanned aerial vehicles (UAVs) require a low complexity video encoder. Additionally, in such applications, the motion in the video is primarily global and due to the known movement of the camera platform. Therefore in this work, we propose and investigate a low complexity encoder with global motion based frame prediction and no block ME. We show that for videos with mostly global motion, this encoder performs better than a baseline H.264 encoder with ME block size restricted to 8×8. Furthermore, the quality degradation of this encoder with decreasing bit rate is more gradual than that of the baseline H.264 encoder since it does not need to allocate bits across motion vectors (MVs) and residue data. We also incorporate a spectral entropy based coefficient selection and quantizer design scheme that entails latency and demonstrate that it helps achieve more consistent frame quality across the video sequence.","1938-1883;1550-3607;1550-3607","978-1-4577-2053-6978-1-4577-2052-9978-1-4577-2051","10.1109/ICC.2012.6364046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6364046","","Bit rate;Entropy;Complexity theory;Transforms;Video coding;PSNR;Streaming media","data compression;entropy;image sequences;motion compensation;motion estimation;quantisation (signal);video cameras;video coding","global motion compensation;spectral entropy bit allocation;low complexity video coding;video compression schemes;H.264/AVC standard;block motion estimation engine;ME engine;video reconnaissance;video surveillance;unmanned aerial vehicles;UAV;low complexity video encoder;camera platform;global motion based frame prediction;baseline H.264 encoder;motion vectors;residue data;spectral entropy based coefficient selection;quantizer design scheme;video sequence","","2","","25","","","","","IEEE","IEEE Conferences"
"Low Complexity Turbo Code Specification for Power-Line Communication (PLC)","A. M. Cruz; R. B. Fern&#x0B4;ndez; J. L. O. Rodriguez; G. A. L. S&#x0B4;nchez","NA; NA; NA; NA","2011 IEEE Electronics, Robotics and Automotive Mechanics Conference","","2011","","","349","354","The development of turbo codes over the past two decades, has resulted in significant performance improvement of PLC communication systems, considerable reduction of the device size, development of more complex chips with channel coding support and implementation of more robust modulation techniques. Today, some of the major research problems are related to the implementation of efficient architectures for decoding algorithms, development of techniques for efficient power consumption management, and latency reduction in the hardware used. In this article we present a specification for a turbo code scheme applied to an OFDM transceiver on a PLC channel based on simulation results using a channel model impaired with AWGN and impulsive noise. The proposed specification includes practical considerations on the basis that a real turbo coding scheme (i.e. on a DSP) has less processing resources than those necessary to implement the original MAP-BCJR algorithm. The specification presented facilitates implementation in a device such as a DSP.","","978-1-4577-1879","10.1109/CERMA.2011.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6125855","Power Line Communications;Turbo code;Max-log-MAP;MAP-BCJR;OFDM","Turbo codes;Noise;Decoding;OFDM;Communication systems","carrier transmission on power lines;channel coding;decoding;maximum likelihood estimation;OFDM modulation;radio transceivers;turbo codes;wireless channels","low complexity turbo code specification;power-line communication;PLC communication systems;channel coding;robust modulation techniques;decoding algorithms;power consumption management;OFDM transceiver;PLC channel;AWGN;impulsive noise;DSP;maximum a posteriori BCJR-MAP algorithm","","1","","18","","","","","IEEE","IEEE Conferences"
"Low-complexity rate control in video coding based on bi-geometric transparent composite models","Y. Gao; E. Yang; D. He","Dept. of Electrical and Computer Engineering, University of Waterloo, 200 University Avenue West, Waterloo, Ontario, N2L 3G1, Canada; Dept. of Electrical and Computer Engineering, University of Waterloo, 200 University Avenue West, Waterloo, Ontario, N2L 3G1, Canada; BlackBerry, 2220 University Ave. E., Waterloo, ON N2K 0A8, Canada","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1419","1423","Bi-geometric transparent composite models (BGTCM) are used to model distributions of transform coefficients in HEVC (High efficiency video coding). Both Kullback-Leibler divergence and χ2test show that, for both original and quantized transform coefficients in HEVC, BGTCMs provide better modelling performance than popular Laplacian and Cauchy models. Based on BGTCMs, a rate control algorithm is proposed for HEVC. Experimental results using the HEVC reference software show that the proposed algorithm achieves better performance in constant-bit-rate control than previous rate control algorithms based on Laplacian models.","","978-1-4799-8339-1978-1-4799-8338","10.1109/ICIP.2015.7351034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351034","Distribution of transformed coefficients;video coding;transparent composite model;rate control","Computational modeling;Transforms;Laplace equations;Video coding;Quantization (signal);Distortion;Computational complexity","computational complexity;geometry;transforms;video coding","low-complexity rate control;bigeometric transparent composite models;BGTCM;transform coefficients;HEVC;high efficiency video coding;Kullback-Leibler divergence;Laplacian models","","","","17","","","","","IEEE","IEEE Conferences"
"Low ML-detection complexity, adaptive 2×2 STBC, with powerful FEC codes","A. E. Falou; C. Langlais; C. A. Nour; C. Douillard","Institut Telecom; Telecom Bretagne; UMR CNRS 3192 Lab-STICC, Electronics Department, Technopôle Brest Iroise CS 83818, 29238 Brest Cedex 3, Université européenne de Bretagne, France; Institut Telecom; Telecom Bretagne; UMR CNRS 3192 Lab-STICC, Electronics Department, Technopôle Brest Iroise CS 83818, 29238 Brest Cedex 3, Université européenne de Bretagne, France; Institut Telecom; Telecom Bretagne; UMR CNRS 3192 Lab-STICC, Electronics Department, Technopôle Brest Iroise CS 83818, 29238 Brest Cedex 3, Université européenne de Bretagne, France; Institut Telecom; Telecom Bretagne; UMR CNRS 3192 Lab-STICC, Electronics Department, Technopôle Brest Iroise CS 83818, 29238 Brest Cedex 3, Université européenne de Bretagne, France","2012 7th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2012","","","195","199","We present an adaptive space time block code (STBC) for 2×2 MIMO systems designed to be used with powerful forward error correcting (FEC) codes and having a low ML-detection complexity. STBCs are commonly designed according to the rank-determinant criteria, suitable for high signal to noise ratio (SNR) values. However, powerful FEC codes like turbo codes achieve iterative convergence at low to moderate SNR. Thus, we first propose a non-asymptotic STBC design criterion based on the bitwise mutual information (BMI) maximization at a specific target SNR. According to this criterion, an adaptive matrix D based STBC is defined for a wide range of SNRs. Resulting code outperforms the original matrix D and closes the gap to the more complex Golden code in the context of a WiMAX system.","2165-4719;2165-4700;2165-4700","978-1-4577-2115-1978-1-4577-2114-4978-1-4577-2113","10.1109/ISTC.2012.6325226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6325226","","MIMO;Complexity theory;Modulation;Signal to noise ratio;Turbo codes;Bit error rate","adaptive codes;convergence;forward error correction;maximum likelihood detection;MIMO communication;optimisation;space-time block codes;turbo codes;WiMax","ML-detection complexity;adaptive STBC;powerful FEC codes;adaptive space time block code;MIMO systems;powerful forward error correcting codes;rank-determinant criteria;high signal to noise ratio values;SNR values;turbo codes;iterative convergence;nonasymptotic STBC design criterion;bitwise mutual information maximization;BMI maximization;target SNR;adaptive matrix D;Golden code;WiMAX system","","3","","13","","","","","IEEE","IEEE Conferences"
"Low-Complexity Detection for Generalized Pre-Coding Aided Spatial Modulation","N. S. Perovic; W. Haselmayr; A. Springer","NA; NA; NA","2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)","","2015","","","1","5","In this paper we consider Generalized Pre-coding aided Spatial Modulation (GPSM), which was recently proposed as a promising alternative to conventional Multiple Input Multiple Output (MIMO) transmission schemes. In GPSM only a part of the receive antennas is activated with the aid of pre- coding at the transmitter. Hence, information bits are mapped to a spatial symbol, corresponding to a particular activation pattern, and to modulation symbols. Optimal performance is achieved with a Maximum Likelihood (ML) detector, but its exhaustive search leads to an intractable complexity. In this paper we present a novel detector, referred to as Soft MMSE with Exhaustive Search (SOMES) detector, that computes soft information for each symbol by employing a soft- output Minimum Mean Square Error (MMSE) detector. The soft information is used to determine the activation pattern using a small exhaustive search and to obtain the symbols in the particular activation pattern. Link level simulations show that the proposed algorithm possesses the near- optimal Bit Error Rate (BER) performance while achieving a remarkable reduction in complexity.","","978-1-4799-8091-8978-1-4799-8090","10.1109/VTCFall.2015.7391015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391015","","Detectors;Modulation;Complexity theory;Receiving antennas;Bit error rate;MIMO;Matrix decomposition","error statistics;least mean squares methods;maximum likelihood detection;MIMO communication;precoding","BER performance;bit error rate performance;soft-output MMSE detector;soft-output minimum mean square error detector;soft information;SOMES detector;soft MMSE;exhaustive search;maximum likelihood detector;spatial symbol;information bits;receive antennas;MIMO transmission schemes;multiple input multiple output transmission schemes;GPSM;generalized precoding aided spatial modulation","","2","","11","","","","","IEEE","IEEE Conferences"
"A Low Complexity Decoding Algorithm for Spinal Codes with Efficiently Distributed Symbols","Y. Hu; R. Liu; A. Kaushik; X. Shi; J. Thompson","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Institute for Digital Communications, The University of Edinburgh, Edinburgh, UK; Institute for Digital Communications, The University of Edinburgh, Edinburgh, UK; Institute for Digital Communications, The University of Edinburgh, Edinburgh, UK","2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","","2018","","","184","191","The following topics are dealt with: field programmable gate arrays; reconfigurable architectures; system-on-chip; feature extraction; fault tolerance; cryptography; aerospace computing; radiation hardening (electronics); reliability; fault tolerant computing.","2471-769X;1939-7003","978-1-5386-7753-7978-1-5386-7752-0978-1-5386-7754","10.1109/AHS.2018.8541487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541487","wireless communications;time-varying channel;spinal codes;low decoding complexity.","Maximum likelihood decoding;Complexity theory;Encoding;Heuristic algorithms;Receivers;Digital communication","fault tolerance;feature extraction;field programmable gate arrays;reconfigurable architectures;system-on-chip","field programmable gate arrays;reconfigurable architectures;system-on-chip;feature extraction;fault tolerance;cryptography;aerospace computing;radiation hardening (electronics);reliability;fault tolerant computing","","","","27","","","","","IEEE","IEEE Conferences"
"Reducing complexity with less than minimum delay space-time lattice codes","R. Vehkalahti; C. Hollanti","Department of Mathematics, FI-20014 University of Turku, Finland; Department of Mathematics, FI-20014 University of Turku, Finland","2011 IEEE Information Theory Workshop","","2011","","","130","134","Recently, several papers have been concentrating on reducing the decoding complexity of high-rate space-time codes. While the research has led to some impressive reductions in decoding complexity, the geometric methods so far used appear to have faced some fundamental limits. In this paper, we study what happens if we let go of the assumption of full diversity and study the possibility of reducing the complexity, while holding on to a high code rate, by reducing the length of codes. We will develop some tools that can be used to measure the changes we will encounter when reducing the code length. We will also study the achievable diversity-multiplexing gain trade-off (DMT) of codes with less than minimum delay (LMD) and discuss some code constructions.","","978-1-4577-0437-6978-1-4577-0438-3978-1-4577-0436","10.1109/ITW.2011.6089361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089361","","Lattices;Encoding;Signal to noise ratio;Complexity theory;Decoding;Delay;Receiving antennas","decoding;space-time codes","minimum delay;space-time lattice codes;decoding complexity;code length reduction;diversity-multiplexing gain trade-off","","6","","14","","","","","IEEE","IEEE Conferences"
"Rate-reliability-complexity tradeoff for ML and lattice decoding of full-rate codes","A. Singh; P. Elia; J. Jaldén","Mobile Communications Department EURECOM, Sophia Antipolis, France; Mobile Communications Department EURECOM, Sophia Antipolis, France; ACCESS Linnaeus Center, Signal Processing Lab, School of Electrical Engineering, KTH, Stockholm, Sweden","2013 IEEE International Symposium on Information Theory","","2013","","","1267","1271","Recent work in [1]-[3] quantified, in the form of a complexity exponent, the computational resources required for ML and lattice sphere decoding to achieve a certain diversity-multiplexing performance. For a specific family of layered lattice designs, and a specific set of decoding orderings, this complexity was shown to be an exponential function in the number of codeword bits, and was shown to meet a universal upper bound on complexity exponents. The same results raised the question of whether complexity reductions away from the universal upper bound are feasible, for example, with a proper choice of decoder (ML vs lattice), or with a proper choice of lattice codes and decoding ordering policies. The current work addresses this question by first showing that for almost any full-rate DMT optimal lattice code, there exists no decoding ordering policy that can reduce the complexity exponent of ML or lattice based sphere decoding away from the universal upper bound, i.e., that a randomly picked lattice code (randomly and uniformly drawn from an ensemble of DMT optimal lattice designs) will almost surely be such that no decoding ordering policy can provide exponential complexity reductions away from the universal upper bound. As a byproduct of this, the current work proves the fact that ML and (MMSE-preprocessed) lattice decoding share the same complexity exponent for a very broad setting, which now includes almost any DMT optimal code (again randomly drawn) and all decoding order policies. Under a basic richness of codes assumption, this is in fact further extended to hold, with probability one, over all full-rate codes. Under the same assumption, the result allows for a meaningful rate-reliability-complexity tradeoff that holds, almost surely in the random choice of the full-rate lattice design, and which holds irrespective of the decoding ordering policy. This tradeoff can be used to, for example, describe the optimal achievable diversity gain of ML or lattice sphere decoding in the presence of limited computational resources.","2157-8117;2157-8095","978-1-4799-0446","10.1109/ISIT.2013.6620430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620430","","Lattices;Complexity theory;Upper bound;Maximum likelihood decoding;MIMO;Fading","communication complexity;diversity reception;maximum likelihood decoding;multiplexing;probability;reliability theory","rate reliability complexity;ML decoding;lattice sphere decoding;diversity multiplexing performance;layered lattice design;exponential function;codeword bit;universal upper bound;complexity exponent;DMT optimal lattice code;exponential complexity reduction;probability;full rate lattice code design;limited computational resource;decoding ordering policy","","1","","10","","","","","IEEE","IEEE Conferences"
"Reduced-complexity decoding of LT codes over noisy channels","I. Hussain; Ming Xiao; L. K. Rasmussen","School of Electrical Engineering, Communication Theory Laboratory, KTH Royal Institute of Technology and the ACCESS Linnaeus Center, Stockholm, Sweden; School of Electrical Engineering, Communication Theory Laboratory, KTH Royal Institute of Technology and the ACCESS Linnaeus Center, Stockholm, Sweden; School of Electrical Engineering, Communication Theory Laboratory, KTH Royal Institute of Technology and the ACCESS Linnaeus Center, Stockholm, Sweden","2013 IEEE Wireless Communications and Networking Conference (WCNC)","","2013","","","3856","3860","We propose an adaptive decoding scheme for Luby Transform (LT) codes over noisy channels which exhibits lower complexity as compared to the conventional LT decoder. The corresponding modified degree distributions have been derived for the low-complexity LT decoder. The complexity and performance comparison demonstrate that the decoding complexity can be reduced with negligible degradation in bit error rate performance.","1525-3511;1525-3511;1558-2612","978-1-4673-5939-9978-1-4673-5938-2978-1-4673-5937","10.1109/WCNC.2013.6555190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6555190","","Decoding;Complexity theory;Iterative decoding;Bit error rate;Noise measurement;Degradation;Reliability","adaptive codes;channel coding;decoding;error statistics;transform coding","reduced-complexity decoding;LT codes;noisy channels;adaptive decoding scheme;Luby transform codes;modified degree distributions;low-complexity LT decoder;bit error rate performance","","2","","14","","","","","IEEE","IEEE Conferences"
"Power-constrained low-complexity coding of compressed sensing measurements","A. A. Saleh; W. Chan; F. Alajaji","Queen's University, Kingston, ON, K7L 3N6, Canada; Queen's University, Kingston, ON, K7L 3N6, Canada; Queen's University, Kingston, ON, K7L 3N6, Canada","2014 IEEE 15th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2014","","","439","443","We study a low delay and low complexity sensor-communication system based on compressed sensing (CS) and scalar coding for transmission. The proposed scheme uses a 1 : r channel dimension expansion on the CS measurements for protection against channel noise. Simulation results show that optimizing the choice of r and the power allocation between the r transmissions significantly improve the system performance when compared to existing CS-communication schemes. Moreover, we consider the asymptotic behaviour of our CS system as the channel signal-to-noise ratio grows without bound and show that the proposed scheme achieves the optimal scaling exponent.","1948-3244;1948-3252","978-1-4799-4903","10.1109/SPAWC.2014.6941855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6941855","","Channel coding;Resource management;Noise measurement;Compressed sensing;Delays;Vectors","compressed sensing;delays;encoding;telecommunication channels;wireless sensor networks","power-constrained low-complexity coding;compressed sensing measurements;low complexity sensor-communication system;low delay sensor communication system;scalar coding;channel dimension expansion;CS measurements;channel noise;power allocation;CS-communication schemes;channel signal-to-noise ratio;wireless sensors networks","","","","21","","","","","IEEE","IEEE Conferences"
"A low complexity iterative multiuser detector for LDPC-coded TDRSS return link MA service","C. Xiaoguang; J. Xu; W. Zhilu; M. Bo","School of Astronautics, Harbin Institute of Technology, Harbin, China; School of Electronics and Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Engineering, Harbin Institute of Technology, Harbin, China","2016 IEEE 13th International Conference on Signal Processing (ICSP)","","2016","","","1214","1218","In coded multiuser tracking and data relay satellite system (TDRSS) return link, multiple access interference and inter-symbol interference are major impediment to reliable communication. While the optimal soft-input soft-output multiuser detection scheme results in a prohibitive computational complexity. In this paper, a simplified hard-input soft-output (HISO) scheme for low-density parity-check (LDPC) coded multiuser TDRS system is proposed. The hard decisions of multiuser detection are used as the initial values of the decoding method. After each iteration, the hard output of log-likelihood ratio (LLR) belief propagation (BP) decoding algorithm is fed back as the input of decoding for the next iteration. Furthermore, interleaving is not a necessary part in the proposed method. The computational complexity is much lower than the optimal algorithm. However, simulation results show that the performance of the proposed scheme approaches that of the single-user system for moderate to high signal-to-noise ratios (SNRs).","2164-5221;2164-5221","978-1-5090-1345-6978-1-5090-1344-9978-1-5090-1343-2978-1-5090-1346","10.1109/ICSP.2016.7878020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878020","iterative multiuser detector;TDRSS;coded CDMA;LDPC codes","Detectors;Iterative decoding;Multiaccess communication;Multiuser detection;Computational complexity","code division multiple access;intersymbol interference;iterative decoding;multiuser detection;parity check codes;radiofrequency interference;relay networks (telecommunication);satellite communication;telecommunication network reliability","low complexity iterative multiuser detector;LDPC-coded TDRSS return link MA service;coded multiuser tracking-and-data relay satellite system return link;multiple access interference;intersymbol interference;optimal soft-input soft-output multiuser detection scheme;prohibitive computational complexity;hard-input soft-output scheme;HISO scheme;low-density parity-check coded multiuser TDRS system;log-likelihood ratio belief propagation decoding algorithm;LLR BP decoding algorithm;signal-to-noise ratios;coded CDMA","","","","11","","","","","IEEE","IEEE Conferences"
"A low complexity deblocking filtering for multiview video coding","Y. Wang; W. Zhang; Z. Zhang; P. An","Key Lab. of Advanced Display and System Application, Ministry of Education, University of Shanghai, Shanghai, 200072, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Hubei, 430074, China; Key Lab. of Advanced Display and System Application, Ministry of Education, University of Shanghai, Shanghai, 200072, China; Key Lab. of Advanced Display and System Application, Ministry of Education, University of Shanghai, Shanghai, 200072, China","IEEE Transactions on Consumer Electronics","","2013","59","3","666","671","Deblocking filtering (DF) plays a major role in improving the visual quality for image and video coding. A well-designed DF scheme should achieve a good tradeoff between coding efficiency and coding complexity. The DF algorithm for multiview video coding (MVC) has adopted H.264 DF scheme to improve the video quality, which, however, leads to relatively high coding complexity. In this paper, we propose a low complexity DF algorithm in consideration of view correlation, which exploits the view correlation to skip the boundary strength (BS) computation of macroblocks (MBs) that are coded in motion skip mode. Experimental results show that the proposed algorithm can dramatically reduce DF time (35%-6%) without degrading the visual quality in comparison to the conventional algorithm.","0098-3063;1558-4127","","10.1109/TCE.2013.6626254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626254","Deblocking filtering;multiview video coding;motion skip mode;boundary strength","Encoding;Complexity theory;Correlation;Video coding;PSNR;Filtering;Algorithm design and analysis","filtering theory;video coding","multiview video coding;deblocking filtering;H.264 DF scheme;visual quality;image coding;coding efficiency;coding complexity;boundary strength computation;motion skip mode","","1","","14","","","","","IEEE","IEEE Journals & Magazines"
"Coding Tree Depth Estimation for Complexity Reduction of HEVC","G. Correa; P. Assuncao; L. Agostini; L. A. D. S. Cruz","NA; NA; NA; NA","2013 Data Compression Conference","","2013","","","43","52","The emerging HEVC standard introduces a number of tools which increase compression efficiency in comparison to its predecessors at the cost of greater computational complexity. This paper proposes a complexity control method for HEVC encoders based on dynamic adjustment of the newly proposed coding tree structures. The method improves a previous solution by adopting a strategy that takes into consideration both spatial and temporal correlation in order to decide the maximum coding tree depth allowed for each coding tree block. Complexity control capability is increased in comparison to a previous work, while compression losses are decreased by 70%. Experimental results show that the encoder computational complexity can be downscaled to 60% with an average bit rate increase around 1.3% and a PSNR decrease under 0.07 dB.","1068-0314","978-0-7695-4965-1978-1-4673-6037","10.1109/DCC.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6543040","","Encoding;Computational complexity;Standards;Video coding;Image coding;Bit rate","computational complexity;encoding;video coding","coding tree depth estimation;complexity reduction;compression efficiency;computational complexity;complexity control method;HEVC encoders;dynamic adjustment;tree structures;temporal correlation;spatial correlation;maximum coding tree;coding tree block;complexity control capability;high efficiency video coding;gain 0.07 dB","","26","","10","","","","","IEEE","IEEE Conferences"
"Low-latency low-complexity heap-based extended min-sum algorithms for non-binary low-density parity-check codes","Y. Hwang; K. Yang; K. Cheun","Pohang University of Science and Technology, Korea; Pohang University of Science and Technology, Korea; Pohang University of Science and Technology, Korea","IET Communications","","2015","9","9","1191","1198","The extended min-sum (EMS) and improved EMS (I-EMS) algorithms for non-binary low-density parity-check codes over GF(q) significantly reduce the decoding complexity with an acceptable performance degradation, but they suffer from high latency because of many serial computations, including a sorting process. On the other hand, the trellis-based EMS algorithm can greatly reduce the latency, but it does not solve the complexity problem in high-order fields (q≥ 64). To improve the latency problem with low-complexity advantages, the authors propose heap-based EMS (H-EMS) and heap-based I-EMS (HI-EMS) algorithms that are modifications of the EMS and I-EMS algorithms, respectively. The authors also propose double H-EMS and double HI-EMS algorithms trading off the latency against the performance by heaping messages twice. Numerical results show that the H-EMS algorithm has 2.74-9.52 times lower latency than the EMS algorithm with a negligible performance degradation over a wide range of code rates, whereas the HI-EMS algorithm has 1.20-1.62 times lower latency than the I-EMS algorithm. Furthermore, the proposed algorithms may be employed regardless of the decoding schedules.","1751-8628;1751-8636","","10.1049/iet-com.2014.1107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7115221","","","decoding;Galois fields;parity check codes","low-latency low-complexity heap-based extended min-sum algorithms;nonbinary low-density parity-check codes;improved EMS algorithms;I-EMS algorithms;GF(q);decoding complexity reduction;performance degradation;sorting process;trellis-based EMS algorithm;heap-based EMS algorithm;H-EMS algorithm;heap-based I-EMS algorithm;HI-EMS algorithms","","1","","20","","","","","IET","IET Journals & Magazines"
"Reduced complexity window decoding schedules for coupled LDPC codes","N. ul Hassan; A. E. Pusane; M. Lentmaier; G. P. Fettweis; D. J. Costello","Vodafone Chair Mobile Communications Systems, Dresden University of Technology (TU Dresden), Germany; Vodafone Chair Mobile Communications Systems, Dresden University of Technology (TU Dresden), Germany; Dept. of Electrical and Electronics Engineering, Bogazici University, Istanbul, Turkey; Dept. of Electrical and Electronics Engineering, Bogazici University, Istanbul, Turkey; Dept. of Electrical Engineering, University of Notre Dame, Indiana, USA","2012 IEEE Information Theory Workshop","","2012","","","20","24","Window decoding schedules are very attractive for message passing decoding of spatially coupled LDPC codes. They take advantage of the inherent convolutional code structure and allow continuous transmission with low decoding latency and complexity. In this paper we show that the decoding complexity can be further reduced if suitable message passing schedules are applied within the decoding window. An improvement based schedule is presented that easily adapts to different ensemble structures, window sizes, and channel parameters. Its combination with a serial (on-demand) schedule is also considered. Results from a computer search based schedule are shown for comparison.","","978-1-4673-0223-4978-1-4673-0224-1978-1-4673-0222","10.1109/ITW.2012.6404660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6404660","","Parity check codes;Decoding","communication complexity;convolutional codes;decoding;message passing;parity check codes","reduced complexity window decoding schedule;message passing decoding;spatially coupled LDPC code;convolutional code structure;decoding latency;decoding complexity;message passing schedule;window size;channel parameter;serial on-demand schedule","","7","","13","","","","","IEEE","IEEE Conferences"
"Error-Resilient and Low-Complexity Onboard Lossless Compression of Hyperspectral Images by Means of Distributed Source Coding","A. Abrardo; M. Barni; E. Magli; F. Nencini","Dipartimento di Ingegneria dell'Informazione, Università di Siena, Siena, Italy; Dipartimento di Ingegneria dell'Informazione, Università di Siena, Siena, Italy; Dipartimento di Elettronica, Politecnico di Torino, Torino, Italy; Dipartimento di Ingegneria dell'Informazione, Università di Siena, Siena, Italy","IEEE Transactions on Geoscience and Remote Sensing","","2010","48","4","1892","1904","In this paper, we propose a lossless compression algorithm for hyperspectral images inspired by the distributed-source-coding (DSC) principle. DSC refers to separate compression and joint decoding of correlated sources, which are taken as adjacent bands of a hyperspectral image. This concept is used to design a compression scheme that provides error resilience, very low complexity, and good compression performance. These features are obtained employing scalar coset codes to encode the current band at a rate that depends on its correlation with the previous band, without encoding the prediction error. Iterative decoding employs the decoded version of the previous band as side information and uses a cyclic redundancy code to verify correct reconstruction. We develop three algorithms based on this paradigm, which provide different tradeoffs between compression performance, error resilience, and complexity. Their performance is evaluated on raw and calibrated AVIRIS images and compared with several existing algorithms. Preliminary results of a field-programmable gate array implementation are also provided, which show that the proposed algorithms can sustain an extremely high throughput.","0196-2892;1558-0644","","10.1109/TGRS.2009.2033470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5338026","Distributed source coding (DSC);error resilience;hyperspectral images;lossless compression","Image coding;Hyperspectral imaging;Source coding;Resilience;Iterative decoding;Compression algorithms;Encoding;Redundancy;Image reconstruction;Iterative algorithms","cyclic redundancy check codes;error correction codes;field programmable gate arrays;geophysical image processing;image coding;image reconstruction;iterative decoding;source coding","error resilient onboard lossless compression;low complexity onboard lossless compression;hyperspectral images;distributed source coding;joint decoding;scalar coset codes;prediction error;iterative decoding;cyclic redundancy code;calibrated AVIRIS images;field programmable gate array implementation","","53","","41","","","","","IEEE","IEEE Journals & Magazines"
"Multiplicative repetition-based spinal codes with low computational complexity","P. Chen; B. Bai; Q. Li; R. Zhang","Xidian University, People's Republic of China; Xidian University, People's Republic of China; Xidian University, People's Republic of China; Xidian University, People's Republic of China","IET Communications","","2017","11","17","2660","2666","Spinal codes, a new class of rateless codes, have received considerable attention for their capacity-approaching performance over noisy channels. Hash function is the core of a spinal encoder to generate infinite coded symbols, which has higher hardware complexity. In this study, the authors propose a multiplicative repetition-based method instead of the hash function to generate innumerable symbols with low encoding complexity. Furthermore, both frozen-aided and cyclic redundancy check-aided decoding methods are proposed to improve the spectral efficiency. Simulation results show that the proposed spinal codes have lower computational complexity and can achieve higher spectral efficiency in the high signal-to-noise ratio region compared with the conventional ones over both additive white Gaussian noise and Rayleigh fading channels.","1751-8628;1751-8636","","10.1049/iet-com.2016.1401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8181579","","","computational complexity;cyclic redundancy check codes;decoding","spinal codes;multiplicative repetition;computational complexity;rateless codes;capacity-approaching performance;noisy channels;hash function;spinal encoder;infinite coded symbols;hardware complexity;encoding complexity;frozen-aided decoding;cyclic redundancy check decoding;spectral efficiency;signal-to-noise ratio","","3","","20","","","","","IET","IET Journals & Magazines"
"Complexity-driven rate-control for parallel HEVC coding","J. Carter; S. Blasi; M. Mrak","British Broadcasting Corporation, Research and Development Department, London, UK; British Broadcasting Corporation, Research and Development Department, London, UK; British Broadcasting Corporation, Research and Development Department, London, UK","2017 IEEE Visual Communications and Image Processing (VCIP)","","2017","","","1","4","When using adaptive streaming, the content needs to be segmented so that clients can seamlessly switch to different rates depending on network conditions. On the video server each segment is stored in various bitrate representations, which are in practice provided by very fast encoders. Such encoders rely on parallelization strategies to limit the encoder complexity. Parallelization strongly affects the performance of rate-control (RC) algorithms, since different segments and parts of segments are encoded independently from each other. A new approach is proposed in this paper to tackle these issues, based on the optimization of the initial parameters of a state-of-the-art RC model for inter-predicted frames in an HEVC/H.265 codec. The model makes use of an estimate of the texture complexity of the first frame in the segment to efficiently tune the parameters depending on the target rate. The approach is consistently improving the accuracy of RC schemes as well as the visual quality, with negligible impact on the encoding efficiency.","","978-1-5386-0462-5978-1-5386-0463","10.1109/VCIP.2017.8305101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305101","Rate-control;HEVC;video streaming;DASH","Bit rate;Encoding;Complexity theory;Visualization;Adaptation models;Optimization;Streaming media","computational complexity;video codecs;video coding;video servers","target rate;RC schemes;encoding efficiency;rate-control;parallel HEVC coding;adaptive streaming;clients;network conditions;video server;bitrate representations;fast encoders;parallelization strategies;encoder complexity;different segments;initial parameters;state-of-the-art RC model;texture complexity;HEVC/H.265 codec","","1","","16","","","","","IEEE","IEEE Conferences"
"Two-Layer LDPC Codes for Low Complexity ML Detection in GSM Systems","X. Jiang; Y. Zheng; W. Chen; M. Wen; J. Li","School of Information Science and Technology, Donghua University, Shanghai, China; School of Information Science and Technology, Donghua University, Shanghai, China; School of Information Science and Technology, Donghua University, Shanghai, China; School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; School of Mechanical and Electrical Engineering, Guangzhou University, Guangzhou, China","IEEE Wireless Communications Letters","","2018","7","3","408","411","Although maximum-likelihood (ML) detection is able to achieve optimal performance in generalized spatial modulation (GSM) systems, its exhaustive search leads to an intractable computational complexity. In this letter, two-layer low-density parity-check (TL-LDPC) codes are proposed to reduce the combinations of transmission vectors in LDPC coded GSM systems. Simulation results show that the complexity of the ML detection in the TL-LDPC-coded GSM systems is much lower than that of the ML detection in conventional LDPC-coded GSM systems with a negligible performance loss.","2162-2337;2162-2345","","10.1109/LWC.2017.2780080","Shanghai Rising-Star Program; National Nature Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8166736","Generalized spatial modulation (GSM);maximum-likelihood (ML);two-layer low-density parity-check (TL-LDPC) codes","Maximum likelihood detection;Parity check codes;Spatial modulation","computational complexity;maximum likelihood detection;modulation;parity check codes","low complexity ML detection;maximum-likelihood detection;generalized spatial modulation systems;intractable computational complexity;two-layer low-density parity-check codes;two-layer LDPC codes;transmission vectors;LDPC coded GSM systems;TL-LDPC-coded GSM systems","","2","","19","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Iterative Phase Noise Tracker for Bit-Interleaved Coded CPM Signals in AWGN","N. Noels; M. Moeneclaey; F. Simoens; D. Delaruelle","TELIN Department, Ghent University, Gent, Belgium; TELIN Department, Ghent University, Gent, Belgium; Newtec NV, Sint Niklaas, Belgium; Newtec NV, Sint Niklaas, Belgium","IEEE Transactions on Signal Processing","","2011","59","9","4271","4285","This paper considers iterative detection of bit-interleaved coded continuous phase modulation in the presence of both phase noise (PN) and additive white Gaussian noise (AWGN). The proposed receiver iterates between a detection module and an estimation module. The detection module operates according to the sum-product algorithm and the factor graph framework in order to perform coherent maximum a posteriori bit detection in AWGN, using a PN estimate provided by the estimation module. The latter module, which results from the expectation-maximization algorithm for maximum likelihood estimation of the unknown PN samples, is implemented as a smoothing phase-locked loop that uses soft decisions provided by the detector. The separation between the detection and the estimation modules allows the use of an off-the-shelf (coherent) bit detector. The technique is further characterized by a very low computational complexity, a small error performance degradation and a small number of overhead symbols.","1053-587X;1941-0476","","10.1109/TSP.2011.2159498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5875905","Belief propagation;bit-interleaved coded modulation;continuous-phase modulation (CPM);factor graph;iterative estimation and detection;phase noise tracking;sum-product algorithm;turbo carrier synchronization","Tin;Receivers;Detectors;Phase locked loops;Estimation;Phase noise;Modulation","AWGN;communication complexity;continuous phase modulation;graph theory;iterative methods;maximum likelihood estimation;phase locked loops;radio receivers","low-complexity iterative phase noise tracker;AWGN;iterative detection;bit-interleaved coded continuous phase modulation;additive white Gaussian noise;receiver;detection module;estimation module;sum-product algorithm;factor graph framework;coherent maximum a posteriori bit detection;PN estimation;expectation-maximization algorithm;maximum likelihood estimation;smoothing phase-locked loop;soft decision;off-the-shelf coherent bit detector;computational complexity;overhead symbols","","4","","24","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Puncturing and Shortening of Polar Codes","V. Bioglio; F. Gabry; I. Land","NA; NA; NA","2017 IEEE Wireless Communications and Networking Conference Workshops (WCNCW)","","2017","","","1","6","In this work, we address the low-complexity construction of shortened and punctured polar codes from a unified view. While several independent puncturing and shortening designs were attempted in the literature, our goal is a unique, low-complexity construction encompassing both techniques in order to achieve any code length and rate. We observe that our solution significantly reduces the construction complexity as compared to state-of-the-art solutions while providing a block error rate performance comparable to constructions that are highly optimized for specific lengths and rates. This makes the constructed polar codes highly suitable for practical application in future communication systems requiring a large set of polar codes with different lengths and rates.","","978-1-5090-5908-9978-1-5090-5909","10.1109/WCNCW.2017.7919040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7919040","","Decoding;Complexity theory;Measurement;Reliability;Algorithm design and analysis;Encoding;Error analysis","block codes;communication complexity;error correction codes;linear codes","low-complexity puncturing;low-complexity shortening;shortened polar codes;punctured polar codes;low-complexity code construction;code length;code rate;construction complexity reduction;block error rate performance;communication systems","","13","","11","","","","","IEEE","IEEE Conferences"
"Reduced-complexity column-layered decoding and implementation for LDPC codes","Z. Cui; Z. Wang; X. Zhang","Qualcomm Inc.; Broadcom Corp.; Department of EECS, Case Western Reserve University","IET Communications","","2011","5","15","2177","2186","Layered decoding is well appreciated in low-density parity-check (LDPC) decoder implementation since it can achieve effectively high decoding throughput with low computation complexity. This work, for the first time, addresses low-complexity column-layered decoding schemes and very-large-scale integration (VLSI) architectures for multi-Gb/s applications. At first, the min-sum algorithm is incorporated into the column-layered decoding. Then algorithmic transformations and judicious approximations are explored to minimise the overall computation complexity. Compared to the original column-layered decoding, the new approach can reduce the computation complexity in check node processing for high-rate LDPC codes by up to 90% while maintaining the fast convergence speed of layered decoding. Furthermore, a relaxed pipelining scheme is presented to enable very high clock speed for VLSI implementation. Equipped with these new techniques, an efficient decoder architecture for quasi-cyclic LDPC codes is developed and implemented with 0.13%%m VLSI implementation technology. It is shown that a decoding throughput of nearly 4%Gb/s at a maximum of 10 iterations can be achieved for a (4096, 3584) LDPC code. Hence, this work has facilitated practical applications of column-layered decoding and particularly made it very attractive in high-speed, high-rate LDPC decoder implementation.","1751-8628;1751-8636","","10.1049/iet-com.2010.1002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6069640","","","codecs;computational complexity;parity check codes;VLSI","reduced complexity column layered decoding;LDPC codes;low density parity check decoder;very large scale integration architectures;algorithmic transformation;judicious approximations;computation complexity","","17","","","","","","","IET","IET Journals & Magazines"
"On the Encoding Complexity of Quasi-Cyclic LDPC Codes","A. Mahdi; V. Paliouras","VLSI Design Laboratory, Electrical and Computer Engineering Department, University of Patras, Patras, Greece; VLSI Design Laboratory, Electrical and Computer Engineering Department, University of Patras, Patras, Greece","IEEE Transactions on Signal Processing","","2015","63","22","6096","6108","In this paper, we propose a parity check matrix (PCM) construction technique that reduces the encoding complexity of quasi-cyclic (QC)-LDPC codes. The proposed construction method is based on a constraint selection of shifting factors, shown here to reduce the density of an inverted matrix used in several encoding algorithms. Furthermore, it demonstrates that the complexity of encoding schemes involving inverted matrices, can be defined by the density of the small inverted binary base matrix and not by the extended QC-PCM. Comparisons of the proposed codes with codes employed in international standards and with randomly shifted QC-LDPC codes of comparable characteristics, show the low complexity of the corresponding hardware implementations and a BER performance equivalent to that of previously reported codes without increasing the decoding complexity. Furthermore, adoption of the proposed method can decrease the complexity of several encoding procedures; in particular, an area reduction of 40%-55% is reported for QC-LDPC encoders, while a reduction of 86% is reported for Multi-Level-QC-LDPC encoders.","1053-587X;1941-0476","","10.1109/TSP.2015.2463258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7174548","BER performance;density reduction;encoding complexity;hardware architecture;LDPC encoding;matrix inversion;quasi-cyclic LDPC","Encoding;Complexity theory;Parity check codes;Bit error rate;Phase change materials;Hardware;Decoding","error statistics;matrix algebra;parity check codes","encoding complexity;quasi-cyclic LDPC codes;parity check matrix construction technique;PCM construction technique;small inverted binary base matrix;QC-LDPC codes;BER performance;low-density parity-check codes","","4","","33","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Joint RDO of Prediction Units Couples for HEVC Intra Coding","M. Bichon; J. L. Tanou; M. Ropert; W. Hamidouche; L. Morin; L. Zhang","Ericsson, Saint-Jacques-de-la-Lande, 35136, France; Ericsson, Saint-Jacques-de-la-Lande, 35136, France; Ericsson, Saint-Jacques-de-la-Lande, 35136, France; INSA de Rennes (IETR - UMR6164), Rennes, 35708, France; INSA de Rennes (IETR - UMR6164), Rennes, 35708, France; INSA de Rennes (IETR - UMR6164), Rennes, 35708, France","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2018","","","1733","1737","HEVC is the latest block-based video compression standard, outperforming H.264/AVC by 50% bitrate savings for the same perceptual quality. An HEVC encoder provides Rate-Distortion optimization coding tools for block-wise compression. Because of complexity limitations, Rate-Distortion Optimization (RDO) is usually performed independently for each block, assuming coding efficiency losses to be negligible. In this paper, we propose an acceleration solution for the Intra coding scheme named Dual-JRDO, which takes advantage of Inter-Block dependencies related to both predictive coding and CABAC. The Dual-JRDO improves Intra coding efficiency at the expense of higher computational complexity. The acceleration of the Dual-JRDO scheme includes adaptive use of the Dual-JRDO model based on source analysis, short-listing and early decisions strategies. The proposed Fast Dual-JRDO reduces the original model complexity by 89.54%, while providing tractable computation for average R-D gains of -0.45% (up to -0.82%) in the HM16.12 reference software model.","2379-190X","978-1-5386-4658-8978-1-5386-4657-1978-1-5386-4659","10.1109/ICASSP.2018.8462489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462489","Intra Coding;Joint Block Optimization;HEVC;Dual-JRDO","Encoding;Acceleration;Optimization;Computational modeling;Estimation;Complexity theory;Distortion","computational complexity;data compression;rate distortion theory;video coding","computational complexity;block-based video compression standard;rate-distortion optimization coding tools;inter-block dependencies;dual-JRDO scheme;HM16.12 reference software model;fast dual-JRDO;HEVC Intra coding;prediction units couples;low complexity joint RDO;Intra coding efficiency;predictive coding;block-wise compression;HEVC encoder;perceptual quality","","","","17","","","","","IEEE","IEEE Conferences"
"Novel blind encoder identification of Reed-Solomon codes with low computational complexity","Hongting Zhang; H. Wu; H. Jiang","School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, 70803, U. S. A.; School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, 70803, U. S. A.; Bell Labs, Alcatel-Lucent Inc., Murray Hill, NJ 07974, U. S. A.","2013 IEEE Global Communications Conference (GLOBECOM)","","2013","","","3294","3299","Adaptive modulation and coding (AMC) is commonly used in wireless systems to dynamically change the modulation and coding schemes (MCSs) in subsequent frames such that the spectral efficiency can be adapted to various channel conditions. The spectrum and energy efficiency would decrease if the adopted MCS option at the transmitter needs to be dynamically transmitted to the receiver through a secure control channel. To combat this problem, in this paper, we would like to propose a novel blind channel-encoder identification scheme with low computational complexity for Reed-Solomon (RS) codes over Galois field GF(q), which could also be applied to other similar non-binary channel codes as well. Our proposed new scheme involves the estimation of the channel parameters using the expectation-maximization (EM) algorithm, the calculation of the log-likelihood ratio vectors (LLRVs) of the syndrome a posteriori probabilities over GF(q), and the identification of the non-binary RS encoder in use subject to the maximum average log-likelihood ratio (LLR) over the pre-selected candidate encoder set. Simulation results justify the effectiveness of this new mechanism.","1930-529X","978-1-4799-1353","10.1109/GLOCOM.2013.6831580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831580","Blind encoder identification;expectation maximization;log-likelihood ratio;Reed Solomon codes","Encoding;Vectors;Probability;Computational complexity;Signal to noise ratio;Modulation;Transmitters","adaptive modulation;computational complexity;expectation-maximisation algorithm;probability;Reed-Solomon codes","blind encoder identification;Reed-Solomon codes;low computational complexity;adaptive modulation and coding;AMC;channel-encoder identification scheme;Galois field;expectation-maximization algorithm;log-likelihood ratio vectors;syndrome a posteriori probabilities;maximum average log-likelihood ratio;preselected candidate encoder set","","","","11","","","","","IEEE","IEEE Conferences"
"Performance analysis and peak-to-average power ratio reduction of concatenated LDPC coded OFDM system using low complexity PTS","A. Joshi; D. S. Saini","Dept. of ECE, Jaypee Institute of Information Technology, Noida, India; Dept. of ECE, Jaypee University of Information Technology, Waknaghat, India","2015 International Conference on Signal Processing and Communication (ICSC)","","2015","","","195","200","OFDM suffers with two major drawbacks, poor bit error rate (BER) performance in fading environment and high value of peak to average power ratio (PAPR). OFDM system can be combined with channel coding schemes to improve BER in fading environment. Convolution codes, Reed Solomon (RS) and Turbo codes are widely used channel codes, but in recent studies low density parity check codes (LDPC) have outperformed every other code in terms of error correction capabilities. High PAPR increases circuit complexity and reduce RF amplifier efficiency. Partial Transmit Sequence (PTS) is one of the most preferred techniques for PAPR reduction, however in PTS scheme the computation of optimal phase factors necessitates exhaustive searching among all allowable phase factors. This leads to exponential increase in computation complexity with the increase in number of sub-blocks. In this paper a coded OFDM scheme with LDPC-RS concatenated codes using PTS is proposed. The design improves BER performance compared to traditional RS-convolution concatenated codes and the proposed modified PTS scheme reduces the computation complexity in terms of reduced complex multiplications and additions by significant margin and at the same time maintaining same PAPR reduction as conventional PTS scheme. OFDM system used in the paper complies with IEEE 802.11 a standard.","","978-1-4799-6761-2978-1-4799-6760","10.1109/ICSPCom.2015.7150646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150646","OFDM;PAPR;LDPC;RS codes;Convolution codes;CCRR;Complex multiplication and addition","Peak to average power ratio;Partial transmit sequences;Parity check codes;Bit error rate;Complexity theory;Fading","channel coding;convolutional codes;error correction codes;error statistics;OFDM modulation;parity check codes;Reed-Solomon codes;turbo codes","peak-to-average power ratio reduction;PAPR reduction;low density parity check codes;LDPC coded;OFDM system;partial transmit sequence;PTS scheme;bit error rate;BER;channel coding schemes;convolution codes;Reed Solomon codes;turbo codes;error correction;IEEE 802.11 a standard","","1","","14","","","","","IEEE","IEEE Conferences"
"A computational complexity measure for trellis modules of convolutional codes","I. B. Benchimol; C. Pimentel; R. Demo Souza; B. F. Uchôa-Filho","CMDI, Federal Institute of Amazonas (IFAM), Manaus-AM, Brazil; CODEC/DES, Federal University of Pernambuco (UFPE), Recife-PE, Brazil; CPGEI, Federal University of Technology - Paraná (UTFPR), Curitiba-PR, Brazil; GPqCom/EEL, Federal University of Santa Catarina (UFSC), Florianópolis-SC, Brazil","2013 36th International Conference on Telecommunications and Signal Processing (TSP)","","2013","","","144","148","This paper presents a computational complexity measure of convolutional codes well suitable for software implementations of the Viterbi algorithm (VA) operating with hard decision. We investigate the number of arithmetic operations performed by the decoding process over the conventional and minimal trellis modules. The computational cost of implementation of each operation is determined in terms of machine cycles taken by its execution. A relation between the complexity measure defined in this work and the one defined by McEliece and Lin is investigated. The system architecture adopted is the TMS320C55xx fixed-point digital signal processor family from Texas Instruments.","","978-1-4799-0404-4978-1-4799-0402-0978-1-4799-0403","10.1109/TSP.2013.6613908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613908","Convolutional codes;decoding complexity;trellis module;Viterbi algorithm","Convolutional codes;Computational complexity;Maximum likelihood decoding;Software measurement","computational complexity;convolutional codes;decoding;trellis codes;Viterbi detection","computational complexity measure;trellis modules;convolutional codes;software implementations;Viterbi algorithm;hard decision;arithmetic operations;decoding process","","2","","19","","","","","IEEE","IEEE Conferences"
"A low-complexity near-ML detector for a 3 × nRhybrid space-time code","J. Cortez; M. Bazdresch; O. Longoria-Gandara","Electrical Engineering Department, Instituto Tecnologico de Sonora, Cd. Obregon, Mexico; Electrical, Computer & Telecom Eng. Tech. Department, Rochester Institute of Technology, Rochester, NY 14623, USA; Department of Electronics, Systems and IT, ITESO University, Guadalajara, Mexico","2017 IEEE 9th Latin-American Conference on Communications (LATINCOM)","","2017","","","1","6","In a MIMO wireless communications system, a space-time block code specifies how the data symbols are transmitted over different antennas at different time instants. A hybrid space-time code attempts to obtain some of the available diversity and multiplexing gains, achieving low error probability and high data rate. The LD StBc-VBLAST hybrid code layers one spatial-multiplexing antenna (to increase the data rate) and one Alamouti encoder (to provide diversity). A low-complexity, nearly optimum decoder algorithm for LD STBC-VBLAST is presented. Its error probability is comparable to that of a maximum-likelihood detector, with much lower computational complexity. The performance of the detector is evaluated over both uncorrelated and correlated channels. It is shown that the proposed decoder remains nearly optimum despite channel correlation.","","978-1-5386-2098-4978-1-5386-2097-7978-1-5386-2099","10.1109/LATINCOM.2017.8240182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240182","Space-time coding;Near-ML detectors;Correlated Channels","Decoding;Receivers;Detectors;Transmitting antennas;Correlation","antenna arrays;block codes;computational complexity;decoding;diversity reception;error statistics;maximum likelihood detection;MIMO communication;multiplexing;space-time codes","low error probability;LD StBc-VBLAST hybrid code layers;spatial-multiplexing antenna;optimum decoder algorithm;maximum-likelihood detector;near-ML detector;communications system;space-time block code;multiplexing gains;computational complexity;diversity reception;hybrid space-time code","","","","12","","","","","IEEE","IEEE Conferences"
"On the performance of complexity-optimized bilayer lengthened LDPC codes for relay channels","O. Vahabzadeh; M. Salehi","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115, USA","2012 46th Annual Conference on Information Sciences and Systems (CISS)","","2012","","","1","5","In this paper, we design an extended class of complexity-optimized bilayer lengthened low-density parity-check (LDPC) codes by considering upper check nodes with different degrees. We show that by implementing a low complexity relay decoder, our designed codes outperform the rate-optimized codes in a wide range of Eb/N0's.","","978-1-4673-3140-1978-1-4673-3139-5978-1-4673-3138","10.1109/CISS.2012.6310743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6310743","Relay channels;bilayer low-density parity-check (LDPC) codes;complexity measure;complexity-optimized codes;rate-optimized codes","Parity check codes;Erbium;Irrigation","computational complexity;decoding;parity check codes","complexity-optimized bilayer lengthened LDPC codes;relay channels;extended class;complexity-optimized bilayer lengthened low-density parity-check codes;upper check nodes;low complexity relay decoder;rate-optimized codes","","","","10","","","","","IEEE","IEEE Conferences"
"Rate-Complexity-Distortion Optimization for Hybrid Video Coding","X. Li; M. Wien; J. Ohm","MediaTek, Inc., Beijing, China; Institute of Communications Engineering, RWTH Aachen University, Aachen, Germany; Institute of Communications Engineering, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Circuits and Systems for Video Technology","","2011","21","7","957","970","In recent years, video applications on handheld devices became more and more popular. Due to limited computational capability and power supply in handheld devices, rate-complexity-distortion optimization (RCDO) algorithms at encoder side draw increasing attention. The target of RCDO is to obtain the best rate-distortion (R-D) performance under a constraint of complexity. Generally, there are three essential problems in RCDO. First, complexity needs to be properly mapped to a target in terms of coding parameters such that the control over complexity can be achieved. Second, the complexity budget should be efficiently distributed among frames or other coding units. Third, the allocated budget for each coding unit has to be effectively used to obtain good R-D performance. In this paper, these problems are well addressed. To obtain a large dynamic range in complexity control, medium-granularity control methods are presented. Then, a frame level complexity allocation algorithm is developed based on dependent rate-distortion function. Finally, an adaptive mode and reference searching method is proposed for motion compensation process. Comprehensive simulations verify the proposed algorithms. In the environment of the H.264/AVC reference software, an average gain of over 0.5 dB and 0.7 dB in BD-PSNR was achieved for nine sequences at low complexity when compared to two RCDO methods from literature. Moreover, experiments on x264 (a practical implementation of H.264/AVC) show that the proposed algorithms outperform predefined complexity levels by x264 in terms of both coding efficiency and computational scalability.","1051-8215;1558-2205","","10.1109/TCSVT.2011.2133750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5740312","Adaptive mode and reference searching;complexity allocation;complexity control;hybrid video coding;rate-complexity-distortion optimization","Complexity theory;Encoding;Video coding;Automatic voltage control;Resource management;Motion estimation;Dynamic range","adaptive modulation;video coding","rate complexity distortion optimization;hybrid video coding;handheld devices;computational capability;encoder side;rate distortion performance;constraint of complexity;complexity control;medium granularity control;motion compensation process;coding efficiency;computational scalability","","40","","50","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Multiuser Coding Scheme With Near-Capacity Performance","G. Song; X. Wang; J. Cheng","Cluster of Science, Singapore University of Technology and Design, Singapore; Department of Electrical and Computer Engineering, University of Western Ontario, London, ON, Canada; Department of Intelligent Information Engineering and Sciences, Doshisha University, Kyoto, Japan","IEEE Transactions on Vehicular Technology","","2017","66","8","6775","6786","Nonorthogonal multiuser coding is an essential technique for superimposed transmission and improving spectrum efficiency of future wireless communication networks. In this paper, a low-complexity nonorthogonal coding scheme, called multiuser repetition-aided irregular repeat-accumulate (IRA) code, is proposed for a multiple access channel (MAC), and the scheme can be applied to fifth-generation (5G) nonorthogonal multiple access systems. The main idea is that not only parity checks, which are generated by an IRA encoder, but repetitions as well, are used in each user's codeword to reduce the coding and decoding complexities. Repetition is a simple way to construct a low-rate code and is shown to be beneficial for multiuser decoding iteration. With a deliberate control of the fraction of repetitions in the codeword and a degree distribution optimization of the IRA encoder, near MAC capacity performance is achieved. It is shown that as the number of users increases, more repetitions can be used, and therefore, very low encoding and decoding complexities are required.","0018-9545;1939-9359","","10.1109/TVT.2017.2650955","Japan Society for the Promotion of Science; Ministry of Education, Culture, Sports, Science and Technology; Strategic Research Foundation at Private Universities (2014-2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7812743","Coding complexity;low-rate code;nonorthogonal multiple access (NOMA);repetition-aided irregular repeat-accumulate (IRA) code","Iterative decoding;Encoding;Decoding;Complexity theory;Phase shift keying;Optimization","5G mobile communication;channel capacity;channel coding;iterative decoding;multi-access systems;multiuser channels;optimisation;parity check codes;wireless channels","low-complexity multiuser coding scheme;near-capacity performance;nonorthogonal multiuser coding;superimposed transmission;spectrum efficiency improvement;future wireless communication networks;low-complexity nonorthogonal coding scheme;multiuser repetition-aided irregular repeat-accumulate code;IRA code;multiple access channel;fifth-generation nonorthogonal multiple access systems;5G nonorthogonal multiple access systems;decoding complexity reduction;coding complexity reduction;multiuser decoding iteration;degree distribution optimization;MAC capacity performance","","11","","36","","","","","IEEE","IEEE Journals & Magazines"
"A reduced-complexity multilevel coded modulation for APSK signaling","D. Yoda; H. Ochiai","Department of Electrical and Computer Engineering, Yokohama National University, 79-5 Tokiwadai, Hodogaya, Yokohama, Kanagawa 240-8501, Japan; Department of Electrical and Computer Engineering, Yokohama National University, 79-5 Tokiwadai, Hodogaya, Yokohama, Kanagawa 240-8501, Japan","2013 IEEE International Symposium on Information Theory","","2013","","","1994","1998","The amplitude and phase shift keying (APSK) signal has been adopted in the recent satellite communication standards such as DVB-S2 due to its peak-to-average power ratio (PAPR) property lower than the quadrature amplitude modulation (QAM). Unlike square QAM constellations that allow separate detection of in-phase and quadrature components (i.e., I-Q decomposition), the detection process for APSK is generally complex. This paper investigates the use of multilevel coding (MLC) together with multistage decoding (MSD) for APSK with particular emphasis on an introduction of a novel labeling that allows I-Q decomposition at the highest level, thereby significantly reducing the decoding complexity at the cost of slight performance degradation.","2157-8117;2157-8095","978-1-4799-0446","10.1109/ISIT.2013.6620575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620575","","Encoding;Peak to average power ratio;Decoding;Labeling;Complexity theory;Quadrature amplitude modulation","amplitude shift keying;decoding;phase shift keying;quadrature amplitude modulation","reduced-complexity multilevel coded modulation;APSK signaling;amplitude and phase shift keying;satellite communication standards;DVB-S2;peak-to-average power ratio;PAPR property;quadrature amplitude modulation;square QAM constellations;in-phase detection;quadrature components;detection process;multilevel coding;multistage decoding;I-Q decomposition;performance degradation","","1","","8","","","","","IEEE","IEEE Conferences"
"Low-complexity Chase decoding of algebraic-geometric codes using Koetter's interpolation","Siyuan Wu; L. Chen; M. Johnston","School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China, 510006; School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China, 510006; School of Electrical and Electronic Engineering, Newcastle University, Newcastle-upon-Tyne, United Kingdom, NE1 7RU","2016 IEEE Information Theory Workshop (ITW)","","2016","","","414","418","Algebraic-geometric (AG) codes have long been considered as a possible candidate to replace Reed-Solomon (RS) codes. However, their decoding remains complex and infeasible to implement. Addressing this challenge, our paper proposes a low-complexity Chase (LCC) decoding algorithm for the most popular class of AG codes - Hermitian codes. The LCC decoding is realised by formulating decoding test-vectors, which allows Koetter's interpolation to be performed for common and uncommon elements. This reduces redundant computations and also removes the need to calculate the corresponding coefficients of a Hermitian curve, thus facilitating message recovery. Our simulation results show that significant coding gains can be achieved over the conventional Koetter-Vardy (KV) soft decoding algorithm, but with a much lower computational cost. Moreover, we also show that in comparison with RS codes of a similar length, Chase decoding has a more significant impact on enhancing the performance of Hermitian codes.","","978-1-5090-1090-5978-1-5090-1091","10.1109/ITW.2016.7606867","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7606867","Algebraic-geometric codes;Chase decoding;decoding complexity;Hermitian codes;interpolation","Decoding;Interpolation;Complexity theory;Encoding;Reliability;Conferences","algebraic geometric codes;decoding;interpolation;vectors","low-complexity chase decoding;algebraic-geometric code;Koetter interpolation;AG code;Reed-Solomon code;RS code;low complexity chase decoding algorithm;LCC decoding algorithm;Hermitian code;test-vector;message recovery;Koetter-Vardy soft decoding algorithm;KV soft decoding algorithm","","","","16","","","","","IEEE","IEEE Conferences"
"Network-coded multihop multicast: Topology and encoding complexity","M. Martalò; M. Mohorovicich; G. Ferrari; C. Fragouli","WASN Laboratory, Department of Information Engineering, University of Parma, Italy; WASN Laboratory, Department of Information Engineering, University of Parma, Italy; WASN Laboratory, Department of Information Engineering, University of Parma, Italy; ARNI Laboratory, EPFL, Lausanne, Switzerland","2012 IEEE International Conference on Communications (ICC)","","2012","","","2501","2505","In this paper, some novel results on the encoding complexity of network coding and its relation with the network topology are reported. The encoding complexity in network coding is defined as the number of nodes which have to perform coding operations in order to achieve the multicast capacity. These nodes are referred to as coding points. Known results state that the number of coding points is cubic in the mincut and quadratic in the number of receivers. In this paper, we show, through extensive simulations and through analysis of these results, that the number of coding points tends to increase linearly in the min-cut and the number of receivers in random graphs. We show that this is correlated to the length of path from the source to the receivers. To verify this, we also analyze pseudo-random graphs with a larger path length.","1938-1883;1550-3607;1550-3607","978-1-4577-2053-6978-1-4577-2052-9978-1-4577-2051","10.1109/ICC.2012.6364588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6364588","Network coding;multicast;multihop networks;complexity;random graph","Receivers;Complexity theory;Network coding;Network topology;Topology;Encoding;Computational modeling","graph theory;multicast communication;network coding;telecommunication network topology","network-coded multihop multicast;encoding complexity;network topology;coding points;min-cut;pseudorandom graphs;path length","","","","13","","","","","IEEE","IEEE Conferences"
"Low complexity in-loop skin tone detection for ROI coding in the HEVC encoder","P. Goswami; P. V. Srikanth; J. Rahiman","Texas Instruments India Pvt. Ltd., United States of America; Dept. of Electronics and Communication, NITK Surathkal, India; Dept. of Electronics and Communication, NITK Surathkal, India","2016 Twenty Second National Conference on Communication (NCC)","","2016","","","1","6","Perceptual video coding with rate control is a technique commonly used to improve the perceptual video quality without compromising on the video bit rate. Region of Interest (ROI) detection has traditionally been done by a dedicated software or hardware block which provides the video encoder the coordinates of the regions whose perceptual video quality needs to be higher than non-ROI regions. This paper introduces a low complexity human skin color based ROI detection technique in-loop with the HEVC encoder which classifies a coding unit as skin or not. The proposed method utilizes the HEVC encoder tools of spatial predictive encoding, variable coding unit block sizes and motion vectors generated during motion estimation to optimize skin tone detection in Intra and Inter predicted frames. The in-loop classification was found to improve the detection precision per operation per pixel by 4x compared to other low cost skin classifiers.","","978-1-5090-2361-5978-1-5090-2362","10.1109/NCC.2016.7561096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7561096","","Skin;Encoding;Color;Streaming media;Standards;Complexity theory;Video recording","computational complexity;image classification;image colour analysis;motion estimation;object detection;vectors;video coding;visual perception","skin classifiers;in-loop classification;intrapredicted frames;motion estimation;motion vectors;variable coding unit block sizes;spatial predictive encoding;HEVC encoder tools;ROI detection technique;human skin color;video encoder;ROI coding;region of interest;video bit rate;perceptual video quality;rate control;perceptual video coding;low complexity in-loop skin tone detection","","1","","15","","","","","IEEE","IEEE Conferences"
"Coupled LDPC codes: Complexity aspects of threshold saturation","M. Lentmaier; G. P. Fettweis","Vodafone Chair Mobile Communications Systems, Dresden University of Technology (TU Dresden), 01062 Dresden, Germany; Vodafone Chair Mobile Communications Systems, Dresden University of Technology (TU Dresden), 01062 Dresden, Germany","2011 IEEE Information Theory Workshop","","2011","","","668","672","We analyze the convergence behavior of iteratively decoded coupled LDPC codes from a complexity point of view. It can be observed that the thresholds of coupled regular LDPC codes approach capacity as the node degrees and the number L of coupled blocks tend to infinity. The absence of degree two variable nodes in these capacity achieving ensembles implies for any fixed L a doubly exponential decrease of the error probability with the number of decoding iterations I, which guarantees a vanishing block error probability as the overall length n of the coupled codes tends to infinity at a complexity of O(n log n). On the other hand, an initial number of iterations I<sub>br</sub> is required until this doubly exponential decrease can be guaranteed, which for the standard flooding schedule increases linearly with L. This dependence of the decoding complexity on L can be avoided by means of efficient message passing schedules that account for the special structure of the coupled ensembles.","","978-1-4577-0437-6978-1-4577-0438-3978-1-4577-0436","10.1109/ITW.2011.6089581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6089581","","Decoding;Convolutional codes;Complexity theory;Iterative decoding;Error probability;Signal to noise ratio","error statistics;iterative decoding;parity check codes","threshold saturation complexity aspects;iteratively decoded coupled LDPC codes;coupled regular LDPC code approach;variable nodes;vanishing block error probability;standard flooding schedule;decoding complexity;message passing schedules","","6","","17","","","","","IEEE","IEEE Conferences"
"Efïicient and low-complexity space time code for massive MIMO RFID systems","M. S. Abouzeid; L. Lopacinski; E. Grass; T. Kaiser; R. Kraemer","IHP, Frankfurt (Oder), Germany; Brandenburg University of Technology, Cottbus, Germany; Department of Computer Science, Humboldt-University of Berlin, Berlin, Germany; Brandenburg University of Technology, Cottbus, Germany; IHP, Frankfurt (Oder), Germany","2017 12th Iberian Conference on Information Systems and Technologies (CISTI)","","2017","","","1","6","In this paper, a Space-Time Block Code (STBC) based on the Golden number, Golden code is proposed for a massive MIMO-RFID systems. Based on channel modelling for massive MIMO-RFID system, the proposed space-time code is applied to the tag side. Simulation results show that the proposed code for massive MIMO-RFID systems outperforms Alamouti code while simplifying the receiver's complexity. The Bit Error Rate (BER) performance of the proposed technique demonstrates that high diversity gain for the tag is achieved leading to a highly reliable and more robust RFID range. Furthermore, the link capacity between the tagged item and the reader can be increased. The proposed RFID technique provides superior performance against the state-of-the-art RFID techniques.","","978-9-8998-4347-9978-1-5090-5047","10.23919/CISTI.2017.7976046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976046","RFID;Massive MIMO;Massive MIMO-RFID Channel Modelling;Golden Code;Alamouti Code","Radiofrequency identification;Fading channels;MIMO;Encoding;Transmitting antennas;Complexity theory","error statistics;MIMO communication;radiofrequency identification","low-complexity space time code;massive MIMO RFID systems;space-time block code;STBC;Golden number;golden code;channel modelling;receiver complexity;Alamouti code;Bit Error Rate;BER performance;RFID techniques","","","","18","","","","","IEEE","IEEE Conferences"
"A Class of Low-Complexity Codes Based on Doubly Recursive Block Markov Superposition Transmission","S. Zhao; X. Ma; Q. Huang; B. Bai","Jinan University, College of Information Science and Technology, Guangzhou, China; Sun Yat-sen University, School of Data and Computer Science, Guangzhou, China; Beihang University, School of Electronic and Information Engineering, Beijing, China; Xidian University, State Key Laboratory of ISN, Xi'an, China","2018 IEEE International Symposium on Information Theory (ISIT)","","2018","","","1365","1369","In this paper, we introduce the doubly recursive block Markov superposition transmission (DrBMST) of short code. An important characteristic of DrBMST codes is that the degrees of the constraint nodes in their normal graphical realizations are at most three. As a result, DrBMST codes can be decoded with low complexity. We propose to use an enlarged code ensemble to analyze the performance of DrBMST under windowed maximum likelihood decoding. Further, the extrinsic information transfer (EXIT) chart analysis is used to study the iterative decoding thresholds of DrBMST code ensembles. The EXIT analysis shows that the iterative decoding thresholds of DrBMST code ensembles are comparable to those of the BMST codes. We have also compared the error performance and the decoding complexity of finite-length DrBMST codes with regular spatial-coupled low-density parity-check (SC-LDPC) codes under equal decoding latency. The comparison results show that the DrBMST code performs about 0.1 dB better than a (4, 8)-regular SC-LDPC code, but with lower computational complexity.","2157-8117","978-1-5386-4781-3978-1-5386-4102","10.1109/ISIT.2018.8437683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8437683","Block Markov superposition transition (BMST);block-oriented convolutional code;SC-LDPC codes;spatial coupling","","binary codes;block codes;computational complexity;iterative decoding;Markov processes;maximum likelihood decoding;parity check codes","low-complexity codes;enlarged code ensemble;windowed maximum likelihood decoding;extrinsic information transfer chart analysis;DrBMST code ensembles;BMST codes;decoding complexity;finite-length DrBMST codes;doubly recursive block Markov superposition transmission;constraint nodes;normal graphical realizations;EXIT chart analysis;iterative decoding thresholds;error performance;regular spatial-coupled low-density parity-check codes;SC-LDPC;equal decoding latency;computational complexity;(4, 8) regular SC-LDPC code;noise figure 0.1 dB;SC","","","","17","","","","","IEEE","IEEE Conferences"
"Energy complexity of polar codes","C. G. Blake; F. R. Kschischang","Department of Electrical and Computer Engineering, University of Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Canada","2016 IEEE International Symposium on Information Theory (ISIT)","","2016","","","810","814","Sequences of VLSI circuits implemented according to the Thompson VLSI model that compute encoding and decoding functions, called coding schemes, are classified according to the rate at which their associated block error probability scales with block length N. It is shown that coding schemes for binary symmetric channels with probability of error that scales as O(f(N)) must have encoding and decoding energy that scales at least as Ω(N√(-ln f (N))). Polar coding schemes of rate greater than 1/2 are shown to have encoding and decoding energy that scales at least as Ω(N3/2). This lower bound is achievable up to polylogarithmic factors on a mesh-network.","2157-8117","978-1-5090-1806-2978-1-5090-1807","10.1109/ISIT.2016.7541411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7541411","","Encoding;Decoding;Integrated circuit modeling;Computational modeling;Very large scale integration;Complexity theory;Wires","channel coding;decoding;error statistics;VLSI","polar code energy complexity;VLSI circuit sequence;Thompson VLSI model;encoding function;decoding function;block error probability scale;binary symmetric channel;polar coding scheme;mesh-network;polylogarithmic factor","","2","","20","","","","","IEEE","IEEE Conferences"
"Reduced-Complexity Decoder Architecture for Non-Binary LDPC Codes","X. Zhang; F. Cai","Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2011","19","7","1229","1238","Non-binary low-density parity-check (NB-LDPC) codes can achieve better error-correcting performance than binary LDPC codes when the code length is moderate at the cost of higher decoding complexity. The high complexity is mainly caused by the complicated computations in the check node processing and the large memory requirement. In this paper, a novel check node processing scheme and corresponding VLSI architectures are proposed for the Min-max NB-LDPC decoding algorithm. The proposed scheme first sorts out a limited number of the most reliable variable-to-check (v-to-c) messages, then the check-to-variable (c-to-v) messages to all connected variable nodes are derived independently from the sorted messages without noticeable performance loss. Compared to the previous iterative forward-backward check node processing, the proposed scheme not only significantly reduced the computation complexity, but eliminated the memory required for storing the intermediate messages generated from the forward and backward processes. Inspired by this novel c-to-v message computation method, we propose to store the most reliable v-to-c messages as “compressed” c-to-v messages. The c-to-v messages will be recovered from the compressed format when needed. Accordingly, the memory requirement of the overall decoder can be substantially reduced. Compared to the previous Min-max decoder architecture, the proposed design for a (837, 726) code overGF(25) can achieve the same throughput with only 46% of the area.","1063-8210;1557-9999","","10.1109/TVLSI.2010.2047956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5460996","Layered decoding;low-density parity-check (LDPC) codes;min-max;non-binary;VLSI design","Parity check codes;Costs;Very large scale integration;Computer architecture;Iterative decoding;Gain;Convolutional codes;Iterative algorithms;Performance loss;Throughput","binary codes;computational complexity;decoding;error correction codes;minimax techniques;parity check codes;VLSI","reduced-complexity decoder architecture;nonbinary LDPC Codes;nonbinary low-density parity-check codes;NB-LDPC codes;error-correcting performance;binary LDPC codes;code length;higher decoding complexity;complicated computations;large memory requirement;check node processing scheme;VLSI architectures;min-max NB-LDPC decoding algorithm;variable-to-check messages;v-to-c messages;check-to-variable messages;c-to-v messages;sorted messages;noticeable performance loss;iterative forward-backward check node processing;computation complexity;intermediate messages;forward process;backward process;compressed format;overall decoder;min-max decoder architecture","","63","","15","","","","","IEEE","IEEE Journals & Magazines"
"Energy-Efficient and Low Complexity Channel Coding for Wireless Body Area Networks","H. T. Nguyen; T. V. Nguyen","Faculty of Technology Natural Sciences and Maritime Sciences University College of Southeast Norway, Norway; Faculty of Information Technology Posts and Telecommunications Institute of Technologies, Hanoi, Vietnam","2018 5th NAFOSTED Conference on Information and Computer Science (NICS)","","2018","","","265","269","A new design framework based on the modified protograph extrinsic information transfer (PEXIT) algorithm is proposed for wireless body area networks (WBAN) communication systems. Using the proposed framework, a protograph LDPC code of rate 1/2 is designed for the WBAN channel which is statistically modelled as Rician distribution with K-factor obtained from the path loss model. The proposed protograph LDPC coded system achieves a significant coding gain of 17.5 dB over the uncoded system. This coding gain is much higher than the coding gain (6.1 dB) of irregular LDPC coded system in the additive white Gaussian noise (AWGN) channel reported previously for wireless sensor networks. This significant coding gain together with the low complexity encoder/decoder makes the protograph LDPC code an excellent candidate for WBAN communication systems where energy and complexity are among very demanding technical requirements.","","978-1-5386-7983-8978-1-5386-7982-1978-1-5386-7984","10.1109/NICS.2018.8606883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606883","","Wireless communication;Body area networks;Iterative decoding;Complexity theory;Encoding;Wireless sensor networks","AWGN channels;body area networks;channel coding;decoding;energy conservation;parity check codes;statistical distributions;telecommunication power management","low complexity encoder;coding gain;energy-efficient channel coding;Rician distribution;K-factor;low complexity decoder;irregular LDPC coded system;additive white Gaussian noise channel;uncoded system;path loss model;WBAN channel;wireless body area networks communication systems;modified protograph extrinsic information transfer algorithm;design framework;low complexity channel coding;WBAN communication systems;protograph LDPC code;gain 6.1 dB;gain 17.5 dB","","","","21","","","","","IEEE","IEEE Conferences"
"On low-complexity full-diversity detection of multi-user space-time coding","A. Ismail; M. Alouini","Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Makkah Province, Kingdom of Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Makkah Province, Kingdom of Saudi Arabia","2013 IEEE International Conference on Communications (ICC)","","2013","","","3204","3208","The incorporation of multiple input multiple output (MIMO) schemes in recent wireless communication standards paved the way to exploit the newly introduced dimension (i.e. space) to efficiently cancel the interference without requiring additional resources. In this paper, we focus on multiple input multiple ouitput (MIMO) multiple access channel (MAC) case and we answer the question about whether it is possible to suppress the interference in a MIMO MAC channel for completely blind users while achieving full-diversity with a simplified decoder in the affirmative. In fact, this goal can be attained by employing space-time block codes (STBC)s that achive full-diversity under partial interference cancellation (PIC). We derive sufficient conditions for a wide range of STBCs to achieve full-diversity under PIC group decoding with or without successive interference cancellation (SIC). Based on the provided design criteria we derive an upper-bound on the achievable rate for a class of codes. A two-user MIMO MAC interference cancellation scheme is presented and proved to achieve full-diversity under PIC group decoding. We compare our scheme to existing beamforming schemes with full or limit feedback.","1550-3607;1938-1883","978-1-4673-3122","10.1109/ICC.2013.6655037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6655037","Interference cancellation;full-diversity;low-complexity decoding;PIC group decoding","Decoding;MIMO;Interference cancellation;Receiving antennas;Transmitting antennas;Complexity theory;Bit error rate","block codes;communication complexity;decoding;design;interference suppression;MIMO communication;multi-access systems;space-time codes","low-complexity full-diversity detection;multiuser space-time coding;multiple input multiple output schemes;wireless communication standards;multiple access channel;MIMO MAC channel;simplified decoder;space-time block codes;STBC;partial interference cancellation;PIC group decoding;successive interference cancellation;SIC;design criteria;two-user MIMO MAC interference cancellation scheme","","3","","16","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Decoding Algorithm for Coded Hierarchical Modulation in Single Frequency Networks","Z. Hu; H. Jiang; H. Li; Z. Chen; H. Liu","Qualcomm Inc., San Jose, CA, USA; Bell Labs, Alcatel-Lucent, Murray Hill, NJ, USA; Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY, USA; Department of Electronic Engineering, Shanghai Jiao Tong University, Institute of Communications Technology, Shanghai, USA; Department of Electronic Engineering, Shanghai Jiao Tong University, Institute of Communications Technology, Shanghai, USA","IEEE Transactions on Broadcasting","","2014","60","2","302","312","In this paper, the hierarchical modulation (HM) technique is adopted in a single frequency network (SFN) to provide both global and local information. In order to mitigate the interlayer interference and intercell interference, we develop a low-complexity successive interference cancellation (SIC) algorithm for the coded HM signals in the SFN. The proposed decoding algorithm can be applied to different soft-decision channel coding schemes (e.g., Turbo codes, LDPC codes) under various channel profiles. We analyzed the decoding complexity of the proposed algorithm, and evaluated the bit error rate performance. The simulations show that the new decoding algorithm can offer up to 0.7 dB carrier to noise ratio ((C/N)) gain compared with the traditional SIC approach under different channel models, while providing the comparable performance (up to 95% decoding complexity savings) with the multilayer iterative decoding approach. The performance evaluation and decoding complexity comparisons indicate that the proposed structured SIC approach offers a good performance-complexity trade-off, especially for the HM-based SFN scenarios.","0018-9316;1557-9611","","10.1109/TBC.2014.2318554","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6819042","Hierarchical modulation;single frequency network;structured SIC;low decoding complexity;Hierarchical modulation;single frequency network;structured SIC;low decoding complexity","Iterative decoding;Decoding;Silicon carbide;Transmitters;Phase shift keying;Complexity theory;Interference","channel coding;error statistics;interference suppression;iterative decoding;modulation coding","decoding complexity;multilayer iterative decoding;carrier-to-noise ratio;bit error rate performance;soft-decision channel coding;SIC;successive interference cancellation;intercell interference;interlayer interference;single frequency networks;coded hierarchical modulation;low-complexity decoding algorithm","","3","","39","","","","","IEEE","IEEE Journals & Magazines"
"A Low-Complexity Coded Transmission Scheme Over Finite-Buffer Relay Links","Y. Li; S. Zhang; J. Wang; X. Ji; H. Wu; Z. Bao","School of Electronics and Information, Nantong University, Nantong, China; School of Electronics and Information, Nantong University, Nantong, China; School of Electronics and Information, Nantong University, Nantong, China; School of Electronics and Information, Nantong University, Nantong, China; School of Electronics and Information, Nantong University, Nantong, China; School of Electronics and Information, Nantong University, Nantong, China","IEEE Transactions on Communications","","2018","66","7","2873","2887","Relay transmissions play an important role in many types of communication systems. In this paper, we consider packet-level coded transmissions over lossy relay links with the finite-buffer and coding coefficients delivery cost constraints. We propose a low-complexity coding scheme where packets are encoded from sequentially formed random subsets of source packets called batches. The relay recodes only from the buffered packets belonging to the same batch to maintain the code sparsity, lowering the packet header overhead and the decoding complexity compared to the random linear network coding (RLNC). To analyze the performance, we first propose an absorbing Markov chain model to analyze the RLNC transmission over finite-buffer relay links. The finite-length analysis not only provides a lower bound on the completion time using any sparser random codes but also characterizes each individual batch's transmission of the proposed code. Based on the analysis, another Markov chain is proposed to determine the decoding failure probability and the expected completion time of the batch coding scheme. As shown through analysis and simulations, the proposed scheme achieves higher effective end-to-end rates than RLNC when the coding coefficients delivery cost is considered and is also with much lowered decoding complexity thanks to its sparseness.","0090-6778;1558-0857","","10.1109/TCOMM.2018.2809627","National Natural Science Foundation of China; Nantong Science and Technology Bureau; NanTong Key Laboratory of High Performance Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302548","Relay;finite-buffer;sparse network codes;decoding complexity;absorbing Markov chain","Encoding;Relays;Decoding;Markov processes;Complexity theory;Network coding;Payloads","channel coding;decoding;linear codes;Markov processes;network coding;probability;random codes;relay networks (telecommunication)","code sparsity;packet header overhead;decoding complexity;random linear network coding;absorbing Markov chain model;RLNC transmission;finite-buffer relay links;finite-length analysis;sparser random codes;batch coding scheme;relay transmissions;packet-level coded transmissions;lossy relay links;coding coefficients delivery cost constraints;low-complexity coding scheme;sequentially formed random subsets;source packets;buffered packets;Markov chain;decoding failure probability;low-complexity coded transmission scheme","","3","","42","","","","","IEEE","IEEE Journals & Magazines"
"Block-Orthogonal Space–Time Code Structure and Its Impact on QRDM Decoding Complexity Reduction","T. P. Ren; Y. L. Guan; C. Yuen; E. Y. Zhang","College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; College of Electronic Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Signal Processing","","2011","5","8","1438","1450","Full-rate space time codes (STCs) with rate number of transmit antennas have high multiplexing gain, but high decoding complexity even when decoded using reduced-complexity decoders such as sphere or QRDM decoders. In this paper, we introduce a new code property of STC called block-orthogonal property, which can be exploited by QR-decomposition-based decoders to achieve significant decoding complexity reduction without performance loss. We show that such complexity reduction principle can benefit the existing algebraic codes such as Perfect and DjABBA codes due to their inherent (but previously undiscovered) block-orthogonal property. In addition, we construct and optimize new full-rate block-orthogonal STC (BOSTC) that further maximize the QRDM complexity reduction potential. Simulation results of bit error rate (BER) performance against decoding complexity show that the new BOSTC outperforms all previously known codes as long as the QRDM decoder operates in reduced-complexity mode, and the code exhibits a desirable complexity saturation property.","1932-4553;1941-0484","","10.1109/JSTSP.2011.2166755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007039","Block-orthogonal space–time codes (STC);decoding complexity;orthogonal STC;QRD-M algorithm;quasi-orthogonal STC","Complexity theory;Space time codes;Maximum likelihood decoding;Matrix decomposition;Bit error rate;Transmitting antennas;Decoding","algebraic codes;decoding;error statistics;orthogonal codes;space-time block codes","block-orthogonal space-time code structure;QRDM decoding complexity reduction;transmit antenna;multiplexing gain;sphere decoder;QRDM decoder;block-orthogonal property;QR-decomposition-based decoder;algebraic codes;perfect code;DjABBA code;full-rate block-orthogonal STC;BOSTC;bit error rate performance","","26","","35","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Depth Coding by Depth Sensitivity Aware Rate-Distortion Optimization","F. Shao; W. Lin; G. Jiang; M. Yu","Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Centre for Multimedia and Network Technology, School of Computer Engineering, Nanyang Technological University, Singapore; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China","IEEE Transactions on Broadcasting","","2016","62","1","94","102","In this paper, a low-complexity coding approach is explored for depth video under the framework of high efficiency video coding based 3-D video coding standard. Unlike the existing low-complexity coding approaches that optimize motion search and mode decision, the major technical innovation of this paper is to incorporate depth sensitivity fidelity (DSF) into the rate-distortion optimization (RDO) process. Specifically, a quantitative maximum tolerable depth distortion is derived to measure the DSF, and jointly estimate the view synthesis distortion to account for the DSF. Then, a DSF-aware RDO scheme is proposed by developing new quantization parameter and Lagrangian multiplier determination strategies. Extensive experimental results demonstrate that the proposed method can reduce the computational complexity of encoding without significant view synthesis performance loss.","0018-9316;1557-9611","","10.1109/TBC.2015.2496818","Natural Science Foundation of China; K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378933","Low-complexity coding;depth sensitivity fidelity;rate-distortion optimization;view synthesis distortion;Low-complexity coding;depth sensitivity fidelity;rate-distortion optimization;view synthesis distortion","Distortion;Encoding;Distortion measurement;Optimization;Video coding;Sensitivity;Three-dimensional displays","computational complexity;optimisation;quantisation (signal);rate distortion theory;video coding","low-complexity depth coding;depth sensitivity aware rate-distortion optimization;high efficiency video coding;3-D video coding standard;motion search optimization;mode decision;depth sensitivity fidelity;DSF-aware RDO scheme;quantitative maximum tolerable depth distortion;view synthesis distortion;quantization parameter;Lagrangian multiplier determination strategies;computational complexity reduction;synthesis performance loss","","7","","32","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Hybrid ARQ Scheme for Polar Codes with Higher-Order Modulation","K. Tian; R. Liu; A. Vardy; R. Wang","NA; NA; NA; NA","GLOBECOM 2017 - 2017 IEEE Global Communications Conference","","2017","","","1","6","Combining the polar coded hybrid automatic repeat request (HARQ) with higher-order modulation has been proposed recently. To further improve the throughput of HARQ, a novel polar coded HARQ cheme with higher-order modulation is proposed. Wherein the proposed HARQ scheme, the unequal rror protection (UEP), caused by bit-mapping, in bit- interleaved polar coded modulation (BIPCM) is considered. Based on UEP, a non-linear integer programming (NLIP) problem is constructed to determine the optimal retransmitted sequence. Furthermore, a low-complexity suboptimal algorithm is proposed to solve the resulting NLIP problem. Simulation results show that the throughput of the proposed HARQ scheme outperforms the existing polar coded HARQ for higher-order modulation; in particular, our scheme easily accommodates arbitrarily puncturing methods. This makes it possible to use recently proposed puncturing methods for polar codes to further improve the performance of our HARQ scheme.","","978-1-5090-5019-2978-1-5090-5020","10.1109/GLOCOM.2017.8254939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8254939","","Modulation;Decoding;Throughput;Receivers;Complexity theory;AWGN channels;Error probability","automatic repeat request;integer programming;interleaved codes;modulation coding;nonlinear programming","polar coded hybrid automatic repeat request;nonlinear integer programming problem;hybrid ARQ scheme;polar coded HARQ","","1","","17","","","","","IEEE","IEEE Conferences"
"Complexity of Dependences in Bounded Domains, Armstrong Codes, and Generalizations","Y. M. Chee; H. Zhang; X. Zhang","School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore; School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore; School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","IEEE Transactions on Information Theory","","2015","61","2","812","819","The study of Armstrong codes is motivated by the problem of understanding complexities of dependences in relational database systems, where attributes have bounded domains. A (q, k, n)-Armstrong code is a q-ary code of length n with minimum Hamming distance n - k + 1, and for any set of k - 1 coordinates, there exist two codewords that agree exactly there. Let f (q, k) be the maximum n for which such a code exists. In this paper, f (q, 3) = 3q -1 is determined for all q ≥ 5 with three possible exceptions. This disproves a conjecture of Sali. Furthermore, we introduce generalized Armstrong codes for branching, or (s, t)-dependences, construct several classes of optimal Armstrong codes, and establish lower bounds for the maximum length n in this more general setting.","0018-9448;1557-9654","","10.1109/TIT.2014.2377735","Singapore National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977949","relational database;Armstrong codes;functional dependency;extorthogonal double covers;Relational database;Armstrong codes;functional dependency;extorthogonal double covers","Arrays;Upper bound;Relational databases;Complexity theory;Indexes;Hamming distance","Hamming codes;relational databases","bounded domain dependence complexity;relational database systems;(q, k, n)-Armstrong code;q-ary code;minimum Hamming distance;k-1 coordinates;codewords;generalized Armstrong codes;(s, t)-dependences;optimal Armstrong codes;lower bounds","","","","27","","","","","IEEE","IEEE Journals & Magazines"
"Filter optimization and complexity reduction for video coding using graph-based transforms","E. Martinez-Enriquez; F. Diaz-de-Maria; J. Cid-Sueiro; A. Ortega","Department of Signal Theory and Communications, Universidad Carlos III; Department of Signal Theory and Communications, Universidad Carlos III; Department of Signal Theory and Communications, Universidad Carlos III; Department of Electrical Engineering, University of Southern California","2013 IEEE International Conference on Image Processing","","2013","","","1948","1952","The basis functions of lifting transform on graphs are completely determined by finding a bipartition of the graph and defining the prediction and update filters to be used. In this work we consider the design of prediction filters that minimize the quadratic prediction error and therefore the energy of the detail coefficients, which will give rise to higher energy compaction. Then, to determine the graph bipartition, we propose a distributed maximum-cut algorithm that significantly reduces the computational cost with respect to the centralized version used in our previous work. The proposed techniques show improvements in coding performance and computational cost as compared to our previous work.","1522-4880;2381-8549","978-1-4799-2341","10.1109/ICIP.2013.6738401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738401","Wavelet transforms;Video coding;MCTF;Lifting;Graphs","","computational complexity;filtering theory;graph theory;transforms;video coding","filter optimization;complexity reduction;video coding;graph-based transforms;lifting transform;graph bipartition;update filters;prediction filters;quadratic prediction error;detail coefficient energy;energy compaction;distributed maximum-cut algorithm;computational cost;centralized version;coding performance","","2","","18","","","","","IEEE","IEEE Conferences"
"High Rate Robust Codes with Low Implementation Complexity","H. Rabii; Y. Neumeier; O. Keren","Faculty of Engineering, Bar-Ilan University, Ramat Gan, Israel; Faculty of Engineering, Bar-Ilan University, Ramat Gan, Israel; Faculty of Engineering, Bar-Ilan University, Ramat Gan, Israel","IEEE Transactions on Dependable and Secure Computing","","2019","16","3","511","520","Robust codes ${\mathcal {C}}(n,k)_q$C(n,k)q are nonlinear $q$q-ary codes of dimension $k$k and length $n\leq 2k$n≤2k. Robust codes can detect any error with nonzero probability; hence, they can effectively detect fault injection attacks. Most high rate robust codes are either restricted to certain ratios between $n$n and $k$k, or have relatively high hardware complexity. This paper presents new constructions for optimum or close to optimum low complexity high rate robust codes. These codes exist for any $k$k and $n$n. The hardware complexity of each construction is discussed, and a method to choose the one with the smallest implementation cost is presented.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2018.2816638","ISRAEL SCIENCE FOUNDATION; Trudevice FCTRU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8318671","High rate robust codes;fault injection;hardware security;security oriented codes;lightweight secure architecture","Robustness;Hardware;Complexity theory;Security;Systematics;Linear codes","","","","1","","27","","","","","IEEE","IEEE Journals & Magazines"
"Rate adaptive non-binary LDPC codes with low encoding complexity","N. B. Chang","MIT Lincoln Laboratory, 244 Wood Street, Lexington, MA 02420, USA","2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)","","2011","","","664","668","For error-correction codes, the optimal coding rate can vary and depend on factors including channel, time-varying fading, environmental interference, power, bandwidth allocation, communication content, and application. Rate adaptive coding schemes are thus important for robust communications. This writeup proposes and studies a rate adaptive low density parity check (LDPC) coding scheme using non-binary Galois fields (GF). The algorithm uses a single low complexity encoding structure, but maintains strong near-capacity performance at arbitrary rational rates. The rate adaptive encoder can be used in a space-time code for multiple-input multiple-output (MIMO) communication systems and is shown to achieve near capacity performance at various rates and different MIMO configurations.","1058-6393;1058-6393;1058-6393","978-1-4673-0323-1978-1-4673-0321-7978-1-4673-0322","10.1109/ACSSC.2011.6190086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190086","error correction coding;low-density parity-check;encoding complexity;outage capacity;space-time coding;multiple-input multiple-output;rate-adaptive codes;puncturing","Parity check codes;Vectors;Channel coding;MIMO;Complexity theory;Fading","adaptive codes;bandwidth allocation;channel capacity;channel coding;communication complexity;error correction codes;fading channels;Galois fields;MIMO communication;parity check codes;radiofrequency interference;space-time codes","rate adaptive nonbinary LDPC codes;error correction codes;optimal coding rate;environmental interference;bandwidth allocation;communication content;robust communications;rate adaptive low density parity check coding scheme;nonbinary Galois fields;nonbinary GF;low complexity encoding structure;near-capacity performance;space-time code;multiple-input multiple-output communication system;MIMO communication system","","5","","17","","","","","IEEE","IEEE Conferences"
"Space-Frequency Coded Index Modulation With Linear-Complexity Maximum Likelihood Receiver in the MIMO-OFDM System","L. Wang; Z. Chen; Z. Gong; M. Wu","School of Electronics and Information Engineering, Xi’an Jiaotong University, Xi’an, China; School of Electronics and Information Engineering, Xi’an Jiaotong University, Xi’an, China; Huawei Technologies Company, Ltd., Shanghai, China; Huawei Technologies Company, Ltd., Shanghai, China","IEEE Signal Processing Letters","","2016","23","10","1439","1443","In this letter, a new multiple-input multiple-output orthogonal frequency division multiplexing with index modulation (MIMO-OFDM-IM) scheme achieving diversity is proposed. It combines space-frequency code and MIMO-OFDM-IM to take advantage of both, and we call it as the space-frequency coded index modulation (SFC-IM) scheme. The proposed SFC-IM scheme can achieve the transmit diversity order of 2 by optimizing an angle parameter. A linear-complexity maximum likelihood receiver is given for the new scheme, which profits from the orthogonality between the real and imaginary parts of the symbols contained in the SFC-IM codewords. The simulation results show the performance advantage of the SFC-IM scheme over the MIMO-OFDM-IM scheme.","1070-9908;1558-2361","","10.1109/LSP.2016.2599020","Huawei Innovation Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539699","Index modulation (IM);maximum likelihood (ML) receiver;multiple-input multiple-output (MIMO);orthogonal frequency division multiplexing (OFDM);space-frequency code;transmit diversity","Indexes;Transmitting antennas;Noise measurement;Modulation;OFDM;Receiving antennas","communication complexity;diversity reception;maximum likelihood decoding;MIMO communication;modulation coding;OFDM modulation;optimisation;radio receivers","space-frequency coded index modulation;linear-complexity maximum likelihood receiver;MIMO-OFDM system;multiple-input multiple-output orthogonal frequency division multiplexing;MIMO-OFDM-IM scheme;SFC-IM scheme;transmit diversity order;angle parameter optimization","","14","","8","","","","","IEEE","IEEE Journals & Magazines"
"The encoding complexity of network coding with two simple multicast sessions","W. Song; K. Cai; R. Feng","LMAM, School of Mathematical Sciences, Peking University, Beijing 100871, China; School of ECEE, Arizona State University, Tempe, 85287-5706 USA; LMAM, School of Mathematical Sciences, Peking University, Beijing 100871, China","2012 9th International Conference on Fuzzy Systems and Knowledge Discovery","","2012","","","2253","2257","We consider the encoding complexity of two simple multicast network coding problem (2-SMNC). The network is a directed acyclic graph, where two messages are required to send from two sources to two groups of sinks respectively. We proved that the number of encoding links required to achieve a network coding solution is upper-bounded by max{3, 2N - 2} and the field size required to achieve a linear solution is upper-bounded by max{2,⌊(√2N-7/4)+1/2⌋}, where N is the number of sinks. Both bounds are shown to be tight.","","978-1-4673-0024-7978-1-4673-0025-4978-1-4673-0023","10.1109/FSKD.2012.6233999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6233999","","Encoding;Complexity theory;Network coding;Educational institutions;Vectors;Equations;Labeling","multicast communication;network coding","encoding complexity;network coding;simple multicast sessions;directed acyclic graph;encoding links;linear solution","","1","","10","","","","","IEEE","IEEE Conferences"
"On the decoding complexity of cyclic codes up to the BCH bound","D. Schipani; M. Elia; J. Rosenthal","Mathematics Institute, University of Zürich, CH-8057, Switzerland; Dipartimento di Elettronica, Politecnico di Torino, IT-10129, Italy; Mathematics Institute, University of Zürich, CH-8057, Switzerland","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","835","839","The standard algebraic decoding algorithm of cyclic codes [n, k, d] up to the BCH bound δ = 2t + 1 is very efficient and practical for relatively small n while it becomes unpractical for large n as its computational complexity is O(nt). Aim of this paper is to show how to make this algebraic decoding computationally more efficient: in the case of binary codes, for example, the complexity of the syndrome computation drops from O(nt) to O(t√n), while the average complexity of the error location drops from O(nt) to max{O(t√n), O(t log2(t)log log(t)log(n))}.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6034253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034253","Cyclic codes;Syndrome computation;Decoding complexity;Reed Solomon codes;Root search","Polynomials;Decoding;Complexity theory;Galois fields;Presses;Error correction codes","algebra;BCH codes;computational complexity;cyclic codes;decoding","decoding complexity;cyclic codes;BCH bound;algebraic decoding algorithm;computational complexity;binary codes;syndrome computation drops","","5","","29","","","","","IEEE","IEEE Conferences"
"A low-complexity power allocation for cooperative bit-interleaved coded modulation systems with adaptive decode-and-forward relaying","Tsang-Wei Yu; Wern-Ho Sheen; Chung-Hsuan Wang","Department of Electrical Engineering, National Chiao Tung University, Hsinchu, 30010 Taiwan; Department of Information and Communication Engineering, Chaoyang University of Technology, Wufong, Taichung, 41349 Taiwan; Department of Electrical Engineering, National Chiao Tung University, Hsinchu, 30010 Taiwan","2011 IEEE 22nd International Symposium on Personal, Indoor and Mobile Radio Communications","","2011","","","904","908","Transmit power allocation between source and relays is a critical design issue in a cooperative relaying system where source and relays are powered by batteries. In this paper, a low-complexity power allocation method is proposed for a 3-node cooperative bit-interleaved coded modulation system with adaptive decode-and-forward relaying, aiming to minimize the bit-error-rate at the destination. The key novelty of the method lies in transforming the power allocation into a max-min optimization problem which can be solved in a first-order equation with very low-complexity. Computer simulations show that the proposed method outperforms the equal gain power allocation with large margins in different channel conditions.","2166-9589;2166-9570","978-1-4577-1348-4978-1-4577-1346-0978-1-4577-1347","10.1109/PIMRC.2011.6140099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140099","","Resource management;Bit error rate;Relays;Decoding;Signal to noise ratio;Modulation;Adaptive systems","cooperative communication;decode and forward communication;error statistics;interleaved codes;minimax techniques;modulation coding","low-complexity power allocation;adaptive decode-and-forward relaying;transmit power allocation;cooperative relaying system;3-node cooperative bit-interleaved coded modulation system;bit-error-rate;max-min optimization problem;first-order equation;equal gain power allocation","","","","16","","","","","IEEE","IEEE Conferences"
"An efficient complexity-optimizing LDPC code design for the binary erasure channel","V. Jamali; Y. Karimian; J. Huber; M. Ahmadian","Friedrich Alexander-University Erlangen-N&#x00FC;rnberg (FAU), Erlangen, Germany; K. N. Toosi University of Technology (KNTU), Tehran, Iran; Friedrich Alexander-University Erlangen-N&#x00FC;rnberg (FAU), Erlangen, Germany; K. N. Toosi University of Technology (KNTU), Tehran, Iran","2014 8th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2014","","","238","242","The complexity-performance trade-off is a fundamental aspect of the design of low-density parity-check (LDPC) codes. In this paper, we consider LDPC codes for the binary erasure channel (BEC), use code rate for performance metric, and number of decoding iterations to achieve a certain residual erasure probability for complexity metric. The available complexity-optimizing problems in the literature for the BEC are either non-convex or belong to the class of semi-infinite problems which are computationally challenging to be solved. Hence, in this paper, we first propose a lower bound on the number of iterations for the BEC. Moreover, a simple but efficient utility function corresponding to the number of iterations is developed. Using this utility function, an optimization problem w.r.t. complexity is formulated to find complexity-optimized code degree distributions. We prove that the considered problem with the proposed utility function falls into the class of semi-definite programming (SDP) and thus, the global solution can be found efficiently using available SDP solvers. Numerical results reveal the superiority of the proposed code design compared to existing code designs from literature.","2165-4719;2165-4700","978-1-4799-5985","10.1109/ISTC.2014.6955121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955121","","Iterative decoding;Optimization;Approximation methods;Programming;Complexity theory;Turbo codes","binary codes;channel coding;computational complexity;iterative decoding;mathematical programming;parity check codes","semidefinite programming;semiinfinite problems;SDP solvers;utility function;code degree distributions;complexity-optimizing problems;decoding iterations;BEC;low-density parity-check codes;LDPC code design;binary erasure channel;complexity-performance trade-off","","1","","12","","","","","IEEE","IEEE Conferences"
"Error-resilient and complexity-constrained distributed coding for large scale sensor networks","K. Viswanatha; S. Ramaswamy; A. Saxena; K. Rose","University of California - Santa Barbara Santa Barbara, CA - 93106-9560, USA; University of California - Santa Barbara Santa Barbara, CA - 93106-9560, USA; Samsung Telecommunications Amer-ica, 1301 E. Lookout Dr., Richardson, TX, USA - 75082.; University of California - Santa Barbara Santa Barbara, CA - 93106-9560, USA","2012 ACM/IEEE 11th International Conference on Information Processing in Sensor Networks (IPSN)","","2012","","","293","304","There has been considerable interest in distributed source coding within the compression and sensor network research communities in recent years, primarily due to its potential contributions to low-power sensor networks. However, two major obstacles pose an existential threat on practical deployment of such techniques in real world sensor networks, namely, the exponential growth of decoding complexity with network size and coding rates, and the critical requirement for error-resilience given the severe channel conditions in many wireless sensor networks. Motivated by these chal-lenges, this paper proposes a novel, unified approach for large scale, error-resilient distributed source coding, based on an optimally designed classifier-based decoding frame-work, where the design explicitly controls the decoding com-plexity. We also present a deterministic annealing (DA) based global optimization algorithm for the design due to the highly non-convex nature of the cost function, which further enhances the performance over basic greedy iterative descent technique. Simulation results on data, both synthetic and from real sensor networks, provide strong evidence that the approach opens the door to practical deployment of distributed coding in large sensor networks. It not only yields substantial gains in terms of overall distortion, compared to other state-of-the-art techniques, but also demonstrates how its decoder naturally scales to large networks while constraining the complexity, thereby enabling performance gains that increase with network size.","","978-1-4799-6469","10.1109/IPSN.2012.6920944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6920944","Distributed source;channel coding;Large scale sensor net-works;Error resilient coding","Decoding;Prototypes;Encoding;Complexity theory;Indexes;Training;Correlation","convex programming;decoding;iterative methods;source coding;wireless channels;wireless sensor networks","complexity constrained distributed coding;error resilient coding;large scale sensor networks;distributed source coding;sensor network research communities;compression network research communities;real world sensor networks;decoding complexity;network size;coding rates;wireless sensor networks;deterministic annealing;DA;global optimization algorithm;cost function;greedy iterative descent technique","","1","","20","","","","","IEEE","IEEE Conferences"
"An In-Loop Filter Based on Low-Complexity CNN using Residuals in Intra Video Coding","D. Li; L. Yu","Institute of Information and Communication Engineering, Zhejiang University, Zhejiang Provincial Key Laboratory of Information Processing, Communication and Networking, Hangzhou, China; Institute of Information and Communication Engineering, Zhejiang University, Zhejiang Provincial Key Laboratory of Information Processing, Communication and Networking, Hangzhou, China","2019 IEEE International Symposium on Circuits and Systems (ISCAS)","","2019","","","1","5","Neural network-based filters have shown their potential in removing video compression artifacts. However, previously studied neural networks have achieved boosted filtering performance by continuously increasing network complexity, causing heavy burden on memory cost and computation speed. In this paper, we firstly analyze properties of original residuals which are the difference between original and predicted pixel values. Then an in-loop filter based on low-complexity CNN using residuals(CNNF-R), which are generated after compression and reconstruction from original residuals, is proposed for intra video coding. Insights of designing the network are also demonstrated. Compared with the state-of-the-art video coding standard HEVC, CNNF-R achieves up to 6.8% BD-rate reduction and 4.8% on average under all intra configuration, and 2.3% on average under random access configuration. Meanwhile, CNNF-R outperforms the previous network VRCNN in terms of nearly 70% decrease in computation complexity, considerable decrease in memory consumption and 1.2% increase in BD-rate reduction.","2158-1525;0271-4302","978-1-7281-0397-6978-1-7281-0398","10.1109/ISCAS.2019.8702443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8702443","Convolutional Neural Network(CNN);Residual;Low Complexity;In-Loop Filter;Intra Video Coding","Feature extraction;Image reconstruction;Video coding;Noise measurement;Training;Complexity theory;Neural networks","","","","","","18","","","","","IEEE","IEEE Conferences"
"Improved Circuit Design of Analog Joint Source Channel Coding for Low-Power and Low-Complexity Wireless Sensors","X. Zhao; V. Sadhu; A. Yang; D. Pompili","Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, NJ, USA","IEEE Sensors Journal","","2018","18","1","281","289","To enable low-power and low-complexity wireless monitoring, an improved circuit design of Analog Joint Source Channel Coding (AJSCC) is proposed for wireless sensor nodes. This innovative design is based on Analog Divider Blocks (ADB) with tunable spacing between AJSCC levels. The ADB controls the switching between two types of Voltage Controlled Voltage Sources (VCVS). LTSpice simulations have been performed to evaluate the performance of the circuit, and the power consumption and circuit complexity of this new ADB-based design have been compared with our previous parallel-VCVS design. It is found that this improved circuit design based on ADB outperforms the design based on parallel VCVS for a large number of AJSCC levels (≥16), both in terms of power consumption as well as circuit complexity, thus enabling persistent and higher temporal/spatial resolution environmental sensing.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2017.2761765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8063881","Power efficiency;analog compression;circuit complexity;signal multiplexing;environmental monitoring","Sensors;Wireless sensor networks;Wireless communication;Power demand;Channel coding;Circuit synthesis;Complexity theory","circuit complexity;combined source-channel coding;communication complexity;dividing circuits;low-power electronics;wireless channels;wireless sensor networks","improved circuit design;analog joint source channel coding;low-power low-complexity wireless sensor node;low-power low-complexity wireless monitoring;AJSCC;analog divider block;voltage controlled voltage source;LTSpice simulation;circuit complexity;power consumption;ADB-based design;parallel-VCVS design;temporal-spatial resolution environmental sensing","","2","","21","","","","","IEEE","IEEE Journals & Magazines"
"An Approach for Detecting Unnecessary Cyclomatic Complexity on Source Code","H. d. S. Campos Junior; L. R. V. Martins Filho; M. A. P. Araujo","NA; NA; NA","IEEE Latin America Transactions","","2016","14","8","3777","3783","Seeking product's quality is essential nowadays. One of the many quality aspects in software development is the source code complexity. Not paying attention to the complexity during the development can result in unexpected cost, caused by the difficulty on the source code understanding. The goal of this paper is to introduce an initial approach to identify unnecessary complexity in source code. Besides identifying, the approach can also show to its user how to properly rewrite the source code without the unnecessary complexity. The approach is based on the static analysis of the source code control flow graph. Once the unnecessary complexity is identified, the graph is refactored in order to allow the user to understand the improvement on the source code. The approach was included in a software tool in order to prove its concept. A performance evaluation of the approach was performed, resulting in a high accuracy. Two experimental studies were also performed to assess the viability of the approach when used by real users. The evidences provided by these studies suggests that the approach support the unnecessary complexity removal.","1548-0992","","10.1109/TLA.2016.7786363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7786363","control flow graph;cyclomatic complexity;experimental software engineering;unnecessary complexity","Complexity theory;IEEE transactions;Flow graphs;Software tools;Performance evaluation;Software engineering","flow graphs;program diagnostics;software maintenance;software metrics;software quality;software tools;source code (software)","cyclomatic complexity detection;product quality;software development;source code complexity;source code understanding;source code rewrite;static analysis;control flow graph;refactored graph;software tool","","","","","","","","","IEEE","IEEE Journals & Magazines"
"A complexity scalable mode decision algorithm for interframe coding in HEVC","B. Yang; G. Chen; X. Zhang; Z. Gao; L. Chen","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai 200240, China","2016 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2016","","","1","4","Due to limited computational capability in handheld devices, complexity constrained video coding draws great attention in recent years. In this paper, a complexity scalable mode decision algorithm is proposed for complexity control of HEVC. First, complexity is properly mapped to a target in terms of prediction modes. Then a mode decision algorithm is proposed through statistics to get the optimal mode combination for each target complexity. Experimental results show that the proposed algorithm achieves a very large complexity control range of 10%-100% while maintaining good Rate-Distortion performance. For lowdelayP condition, an average gain of 1.1 dB in BDPSNR is obtained for 22 sequences when the complexity is around 20%.","2155-5052","978-1-4673-9044-6978-1-4673-9045","10.1109/BMSB.2016.7521939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7521939","High Efficiency Video Coding;Mode Decision;Complexity Control","Encoding;Resource management;Prediction algorithms;Computational complexity","computational complexity;rate distortion theory;video coding","complexity constrained video coding;complexity scalable mode decision algorithm;complexity control;HEVC;prediction modes;optimal mode combination;rate-distortion performance;lowdelayP condition;BDPSNR;interframe coding","","","","10","","","","","IEEE","IEEE Conferences"
"Efficient adaptive bit-rate control for Scalable Video Coding by using Computational Complexity-Rate-Distortion analysis","D. Grois; O. Hadar","Ben-Gurion University, Israel; Ben-Gurion University, Israel","2011 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2011","","","1","6","We present an adaptive pre-processing (pre-filtering) method and system for the efficient Scalable Video Coding (SVC). Our adaptive SVC prefiltering system enables to decrease motion activity, and in turn, enables to decrease quantization fluctuations within the background region of each layer of a SVC video sequence, which contains two or more layers. In turn, this enables to use smaller quantization parameters, but still to obtain the same compression rate for the optimal usage of encoder and decoder computational resources in order to achieve optimal video presentation quality. Our adaptive prefiltering system is very efficient since it is based on a SVC Computational Complexity-Rate-Distortion (C R-D) analysis, adding a complexity dimension to the conventional rate-distortion (R-D) analysis for the SVC coding. As a result, the SVC visual presentation quality at the decoder side is significantly improved even for the decoders with very limited computational resources, which can be very useful for various mobile devices, such as cellular phones. Also, we perform Region-of-Interest (ROI) SVC pre-processing for enabling providing efficient coding of the desired Region-of-Interest. The performance of the presented systems is demonstrated and compared with the Joint Scalable Video Model 9.19 (JSVM 9.19) reference software.","2155-5052;2155-5044;2155-5044","978-1-61284-122-9978-1-61284-121-2978-1-61284-120","10.1109/BMSB.2011.5954877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954877","Scalable Video Coding (SVC);Regions-of-interest (ROI) video coding;preprocessing/pre-filtering;image/video coding;ROI scalability;high-quality visual presentation;H.264/AVC","Static VAr compensators;Encoding;Video coding;Computational complexity;PSNR;Scalability;Video sequences","adaptive filters;data compression;image motion analysis;image sequences;quantisation (signal);rate distortion theory;video coding","adaptive bit-rate control;adaptive preprocessing method;adaptive prefiltering method;scalable video coding;SVC prefiltering system;motion activity;quantization fluctuation;background region;SVC video sequence;quantization parameter;compression rate;encoder;decoder;optimal video presentation quality;computational complexity-rate-distortion analysis;C R-D analysis;SVC coding;region-of-interest","","11","","25","","","","","IEEE","IEEE Conferences"
"Low complexity memory architectures based on LDPC codes: Benefits and disadvantages","B. Vasić; P. Ivaniš; S. Brkic","Department of Electrical and Computer Engineering, University of Arizona, Tucson, 85721 USA; University of Belgrade, Serbia, School of Electrical Engineering; University of Belgrade, Serbia, School of Electrical Engineering","2015 12th International Conference on Telecommunication in Modern Satellite, Cable and Broadcasting Services (TELSIKS)","","2015","","","11","18","In this paper we investigate the problem of information storage in inherently unreliable memory cells. In order to increase the memory reliability, information is stored in memory cells as a codeword of a low-density parity-check (LDPC) code, while the memory content is updated periodically by an error correction scheme. We first present an overview on the state-of-the memory architectures based on LDPC codes, and then asses the benefits of using the coded architectures expressed through the increased reliability. In addition, we provide upper bounds on the complexity of such memories.","","978-1-4673-7516-0978-1-4673-7515-3978-1-4673-7514","10.1109/TELSKS.2015.7357727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357727","","Decoding;Logic gates;CMOS integrated circuits;CMOS technology","error correction;memory architecture;parity check codes;reliability","low density parity check code;LDPC codes;low complexity memory architectures;information storage problem;memory cells;memory reliability;memory content;error correction scheme;state-of-the memory architectures","","4","","38","","","","","IEEE","IEEE Conferences"
"Turbo Decoding Using the Sectionalized Minimal Trellis of the Constituent Code: Performance-Complexity Trade-Off","G. L. Moritz; R. D. Souza; C. Pimentel; M. E. Pellenz; B. F. Uchoa-Filho; I. Benchimol","CPGEI, Federal University of Technology - Parana (UTFPR), Curitiba-PR, Brazil; CPGEI, Federal University of Technology - Parana (UTFPR), Curitiba-PR, Brazil; CODEC/DES, Federal University of Pernambuco (UFPE), Recife-PE, Brazil; PPGIA, Pontifical Catholic University (PUC-PR), Curitiba-PR, Brazil; GPqCom/EEL, Federal University of Santa Catarina (UFSC), Florianopolis-SC, Brazil; CMDI, Federal Institute of Amazonas (IFAM), Manaus-AM, Brazil","IEEE Transactions on Communications","","2013","61","9","3600","3610","The performance and complexity of turbo decoding using rate k/n constituent codes are investigated. The conventional, minimal and sectionalized trellis modules of the constituent convolutional codes are utilized. The performance metric is the bit error rate (BER), while complexity is analyzed based on the number of multiplications, summations and comparisons required by the max-log-MAP decoding algorithm. Our results show that the performance depends on how the systematic bits are grouped in a trellis module. The best performance is achieved when the k systematic bits are grouped together in the same section of the module, so that the log-likelihood ratio (LLR) of the k-bit vector is calculated at once. This is a characteristic of the conventional trellis module and of some of the sectionalizations of the minimal trellis module. Moreover, we show that it is possible to considerably reduce the decoding complexity with respect to the conventional trellis if a particular sectionalization of the minimal trellis module is utilized. In some cases, this sectionalization is found within the best performing group, while in some other cases a small performance loss can be traded off for a large complexity reduction.","0090-6778;1558-0857","","10.1109/TCOMM.2013.072913.120912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573237","Convolutional codes;turbo codes;decoding complexity;minimal trellis","Complexity theory;Decoding;Turbo codes;Bit error rate;Measurement;Convolutional codes;Systematics","computational complexity;convolutional codes;error statistics;maximum likelihood decoding;trellis codes","sectionalized minimal Trellis;constituent code;performance-complexity trade-off;turbo decoding complexity;decoding performance;sectionalized Trellis modules;convolutional codes;performance metric;bit error rate;BER;max-log-MAP decoding algorithm;systematic bits;trellis module;log-likelihood ratio;k-bit vector;decoding complexity;complexity reduction","","3","","25","","","","","IEEE","IEEE Journals & Magazines"
"Floating-point to fixed-point code conversion with variable trade-off between computational complexity and accuracy loss","A. Bârleanu; V. Băitoiu; A. Stan","NA; NA; NA","15th International Conference on System Theory, Control and Computing","","2011","","","1","6","This paper describes a method of converting floating-point expressions into equivalent fixed-point code in DSP software. Replacing floating-point expressions with specialized integer operations can greatly improve the performance of embedded applications. This is a new method that is developed for Direct-Form I filters with constant coefficients and input variables whose low/high bounds are known. Two conflicting objectives are considered simultaneously: computational complexity and accuracy loss. The algorithm presented here can construct multiple fixed-point solutions for the same floating-point code: from “high-complexity-high-accuracy” to “low-complexity-low-accuracy”. A so-called cost function conducts the data flow transformation decisions. By changing the cost function coefficients, different fixed-point forms can be obtained. The data flow transformation takes very little time: less than 100 milliseconds for a 32-tap FIR filter. The generated fixed-point code is tested on 8-bit (AVR ATmega), 16-bit (MSP430), and 32-bit (ARM Cortex-M3) microcontrollers. It provides, in all cases, execution speeds better than if using floating-point code.","","978-973-621-321-2978-1-4577-1173-2978-973-621-323","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6085687","","Accuracy;Cost function;Computational complexity;Microprocessors;Java","computational complexity;fixed point arithmetic;floating point arithmetic","floating-point code conversion;fixed-point code conversion;computational complexity;equivalent fixed-point code;DSP software;floating-point expression;specialized integer operation;embedded application;data flow transformation decision;cost function coefficient;microcontrollers","","","","9","","","","","IEEE","IEEE Conferences"
"Low-complexity mode-dependent KLT for block-based intra coding","C. Yeo; Y. H. Tan; Z. Li","Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01, Connexis (South Tower), Singapore 138632; Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01, Connexis (South Tower), Singapore 138632; Signal Processing Department, Institute for Infocomm Research, 1 Fusionopolis Way, #21-01, Connexis (South Tower), Singapore 138632","2011 18th IEEE International Conference on Image Processing","","2011","","","3685","3688","Applying mode-dependent separable transforms, e.g., mode- dependent directional transform (MDDT), is an effective method for improving transform coding of intra prediction residuals. However, two transform matrices typically need to be stored for each intra prediction mode. By using a simple image correlation mode, we have previously derived and proposed a simplified mode-dependent separable transforms scheme that uses a combination of two well-known trans- forms: Discrete Cosine Transform (DCT) and Discrete Sine Transform (DST). In this paper, we propose an orthogonal 4-point integer DST that has a multiplier-less implementation consisting of only adds and bit-shifts. We also propose a simple set of mode-dependent scans for coefficient coding that can be used on top of mode-dependent transforms. Our experimental results on the current HEVC reference software show that in terms of coding efficiency, our proposed approach has comparable performance to MDDT. More importantly, compared to MDDT, our approach requires no training and has lower computational and storage costs.","2381-8549;1522-4880;1522-4880","978-1-4577-1303-3978-1-4577-1304-0978-1-4577-1302","10.1109/ICIP.2011.6116519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6116519","HEVC;Intra coding;Transforms","Image coding;Discrete cosine transforms;Conferences;Covariance matrix;Software","computational complexity;discrete cosine transforms;image coding","low-complexity mode-dependent KLT;block-based intra coding;mode-dependent separable transforms;transform coding;intra prediction residuals;discrete cosine transform;discrete sine transform;orthogonal 4-point integer DST;DCT;mode-dependent transforms;HEVC reference software;MDDT","","11","","12","","","","","IEEE","IEEE Conferences"
"A low-complexity multicarrier scheme with LDPC coding for mobile-to-mobile environment","D. Roque; C. Siclet; J. Brossier","Grenoble Images Parole Signal Automatique (GIPSA-lab), 11, rue des Math&#x00E9;matiques, 38402 Grenoble, France; Grenoble Images Parole Signal Automatique (GIPSA-lab), 11, rue des Math&#x00E9;matiques, 38402 Grenoble, France; Grenoble Images Parole Signal Automatique (GIPSA-lab), 11, rue des Math&#x00E9;matiques, 38402 Grenoble, France","MILCOM 2012 - 2012 IEEE Military Communications Conference","","2012","","","1","6","In this paper, we consider a mobile ad-hoc radio network in an urban area. The propagation environment between two endpoints can be modeled by a double-Rayleigh fading multipath channel. Such a mobile scenario justifies the use of filter bank based multicarrier (FBMC) transmission systems. This technique generalizes traditional cyclic prefix orthogonal frequency-division multiplexing (CP-OFDM), allowing the design of non-rectangular pulse shape filters. We show that this approach leads to a better interference mitigation in time-variant channels. We restrict our study to short filters and single-tap per sub-channel equalization in order to preserve a low-complexity transmultiplexer. In this study, we compare FBMC with short filters to CP-OFDM in terms of coded bit-error-rate performances, using a realistic mobile-to-mobile channel model.","2155-7586;2155-7578;2155-7578","978-1-4673-1731-3978-1-4673-1729-0978-1-4673-1730","10.1109/MILCOM.2012.6415654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415654","","Receivers;Prototypes;Parity check codes;Mobile communication;Interference;Equalizers;Doppler effect","channel bank filters;error statistics;interference (signal);mobile ad hoc networks;multipath channels;OFDM modulation;parity check codes;pulse shaping;Rayleigh channels;transmultiplexing","low-complexity multicarrier scheme;LDPC coding;mobile-to-mobile environment;mobile ad-hoc radio network;propagation environment;double-Rayleigh fading multipath channel;mobile scenario;filter bank based multicarrier transmission systems;FBMC transmission systems;cyclic prefix orthogonal frequency-division multiplexing;CP-OFDM;nonrectangular pulse shape filters;interference mitigation;time-variant channels;subchannel equalization","","1","","17","","","","","IEEE","IEEE Conferences"
"Robust and Low-Complexity Space Time Code for Industrial Automation","M. S. Abouzeid; M. Ehrig; N. A. Odhah; E. Grass; R. Kraemer","IHP, Frankfurt, Oder, 15236, Germany; IHP, Frankfurt, Oder, 15236, Germany; IHP, Frankfurt, Oder, 15236, Germany; Department of Computer Science, Humboldt-University of Berlin, Berlin, 10099, Germany; Department of Systems, Brandenburg University of Technology, Cottbus, 03046, Germany","2018 10th International Conference on Advanced Infocomm Technology (ICAIT)","","2018","","","110","114","This paper addresses an uplink system configuration for an Industrial Environment, in addition, a novel robust and low-complexity space time code is proposed. Based on channel modelling of Quadriga which follows a geometry-based stochastic channel modeling approach, the proposed space time code is applied in the uplink of an industrial communication system. Simulation results show that the proposed code outperforms Alamouti code for Industrial Automation. The bit error rate (BER) performance demonstrates that the achieved coding gain for the proposed code is higher than Alamouti code. Furthermore, a low complexity decoder based on minimum mean squared error is designed at the receiver side. The proposed space time code is designed basically for increasing the reliability and robustness in industrial communication systems where BER of 10-9 is demanded. The proposed code provides superior performance against the state-of-the-art space time code systems.","","978-1-5386-7936-4978-1-5386-7935-7978-1-5386-7937","10.1109/ICAIT.2018.8686600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8686600","industrial communication;space-time code;quadriga;minimum mean squared error decoder;factory automation","","channel coding;decoding;error statistics;geometry;space-time codes","low-complexity space time code;geometry-based stochastic channel modeling approach;industrial communication system;Alamouti code;Industrial Automation;achieved coding gain;low complexity decoder;uplink system configuration;Industrial Environment;space time code systems","","","","21","","","","","IEEE","IEEE Conferences"
"Reducing the Complexity of Quasi-Maximum-Likelihood Detectors Through Companding for Coded MIMO Systems","X. Dai; R. Zou; J. An; X. Li; S. Sun; Y. Wang","China Academy of Telecommunication Technology, Beijing, China; School of Information Science and Engineering, Central South University, Changsha, China; School of Information and Electronics Engineering, Beijing Institute of Technology, Beijing, China; School of Information and Electronics Engineering, Beijing Institute of Technology, Beijing, China; China Academy of Telecommunication Technology, Beijing, China; China Academy of Telecommunication Technology, Beijing, China","IEEE Transactions on Vehicular Technology","","2012","61","3","1109","1123","A companding-based transformation method is introduced to quasi-maximum-likelihood (ML) detectors, such as the QR-decomposition-based M-algorithm (QRD-M) and list sphere decoding, for coded multiple-input-multiple-output (MIMO) systems in this paper. The key idea of the proposed companding technique is to compress (i.e., down-weight) the dubious observation of the accumulated branch metric by taking into account its statistical characteristics so that, after companding, the estimation error of the unreliable detected information bits due to insufficient candidate size and/or channel estimation error is significantly mitigated without disproportionate compromise of the reliable information bits. By employing the proposed companding method, the original leptokurtically distributed log-likelihood ratio of the detected information bits becomes more Gaussian distributed. As an illustrative example, the QRD-M detector is employed in this paper. Numerical results show that the QRD-M detector based on the proposed companding paradigm achieves significant performance gain over the conventional method and approaches the performance of the ML detector for a 16-ary quadrature-amplitude-modulated (16-QAM) 4×4 MIMO multiplexing system with lower-than-linear-detector computational complexity.","0018-9545;1939-9359","","10.1109/TVT.2012.2183008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6123218","Accumulated branch metric (ABM);list sphere decoding (LSD);maximum likelihood (ML);multiple-input–multiple-output (MIMO);QR-decomposition-based M-algorithm (QRD-M)","Detectors;Measurement;Channel estimation;MIMO;Decoding;Vectors;Multiplexing","channel estimation;computational complexity;Gaussian distribution;matrix decomposition;maximum likelihood decoding;MIMO communication;multiplexing;quadrature amplitude modulation","quasimaximum-likelihood detector complexity reduction;coded MIMO system;companding-based transformation method;quasiML detector complexity reduction;QR-decomposition-based M-detector algorithm;QRD-M detector algorithm;list sphere decoding;coded multiple-input-multiple-output system;accumulated branch metric;unreliable detected information bit detection;channel estimation error;reliable information bit detection;original leptokurtically distributed log-likelihood ratio;Gaussian distribution;16-ary quadrature-amplitude-modulation;16-QAM;MIMO multiplexing system;lower-than-linear-detector computational complexity","","15","","42","","","","","IEEE","IEEE Journals & Magazines"
"Complexity-adaptive Random Network Coding for Peer-to-Peer video streaming","A. Fiandrotti; S. Zezza; E. Magli","Dipartimento di Elettronica - Politecnico di Torino; Dipartimento di Elettronica - Politecnico di Torino; Dipartimento di Elettronica - Politecnico di Torino","2011 IEEE 13th International Workshop on Multimedia Signal Processing","","2011","","","1","6","We present a novel architecture for complexity-adaptive Random Network Coding (RNC) and its application to Peer-to-Peer (P2P) video streaming. Network coding enables the design of simple and effective P2P video distribution systems, however it relies on computationally intensive packet coding operations that may exceed the computational capabilities of power constrained devices. It is hence desirable that the complexity of network coding can be adjusted at every node according to its computational capabilities, so that different classes of nodes can coexist in the network. To this end, we model the computational complexity of network coding as the sum of a packet decoding cost, which is centrally minimized at the encoder, and a packet recoding cost, which is locally controlled by each node. Efficient network coding is achieved exploiting the packet decoding process as a packet pre-recoding stage, hence increasing the chance that transmitted packets are innovative without increasing the recoding cost. Experiments in a P2P video streaming framework show that the proposed design enables the nodes of the network to operate at a wide range of computational complexity levels, while a higher number of low complexity nodes are able to join the network and experience high-quality video.","","978-1-4577-1434-4978-1-4577-1432-0978-1-4577-1433","10.1109/MMSP.2011.6093834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093834","","Peer to peer computing;Streaming media;Decoding;Encoding;Network coding;Protocols;Computer architecture","computational complexity;network coding;peer-to-peer computing;video streaming","complexity-adaptive random network coding;peer-to-peer video streaming;RNC;P2P video distribution systems;power constrained devices;packet decoding process;packet prerecoding stage;computational complexity;low complexity nodes;high-quality video","","3","","10","","","","","IEEE","IEEE Conferences"
"Low complexity codes for writing a write-once memory twice","Y. Wu","Microsoft Research, One Microsoft Way, Redmond, WA USA","2010 IEEE International Symposium on Information Theory","","2010","","","1928","1932","A write-once memory (wom) is a storage medium formed by a number of “write-once” binary cells, where each cell initially is in a `0' state and can be changed to a `1' state irreversibly. Examples of write-once memories include SLC flash memories and optical disks. This paper presents some low complexity codes for writing such write-once memories twice.","2157-8095;2157-8117","978-1-4244-7892-7978-1-4244-7890-3978-1-4244-7891","10.1109/ISIT.2010.5513379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513379","","Writing;Flash memory;Block codes;Bipartite graph;Encoding;Decoding;Vectors;Error analysis;Delay;Measurement","codes;computational complexity;optical disc storage;write-once storage","low-complexity codes;write-once memory;storage medium;write-once binary cells;SLC flash memories;optical disks","","27","","12","","","","","IEEE","IEEE Conferences"
"High Performance, Low Complexity Video Coding and the Emerging HEVC Standard","K. Ugur; K. Andersson; A. Fuldseth; G. Bjontegaard; L. P. Endresen; J. Lainema; A. Hallapuro; J. Ridge; D. Rusanovskyy; C. Zhang; A. Norkin; C. Priddle; T. Rusert; J. Samuelsson; R. Sjoberg; Z. Wu","Nokia Corporation, Tampere, Finland; Ericsson Research, Stockholm, Sweden; Tandberg Telecom (Cisco company), Lysaker, Norway; Tandberg Telecom (Cisco company), Lysaker, Norway; Tandberg Telecom (Cisco company), Lysaker, Norway; Nokia Corporation, Tampere, Finland; Nokia Corporation, Tampere, Finland; Nokia Corporation, Tampere, Finland; Nokia Corporation, Tampere, Finland; Nokia Corporation, Tampere, Finland; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden; Ericsson Research, Stockholm, Sweden","IEEE Transactions on Circuits and Systems for Video Technology","","2010","20","12","1688","1697","This paper describes a low complexity video codec with high coding efficiency. It was proposed to the high efficiency video coding (HEVC) standardization effort of moving picture experts group and video coding experts group, and has been partially adopted into the initial HEVC test model under consideration design. The proposal utilizes a quadtree-based coding structure with support for macroblocks of size 64 × 64, 32 × 32, and 16 × 16 pixels. Entropy coding is performed using a low complexity variable length coding scheme with improved context adaptation compared to the context adaptive variable length coding design in H.264/AVC. The proposal's interpolation and deblocking filter designs improve coding efficiency, yet have low complexity. Finally, intra-picture coding methods have been improved to provide better subjective quality than H.264/AVC. The subjective quality of the proposed codec has been evaluated extensively within the HEVC project, with results indicating that similar visual quality to H.264/AVC High Profile anchors is achieved, measured by mean opinion score, using significantly fewer bits. Coding efficiency improvements are achieved with lower complexity than the H.264/AVC Baseline Profile, particularly suiting the proposal for high resolution, high quality applications in resource-constrained environments.","1051-8215;1558-2205","","10.1109/TCSVT.2010.2092613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5638615","H264/AVC;HEVC;standardization;video coding","Encoding;Transforms;Automatic voltage control;Complexity theory;Interpolation;Video codecs","entropy codes;interpolation;quadtrees;variable length codes;video codecs;video coding","HEVC standard;moving picture experts group;video coding experts group;quadtree-based coding structure;entropy coding;low complexity variable length coding scheme;interpolation;deblocking filter design;intrapicture coding","","104","","16","","","","","IEEE","IEEE Journals & Magazines"
"A pixel orientation and adaptive search range based complexity reduction in H.264 scalable video coding","L. Balaji; K. K. Thyagharajan","Dept of ECE, Velammal Institute of Technology, Faculty of Information and Communication, Anna University, Chennai, India; RMD Engineering College, Chennai, India","2017 4th International Conference on Advanced Computing and Communication Systems (ICACCS)","","2017","","","1","5","H.264 advanced video coding (AVC) is prolonged to scalable video coding (SVC) for all categories of network transmission. Due to the adoption of exhaustive full search method, SVC computational complexity (CC) has increased when compared to AVC. To reduce the computational complexity, various fast mode decision (FMD) algorithms have been developed. In FMD algorithms, the selection of mode in macro block (MB) needs an efficient algorithm to reduce computational complexity. These FMD algorithms reduce Computational Complexity in saving time alone with no increment in peak signal to noise ratio (PSNR) or no detriments in bit rate. In other words, the existing FMD algorithms are effective in one measure viz., time saving as a reduction of computational complexity but doesn't affect PSNR and bit rate. This research paper describes how a pixel orientation algorithm (POA) executed for SVC will result in reduction of computations complexity in all three measures viz., time, PSNR and bit rate compared with the all existing algorithms. From the results, the overall performance of POA resulted in 3.76 % faster time, 1.43 dB increments in PSNR and 0.45 kbps detriments in bit rate as compared to joint scalable video model (JSVM).","","978-1-5090-4559-4978-1-5090-4560","10.1109/ICACCS.2017.8014625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014625","SVC;Computational Complexity;pixel orientation mode;PSNR;Encoding Time","Encoding;Static VAr compensators;Video coding;Scalability;Bit rate;Computational complexity;Algorithm design and analysis","computational complexity;image resolution;search problems;video coding","adaptive search range based complexity reduction;H.264 scalable video coding;H.264 advanced video coding;AVC;network transmission;exhaustive full search method;SVC computational complexity;CC;fast mode decision algorithms;FMD;macro block;MB;computational complexity;peak signal to noise ratio;PSNR;pixel orientation algorithm;POA;joint scalable video model;JSVM","","","","14","","","","","IEEE","IEEE Conferences"
"Algebraic Quasi-Cyclic LDPC Codes: Construction, Low Error-Floor, Large Girth and a Reduced-Complexity Decoding Scheme","J. Li; K. Liu; S. Lin; K. Abdel-Ghaffar","NA; NA; NA; NA","IEEE Transactions on Communications","","2014","62","8","2626","2637","This paper presents a simple and very flexible method for constructing quasi-cyclic (QC) low density paritycheck (LDPC) codes based on finite fields. The code construction is based on two arbitrary subsets of elements from a given field. Some well known constructions of QC-LDPC codes based on finite fields and combinatorial designs are special cases of the proposed construction. The proposed construction in conjunction with a technique, known as masking, results in codes whose Tanner graphs have girth 8 or larger. Experimental results show that codes constructed using the proposed construction perform well and have low error-floors. Also presented in the paper is a reduced-complexity iterative decoding scheme for QC-LDPC codes based on the section-wise cyclic structure of their parity-check matrices. The proposed decoding scheme is an improvement of an earlier proposed reduced-complexity iterative decoding scheme.","0090-6778;1558-0857","","10.1109/TCOMM.2014.2339329","NSF; Northrop Grumman Corporation, LSI Corporation and SanDisk; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6856152","Binary codes;channel coding;parity-check codes;iterative coding","Iterative decoding;Arrays;Decoding;Null space;Bit error rate;Sparse matrices","algebraic codes;channel coding;graph theory;parity check codes","Tanner graphs;parity-check matrices;channel coding;masking;quasi-cyclic low density parity check codes;reduced-complexity decoding scheme;algebraic quasicyclic LDPC codes","","63","","36","","","","","IEEE","IEEE Journals & Magazines"
"Joint Encoding and Grouping Multiple Node Pairs for Physical-Layer Network Coding With Low-Complexity Algorithm","L. Guo; Z. Ning; Q. Song; F. Huang; A. Jamalipour","Key Laboratory of Medical Image Computing and the School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Software, Dalian University of Technology, Dalian, China; Key Laboratory of Medical Image Computing and the School of Computer Science and Engineering, Northeastern University, Shenyang, China; Key Laboratory of Medical Image Computing and the School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Vehicular Technology","","2017","66","10","9275","9286","How to put fifth-generation (5G) into the commercial service by 2020 is still challenging. Since physical-layer network coding (PNC) does not bring unnecessary redundancy and offers satisfactory end-to-end services, it is promising to provide network services in 5G networks. Existing works on PNC mainly focus on the case that two packets are simultaneously sent by a node pair for information exchange, and the transmitted signals are superposed and encoded at a relay. To further exploit the potential performance improvement of PNC, this study aims at encoding multiple packets through jointly processing multiple superposed signals sent by diverse node pairs. Particularly, we formulate the problem of joint-encoding method design as a binary integer programming model and propose a low-complexity solution based on the idea of filling a Latin rectangle. We find that network throughput enhancement does not accompany the increasing number of coded node pairs, and some node pairs cannot be encoded together since they fail to decode the resulting encoded packet. Therefore, the decodability-based node pair grouping scheme is further investigated. Simulation results in various scenarios show that the proposed low-complexity scheme performs neck to neck to the optimum one with high computational complexity.","0018-9545;1939-9359","","10.1109/TVT.2017.2696709","National Natural Science Foundation of China; Fundamental Research Funds for Central Universities; China Post-Doctoral Science Foundation Project; Liaoning Province Doctor Startup Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907326","Constellation mapping;mobile service;physical-layer network coding (PNC);quadrature amplitude modulation (QAM);wireless networks","Relays;Encoding;Artificial neural networks;Scheduling;Network coding;Scheduling algorithms;Wireless communication","5G mobile communication;computational complexity;decoding;diversity reception;integer programming;network coding;relay networks (telecommunication)","end-to-end services;encoded packet;fifth-generation networks;5G networks;binary integer programming model;Latin rectangle;decodability-based node pair grouping scheme;computational complexity;coded node pairs;joint-encoding method design;diverse node pairs;multiple superposed signals;network services;PNC;commercial service;physical-layer network coding;grouping multiple node pairs","","1","","24","Traditional","","","","IEEE","IEEE Journals & Magazines"
"H.264/AVC rate control scheme with frame coding complexity optimized selection","X. Liu; Y. Zhou; L. Tian","School of Software, University of Electronic Science and Technology of China, Chengdu, China, 610054; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China, 610054; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China, 610054","2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","","2011","4","","2181","2185","Rate control regulates the output bit rate of a video encoder to obtain optimum visual quality within the available network bandwidth and maintain buffer fullness within a specified tolerance range. The quadratic rate-distortion model is widely adopted in H.264/AVC rate control. However the quadratic model is not precise in terms of the estimation and computation of frame coding complexity. To achieve excellent rate control performance, the paper proposes a novel rate control scheme based on the optimized selection for frame coding complexity. Simulation results indicate that, compared with JM13.2, the new rate control algorithm gets more accurate rate regulation, provides robust buffer control, and efficiently reduces frame skipping. Furthermore, the PSNR gains for QCIF sequences are high up to 0.99dB under all inter-frame encoding formats.","","978-1-61284-181-6978-1-61284-180-9978-1-61284-179","10.1109/FSKD.2011.6020033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6020033","H.264/AVC;Rate Control;Video Encoding;Frame Complexity","Encoding;Complexity theory;Bit rate;Object oriented modeling;Computational modeling;Video coding;PSNR","rate distortion theory;video coding","H.264-AVC rate control scheme;frame coding complexity optimized selection;video encoder;visual quality;network bandwidth;quadratic rate-distortion model;robust buffer control;frame skipping reduction;PSNR gains;inter-frame encoding formats","","","","14","","","","","IEEE","IEEE Conferences"
"Non-uniformly coupled LDPC codes: Better thresholds, smaller rate-loss, and less complexity","L. Schmalen; V. Aref; F. Jardel","Nokia Bell Labs, Stuttgart, Germany; Nokia Bell Labs, Stuttgart, Germany; Nokia Bell Labs, Stuttgart, Germany","2017 IEEE International Symposium on Information Theory (ISIT)","","2017","","","376","380","We consider spatially coupled low-density parity-check codes with finite smoothing parameters. A finite smoothing parameter is important for designing practical codes that are decoded using low-complexity windowed decoders. By optimizing the amount of coupling between spatial positions, we show that we can construct codes with excellent thresholds and small rate loss, even with the lowest possible smoothing parameter and large variable node degrees, which are required for low error floors. We also establish that the decoding convergence speed is faster with non-uniformly coupled codes, which we verify by density evolution of windowed decoding with a finite number of iterations. We also show that by only slightly increasing the smoothing parameter, practical codes with potentially low error floors and thresholds close to capacity can be constructed. Finally, we give some indications on protograph designs.","2157-8117","978-1-5090-4096-4978-1-5090-4097","10.1109/ISIT.2017.8006553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006553","","Couplings;Decoding;Smoothing methods;Complexity theory;Iterative decoding;Sockets","convergence;decoding;parity check codes;smoothing methods","nonuniformly coupled LDPC codes;smaller rate-loss;spatially coupled low-density parity check codes;finite smoothing parameters;low-complexity windowed decoders;excellent thresholds;variable node degrees;error floors;convergence speed decoding;nonuniformly coupled codes;density evolution;finite iteration number;practical codes;protograph designs","","3","","17","","","","","IEEE","IEEE Conferences"
"Low-complexity collision detection scheme using pseudo-coded ON-OFF pilot transmission per-packet for Wireless Sensor Networks","F. Alassery; W. K. M. Ahmed; M. Sarraf; V. Lawrence","Stevens Institute of Technology, New Jersey, USA; Stevens Institute of Technology, New Jersey, USA; Stevens Institute of Technology, New Jersey, USA; Stevens Institute of Technology, New Jersey, USA","2015 36th IEEE Sarnoff Symposium","","2015","","","167","172","Power consumption minimization in Wireless Sensor Networks (WSNs) has been discussed extensively in literature. Usually, a central node (a receiver) in WSNs consumes large amount of power due to the necessity to decode every received packet regardless of the fact that the transmission may suffer from packets collision. Current collision detection mechanisms in WSNs have largely been revolving around direct demodulation and full decoding of received packets. The obvious drawback of full decoding of a received packet is the need to expend a significant amount of energy and processing complexity in order to fully-decode a packet, only to discover the packet is illegible due to a collision. In this paper, we propose a suite of novel, yet simple and power-efficient technique to detect a collision at the receiver side of WSNs without the need for full-decoding of the received packet. Our novel approach aims at detecting collision through fast examination of the signal statistics of a short snippet of the received packet via a relatively small number of computations over a small number of received IQ samples. We also present a complexity and power-saving comparison between our novel approach and a conventional full-decoding approach (Log-MAP algorithm) to demonstrate the significant power and complexity saving advantage of our approach. In addition, we demonstrate how to tune various design parameters in order to allow a system designer multiple degrees of freedom for design trade-offs and optimization.","","978-1-4673-8042-3978-1-4673-8043","10.1109/SARNOF.2015.7324663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324663","Low Computational Complexity;Zero-Power Pilot Transmission;Collisions Detection;Power Efficient Protocol","Wireless sensor networks;Signal to noise ratio;Interference;Decoding;Receivers;Computational complexity","decoding;minimisation;power consumption;wireless sensor networks","low-complexity collision detection scheme;pseudo-coded ON-OFF pilot transmission;wireless sensor networks;WSN;power consumption minimization;full-decoding approach","","","","9","","","","","IEEE","IEEE Conferences"
"Complexity evaluation of non-binary Galois field LDPC code decoders","T. Lehnigk-Emden; N. Wehn","Microelectronic Systems Design Research Group, University of Kaiserslautern, Erwin-Schroedinger Str., 67663, Germany; Microelectronic Systems Design Research Group, University of Kaiserslautern, Erwin-Schroedinger Str., 67663, Germany","2010 6th International Symposium on Turbo Codes & Iterative Information Processing","","2010","","","53","57","Forward error correction is an essential part of digital communication systems. Non-binary low-density parity-check (NB-LDPC) codes have an excellent communications performance for short block lengths. The higher the field size is, the better the communications performance is. Non-binary LDPC codes can outperform all other state-of-the-art code classes. However, the algorithmic decoding complexity grows with the field size. For an intended, future use of these codes in real systems, an efficient hardware implementation is mandatory. Hardware architectures for binary LDPC codes are well investigated, but efficient implementation of non-binary LDPC decoders is still an open field. To the best of our knowledge, this paper studies for the first time the hardware implementation costs for non-binary LDPC decoders with Galois field sizes of 4, 16 and 256 on FPGA and a 65nm ASIC technology.","2165-4700;2165-4719","978-1-4244-6746-4978-1-4244-6744-0978-1-4244-6745","10.1109/ISTC.2010.5613874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613874","LDPC;non-binary;implementation;higher field;Galois field;decoder;complexity;hardware;FPGA;ASIC;65nm","Wideband;Complexity theory;Logic gates;Digital video broadcasting;Quantization;Parallel processing;Manganese","application specific integrated circuits;binary codes;block codes;digital communication;field programmable gate arrays;forward error correction;Galois fields;parity check codes","nonbinary Galois field LDPC code decoder;forward error correction;digital communication system;nonbinary low density parity check codes;short block lengths;state-of-the-art code class;algorithmic decoding complexity;hardware architectures;binary LDPC codes;ASIC technology","","15","","19","","","","","IEEE","IEEE Conferences"
"Performance and complexity analysis of channel coding schemes for multi-Gbps wireless communications","M. Marinkovic; M. Krstic; E. Grass; M. Piz","IHP, Im Technologiepark 25, D-15236, Frankfurt (Oder), Germany; IHP, Im Technologiepark 25, D-15236, Frankfurt (Oder), Germany; IHP, Im Technologiepark 25, D-15236, Frankfurt (Oder), Germany; IHP, Im Technologiepark 25, D-15236, Frankfurt (Oder), Germany","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)","","2012","","","1937","1943","In this paper, a trade-off analysis between concatenated codes consisting of a convolutional (CC) followed by a Reed-Solomon (RS) code versus low-density parity-check (LDPC) codes is presented. The analysis is based on a twofold criterion: coding gain for a target bit error rate (BER) of 10<sup>-6</sup> and required decoder hardware complexity for a target data throughput of 10 Gbps. Furthermore, we have investigated relevant parameters which directly impact an efficient hardware implementation as well as the error correction performance of the LDPC decoder. These parameters include an attenuation factor in the min-sum layered (MSL) decoding algorithm, the finite word length of soft information and an early-termination (ET) strategy. The error correction performances are evaluated for 16-QAM modulation over an independent Rayleigh fading channel. The complexity of the RS-CC and LDPC decoders is estimated based on synthesis results using an Infineon 40 nm CMOS design kit.","2166-9589;2166-9570;2166-9570","978-1-4673-2569-1978-1-4673-2566-0978-1-4673-2568","10.1109/PIMRC.2012.6362670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6362670","Reed-Solomon;LDPC;convolutional code;concatenated code;Rayleigh fading;BER performance;decoder complexity;data throughput","Decoding;Parity check codes;Bit error rate;Encoding;Complexity theory;Throughput;IEEE 802.16 Standards","channel coding;CMOS integrated circuits;concatenated codes;convolutional codes;decoding;error correction codes;error statistics;parity check codes;quadrature amplitude modulation;Rayleigh channels;Reed-Solomon codes","channel coding schemes;multiGbps wireless communications;concatenated codes;convolutional codes;Reed-Solomon code;RS codes;low-density parity-check codes;LDPC codes;coding gain;bit error rate;BER;decoder hardware complexity analysis;error correction performance;LDPC decoder;min-sum layered decoding algorithm;attenuation factor;MSL decoding algorithm;finite word length;soft information;early-termination strategy;ET strategy;independent Rayleigh fading channel;16-QAM modulation;RS-CC decoders;Infineon CMOS design kit;bit rate 10 Gbit/s;size 40 nm","","4","","18","","","","","IEEE","IEEE Conferences"
"A Practical Low-Complexity Coding Scheme for the Multiple Access Channel Inspired by the Compute-and-Forward Strategy","X. Insausti; A. Saez; P. M. Crespo","NA; NA; NA","2015 IEEE 81st Vehicular Technology Conference (VTC Spring)","","2015","","","1","6","This paper proposes a low-complexity coding strategy for the flat fading Gaussian Multiple Access Channel (MAC) based on the concept of Compute-and-Forward. To assess the performance of the proposed coding scheme, we compare the block error probability of a particular code implementation with that of an optimal Gaussian MAC code with the same rates, when applied to our slow fading Gaussian MAC. We show by simulation that even using very short codewords, our strategy beats standard Successive Interference Cancellation techniques by obtaining a good performance with low coding and decoding complexity.","1550-2252","978-1-4799-8088","10.1109/VTCSpring.2015.7145640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7145640","","Decoding;Mathematical model;Fading;Complexity theory;Encoding;Transmitters;Error probability","decoding;encoding;error statistics;fading channels;Gaussian channels;interference suppression;multi-access systems","low-complexity coding strategy;flat fading Gaussian multiple access channel;compute-and-forward;block error probability;optimal Gaussian MAC code;very short codewords;successive interference cancellation techniques;decoding complexity","","","","11","","","","","IEEE","IEEE Conferences"
"Low-Complexity Decoding of Repeat-Accumulate Codes over Quasi-Static Fading Channels","H. Yuan; P. Y. Kam","NA; NA","2015 IEEE Global Communications Conference (GLOBECOM)","","2015","","","1","6","We consider iterative decoding of repeat- accumulate (RA) codes over frequency-flat, quasi- static fading channels. A soft-input, soft-output decoder is proposed for the inner convolutional decoding, which fuses the decoding approach of the soft-output Viterbi algorithm and the estimation approach of the maximum-likelihood sequence detector. The decoder deploys trellis search algorithm based on the generalized likelihood ratio test, whereby the channel state information is acquired implicitly using both the pilot and data signals during the decoding process. Through simulations, we show that the RA decoding with the proposed decoder has much better error performance than standard RA decoding with pilot-symbol- assisted channel estimation, while having approximately the same computational complexity. Compared with the conventional scheme of iterative channel estimation and decoding, the proposed decoder has much simpler structure and requires significantly less computational power, although it incurs some loss in error performance.","","978-1-4799-5952-5978-1-4799-5951","10.1109/GLOCOM.2015.7417667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417667","","Iterative decoding;Channel estimation;Maximum likelihood decoding;Measurement;Convolutional codes;Reliability","channel estimation;computational complexity;convolutional codes;fading channels;iterative decoding;maximum likelihood detection;maximum likelihood estimation;search problems;statistical testing;Viterbi decoding","low-complexity decoding;repeat-accumulate codes;quasi-static fading channels;frequency-flat fading channels;iterative decoding;soft-input soft-output decoder;inner convolutional decoding;soft-output Viterbi algorithm;estimation approach;maximum-likelihood sequence detector;trellis search algorithm;generalized likelihood ratio test;channel state information acquisition;pilot signals;data signals;pilot-symbol-assisted channel estimation;computational complexity","","","","27","","","","","IEEE","IEEE Conferences"
"Best of Both Worlds: Prioritizing Network Coding without Increased Space Complexity","R. Naumann; S. Dietzel; B. Scheuermann","NA; NA; NA","2016 IEEE 41st Conference on Local Computer Networks (LCN)","","2016","","","723","731","Random linear network coding simplifies routing decisions, improves throughput, and increases tolerance against packet loss. A substantial limitation, however, is delay: decoding requires as many independent linear combinations as data blocks. Hierarchical network coding purportedly solves this delay problem. It introduces layers to decode prioritized data blocks early, which may benefit video streaming applications or applications for sensor information collection. While hierarchical network coding reduces decoding delays, it introduces significant space complexity and additional decoding time. We propose a decoding algorithm that manages all prioritization layers in a joint decoder matrix. Analytical evaluation and performance measurements show that we maintain prioritization benefits without increased space complexity and improve decoding performance. With memory requirements independent of the number of layers, our algorithm facilitates more fine-grained prioritization layers to further the benefits of hierarchical network coding.","","978-1-5090-2054-6978-1-5090-2055","10.1109/LCN.2016.123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796872","network coding;random linear network coding;hierarchical network coding;prioritization;decoding","Decoding;Network coding;Encoding;Delays;Streaming media;Complexity theory;Matrices","computational complexity;decoding;matrix algebra;network coding","fine-grained prioritization;joint decoder matrix;prioritization layers;space complexity;decoding delays;hierarchical network coding;data blocks;independent linear combinations;random linear network coding","","2","","19","","","","","IEEE","IEEE Conferences"
"A Low Complexity Encoding Algorithm for Systematic Polar Codes","G. T. Chen; Z. Zhang; C. Zhong; L. Zhang","Fuqing Branch of Fujian Normal University, Fuqing, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Communications Letters","","2016","20","7","1277","1280","Arıkan has shown that systematic polar codes (SPC) outperform non-SPC (NSPC). However, the performance gain comes at the price of elevated encoding complexity, i.e., compared with NSPC, the available encoding methods for SPC require higher memory and computation. In this letter, we propose an efficient encoding algorithm requiring only N bits of memory and having (N/2)log2N XOR operations. Moreover, the auxiliary variables in the algorithm can share the memory to reduce extra memory requirement. Furthermore, a parallel 2-bit encoding algorithm is also presented to improve the encoding throughput. Remarkably, we show that parallel encoding can be implemented with the same number of XOR operations and memory bits. Finally, the proposed encoding algorithm can be directly used for NSPC with the same complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2560169","National Key Basic Research Program of China; National Natural Science Foundation of China; Huawei HIRP Flagship Projects; Zhejiang Provincial Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Research Funds of the Education Department of Fujian Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462203","Polar codes;systematic polar codes;encoding algorithm;parallel encoding","Encoding;Memory management;Systematics;Complexity theory;Indexes;Decoding;Electronic mail","block codes;communication complexity;error correction codes;linear codes;parallel algorithms","low complexity encoding algorithm;systematic polar codes;SPC;performance gain;memory sharing;memory requirement;parallel 2-bit encoding algorithm;encoding throughput improvement;XOR operations","","2","","13","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Encoding of Quasi-Cyclic Codes Based on Galois Fourier Transform","Q. Huang; L. Tang; S. He; Z. Xiong; Z. Wang","NA; NA; NA; NA; NA","IEEE Transactions on Communications","","2014","62","6","1757","1767","This paper presents two novel low-complexity encoding algorithms for quasi-cyclic (QC) codes based on Galois Fourier transform. The key idea behind them is making use of the block diagonal structure of the transformed generator matrix. The first one, named encoding by Galois Fourier transform, is equivalent to the fast implementations of the traditional encoding by Galois Fourier transform. The second one, named encoding in the transform domain (ETD), requires much less computational complexity for encoding binary QC codes. It skips the first step of the first algorithm and applies post-processing to save a large number of Galois field multiplications. Its application to QC-LDPC codes is also studied in this paper. Particularly, the hardware cost of the ETD for RS-based LDPC codes can be greatly reduced by short linear-feedback shift registers.","0090-6778;1558-0857","","10.1109/TCOMM.2014.2316174","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784391","Encoding complexity;Galois Fourier transform (GFT);low-density parity-check (LDPC) codes;matrix transformation;quasi-cyclic (QC) codes;redundant rows","Encoding;Vectors;Generators;Computational complexity;Fourier transforms;Parity check codes","binary codes;block codes;computational complexity;cyclic codes;Fourier transforms;Galois fields;matrix algebra;parity check codes;transform coding","quasicyclic codes;Galois Fourier transform;low-complexity encoding algorithms;block diagonal structure;transformed generator matrix;encoding in the transform domain;ETD;computational complexity;encoding binary QC codes;Galois field multiplications;hardware cost;RS-based LDPC codes;linear-feedback shift registers","","26","","29","","","","","IEEE","IEEE Journals & Magazines"
"Complexity Reduction and Performance Improvement for Geometry Partitioning in Video Coding","Q. Wang; X. Ji; M. Sun; G. J. Sullivan; J. Li; Q. Dai","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China; Department of Electrical Engineering, University of Washington, Seattle, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","","2013","23","2","338","352","Geometry partitioning for video coding involves establishing a partition line boundary within each block-shaped region and applying motion-compensated prediction to the two sub-regions created by the partition line. This paper presents techniques for enhancing the effectiveness and reducing the complexity of geometry partitioning schemes. A texture-difference-based approach is described to simplify the process of selecting the partition lines. Applying this approach together with a described skipping strategy for blocks with uniform texture can achieve a 94% reduction of encoding time while retaining a similar rate-distortion (R-D) performance to the full-search partitioning approach, when implemented for wedge-based geometry partitioning (WGP) in the context of H.264/MPEG-4 AVC JM 16.2. A bit rate improvement of approximately 6% is shown relative to not using geometry partitioning. For further R-D improvement, we describe a background-compensated prediction scheme to reduce the number of overhead bits used for motion vectors. Additionally, for systems in which high-quality depth maps are available, we incorporate depth map usage into the described approaches to generate a more accurate partitioning. Using these approaches with object-boundary-based geometry partitioning can achieve about 9% bit rate savings relative to using WGP, while keeping a similar computational complexity to the described complexity-reduced WGP.","1051-8215;1558-2205","","10.1109/TCSVT.2012.2203743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213532","Computational complexity;depth maps;geometry partitioning;texture difference","Geometry;Vectors;Training;Encoding;Approximation methods;Complexity theory;Image edge detection","computational complexity;geometry;image texture;motion compensation;video coding","complexity reduction;performance improvement;video coding;block-shaped region;motion-compensated prediction;partition line boundary;texture-difference-based approach;skipping strategy;rate-distortion performance;R-D performance;wedge-based geometry partitioning;WGP;H.264-MPEG-4 AVC JM 16.2;background-compensated prediction scheme;motion vectors;high-quality depth maps;object-boundary-based geometry partitioning;computational complexity","","4","","27","","","","","IEEE","IEEE Journals & Magazines"
"Optimizing the transform complexity-quality tradeoff for hardware-accelerated HEVC video coding","M. Masera; L. R. Fiorentin; M. Martina; G. Masera; E. Masala","Electronics and Telecommunication Engineering Department, Politecnico di Torino, Torino, Italy 10129; Electronics and Telecommunication Engineering Department, Politecnico di Torino, Torino, Italy 10129; Electronics and Telecommunication Engineering Department, Politecnico di Torino, Torino, Italy 10129; Electronics and Telecommunication Engineering Department, Politecnico di Torino, Torino, Italy 10129; Control and Computer Engineering Department, Politecnico di Torino, Torino, Italy 10129","2015 Conference on Design and Architectures for Signal and Image Processing (DASIP)","","2015","","","1","6","When designing hardware-accelerated video encoding systems, it is fundamental to determine the maximum throughput needed by each subsystem so that the design can optimize the cost-performance tradeoff. One of the key modules in video coding is the 2D transform operation which is typically subject to heavy optimization efforts. This work investigates the tradeoff between the computational power spent in performing the transform operations for HEVC compression and the corresponding video quality as a function of a number of coding configuration parameters. Results provides a practical method to determine the throughput needed by the transform coding subsystem as well as the optimal configuration of the considered coding parameters for each desired complexity-quality tradeoff, showing that with small quality reduction large computational power savings are possible.","","978-1-4673-7738-6978-1-4673-7737","10.1109/DASIP.2015.7367269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367269","HEVC;Video coding;DCT complexity","Encoding;Transforms;Bit rate;Video sequences;Video coding;Complexity theory;Standards","data compression;discrete cosine transforms;optimisation;transform coding;video coding","transform complexity-quality tradeoff;hardware-accelerated HEVC video coding;hardware-accelerated video encoding systems;maximum throughput;cost-performance tradeoff;2D transform operation;HEVC compression;video quality;coding configuration parameters;transform coding subsystem;optimal configuration;computational power savings;DCT;discrete cosine transform","","2","","12","","","","","IEEE","IEEE Conferences"
"Low decoding complexity STBC design for turbo coded broadcast transmission","A. El Falou; C. A. Nour; C. Langlais; C. Douillard","Department of Electronics, Institut TELECOM-TELECOM Bretagne, Brest, France; Department of Electronics, Institut TELECOM-TELECOM Bretagne, Brest, France; Department of Electronics, Institut TELECOM-TELECOM Bretagne, Brest, France; Department of Electronics, Institut TELECOM-TELECOM Bretagne, Brest, France","2011 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","","2011","","","1","5","In this paper, we study the Space Time Block Code (STBC) called Matrix D (MD) that enjoys low-complexity detection for broadcast transmission. Such a transmission uses a powerful channel code like a turbo code. To produce Log-Likelihood Ratio (LLR) for the channel decoder, we propose a soft Maximum Likelihood (ML) detector for MD STBC while keeping the same low-complexity order as the original one. Then, we optimize the MD STBC for low Signal to Noise Ratio (SNR) under the trace criterion. The performance of the concatenation of a turbo code with optimized MD is given. A gain of almost 0.2 dB is obtained between optimized MD and original MD. However, the proposed code loses its full-diversity as its determinant is equal to 0. Then, a tradeoff angle is chosen and a low-complexity detection full-rate full-diversity STBC is designed for turbo coded broadcast transmission which performs well for both low and high SNR.","2155-5052;2155-5044;2155-5044","978-1-61284-122-9978-1-61284-121-2978-1-61284-120","10.1109/BMSB.2011.5954883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954883","","Detectors;MIMO;Signal to noise ratio;Bit error rate;Complexity theory;Modulation;Gain","broadcast channels;channel coding;matrix algebra;maximum likelihood detection;MIMO communication;space-time block codes;turbo codes","low decoding complexity;STBC design;turbo coded broadcast transmission;space time block code;matrix D;low-complexity detection;log-likelihood ratio;channel decoder;maximum likelihood detector;signal to noise ratio","","3","","8","","","","","IEEE","IEEE Conferences"
"A Low-Complexity Solution to Decode Diversity-Oriented Block Codes in MIMO Systems with Inter-Symbol Interference","C. Xu; H. Gharavi","National Institute of Standards and Technology, Gaithersburg, USA; National Institute of Standards and Technology, Gaithersburg, USA","IEEE Transactions on Wireless Communications","","2012","11","10","3574","3587","In this paper we first propose a block-code based general model to combat the Inter-Symbol Interference (ISI) caused by frequency selective channels in a Multi-Input Multi-Output (MIMO) system and/or by asynchronous cooperative transmissions. The general model is not only exemplified by the Time-Reversed Space-Time Block Code (TR-STBC) scheme, but also by the Asynchronous Cooperative Liner Dispersion Codes (ACLDC) scheme. In these schemes a guard interval has to be inserted between adjacent transmission blocks to mitigate the effect of ISI. Consequently, this could degrade the effective symbol rate for a small block size. A larger block size would enhance the effective symbol rate and also substantially increase the decoding complexity. In the general model proposed in this paper, we further present a novel low-complexity breadth-adjustable tree-search algorithm and compare it with Sphere-Decoding (SD) based algorithms. With simulation results we will illustrate that our algorithm is able to achieve the optimal performance in terms of Bit Error Rate (BER) with a complexity much lower than the SD-based algorithms, whether the ACLDC or TR-STBC scheme is employed. Through simulations we further demonstrate that when the block size of the ACLDC is equivalent to 20, the complexity of the proposed algorithm is only a fraction of 10<sup>-8</sup> that of the Maximum Likelihood (ML) algorithm. This would allow us to practically enhance the effective symbol rate without any performance degradation.","1536-1276;1558-2248","","10.1109/TWC.2012.081612.111819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6287531","Multi Input Multi Output (MIMO);Inter-symbol Interference (ISI);Time-Reversed Space-Time Block Code (TR-STBC);Linear Dispersion Code (LDC);asynchronous cooperative transmission;tree-search detection algorithm;sphere decoding algorithm;Viterbi Algorithm (VA);Maximum Likelihood (ML) algorithm","Vectors;Complexity theory;Decoding;Transmitting antennas;MIMO;Linear programming;Block codes","cooperative communication;error statistics;intersymbol interference;maximum likelihood decoding;MIMO communication;space-time block codes","low-complexity solution;diversity-oriented block codes;MIMO systems;inter-symbol interference;ISI;frequency selective channels;multi-input multi-output system;asynchronous cooperative transmissions;time-reversed space-time block code;TR-STBC scheme;asynchronous cooperative liner dispersion codes;ACLDC scheme;low-complexity breadth-adjustable tree-search algorithm;sphere-decoding based algorithms;bit error rate;BER;maximum likelihood algorithm","","5","","18","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity independent multi-view video coding","H. S. Hussein; M. El-Khamy; F. Mehdipour; M. El-Sharkawy","Electronics and Communications Engineering Department, Egypt-Japan University of Science and Technology (E-JUST), New Borg El-Arab 21934, Alexandria, Egypt; Electrical Engineering Department, Alexandria University, 21544, Egypt; E-JUST Center, Kyushu University, Fukuoka, Japan; Electronics and Communications Engineering Department, Egypt-Japan University of Science and Technology (E-JUST), New Borg El-Arab 21934, Alexandria, Egypt","2013 IEEE 10th Consumer Communications and Networking Conference (CCNC)","","2013","","","37","42","In 3D multi-view video coding (MVC), disparity estimation (DE) are used to exploit the correlation among different view sequences. The DE process greatly increases the computational complexity of the MVC. In this paper, a novel independent low complexity multi-view video coder (I-MVC) is introduced. In the proposed MVC, the coding complexity is shifted from the encoder side to the decoder side. Instead of disparity estimation, the proposed I-MVC deploys independent component analysis (ICA) on the video streams to remove the correlation between the view sequences. The correlated (dependent) video sequences are decomposed into uncorrelated (independent) sequences and a mixing matrix. Each independent sequence is independently encoded by the H.264/AVC video coder. Then the mixing matrix is used at decoder to jointly decode the received independent sequences. Our experimental results show that the proposed I-MVC has better coding efficiency than conventional 3D multi-view video coder. The I-MVC gives more than 21% savings in overall bit rate and reduces the MVC computational complexity by 49% with less than 0.2 dB loss in the video peak signal to noise ratio.","2331-9852;2331-9860","978-1-4673-3133-3978-1-4673-3131-9978-1-4673-3132","10.1109/CCNC.2013.6488422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488422","3D video;Independent component analysis (ICA);joint multi-view video model (JMVM);multi-view video coding (MVC)","Streaming media;Decoding;Three-dimensional displays;Vectors;Correlation;Computational complexity;Algorithm design and analysis","computational complexity;correlation methods;error statistics;estimation theory;image sequences;independent component analysis;video coding;video streaming","low complexity independent multiview video coding;3D multiview video coding;disparity estimation;view sequences;DE process;I-MVC;coding complexity;encoder side;decoder side;independent component analysis;ICA;video streams;correlated video sequences;dependent video sequences;uncorrelated sequences;independent sequences;mixing matrix;H.264/AVC video coder;3D multiview video coder;overall bit rate;MVC computational complexity;video peak signal to noise ratio","","","","17","","","","","IEEE","IEEE Conferences"
"Performance-complexity tradeoff of convolutional codes for broadband fixed wireless access systems","I. Chatzigeorgiou; A. Demosthenous; M. R. D. Rodrigues; I. J. Wassell","Digital Technology Group, Computer Laboratory, University of Cambridge; Department of Electronics and Electrical Engineering, University College London; Instituto de Telecomunicacoes and Department of Computer Science, University of Porto; Digital Technology Group, Computer Laboratory, University of Cambridge","IET Communications","","2010","4","4","419","427","In this study, the authors investigate the performance-complexity tradeoff of convolutional codes for broadband fixed wireless access systems by considering the effects of quantisation and path metric memory in practical Viterbi decoding implementations. They show that in systems with limited antenna diversity, low-memory codes achieve a better error-rate performance compared to that of high-memory codes. Only in systems with considerable antenna diversity, can the performance of a convolutional code be improved by increasing its memory size. Nevertheless, the authors demonstrate that the coding advantage offered by the high-memory codes is not large enough to justify the significant increase in implementation complexity. In particular, memory-2 convolutional codes achieve a coding gain of up to 1.2 dB over their memory-8 counterparts in single-input single-output fixed wireless access systems. The situation is reversed when multiple antennas are used, but the decoder of memory-8 codes occupies at least 130 times more silicon area than that of memory-2 codes.","1751-8628;1751-8636","","10.1049/iet-com.2008.0596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5419613","","","broadband antennas;communication complexity;convolutional codes;diversity reception;radio access networks;Viterbi decoding","performance-complexity tradeoff;convolutional codes;broadband fixed wireless access systems;path metric memory;Viterbi decoding;limited antenna diversity;low-memory codes;error-rate performance;high-memory codes;gain 1.2 dB","","","","","","","","","IET","IET Journals & Magazines"
"Context adaptive thresholding and entropy coding for very low complexity JPEG transcoding","X. Xu; Z. Akhtar; R. Govindan; W. Lloyd; A. Ortega","Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Computer Science, University of Southern California; Department of Electrical Engineering, University of Southern California","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2016","","","1392","1396","The ever increasing quantity of user generated photos, nearly all compressed using JPEG, has created a growing storage burden on photo storage and sharing services. This creates the need for compression techniques that take JPEG compressed images as inputs. In this paper we propose two novel very low complexity codecs, ROMP and L-ROMP to recompress JPEG photos, achieving increased coding efficiency by making use of very large entropy coding tables. ROMP is a lossless JPEG recompression codec that achieves 15% average gains over JPEG, while L-ROMP is a lossy codec that can achieve 29% average compression gains over JPEG, by applying coefficient thresholding based on a perceptual criterion to a JPEG image before using the entropy coding of ROMP.","2379-190X","978-1-4799-9988-0978-1-4799-9987","10.1109/ICASSP.2016.7471905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471905","JPEG;image compression","Transform coding;Context;Image coding;Complexity theory;Codecs;Entropy;Encoding","data compression;image coding;image segmentation","JPEG image;JPEG photos;complexity codecs;JPEG compressed images;compression techniques;photo storage;sharing services;very low complexity JPEG transcoding;entropy coding;context adaptive thresholding","","","","25","","","","","IEEE","IEEE Conferences"
"A low-complexity tree-search algorithm to decode diversity-oriented block codes with inter-symbol interference","C. Xu; H. Gharavi","National Institute of Standards and Technology, Gaithersburg, USA; National Institute of Standards and Technology, Gaithersburg, USA","2012 IEEE International Conference on Communications (ICC)","","2012","","","4240","4245","In order to contain a differential propagation delay in a block based cooperative Multiple-Input-Multiple-Output (MIMO) system, a guard interval can be inserted to mitigate the effect of inter-symbol interference. A larger block size could substantially increase the effective symbol rate, although at the expense of decoding complexity. In this paper, we propose a novel low-complexity breadth-adjustable tree-search algorithm, which, as an example, has been applied to decode Asynchronous Cooperative Liner Dispersion Codes (ACLDC). With simulation we demonstrate that when the blocksize is equivalent to 20, the complexity of the proposed algorithm is only a fraction of 10<sup>-8</sup> of that of the Maximum Likelihood (ML) algorithm. This would allow us to practically enhance the effective symbol rate without any performance degradation.","1938-1883;1550-3607;1550-3607","978-1-4577-2053-6978-1-4577-2052-9978-1-4577-2051","10.1109/ICC.2012.6363870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363870","","Wireless communication;Government;Signal to noise ratio","block codes;cooperative communication;delays;interference suppression;intersymbol interference;maximum likelihood decoding;MIMO communication;tree searching","diversity-oriented block codes;intersymbol interference mitigation;differential propagation delay;block based cooperative multiple-input-multiple-output system;MIMO system;guard interval;effective symbol rate;decoding complexity;novel low-complexity breadth-adjustable tree-search algorithm;asynchronous cooperative liner dispersion codes;ACLDC;maximum likelihood decoding algorithm;ML algorithm","","","","14","","","","","IEEE","IEEE Conferences"
"Complexity-reduced geometry partition search and high efficiency prediction for video coding","Q. Wang; M. Sun; G. J. Sullivan; J. Li","TNList and Dept. of Automation, Tsinghua University, Beijing, China; Dept. of Electrical Engineering, University of Washington, Seattle, US; Microsoft Corporation, One Microsoft Way, Redmond, WA, US; Microsoft Corporation, One Microsoft Way, Redmond, WA, US","2012 IEEE International Symposium on Circuits and Systems","","2012","","","133","136","To reduce the complexity of searching for wedge-based geometry partitions in video coding, we propose a texture-difference based partition line selection approach with a skipping strategy. Applying this approach can reduce the encoding time by 90% while retaining the similar rate-distortion performance to that of exhaustive searching. We also propose a background-compensated prediction scheme to improve the rate-distortion performance of the geometry partitioning prediction by reducing its motion vector overhead. Incorporating our proposed approach into object-boundary-based geometry partitioning can achieve about 10% bit-rate savings relative to the full-search approach while keeping the complexity at about the same level as our proposed complexity-reduced wedge-based geometry partitioning.","2158-1525;0271-4302;0271-4302","978-1-4673-0219-7978-1-4673-0218-0978-1-4673-0217","10.1109/ISCAS.2012.6271486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6271486","","Geometry;Video coding;Encoding;PSNR;Vectors;Gain;Computational complexity","","","","2","","11","","","","","IEEE","IEEE Conferences"
"Low-complexity content-adaptive Lagrange multiplier decision for SSIM-based RD-optimized video coding","P. Zhao; Y. Liu; J. Liu; R. Yao; S. Ci; H. Tang","Institute of Acoustics, Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China; Zhejiang Wanli University, Ningbo, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China; Institute of Acoustics, Chinese Academy of Sciences, Beijing, China","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","485","488","The SSIM-based rate distortion optimization (R-DO) has been proved to be an effective way to promote the perceptual video coding performance, and the Lagrange multiplier decision is the key to the SSIM-based RD-optimized video coding. Through extensively analyzing the characteristics of SSIM-based and SSE-based video distortions, this paper presents a low-complexity content-adaptive Lagrange multiplier decision method. The proposed method first estimates frame-level SSIM-based Lagrange multiplier by scaling the traditional SSE-based Lagrange multiplier with the ratio of SSE-based distortion to SSIM-based distortion. Via predicting the macroblock's perceptual importance in the whole frame, the macroblock-level Lagrange multiplier is further refined to promote the accuracy of the Lagrange multiplier decision. Experimental results show that the proposed method can obtain almost the same rate-SSIM performance and subjective quality as the state-of-the-art SSIM-based RD-optimized video coding methods with lower computation overheads.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6571886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571886","","Encoding;Video coding;Decision support systems;Rate-distortion;Optimization;Distortion measurement;Indexes","distortion;video coding","low-complexity content-adaptive lagrange multiplier decision;SSIM-based rate distortion optimization;perceptual video coding performance;Lagrange multiplier decision;macroblock-level Lagrange multiplier;SSIM-based RD-optimized video coding methods","","1","","8","","","","","IEEE","IEEE Conferences"
"An efficient algorithm for irregular Low density Parity Check code with reduced computational complexity and error floor","D. P. Rathod; R. N. Awale","Dept. of Electronics Engineering, VJIT, Mumbai, India-400019, India; Dept. of Electronics Engineering, VJIT, Mumbai, India-400019, India","2012 International Conference on Communication, Information & Computing Technology (ICCICT)","","2012","","","1","4","Low-density Parity check codes (LDPC) are gaining interest for high data rate application in both terrestrial and spatial communications which requires low bit error rate. This paper proposes an algorithm through which the best parity check matrix is obtained for irregular low density parity check codes, which reduces the Bit Error Rate ,Frame Error Rate and computational complexity. Tanner graph also called a bipartite graph format in an Low Density Parity Check (LDPC) code design contains many short cycles; it will produce a computational complexity in a code design process and it degrades the code performance. To overcome this effect, we proposed an new algorithm which obtains a best final parity check matrix. Throughout the paper we will give the detailed description of an efficient algorithm proposed. The complexity is studied graphically and analytically. Using proposed algorithm, cycles of length four observed in corresponding matrix are removed. Each matrix is evaluated over a noisy Additive White Gaussian Noise (AWGN) channel. Bit Error rate and Frame error rate is calculated. The results are compared with randomly generated best parity check matrix. Simulation results shows that proposed method successfully overcomes the computational complexity and meets the near Shannon limit.","","978-1-4577-2078-9978-1-4577-2077-2978-1-4577-2076","10.1109/ICCICT.2012.6398190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6398190","Irregular LDPC code;efficient algorithm;BPSk modulation;short cycles","Parity check codes;Bit error rate;Algorithm design and analysis;Floors;Encoding;Computational complexity","AWGN channels;computational complexity;error statistics;parity check codes","irregular low density parity check code;computational complexity;error floor;LDPC;high data rate application;spatial communications;terrestial communications;parity check matrix;bit error rate;frame error rate;code design process;additive white Gaussian noise;AWGN channel;Shannon limit","","1","","15","","","","","IEEE","IEEE Conferences"
"Low complexity opportunistic decoder for network coding","B. Yin; M. Wu; G. Wang; J. R. Cavallaro","ECE Department, Rice University, 6100 Main St., Houston, TX 77005, USA; ECE Department, Rice University, 6100 Main St., Houston, TX 77005, USA; ECE Department, Rice University, 6100 Main St., Houston, TX 77005, USA; ECE Department, Rice University, 6100 Main St., Houston, TX 77005, USA","2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)","","2012","","","1097","1101","In this paper, we propose a novel opportunistic decoding scheme for network coding decoder which significantly reduces the decoder complexity and increases the throughput. Network coding was proposed to improve the network throughput and reliability, especially for multicast transmissions. Although network coding increases the network performance, the complexity of the network coding decoder algorithm is still high, especially for higher dimensional finite fields or larger network codes. Different software and hardware approaches were proposed to accelerate the decoding algorithm, but the decoder remains to be the bottleneck for high speed data transmission. We propose a novel decoding scheme which exploits the structure of the network coding matrix to reduce the network decoder complexity and improve throughput. We also implemented the proposed scheme on Virtex 7 FPGA and compared our implementation to the widely used Gaussian elimination.","1058-6393;1058-6393;1058-6393","978-1-4673-5051-8978-1-4673-5050-1978-1-4673-5049","10.1109/ACSSC.2012.6489189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6489189","","","communication complexity;decoding;Gaussian processes;network coding;telecommunication network reliability","low complexity opportunistic decoder;network throughput;network reliability;multicast transmissions;high speed data transmission;network coding matrix;network decoder complexity;Virtex 7 FPGA;Gaussian elimination","","2","","12","","","","","IEEE","IEEE Conferences"
"Approximate iterative LP decoding of LDPC codes over GF(q) in linear complexity","D. Goldin; D. Burshtein","School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel; School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel","2010 IEEE 26-th Convention of Electrical and Electronics Engineers in Israel","","2010","","","000960","000964","The problem of low-complexity approximated linear programming (LP) decoding of LDPC codes over GF(q) is considered. An iterative algorithm with linear complexity was proposed by Burshtein for the binary case. However, this algorithm cannot be trivially generalized for the non-binary case, since the derivation uses the even parity property of the check nodes, which does not hold for codes over GF(q). In this work the algorithm is generalized to the non-binary case. We show, that by applying this algorithm to a softened version of the non-binary LP problem proposed by Flanagan, Skachek, Byrne and Grefeath, we can obtain, with complexity linear in the block length, a feasible solution vector for the non-binary LP decoding problem, which is arbitrarily close to optimal in the following sense. The distance between the minimum value achieved by the exact LP decoder and the objective function value of the approximate solution, normalized by the block length of the code, can be made arbitrarily small. We present simulation results and comparisons with the belief propagation (BP) algorithm using-regular ternary LDPC codes. The proposed algorithm can be easily extended to generalized LDPC codes.","","978-1-4244-8682-3978-1-4244-8681-6978-1-4244-8680","10.1109/EEEI.2010.5661933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661933","Low-density parity-check (LDPC) codes;belief propagation (BP);iterative decoding;linear programming (LP) decoding;non-binary codes","Schedules;Decoding;Complexity theory;Iterative decoding;Linear code;Eigenvalues and eigenfunctions","approximation theory;Galois fields;iterative decoding;linear programming;parity check codes;ternary codes","approximate iterative LP decoding;LDPC code;GF(q);linear complexity;low-complexity approximated linear programming decoding;nonbinary linear programming problem;exact linear programming decoder;block length;belief propagation algorithm;[3,6]-regular ternary low-density parity-check codes","","2","","6","","","","","IEEE","IEEE Conferences"
"Low-complexity PHY-layer network coding for two-way compute-and-forward relaying","K. N. Pappi; G. K. Karagiannidis; D. Toumpakaris","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, GR-54124 Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, GR-54124 Thessaloniki, Greece; Department of Electrical and Computer Engineering, University of Patras, 26500 Rio Achaias, Greece","2014 IEEE Wireless Communications and Networking Conference (WCNC)","","2014","","","358","363","We present a novel low-complexity technique that obtains the Physical-Layer Network Coding (PNC) equation coefficient vectors for the two-way relay channel when Compute-and-Forward is employed. The proposed method is based on pre-computed look-up tables that are used for all channel realizations. It is shown that the size of the look-up tables can be made small by taking into account the statistics of the channel coefficients as well as power and performance specifications. Moreover, a low-complexity algorithm is developed for efficient real-time selection of the equation coefficient vectors using the instantaneous channel coefficients and the look-up tables. Although the method may at times exclude some candidate vectors from the search space, simulation results indicate that the effect on the achievable computation rate at the relay is very small. Hence, significant complexity reduction is achieved, while the computation rate remains extremely close to the optimal value.","1525-3511;1558-2612","978-1-4799-3083","10.1109/WCNC.2014.6952034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6952034","two-way relay channel;compute-and-forward;PHY-layer network coding","Vectors;Equations;Relays;Table lookup;Mathematical model;Signal to noise ratio;Optimization","network coding;relay networks (telecommunication);statistics;table lookup","PHY-layer network coding;compute-and-forward relaying;physical-layer network coding equation coefficient vectors;PNC equation coefficient vectors;two-way relay channel;look-up tables;channel coefficients","","","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity interference cancellation receiver for sparse code multiple access","Jia Zou; Hui Zhao; Wenxiu Zhao","Key Laboratory of Universal Wireless Communication, Ministry of Education, Wireless Signal Processing & Network Lab, BUPT, Beijing, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Wireless Signal Processing & Network Lab, BUPT, Beijing, China; Key Laboratory of Universal Wireless Communication, Ministry of Education, Wireless Signal Processing & Network Lab, BUPT, Beijing, China","2015 IEEE 6th International Symposium on Microwave, Antenna, Propagation, and EMC Technologies (MAPE)","","2015","","","277","282","Sparse code multiple access (SCMA) is a proposed candidate for multiple access method in 5G telecommunication. In SCMA system, the classic receiver is message passing algorithm (MPA). MPA can reach a near optimal performance, but the computation process is complicated and time-consuming, so MPA is suitable for advanced terminal. But for simple terminal such as sensor node, large mount of calculations are difficult to bear. In this paper, we propose two interference cancellation (IC) related receivers to reduce complexity, including serial interference cancellation (SIC) and jointed serial interference cancellation with MPA algorithm (Jointed SIC-MPA). It's important to emphasize that these two IC related algorithms are designed jointed with multi-dimensional modulation constellation design. We propose a new method to generate multi-dimensional constellation. Both of them have lower complexity compared with classic MPA receiver.","","978-1-4673-7441-5978-1-4673-7439-2978-1-4673-7442","10.1109/MAPE.2015.7510315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7510315","SCMA;MPA;SIC;jointed SIC-MPA;non-orthogonal multiple access","Receivers;Performance evaluation;Image resolution;TV;Complexity theory","5G mobile communication;code division multiple access;interference suppression;message passing;radio receivers","low-complexity interference cancellation receiver;SCMA system;sparse code multiple access system;5G telecommunication;message passing algorithm;complicated computation process;time-consuming computation process;near optimal performance;complexity reduction;serial interference cancellation;jointed SIC-MPA;multidimensional modulation constellation design","","","","16","","","","","IEEE","IEEE Conferences"
"Low-Complexity Power Allocation With Network-Coded Cooperation for Single-Carrier Transmission","S. Lee; T. Kim; G. Im","NA; NA; NA","IEEE Communications Letters","","2015","19","11","2021","2024","We consider a power allocation algorithm in a network-coded system with iterative detection for single-carrier transmission. To achieve high network coding gain, the power fraction of each user should be allocated according to channel state information. To facilitate the power allocation, we first develop a density evolution technique for the iterative detection scheme. We then investigate the distribution of optimal power fraction by an exhaustive search. Based on the distribution, we propose a quantization-based power allocation algorithm with a low complexity. In simulations, the proposed algorithm significantly improves the network coding gain.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2015.2472408","Basic Science Research Program; National Research Foundation of Korea; Ministry of Science, ICT, and Future Planning; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7222387","SC-FDMA;iterative multiuser detection;network coding;power allocation;SC-FDMA;iterative multiuser detection;network coding;power allocation","Bit error rate;Resource management;Network coding;Relays;Frequency-domain analysis;Approximation algorithms;Iterative decoding","cooperative communication;iterative decoding;network coding","low-complexity power allocation;network-coded cooperation;single-carrier transmission;iterative detection;high network coding gain;channel state information;density evolution technique;optimal power fraction","","","","9","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Reliability-Based Message-Passing Decoder Architectures for Non-Binary LDPC Codes","X. Zhang; F. Cai; S. Lin","Case Western Reserve University; Case Western Reserve University; University of California, Davis","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2012","20","11","1938","1950","Non-binary low-density parity-check (NB-LDPC) codes can achieve better error-correcting performance than their binary counterparts at the cost of higher decoding complexity when the codeword length is moderate. The recently developed iterative reliability-based majority-logic NB-LDPC decoding has better performance-complexity tradeoffs than previous algorithms. This paper first proposes enhancement schemes to the iterative hard reliability-based majority-logic decoding (IHRB-MLGD). Compared to the IHRB algorithm, our enhanced (E-)IHRB algorithm can achieve significant coding gain with small hardware overhead. Then low-complexity partial-parallel NB-LDPC decoder architectures are developed based on these two algorithms. Many existing NB-LDPC code construction methods lead to quasi-cyclic or cyclic codes. Both types of codes are considered in our design. Moreover, novel schemes are developed to keep a small proportion of messages in order to reduce the memory requirement without causing noticeable performance loss. In addition, a shift-message structure is proposed by using memories concatenated with variable node units to enable efficient partial-parallel decoding for cyclic NB-LDPC codes. Compared to previous designs based on the Min-max decoding algorithm, our proposed decoders have at least tens of times lower complexity with moderate coding gain loss.","1063-8210;1557-9999","","10.1109/TVLSI.2011.2164951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018324","Iterative majority-logic decoding;low-density parity-check (LDPC) codes;non-binary;partial-parallel;VLSI","Decoding;Reliability;Algorithm design and analysis;Iterative decoding;Encoding;Complexity theory;Parity check codes","codecs;cyclic codes;iterative decoding;message passing;minimax techniques;parity check codes;reliability","low-complexity reliability-based message-passing decoder architectures;nonbinary LDPC codes;iterative reliability-based majority-logic NB-LDPC decoding;nonbinary low-density parity-check codes;error-correcting performance;decoding complexity;codeword length;performance-complexity tradeoffs;iterative reliability-based majority-logic decoding;IHRB-MLGD;IHRB algorithm;enhanced IHRB algorithm;E-IHRB algorithm;low-complexity partial-parallel NB-LDPC decoder;quasi-cyclic codes;NB-LDPC code construction methods;shift-message structure;partial-parallel decoding;cyclic NB-LDPC codes;min-max decoding algorithm;decoders;coding gain loss","","34","","16","","","","","IEEE","IEEE Journals & Magazines"
"Skip Decision and Reference Frame Selection for Low-Complexity H.264/AVC Surveillance Video Coding","P. Gorur; B. Amrutur","Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India","IEEE Transactions on Circuits and Systems for Video Technology","","2014","24","7","1156","1169","H.264/advanced video coding surveillance video encoders use the Skip mode specified by the standard to reduce bandwidth. They also use multiple frames as reference for motion-compensated prediction. In this paper, we propose two techniques to reduce the bandwidth and computational cost of static camera surveillance video encoders without affecting detection and recognition performance. A spatial sampler is proposed to sample pixels that are segmented using a Gaussian mixture model. Modified weight updates are derived for the parameters of the mixture model to reduce floating point computations. A storage pattern of the parameters in memory is also modified to improve cache performance. Skip selection is performed using the segmentation results of the sampled pixels. The second contribution is a low computational cost algorithm to choose the reference frames. The proposed reference frame selection algorithm reduces the cost of coding uncovered background regions. We also study the number of reference frames required to achieve good coding efficiency. Distortion over foreground pixels is measured to quantify the performance of the proposed techniques. Experimental results show bit rate savings of up to 94.5% over methods proposed in literature on video surveillance data sets. The proposed techniques also provide up to 74.5% reduction in compression complexity without increasing the distortion over the foreground regions in the video sequence.","1051-8215;1558-2205","","10.1109/TCSVT.2014.2319611","TCS Research Fellowship Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805578","Cache optimization;H.264/advanced video coding (AVC);motion detection;reference frame selection;skip decision;video surveillance","Video coding;Surveillance;Streaming media;Encoding;Motion detection;Cameras;Motion segmentation","cameras;data compression;distortion;Gaussian processes;motion compensation;video codecs;video coding;video surveillance","skip decision;reference frame selection;low-complexity H.264/AVC surveillance video coding;H.264/advanced video coding surveillance video encoders;multiple frames;motion-compensated prediction;static camera surveillance video encoders;recognition performance;detection performance;Gaussian mixture model;mixture model;floating point computations;reference frame selection algorithm;coding uncovered background regions;foreground pixels;bit rate savings;video surveillance data sets;compression complexity;distortion;video sequence","","10","","36","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity, near-lossless coding of depth maps from kinect-like depth cameras","S. Mehrotra; Z. Zhang; Q. Cai; C. Zhang; P. A. Chou","Microsoft Research Redmond, WA, USA; Microsoft Research Redmond, WA, USA; Microsoft Research Redmond, WA, USA; Microsoft Research Redmond, WA, USA; Microsoft Research Redmond, WA, USA","2011 IEEE 13th International Workshop on Multimedia Signal Processing","","2011","","","1","6","Depth cameras are gaining interest rapidly in the market as depth plus RGB is being used for a variety of applications ranging from foreground/background segmentation, face tracking, activity detection, and free viewpoint video rendering. In this paper, we present a low-complexity, near-lossless codec for coding depth maps. This coding requires no buffering of video frames, is table-less, can encode or decode a frame in close to 5ms with little code optimization, and provides between 7:1 to 16:1 compression ratio for near-lossless coding of 16-bit depth maps generated by the Kinect camera.","","978-1-4577-1434-4978-1-4577-1432-0978-1-4577-1433","10.1109/MMSP.2011.6093803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093803","","Sensors;Cameras;Accuracy;Quantization;PSNR;Entropy coding","cameras;codecs;data compression;image segmentation;image sensors;video coding","low-complexity coding;near-lossless coding;depth maps;kinect-like depth cameras;RGB;foreground-background segmentation;face tracking;activity detection;free viewpoint video rendering;near-lossless codec;video frames;code optimization;word length 16 bit","","12","","10","","","","","IEEE","IEEE Conferences"
"Adaptive Steganography Based on Syndrome-Trellis Codes and Local Complexity","G. Liu; W. Liu; Y. Dai; S. Lian","NA; NA; NA; NA","2012 Fourth International Conference on Multimedia Information Networking and Security","","2012","","","323","327","Syndrome-trellis codes provide a flexible framework for adaptive steganography, but how to construct the proper distortion weight function is still a disturbing question. In this paper, we propose to use an exponential function based on the local complexity to define the risk caused by the data embedding. And the steganography based on the payload-adaptive embedding risk and syndrome-trellis codes is presented. Finally, an experiment is performed to verify its validity.","2162-8998","978-1-4673-3093-0978-0-7695-4852","10.1109/MINES.2012.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405689","syndrome-trellis codes;steganography","Complexity theory;Payloads;Security;Multimedia communication;Image edge detection;Encoding;Forensics","data compression;steganography","adaptive steganography;syndrome-trellis code;local complexity;distortion weight function;exponential function;data embedding;payload-adaptive embedding risk","","2","","25","","","","","IEEE","IEEE Conferences"
"A tag complexity reduction approach for code-based cooperative ranging systems","M. Chaabane; E. M. Biebl","Department of Highest Frequency Engineering, TU Munich Arcisstr. 32, 80333 Muenchen, Germany; Department of Highest Frequency Engineering, TU Munich Arcisstr. 32, 80333 Muenchen, Germany","2013 10th Workshop on Positioning, Navigation and Communication (WPNC)","","2013","","","1","7","Cooperative wireless distance measurement based on PN-sequences requires a complex transponder tag. We present in this paper a novel approach to reduce the tag complexity and compensate the introduced errors. The compensation scheme is based on a dithering like approach, which profits from the error repetition along the ranging signal. Simulations and measurements were conducted to prove the approach.","","978-1-4673-6033-3978-1-4673-6031-9978-1-4673-6032","10.1109/WPNC.2013.6533264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6533264","Secondary Radar;Round-Trip time-of-flight;Cooperative Sensor Technology;Dithering","Base stations;Clocks;Distance measurement;Radar;Accuracy;Correlators;Complexity theory","codes;cooperative communication;distance measurement;transponders","tag complexity reduction approach;code-based cooperative ranging systems;cooperative wireless distance measurement;PN-sequences;complex transponder tag;tag complexity;compensation scheme;dithering like approach;error repetition;ranging signal","","1","","8","","","","","IEEE","IEEE Conferences"
"Complexity Modeling of the Motion Compensation Process of the H.264/AVC Video Coding Standard","M. Semsarzadeh; M. J. Langroodi; M. R. Hashemi; S. Shirmohammadi","NA; NA; NA; NA","2012 IEEE International Conference on Multimedia and Expo","","2012","","","925","930","With recent advances in computing and communication technologies, ubiquitous access to high quality multimedia content such as high definition video using smart phones, Net books, or tablets is a fact of our daily life. However, power is still a major concern for any mobile device, and requires optimization of power consumption using a power model for each multimedia application, such as a video decoder. In this paper, a generic decoding complexity model for the motion compensation (MC) process, which constitutes up to 25% of the computational complexity and hence power consumption of an H.264/AVC decoder, has been proposed. For the model to remain independent from a specific implementation or platform, it has been developed by analysing the MC algorithm as described in the standard. Simulation results indicate that the proposed model estimates MC complexity with an average accuracy of 95.63%, for a wide range of test sequences using both JM and x.264 software implementations of H.264/AVC. For a dedicated hardware implementation of the MC module the modeling accuracy is around 89.61%, according to our simulation results. It should be noted that in addition to power consumption control, the proposed model can be used for designing a receiver-aware H.264/AVC encoder, where the complexity constraints of the receiver side are taken into account during compression.","1945-788X;1945-7871;1945-7871","978-1-4673-1659-0978-0-7695-4711","10.1109/ICME.2012.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6298521","Decoder complexity modeling;H.264/AVC decoding;motion compensation","Complexity theory;Decoding;Interpolation;Computational modeling;Video coding;Estimation error;Software","computational complexity;data compression;motion compensation;power consumption;telecommunication power supplies;video coding","motion compensation process;H.264-AVC video coding standard;computing technology;communication technology;ubiquitous access;high-quality multimedia content;high-definition video;smart phones;net books;tablets;mobile device;power consumption optimization;power model;video decoder;generic decoding complexity model;MC process;computational complexity;H.264-AVC decoder;MC complexity estimation;JM implementation;x.264 software implementation;power consumption control;receiver-aware H.264-AVC encoder;data compression","","1","","15","","","","","IEEE","IEEE Conferences"
"A novel low complexity space-time coding schemes for underwater acoustic communications","C. Han; X. Zhang; Y. Yu; T. Li","College of Marine Northwestern Polytechnical University, Xi'an, China; College of Marine Northwestern Polytechnical University, Xi'an, China; College of Marine Northwestern Polytechnical University, Xi'an, China; College of Marine Northwestern Polytechnical University, Xi'an, China","Proceedings of the 32nd Chinese Control Conference","","2013","","","6383","6386","Underwater acoustic (UWA) channels exhibit time-varying fading statistics, thus a coded modulation scheme optimally designed for a specific model (e.g., Rayleigh fading) will perform poorly when the channel statistics change. Exploiting diversity via coded modulation is a robust approach to improve the reliability of the acoustic link in a variety of channel conditions. Herein, protocols coupled with space-time block code(STBC) strategies are proposed and analyzed for underwater communication. In this paper, we consider a time-reversal space-time block code (STBC) with rotated factors(RFTR-STBC) which extend a tranditional STBC scheme to a underwater communication scenario. We formulate its algebraic structure and propose a systematic method for its construction. Based on time-reversal scheme,we presented a new stratgy of STBC known as the block orthogonal STBCs (BOSTBCs), which exploited the time-reversal matrix with the rotated factor to restructure the new coded matrix.Analyzing both simulated and experimental data, the following results are confirmed: the proposed coding scheme yield a lower error rate when spatial diversity is very limited. On the other hand, the decoding complexity of the scheme show a better reduction performance than tranditional OSTBC schemes.","1934-1768","978-9-8815-6383","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640558","MIMO;time reversal (TR);STBC;Communiaction;underwater","Encoding;Maximum likelihood decoding;Complexity theory;Underwater acoustics;Rayleigh channels;Peak to average power ratio","diversity reception;modulation coding;space-time block codes;underwater acoustic communication","low complexity space-time coding;underwater acoustic communication;time-varying fading statistic;coded modulation;time-reversal space-time block code;rotated factor;algebraic structure;time reversal matrix;coded matrix;spatial diversity","","","","7","","","","","IEEE","IEEE Conferences"
"Linear Network Coding for Erasure Broadcast Channel With Feedback: Complexity and Algorithms","C. W. Sung; K. W. Shum; L. Huang; H. Y. Kwan","Department of Electronic Engineering, City University of Hong Kong, Hong Kong; Institute of Network Coding, The Chinese University of Hong Kong, Hong Kong; Department of Electronic Engineering, City University of Hong Kong, Hong Kong; Department of Electronic Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Information Theory","","2016","62","5","2493","2503","This paper investigates the linear network coding problem for erasure broadcast channel with user feedback. An innovative linear network code is shown to be uniformly optimal for the system. In general, determining the existence of innovative packets is proved to be NP-complete. When the finite field size is larger than the number of users, innovative packets always exist and the problem of finding an innovative encoding vector with smallest Hamming weight is considered. The corresponding decision problem is shown to be NP-complete. Optimal and approximate network coding algorithms for maximizing the sparsity of encoding vectors are designed.","0018-9448;1557-9654","","10.1109/TIT.2016.2536612","Research Foundation for Youth Scholars of Sichuan University; Research Grants Council, Hong Kong; University Grants Committee, Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422800","Erasure broadcast channel;innovative encoding vector;sparse network code;computational complexity;Erasure broadcast channel;innovative encoding vector;sparse network code;computational complexity","Encoding;Complexity theory;Network coding;Decoding;Receivers;Indexes;Matrices","broadcast channels;channel coding;computational complexity;decision theory;Hamming codes;linear codes;network coding","encoding vector sparsity maximization;decision problem;Hamming weight;field size;NP-complete;user feedback;erasure broadcast channel;linear network coding","","5","","48","","","","","IEEE","IEEE Journals & Magazines"
"Flicker Mitigating High Rate RLL Codes for VLC with Low Complexity Encoding and Decoding","u. Thummaluri; A. Kumar; L. Natarajan","NA; NA; NA","2018 IEEE International Symposium on Smart Electronic Systems (iSES) (Formerly iNiS)","","2018","","","209","214","Visible light communications integrated with Internet-of-Things has numerous applications in indoor wireless communications and it requires high data rates and flicker mitigation. Flicker mitigation is an important consideration since light emitting diodes are simultaneously used for illumination and communications. Several run-length limited (RLL) codes have been proposed in the literature for flicker mitigation with trade-offs between code rate, bit error performance, encoding and decoding complexities. However, there are no generalized algorithms for high rate codes generation with low complexity encoding and decoding. Hence, in this paper, we propose generalized algorithm for generating high rate RLL codes with low complexity encoding and decoding. The proposed codes can be used for flicker mitigation conditioned on maximum flickering time period (MFTP). The performance of the proposed codes is compared with existing codes in terms of various metrics like code rate, minimum Hamming distance, peak to average power ratio, run-length, and bit error rate. We show that the proposed codes mitigate flicker conditioned on MFTP and provide high data rates with low complexity encoding and decoding compared to the codes in the literature.","","978-1-5386-9172-4978-1-5386-9173","10.1109/iSES.2018.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719334","Bit error rate, code rate, ?icker mitigation, Hamming distance, peak to average power ratio, run-length, visible light communications","Decoding;Encoding;Complexity theory;Lighting;Peak to average power ratio;Light emitting diodes;Measurement","","","","","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity adaptive multiple transforms for post-HEVC video coding","T. Biatek; V. Lorcy; P. Castel; P. Philippe","IRT b&#x003C;&#x003E;com, Cesson-Sevigne, France; IRT b&#x003C;&#x003E;com, Cesson-Sevigne, France; IRT b&#x003C;&#x003E;com, Cesson-Sevigne, France; IRT b&#x003C;&#x003E;com, Cesson-Sevigne, France","2016 Picture Coding Symposium (PCS)","","2016","","","1","5","Since the introduction of the H.261 standard in 1990, the transform step has always been a key feature of video coding technologies. The successive generations of standard have always found a better way to further de-correlate the residual signals and thus improve the coding efficiency. Recently, MPEG and the ITU have jointly launched the Joint Video Exploration Team (or JVET) to prepare the next generation of video coding standard. In this context, a new approach called Adaptive Multiple Transforms (or AMT) has been introduced. Despite the provided coding efficiency, the AMT solution raises several issues especially regarding the complexity of the selected set of transforms. This can be an important issue, particularly for a future industrial adoption. In this paper, a low-complexity transform set for AMT is proposed, which aims at providing a significant reduction of the required computational power. The proposed method has been implemented in the JEM-2.0 and tested under the JVET test conditions where it provides a substantial complexity reduction of -56.5% compared to the current AMT transform set with similar R-D performance.","2472-7822","978-1-5090-5966-9978-1-5090-5967","10.1109/PCS.2016.7906348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906348","","Transforms;Encoding;Complexity theory;Video coding;Algorithm design and analysis;Standards;Software","computational complexity;video coding","low-complexity adaptive multiple-transforms;post-HEVC video coding;H.261 standard;video coding technology;residual signals;coding efficiency;joint video exploration team;next generation video coding standard;low-complexity transform set;JEM-2.0;JVET test condition;substantial complexity reduction;AMT transform set;RD performance","","8","","23","","","","","IEEE","IEEE Conferences"
"Low-Complexity Video Coding Based on Two-Dimensional Singular Value Decomposition","Z. Gu; W. Lin; B. Lee; C. Lau","School of Computer Engineering, Nanyang Technological University, Singapore; Centre for Multimedia and Network Technology, School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","","2012","21","2","674","687","In this paper, we propose a low-complexity video coding scheme based upon 2-D singular value decomposition (2-D SVD), which exploits basic temporal correlation in visual signals without resorting to motion estimation (ME). By exploring the energy compaction property of 2-D SVD coefficient matrices, high coding efficiency is achieved. The proposed scheme is for the better compromise of computational complexity and temporal redundancy reduction, i.e., compared with the existing video coding methods. In addition, the problems caused by frame decoding dependence in hybrid video coding, such as unavailability of random access, are avoided. The comparison of the proposed 2-D SVD coding scheme with the existing relevant non-ME-based low-complexity codecs shows its advantages and potential in applications.","1057-7149;1941-0042","","10.1109/TIP.2011.2166969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6008644","Computational complexity;energy compaction;frame independence;simultaneous low-rank approximation of matrices;video decomposition","Encoding;Discrete cosine transforms;Video coding;Matrix decomposition;Codecs;Complexity theory;Gain","correlation methods;singular value decomposition;video coding","low complexity video coding;two dimensional singular value decomposition;visual signal temporal correlation;energy compaction property","","10","","61","","","","","IEEE","IEEE Journals & Magazines"
"On the design of low complexity decoding (LCD) network codes","D. Y. Hu; M. Z. Wang; F. C. M. Lau; Q. C. Peng","Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Hong Kong SAR, China; School of Communication and Information Engineering, University of Electronic and Science Technology of China, Chengdu, China","2010 IEEE International Conference on Wireless Communications, Networking and Information Security","","2010","","","269","273","Network coding has recently attracted significant attention as its new paradigm promises to benefit various areas of communication and networks. The benefits come at a cost since encoding operations are required at intermediate nodes, and decoding operations are required at destination nodes. In some applications, the receive ends (the destinations) cannot, or may not want to, perform complex decoding operations. Hence, it is desirable to design network codes that can be decoded with a minimal complexity. We propose a design algorithm that results in network codes with a lower decoding complexity. This is achieved by minimizing the number of operations at the sinks to recover data sent by the source. Ideally, the sinks just receive “native” packets (packets that are in their original form) and hence only require perhaps some “shuffling” (permutation) operations. To achieve maximum throughput, in general, the ideal case does not happen too often. Hence there is a need to find low complexity decoding (LCD) network codes. We make use of a concept known as information decomposition to classify different information flows on all the links in a network. A greedy search algorithm examines the information flows on the links to the sinks and assigns global coding vectors in such a way that the resultant network code is a LCD network code, in other words, fewer operations, compared to other network codes, are required to recover data transmitted by the source. We define a complexity measure for the decoding operations and investigate the performance of LCD codes and compare the decoding complexity of LCD codes with that of network codes obtained by the conventional polynomial time algorithm. We demonstrate, by using 500 random networks with 50 nodes, that the average decoding complexity of our network codes is about 20 folds lower than that of the conventional network codes. The experiment also demonstrates that the number of “native” packets arriving at the sinks in our LCD network codes is significantly higher than that in a conventional network code. The network codes found may be useful in some special applications such as data transmission in a sensor network.","","978-1-4244-5849-3978-1-4244-5850-9978-1-4244-5852","10.1109/WCINS.2010.5541935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5541935","Network coding;low complexity decoding;information decomposition;greedy","Network coding;Encoding;Throughput;Computer networks;Iterative decoding;Design engineering;Algorithm design and analysis;Multicast algorithms;Iterative algorithms;Costs","computational complexity;decoding;network coding;search problems","low-complexity decoding;LCD network code design;native packets;shuffling operations;information decomposition;greedy search algorithm;global coding vectors;conventional polynomial time algorithm;data transmission;sensor network","","1","","17","","","","","IEEE","IEEE Conferences"
"On the complexity-performance trade-off in code-aided frame synchronization","D. Jakubisin; R. M. Buehrer","Mobile and Portable Radio Research Group (MPRG), Wireless@VT, Virginia Tech, Blacksburg, USA; Mobile and Portable Radio Research Group (MPRG), Wireless@VT, Virginia Tech, Blacksburg, USA","2014 IEEE 15th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)","","2014","","","364","368","Next generation wireless communication systems are pushing the limits of both energy efficiency and spectral efficiency. This presents a challenge to other functions in the receiver such as frame synchronization. In this paper we examine the trade-off between increased complexity and the improvement in energy and spectral efficiency of code-aided frame synchronization. Parallel and serial approaches to the frame synchronization problem are considered as well as methods for minimizing their complexity. We identify regions over which code-aided frame synchronization improves performance, with respect to a conventional receiver, while maintaining reasonable complexity.","1948-3244;1948-3252","978-1-4799-4903","10.1109/SPAWC.2014.6941765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6941765","","Synchronization;Receivers;Complexity theory;Wireless communication;Detectors;Modulation;Reliability","next generation networks;radio receivers;synchronisation","complexity-performance trade-off;next generation wireless communication systems;energy efficiency;spectral efficiency;receiver;frame synchronization;code-aided frame synchronization","","2","","15","","","","","IEEE","IEEE Conferences"
"Reduced complexity ADMM-based schedules for LP decoding of LDPC convolutional codes","H. Ben Thameur; B. Le Gal; N. Khouja; F. Tlili; C. Jego","SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; Bordeaux INP, CNRS IMS, UMR 5218, Bordeaux University, France; SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; SUPCOM, GRESCOM Lab, University of Carthage, Tunisia; Bordeaux INP, CNRS IMS, UMR 5218, Bordeaux University, France","2017 IEEE International Workshop on Signal Processing Systems (SiPS)","","2017","","","1","6","The ADMM based linear programming (LP) technique shows interesting error correction performance when decoding binary LDPC block codes. Nonetheless, it's applicability to decode LDPC convolutional codes (LDPC-CC) has not been yet investigated. In this paper, a first flooding based formulation of the ADMM-LP for decoding LDPC-CCs is described. In addition, reduced complexity decoding schedules to lessen the storage requirements and improve the convergence speed of an ADMM-LP based LDPC-CC decoder without significant loss in error correction performances are proposed and assessed from an algorithmic and computational/memory complexity perspectives.","2374-7390","978-1-5386-0446-5978-1-5386-0445-8978-1-5386-0447","10.1109/SiPS.2017.8110012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8110012","","Decoding;Schedules;Iterative decoding;Convergence;Computational complexity","block codes;convolutional codes;decoding;linear programming;parity check codes","reduced complexity ADMM;LP decoding;LDPC convolutional codes;linear programming technique;decoding binary LDPC block codes;flooding based formulation;ADMM-LP based LDPC-CC decoder;error correction performances;error correction performance;algorithmic-memory complexity perspectives;computational-memory complexity perspectives","","","","19","","","","","IEEE","IEEE Conferences"
"Low-Complexity Soft-Decision Detection of Coded OFDM With Index Modulation","J. Zheng; Q. Liu","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Transactions on Vehicular Technology","","2018","67","8","7759","7763","In this paper, the low-complexity soft-decision detectors of the orthogonal frequency division multiplexing with index modulation (OFDM-IM) in both single- and two-level coded modulation systems are studied. In the single-level coded OFDM-IM, the bitwise Markov chain Monte Carlo (b-MCMC) detector can overcome the drawback of the conventional symbolwise MCMC detector that the signal vectors obtained by Gibbs sampling are trapped in the activation pattern (AP) of the initial vector. By integrating with the randomized step further to alleviate the stalling problem, the randomized b-MCMC detector is proposed. In the two-level coded OFDM-IM, the framework of iterative multistage decoding (IMSD) with two component detectors for AP and symbol information, respectively, is employed. To reduce the complexity of IMSD, the efficient processing of the multiplicative interference in the component detectors is proposed. Finally, the effectiveness of the proposed detectors is verified by computer simulations.","0018-9545;1939-9359","","10.1109/TVT.2018.2822943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8331093","OFDM;index modulation;iterative multi-stage decoding;soft-input soft-output;Markov chain Monte Carlo","Detectors;Decoding;Indexes;OFDM;Modulation;Iterative decoding","iterative decoding;Markov processes;modulation coding;Monte Carlo methods;OFDM modulation","index modulation;low-complexity soft-decision detectors;bitwise Markov chain Monte Carlo detector;conventional symbolwise MCMC detector;signal vectors;Gibbs sampling;b-MCMC detector;low-complexity soft-decision detection;orthogonal frequency division multiplexing;two-level coded OFDM-IM;single-level coded modulation systems;two-level coded modulation systems;iterative multistage decoding;multiplicative interference","","","","20","","","","","IEEE","IEEE Journals & Magazines"
"Optimal distributed codes with delay four and constant decoding complexity","H. Lu","Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan","2015 IEEE International Symposium on Information Theory (ISIT)","","2015","","","2026","2030","A novel transmission scheme based on the non-orthogonal selection decode-and-forward protocol is presented in this paper for cooperative relay networks. The proposed scheme assumes a low rate feedback channel from the destination to the relays. Benefited from the feedback information, an optimal distributed code that has an extremely short delay equal to four is constructed, and the same code can be applied to networks with arbitrary number of relays to yield optimal cooperative diversity gains. The proposed code is sphere-decodable with decoding complexity again independent of the number of relays. In particular, when operating at multiplexing gain ≥ 1/2, the lattice decoder at the destination has a zero complexity exponent, meaning a constant decoding complexity and independent of transmission rate. Analyses for the decoding complexity of other existing diversity-optimal distributed codes are also provided. It is shown that these codes have a linear growth in delay and an exponential growth in decoding complexity as the number of relays increases.","2157-8117;2157-8095","978-1-4673-7704-1978-1-4673-7703","10.1109/ISIT.2015.7282811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7282811","","Relays;Complexity theory;Decoding;Signal to noise ratio;Delays;Multiplexing;Protocols","cooperative communication;decode and forward communication;decoding;diversity reception;relay networks (telecommunication);wireless channels","optimal distributed code;constant decoding complexity;transmission scheme;nonorthogonal selection decode-and-forward protocol;cooperative relay network;low rate feedback channel;optimal cooperative diversity gain;lattice decoder","","","","12","","","","","IEEE","IEEE Conferences"
"Low Complexity Transform Coding for Millimeter Wave MIMO CSI Compression","B. V. Boas; N. Fonseca; A. Klautau; N. González-Prelcic","Federal University of Pará; Federal University of Pará; Federal University of Pará; The University of Texas, Austin","2018 52nd Asilomar Conference on Signals, Systems, and Computers","","2018","","","1582","1586","Channel state information (CSI) obtained at the user equipment (UE) can be made available at the base stations (BS) using limited feedback strategies. The design of feedback techniques is critical in millimeter wave (mmWave) multiple-input multiple-output (MIMO) systems, due to the high dimension of the channel matrix. In this paper, we assume that the mmWave MIMO channel has been estimated, and develop a compression method that allows trading off rate and distortion. The proposed method, which exhibits low complexity, is based on transform coding and uses a customized significance map to encode the relevant coefficients without using a fixed ordering scheme, such as the zig-zag scanning used in image coding. We show the performance gain of our method compared to previous algorithms using several datasets of MIMO channels obtained with ray tracing simulations, for mobile and fixed access scenarios.","2576-2303;1058-6393","978-1-5386-9218-9978-1-5386-9216-5978-1-5386-9217-2978-1-5386-9219","10.1109/ACSSC.2018.8645142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8645142","","MIMO communication;Transforms;Image coding;Indexes;Ray tracing;Receiving antennas","millimetre wave communication;MIMO communication;mobile radio;ray tracing;transform coding;wireless channels","millimeter wave multiple-input multiple-output systems;fixed ordering scheme;zig-zag scanning;ray tracing simulations;mobile access scenarios;fixed access scenarios;channel matrix;feedback techniques;base stations;user equipment;channel state information;millimeter wave MIMO CSI compression;low complexity transform coding;image coding;compression method;mmWave MIMO channel","","","","17","","","","","IEEE","IEEE Conferences"
"Complexity reduction for the 3D-HEVC depth maps coding","M. Saldanha; G. Sanchez; B. Zatt; M. Porto; L. Agostini","Group of Architecture and Integrated Circuits - GACI, Federal University of Pelotas - Brazil; Group of Architecture and Integrated Circuits - GACI, Federal University of Pelotas - Brazil; Group of Architecture and Integrated Circuits - GACI, Federal University of Pelotas - Brazil; Group of Architecture and Integrated Circuits - GACI, Federal University of Pelotas - Brazil; Group of Architecture and Integrated Circuits - GACI, Federal University of Pelotas - Brazil","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","","2015","","","621","624","This paper presents a qualitative discussion of the depth maps properties that can be considered to achieve complexity reduction for 3D-High Efficiency Video Coding (3D-HEVC) depth maps coding. Both intra and inter-frame predictions are considered in this discussion that conduced to the proposition of two simple complexity reduction techniques: the Simplified Edge Detector (SED) and the Diamond Search (DS) simplified inter-prediction. The SED anticipates the blocks that are likely to be better predicted by the HEVC intra-prediction, avoiding evaluations of Depth Modeling Modes (DMM). The DS and SED were compared to anchor results and experimental analysis showed that the proposed algorithms are able to achieve a time saving of 11.3% encoding time reduction, with acceptable impact on the BD-Rate of the synthesized views of 0.6%.","0271-4302;2158-1525","978-1-4799-8391","10.1109/ISCAS.2015.7168710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168710","3D-HEVC;Motion Estimation;Intra-Prediction;Depth Maps;Complexity Reduction","Encoding;Complexity theory;Algorithm design and analysis;Prediction algorithms;Three-dimensional displays;Image edge detection;Diamonds","edge detection;video coding","complexity reduction;3D-HEVC depth maps coding;high efficiency video coding;simplified edge detector;diamond search simplified inter-prediction;depth modeling modes","","4","","16","","","","","IEEE","IEEE Conferences"
"Low-complexity lossy image coding through a near-optimal general embedded quantizer","F. Aulí-Llinàs; J. L. Monteagudo-Pereira; J. Serra-Sagristà; J. Bartrina-Rapesta","Department of Information and Communications Engineering Universitat Autònoma de Barcelona - 08193 Bellaterra, Spain; Department of Information and Communications Engineering Universitat Autònoma de Barcelona - 08193 Bellaterra, Spain; Department of Information and Communications Engineering Universitat Autònoma de Barcelona - 08193 Bellaterra, Spain; Department of Information and Communications Engineering Universitat Autònoma de Barcelona - 08193 Bellaterra, Spain","IET Conference on Image Processing (IPR 2012)","","2012","","","1","6","Embedded quantization is a mechanism employed by many lossy image codecs to progressively refine the distortion of a (transformed) image. Currently, the most common scheme to do so is to use a uniform scalar deadzone quantizer (USDQ) together with a bitplane coding (BPC) strategy. This scheme is convenient, but does not allow major variations. This paper uses the recently introduced general embedded quantizer (GEQ) to design a multi-stage quantization scheme that can be introduced in the core of modern image coding systems. Experimental results carried out in the framework of JPEG2000 indicate that the proposed scheme achieves same coding performance as that of USDQ+BPC while requiring fewer quantization stages, which reduces the computational costs of codecs without penalizing their performance.","","978-1-84919-632","10.1049/cp.2012.0428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6290623","Image coding;embedded quantization;JPEG2000","","cost reduction;design;image coding;quantisation (signal)","low-complexity lossy image coding;near-optimal general embedded quantizer;embedded quantization;lossy image codecs;uniform scalar deadzone quantizer;bitplane coding strategy;multistage quantization scheme design;JPEG2000;quantization stage;computational cost reduction","","3","","","","","","","IET","IET Conferences"
"Low complexity decoder design of non-binary LDPC codes in amplify-and-forward relay networks with cooperative diversity","Y. Yan; Y. Guo; M. H. Lee","School of Mechanical and Electrical Engineering of Guangzhou University, Guangdong Guangzhou 510006, China; School of Information Science and Engineering Central South University, Changsha 410083, China; Dept. Electrical and Computer Engineering of Chonbuk National University, 561-756, Korea","2010 IEEE 11th International Symposium on Spread Spectrum Techniques and Applications","","2010","","","1","5","In this paper, a simple cooperative diversity scheme in M + 2 relay network with low-density parity-check (LDPC) codes is proposed. By allocating random interleavers to each relay node, the interleaved sequences provide M copies of received vector to achieve the cooperative diversity. An efficiently iterative coding algorithm is proposed with low complexity. The complexity involved grows linearly with the number of relay nodes and size of Galois Field.","1943-7447;1943-7439;1943-7439","978-1-4244-6015-1978-1-4244-6013-7978-1-4244-6014","10.1109/ISSSTA.2010.5650020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650020","","Relays;Decoding;Complexity theory;Bit error rate;Modulation;Iterative decoding","amplify and forward communication;binary codes;cooperative communication;diversity reception;Galois fields;iterative decoding;parity check codes;radio repeaters","low complexity decoder design;nonbinary LDPC codes;amplify-and-forward relay networks;cooperative diversity;M+2 relay network;low-density parity-check codes;random interleaver allocation;relay node;M-copy interleaved sequences;iterative coding algorithm;Galois field","","","","12","","","","","IEEE","IEEE Conferences"
"Low complexity cross parity codes for multiple and random bit error correction","M. Poolakkaparambil; J. Mathew; A. M. Jabir; S. P. Mohanty","Department of Computer Science and Electronics, Oxford Brookes University, UK; Department of Computer Science, University of Bristol, UK; Department of Computer Science and Electronics, Oxford Brookes University, UK; Department of Computer Science and Engineering, University of North Texas, USA","Thirteenth International Symposium on Quality Electronic Design (ISQED)","","2012","","","57","62","Error detection and correction which has been used in communication and memory design is becoming increasingly important in fault tolerant logic circuit design. As a result of the aggressive technology scaling, the current high-density integrated circuits are easily succumbed to faulty operations generated from many sources including stuck-at-faults, radiation induced faults, or malicious eavesdropper attacks. The currently used techniques like low-density parity-check (LDPC) and Hamming code based fault masking to mitigate bit flips in the digital circuits are either single bit error correcting or multiple error correctable with Bose-Choudhury-Hocquenghem (BCH) and Reed-solomon based methods with very large overheads. This paper introduce a novel cross code based method that can correct multiple errors with minimal compromise in error correction capability and area. The key idea of the novel method proposed in this paper is that do not correct all the errors but minimize their probability being escaped. Experimental results of the proposed methods show that the following: (1) area overhead is 101% for Hamming cross code and 106% for BCH cross code for a 90-bit finite field multiplier and (2) 150% for Hamming cross code and 170% for BCH cross codes for practically used 163-bit digit serial polynomial basis multiplier. Thus, the proposed methods are significantly efficient compared to Triple Modular Redundancy (TMR), LDPC, Hamming based methods in terms of area overhead and also the first attempted approach to a low complexity multiple error correctable digit serial multiplier to the best of the authors knowledge.","1948-3295;1948-3287;1948-3287","978-1-4673-1036-9978-1-4673-1034-5978-1-4673-1035","10.1109/ISQED.2012.6187474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6187474","Polynomial Basis Bultiplier;Concurrent Error Detection;Single Error Correction;N-Modular Redundancy;Bose-Choudhury-Hocquenghem Code","Error correction codes;Transient analysis;Decoding;Error correction;Cryptography;Complexity theory;Circuit faults","BCH codes;error correction codes;error detection codes;fault tolerance;Hamming codes;logic circuits;logic design;parity check codes;Reed-Solomon codes","low complexity cross parity codes;multiple bit error correction;random bit error correction;error detection;fault tolerant logic circuit design;low-density parity-check code;Hamming code;fault masking;Bose-Choudhury-Hocquenghem based method;Reed-Solomon based method;triple modular redundancy;word length 90 bit;word length 163 bit","","3","","16","","","","","IEEE","IEEE Conferences"
"Some Results on Update Complexity of a Linear Code Ensemble","A. Jule; I. Andriyanova","NA; NA","2011 International Symposium on Networking Coding","","2011","","","1","5","In this paper, the update complexity of a linear code ensemble (binary or nonbinary) is considered. The update complexity has been proposed in as a measure of the number of updates needed to be done within the bits of a codeword, if one of information bits, encoded in this codeword, has been changed. The update efficiency is a performance measure of distributed storage applications, that naturally use erasure-correction coding. The maximum update complexity γmaxand the average update complexity γavgof a code ensemble are distinguished in this paper. In the first part of the paper, we propose a simple lower bound on the γavgand further evaluate its general expression. In the second part, we show how a simple upper bound on γavgfor sparse graph codes can be obtained, on a particular example of binary LDPC codes. One of interesting results of the paper is that code ensembles with polynomial minimum distance growth have the update complexity which grows linearly in the codelength, i.e. they are not update- efficient.","2374-9660","978-1-61284-140-3978-1-61284-138-0978-1-61284-139","10.1109/ISNETCOD.2011.5979095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979095","","Complexity theory;Parity check codes;Redundancy;Genetic expression;Nickel;Linear code;Upper bound","binary codes;linear codes;parity check codes","update complexity;linear code ensemble;distributed storage applications;erasure-correction coding;sparse graph codes","","4","","11","","","","","IEEE","IEEE Conferences"
"Symbol-index-feedback polar coding schemes for low-complexity devices","X. Ma","Pattern Technology Lab LLC, U.S.A.","2013 International Conference on Computing, Networking and Communications (ICNC)","","2013","","","273","277","Recently, a new class of error-control codes, the polar codes, have attracted much attention. The polar codes are the first known class of capacity-achieving codes for many important communication channels. In addition, polar codes have low-complexity encoding algorithms. Therefore, these codes are favorable choices for low-complexity devices, for example, in ubiquitous computing and sensor networks. However, the polar codes fall short in terms of finite-length error probabilities, compared with the state-of-the-art codes, such as the low-density parity-check codes. In this paper, in order to improve the error probabilities of the polar codes, we propose novel interactive coding schemes using receiver feedback based on polar codes. The proposed coding schemes have very low computational complexities at the transmitter side. By experimental results, we show that the proposed coding schemes achieve significantly lower error probabilities.","","978-1-4673-5288-8978-1-4673-5287-1978-1-4673-5286","10.1109/ICCNC.2013.6504094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504094","","Decoding;Error probability;Reliability;Receivers;Indexes;Channel coding","computational complexity;error correction codes;probability;radio receivers;radio transmitters;wireless channels","symbol-index-feedback polar coding schemes;low-complexity devices;error-control codes;capacity-achieving codes;communication channels;low-complexity encoding algorithms;ubiquitous computing;sensor networks;finite-length error probabilities;interactive coding schemes;receiver feedback;transmitter side","","","","13","","","","","IEEE","IEEE Conferences"
"An LDPC-Coded Generalized Space Shift Keying Scheme Using A Codebook-Assisted Low-Complexity Massive MIMO Detector","Y. Chen; C. Lin; Y. Ueng","Department of Electronic Engineering, Chung Yuan Christian University, Taoyuan, Taiwan; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering and the Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan","IEEE Communications Letters","","2016","20","3","454","457","Aimed at the emerging massive multiple-input multiple-output (MIMO) systems, both hard-decision and soft-decision detectors are investigated for the generalized space shift keying (GSSK) modulation scheme, where a codebook-assisted technique is used to reduce the tree-search complexity. Furthermore, the outer low-density parity-check (LDPC) code is optimized such that the proposed system design can provide an error performance, which is almost the same as that of the scheme that uses the maximum a posteriori (MAP) soft detector, but with a much lower computational complexity.","1089-7798;1558-2558;2373-7891","","10.1109/LCOMM.2016.2518180","Ministry of Science and Technology of Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383250","Massive MIMO;generalized space shift keying (GSSK);low-density parity-check (LDPC) codes;Massive MIMO;generalized space shift keying (GSSK);low-density parity-check (LDPC) codes","Detectors;Complexity theory;MIMO;Parity check codes;Indexes;Transmitting antennas","computational complexity;maximum likelihood estimation;MIMO communication;parity check codes","LDPC-coded generalized space shift keying scheme;codebook-assisted low-complexity massive MIMO detector;massive multiple-input multiple-output systems;hard-decision detectors;soft-decision detectors;low-density parity-check code;tree-search complexity;maximum a posteriori soft detector;MAP soft detector;computational complexity","","1","","8","","","","","IEEE","IEEE Journals & Magazines"
"A low complexity extrinsic message based decoding algorithm for non-binary LDPC codes","N. Qiu; W. Chen; Y. Yu; C. Li","Department of Electronic Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, China","2013 International Workshop on High Mobility Wireless Communications (HMWC)","","2013","","","106","109","In this paper, we propose a low complexity extrinsic message based decoding algorithm for non-binary LDPC codes. This algorithm only requires computations over finite field and integer operations. The novelty of this decoding algorithm lies in that we compute extrinsic message and iteratively update the messages in every iteration. The proposed algorithm provides effective trade-off between computational complexity and performance. Furthermore, complexity issues and decoding performance will be well analyzed in this paper. Simulation results show that we can achieve a better performance than ISRB algorithm with a slight increase in computational complexity.","","978-1-4673-6380-8978-1-4673-6379","10.1109/HMWC.2013.6710288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710288","LDPC;Non-binary LDPC;majority logic decoding;iterative decoding;extrinsic message","Decoding;Reliability;Iterative decoding;Algorithm design and analysis;Computational complexity","computational complexity;iterative decoding;parity check codes","low complexity extrinsic message based decoding algorithm;nonbinary LDPC codes;integer operations;finite field operations;computational complexity;decoding performance;iterative update","","1","","14","","","","","IEEE","IEEE Conferences"
"A low-complexity and lossless reference frame encoder algorithm for video coding","D. Silveira; G. Povala; L. Amaral; B. Zatt; L. Agostini; M. Porto","Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil; Group of Architectures and Integrated Circuits - GACI, Federal University of Pelotas - UFPel, Pelotas - RS - Brazil","2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2014","","","7358","7362","This paper presents a lossless coding solution to reduce the large overhead of external memory communication during the motion estimation process in current video coders. Our solution is called Differential Reference Frame Coder (DRFC), and uses two techniques together to compress the reference frame: a differential coding based on a simplified intra-prediction process to reduce the spatial redundancy of the reference samples, and a simple VLC applied to differential coding residues. The proposed solution reaches an average compression rate higher than 45% for the evaluated HD 1080p video sequences. This is a lossless and low-complexity solution, and could easily be implemented in hardware.","1520-6149;2379-190X","978-1-4799-2893","10.1109/ICASSP.2014.6855029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6855029","Lossless reference frame compression;differential coding;memory bandwidth reduction;motion estimation;video coding","Encoding;Video coding;Bandwidth;Hardware;Computational efficiency;Video sequences;Decoding","data compression;image sequences;motion estimation;redundancy;video coding","lossless reference frame encoder algorithm;video coding;lossless coding;memory communication;motion estimation process;video coder;differential reference frame coder;DRFC;simplified intraprediction process;spatial redundancy reduction;reference samples;VLC;differential coding residue;compression rate;HD 1080p video sequences;lossless reference frame compression","","2","","11","","","","","IEEE","IEEE Conferences"
"Computational Complexity of Decoding Orthogonal Space-Time Block Codes","E. Ayanoglu; E. G. Larsson; E. Karipidis","Center for Pervasive Communications and Computing, Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA; Department of Electrical Engineering, Linkoping University, SE-581 83 Linkoping, Sweden; Department of Electrical Engineering, Linkoping University, SE-581 83 Linkoping, Sweden","IEEE Transactions on Communications","","2011","59","4","936","941","The computational complexity of optimum decoding for an orthogonal space-time block code GNsatisfying GNHGN= c(Σk=1K|sk|2)INwhere c is a positive integer is quantified. Four equivalent techniques of optimum decoding which have the same computational complexity are specified. Modifications to the basic formulation in special cases are calculated and illustrated by means of examples. This paper corrects and extends, and unifies them with the results from the literature. In addition, a number of results from the literature are extended to the case c >; 1.","0090-6778;1558-0857","","10.1109/TCOMM.2011.012711.090613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708206","OSTBC;maximum likelihood decoding;quadrature amplitude modulation (QAM);decoding QAM;square QAM","Maximum likelihood decoding;Computational complexity;Measurement;Quadrature amplitude modulation;Receiving antennas","computational complexity;decoding;orthogonal codes;space-time block codes","computational complexity;decoding orthogonal space-time block codes;optimum decoding;orthogonal space-time block code GN","","2","","11","","","","","IEEE","IEEE Journals & Magazines"
"RDO cost modeling for low-complexity HEVC intra coding","M. Jamali; S. Coulombe","École de technologie supérieure, Université du Québec, Montreal, Canada; École de technologie supérieure, Université du Québec, Montreal, Canada","2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","","2016","","","1","5","High efficiency video coding (HEVC) is the newest international standard for video compression, providing improved coding performance that achieves compression ratios up to 50% higher than those obtained with H.264/AVC. However, this improvement comes at the expense of high computational complexity and coding time. In this paper, we propose a novel method for fast and low-complexity intra HEVC mode decision based on rate-distortion optimization (RDO) cost modeling, which permits the exclusion of non-promising candidates from the RDO processing. To achieve even more complexity reduction, an additional rough most probable modes examination is coupled with the main algorithm. Experimental results show that the proposed algorithms reduce the encoding time by 41.8% on average, with a negligible quality loss of 0.058 dB (BD-PSNR) for all-intra scenarios, as compared to the HEVC reference implementation, the HM 15.0.","","978-1-4673-8721-7978-1-4673-8722","10.1109/CCECE.2016.7726703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726703","HEVC;video coding;intra coding;mode decision;rate-distortion optimization","Conferences;Computers","computational complexity;data compression;optimisation;rate distortion theory;video coding","RDO cost modeling;low-complexity HEVC intra coding;high efficiency video coding;international standard;video compression;coding performance;H.264-AVC;computational complexity;low-complexity intra HEVC mode decision;complexity reduction;rate-distortion optimization cost modeling;probable mode examination","","1","","12","","","","","IEEE","IEEE Conferences"
"Low Complexity X-EMS Algorithms for Nonbinary LDPC Codes","X. Ma; K. Zhang; H. Chen; B. Bai","Department of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510275, China.; Department of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510275, China.; Department of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou 510275, China. H. Chen is also with the School of Computer, Electronics and Information, Guangxi University, Nanning 530004, China; State Key Lab of ISN, Xidian University, Xi'an 710071, China","IEEE Transactions on Communications","","2012","60","1","9","13","The extended min-sum (EMS) algorithm is redescribed as a reduced-search trellis algorithm (called M-EMS algorithm). Two variants of the M-EMS algorithm, called T-EMS algorithm and D-EMS algorithm, are presented. Simulation results show that, these three algorithms (referred to as X-EMS algorithms for convenience), combined with factor correction techniques, perform almost as well as the Q-ary sum-product algorithm (QSPA) but with a much lower complexity.","0090-6778;1558-0857","","10.1109/TCOMM.2011.092011.110082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6029349","EMS algorithms;nonbinary low-density parity-check (LDPC) codes;Q-ary sum-product algorithm (QSPA);reduced-search trellis algorithm","Parity check codes;Complexity theory;Decoding;Algorithm design and analysis;Measurement;Bit error rate;Joining processes","communication complexity;parity check codes;trellis codes","low complexity X-EMS algorithm;nonbinary LDPC code;extended min-sum algorithm;reduced-search trellis algorithm;M-EMS algorithm;D-EMS algorithm;factor correction technique;Q-ary sum-product algorithm;T-EMS algorithm","","38","","15","","","","","IEEE","IEEE Journals & Magazines"
"Lattice sequential decoder for coded MIMO channel: Performance and complexity analysis","W. Abediseid; M. O. Damen","Department of Elect. &amp; Comp. Engineering, University of Waterloo, Ontario N2L 3G1, Canada; Department of Elect. &amp; Comp. Engineering, University of Waterloo, Ontario N2L 3G1, Canada","2010 IEEE International Symposium on Information Theory","","2010","","","2248","2252","In this paper, the performance limits and computational complexity of lattice sequential decoder for coded MIMO channel are analyzed. It is shown that using nested lattice codes, the optimal diversity-multiplexing tradeoff of the channel can be achieved in the presence of such very low complexity decoder. We show that the computational complexity distribution, at high signal-to-noise ratio, is dominated by the outage probability.","2157-8095;2157-8117","978-1-4244-7892-7978-1-4244-7890-3978-1-4244-7891","10.1109/ISIT.2010.5513497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5513497","","Lattices;MIMO;Performance analysis;Maximum likelihood decoding;Computational complexity;Signal to noise ratio;Random variables;Fading;Symmetric matrices;OFDM modulation","channel coding;computational complexity;decoding;diversity reception;MIMO communication;multiplexing;probability","lattice sequential decoder;coded MIMO channel;complexity analysis;nested lattice codes;optimal diversity-multiplexing tradeoff;signal-to-noise ratio","","4","","9","","","","","IEEE","IEEE Conferences"
"A reduced-complexity iterative scheme for decoding quasi-cyclic low-density parity-check codes","S. Lin; K. Liu; J. Li; K. Abdel-Ghaffar","Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA; Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA; Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA; Department of Electrical and Computer Engineering, University of California, Davis, CA 95616, USA","2014 48th Asilomar Conference on Signals, Systems and Computers","","2014","","","119","125","This paper presents a reduced-complexity iterative decoding scheme for quasi-cyclic (QC) low-density parity-check (LDPC) codes which is devised based on the section-wise cyclic structure of their parity-check matrices. The decoding scheme significantly reduces the hardware implementation complexity of a QC-LDPC code decoder in terms of the number of message processing units and the number of wires required to connect the message processing units. A long high-rate high-performance QC-LDPC code is used to demonstrate the effectiveness of the proposed decoding scheme. Using this long code as the mother code, descendant QC-LDPC codes of various lengths and rates can be constructed by puncturing and masking the columns and rows of the parity-check matrix of the mother code. These descendant codes can be decoded by deactivating specific sets of message processing units of the mother decoder.","1058-6393","978-1-4799-8297-4978-1-4799-8295","10.1109/ACSSC.2014.7094410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7094410","","Iterative decoding;Decoding;Arrays;Complexity theory;Reliability;Null space;Wires","computational complexity;cyclic codes;iterative decoding;matrix algebra;parity check codes","quasicyclic low-density parity-check codes;reduced-complexity iterative decoding scheme;QC-LDPC code decoder;section-wise cyclic structure;parity-check matrices;hardware implementation complexity reduction;message processing units;wires;high-rate high-performance QC-LDPC code;descendant QC-LDPC codes","","","","21","","","","","IEEE","IEEE Conferences"
"Construction of polar codes with sublinear complexity","M. Mondelli; S. H. Hassani; R. Urbanke","Department of Electrical Engineering, Stanford University, USA; Department of Electrical and Systems Engineering, University of Pennsylvania, USA; School of Computer and Communication Sciences, EPFL, Switzerland","2017 IEEE International Symposium on Information Theory (ISIT)","","2017","","","1853","1857","Consider the problem of constructing a polar code of block length N for the transmission over a given channel W. Typically this requires to compute the reliability of all the N synthetic channels and then to include those that are sufficiently reliable. However, we know from [1], [2] that there is a partial order among the synthetic channels. Hence, it is natural to ask whether we can exploit it to reduce the computational burden of the construction problem. We show that, if we take advantage of the partial order [1], [2], we can construct a polar code by computing the reliability of roughly N/ log3/2N synthetic channels. Such a set of synthetic channels is universal, in the sense that it allows one to construct polar codes for any W, and it can be identified by solving a maximum matching problem on a bipartite graph. Our proof technique consists in reducing the construction problem to the problem of computing the maximum cardinality of an antichain for a suitable partially ordered set. As such, this method is general and it can be used to further improve the complexity of the construction problem in case a new partial order on the synthetic channels of polar codes is discovered.","2157-8117","978-1-5090-4096-4978-1-5090-4097","10.1109/ISIT.2017.8006850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8006850","Polar codes;partial order;construction problem;antichain;chain","Complexity theory;Degradation;Decoding;Reliability theory;Electronic mail","channel coding;computational complexity;graph theory;reliability","polar code construction;sublinear complexity;block length;synthetic channel reliability;maximum matching problem;bipartite graph","","11","","17","","","","","IEEE","IEEE Conferences"
"RGBW image compression by low-complexity adaptive multi-level block truncation coding","S. Kim; H. Lee","Inter-university Semiconductor Research Center, Department of Electrical Engineering, Seoul National University, Seoul, Republic of Korea; Inter-university Semiconductor Research Center, Department of Electrical Engineering, Seoul National University, Seoul, Republic of Korea","IEEE Transactions on Consumer Electronics","","2016","62","4","412","419","Frame memory compression is a widely used image compression technique that aims to reduce the size of the frame memory in display panels such as those containing LCD and OLED technologies. Recent LCD panels use the RGBW color domain to replace the traditional RGB domain in order to enhance the brightness of LCD panels with the addition of a white component. The additional component increases the size of the frame memory but necessitates an aggressive compression algorithm. This paper proposes a novel compression algorithm for RGBW components that improves the efficiency of block truncation coding (BTC), which is widely used for LCD overdrive. The proposed low-complexity adaptive multi-level block truncation coding (LAM-BTC) algorithm codes RGBW color data with a width of ten bits. It adaptively selects two-level BTC or four-level BTC to enhance the quality of the images, for which a low-complex level selection scheme is used. Four-level BTC in the proposed algorithm codes two representative values (RV) in four RVs and infers the other two RVs using inter-color correlation. In spite of the reduced complexity, the average peak signal-to-noise ratio (PSNR) of the proposed algorithm is 0.54 dB higher than that of the previous AM-BTC at a compression ratio of eight.","0098-3063;1558-4127","","10.1109/TCE.2016.7838094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838094","Liquid-crystal display;RGBW color domain;overdrive technique;block truncation coding","Image coding;Liquid crystal displays;Image color analysis;Image reconstruction;Compression algorithms;Hardware;Brightness","adaptive codes;block codes;brightness;data compression;image coding;image colour analysis;liquid crystal displays","RGBW image compression;low-complexity adaptive multilevel block truncation coding;frame memory size reduction;display panels;LCD technology;OLED technology;LCD panels;RGBW color domain;brightness enhancement;white component addition;block truncation coding efficiency improvement;LAM-BTC;RGBW color data coding;image quality enhancement;low-complex level selection scheme;peak signal-to-noise ratio;PSNR","","3","","16","","","","","IEEE","IEEE Journals & Magazines"
"Invertible Subset LDPC Code for PAPR Reduction in OFDM Systems with Low Complexity","D. Qu; L. Li; T. Jiang","Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China","IEEE Transactions on Wireless Communications","","2014","13","4","2204","2213","In this paper, we introduce a new family of low-density parity-check (LDPC) codes, called as invertible subset LDPC (IS-LDPC) code, for peak-to-average power ratio (PAPR) reduction in OFDM systems with low complexity. An IS-LDPC code has a number of disjoint invertible subsets, and each invertible subset can be independently inverted to generate other valid codewords of the LDPC code. To construct IS-LDPC codes with good error-correcting performance, we propose a modified progressive edge-growth construction algorithm and verify its effectiveness by analyzing the constructed Tanner graphs. Both theoretical analysis and numerical results show that the IS-LDPC codes exhibit good error-correcting performance and the proposed PAPR reduction scheme based on IS-LDPC codes significantly reduces the PAPR. Compared with the existing coding-based candidate generation schemes, the proposed scheme has a much lower searching complexity when the codeword is transmitted by multiple OFDM symbols. With all mentioned advantages, the proposed PAPR reduction scheme based on IS-LDPC codes could serve as an attractive PAPR reduction solution for future multicarrier communication systems.","1536-1276;1558-2248","","10.1109/TWC.2014.031314.131289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6775375","Orthogonal frequency-division multiplexing (OFDM);peak-to-average power ratio (PAPR);low density parity-check codes (LDPC);progressive edge-growth (PEG)","Peak to average power ratio;Parity check codes;Complexity theory;Block codes;Vectors;Indexes","error correction codes;graph theory;parity check codes","invertible subset LDPC code;PAPR reduction;OFDM systems;low complexity code;low density parity check code;disjoint invertible subsets;error correction code;modified progressive edge growth construction algorithm;multicarrier communication;Tanner graphs;peak-to-average power ratio reduction","","17","","33","","","","","IEEE","IEEE Journals & Magazines"
"Low-complexity near-optimal map decoder for convolutional codes in symmetric alpha-stable noise","T. S. Saleh; I. Marsland; M. El-Tanany","Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada","2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","","2012","","","1","4","The design of the MAP decoder for signals in impulsive noise modeled using the symmetric α-stable (SαS) distribution is considered. The conventional MAP decoder, which optimizes the a posteriori probability for Gaussian noise, performs poorly in SαS noise. On the other hand, the optimal MAP decoder possesses impractical complexity due to the lack of a closed form expression of the probability density function. To simplify the implementation of the MAP decoder, the Huber nonlinearity was previously proposed, which results in a performance improvement over the conventional Gaussian MAP decoder. However, the performance is still far from optimal. In this paper, a simple unified approach to design low complexity suboptimal MAP decoder is proposed. The proposed approach uses the log likelihood ratio (LLR) as a metric to evaluate how close the suboptimal MAP decoder from the optimal. Based on this approach, a piecewise linear approximation of the LLR is used to design a sub-optimal MAP decoder which gives near optimal performance with low implementation complexity. The performance improvement of the MAP decoder is approximately 2-6 dB for different values of α compared to the MAP decoder with the Huber nonlinearity, with low complexity.","0840-7789;0840-7789","978-1-4673-1433-6978-1-4673-1431-2978-1-4673-1432","10.1109/CCECE.2012.6334952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6334952","","Decoding;Noise;Complexity theory;Approximation methods;Robustness;Piecewise linear approximation;Measurement","convolutional codes;Gaussian noise;impulse noise;maximum likelihood decoding;piecewise linear techniques;statistical distributions","low-complexity near-optimal MAP decoder;convolutional codes;symmetric alpha-stable noise;impulsive noise;symmetric α-stable distribution;SαS distribution;a posteriori probability;Gaussian noise;probability density function;Huber nonlinearity;Gaussian MAP decoder;log likelihood ratio;piecewise linear approximation","","1","","10","","","","","IEEE","IEEE Conferences"
"Low-complexity decoding architecture for rate-compatible puncturing polar codes","X. Wang; D. Jia; C. Sun; J. Huang; Z. Fei","School of Information and Electronics, Beijing Institute of Technology, Beijing, China, 100081; School of Information and Electronics, Beijing Institute of Technology, Beijing, China, 100081; School of Information and Electronics, Beijing Institute of Technology, Beijing, China, 100081; School of Information and Electronics, Beijing Institute of Technology, Beijing, China, 100081; School of Information and Electronics, Beijing Institute of Technology, Beijing, China, 100081","2017 IEEE 17th International Conference on Communication Technology (ICCT)","","2017","","","97","101","Polar codes are the first family of codes which can achieve the channel capacity. Motivated by the demand of high speed transmission for 5G communications, researchers are keen to explore efficient decoding architecture for polar codes. In this paper, we propose a low-complexity decoding architecture for rate-compatible puncturing polar (RCPP) codes with two different puncturing methods, quasi-uniform puncturing (QUP) and shortening. By removing the redundant processing elements for the recursive calculation of log-likelihood ratio, the proposed decoding architecture significantly reduces the space complexity and latency with no BLER performance loss. Moreover, we illustrate that the proposed decoding architecture reduces more complexity if two puncturing methods are employed simultaneously, with negligible BLER performance loss.","2576-7828","978-1-5090-3944-9978-1-5090-3942-5978-1-5090-3945","10.1109/ICCT.2017.8359611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359611","polar codes;low-complexity architecture;successive cancellation (SC) decoding","Complexity theory;Maximum likelihood decoding;Encoding;Generators;Hardware;Reliability","5G mobile communication;channel capacity;computational complexity;decoding","space complexity;low-complexity decoding architecture;rate-compatible puncturing polar codes;high speed transmission;efficient decoding architecture;different puncturing methods;channel capacity;5G communications;quasiuniform puncturing;log-likelihood ratio","","","","12","","","","","IEEE","IEEE Conferences"
"Low complexity multiview video plus depth coding","Q. Zhang; P. An; Y. Zhang; L. Shen; Z. Zhang","Shanghai University, China; Shanghai University, China; Shanghai University, China; Shanghai University, China; Shanghai University, China","IEEE Transactions on Consumer Electronics","","2011","57","4","1857","1865","Multiview video plus depth (MVD) is a new 3D video format that would support 3D applications developed by MPEG. Such a format is a combination of texture video and associated depth maps. Consequently, for the efficient transmission of 3D video signals, the compression of texture video and also the depth maps is required. Since high computational complexity of jointly coding between the texture video and depth map is still an open question, this paper introduces a low complexity MVD coding algorithm that adaptively utilizes the texture and depth map correlation. Based on the correlation, we propose four efficient techniques, including depth information based fast mode size decision, adaptive disparity estimation in texture coding, motion vector sharing based on the texture image similarity correlation and the SKIP mode decision in depth coding. Experimental results show that the proposed algorithm can significantly reduce the computational complexity of MVD coding while improve the coding performance and achieve better rendering quality<sup>1</sup>.","0098-3063;1558-4127","","10.1109/TCE.2011.6131164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131164","3D video coding;multiview video plus depth(MVD);texture and depth map correlation;depth map coding.","Encoding;Vectors;Three dimensional displays;Complexity theory;Correlation;Video coding;Estimation","computational complexity;image motion analysis;image texture;video coding","low complexity multiview video plus depth coding;3D video format;MPEG;texture video;3D video signals;low complexity MVD coding algorithm;depth map correlation;depth information based fast mode size decision;adaptive disparity estimation;texture coding;motion vector sharing;texture image similarity correlation;SKIP mode decision;computational complexity","","33","","26","","","","","IEEE","IEEE Journals & Magazines"
"Reducing 3D video coding complexity through more efficient disparity estimation","B. W. Micallef; C. J. Debono; R. A. Farrugia","Member; Department of Communications and Computer Engineering (CCE), University of Malta, Msida, MSD 2080, Malta; Senior Member; Department of CCE, University of Malta, Msida, MSD 2080, Malta; Member; Department of CCE, University of Malta, Msida, MSD 2080, Malta","IEEE Transactions on Consumer Electronics","","2014","60","1","74","82","3D video coding for transmission exploits the Disparity Estimation (DE) to remove the inter-view redundancies present within both the texture and the depth map multi-view videos. Good estimation accuracy can be achieved by partitioning the macro-block into smaller subblocks partitions. However, the DE process must be performed on each individual sub-block to determine the optimal mode and their disparity vectors, in terms of rate-distortion efficiency. This vector estimation process is heavy on computational resources, thus, the coding computational cost becomes proportional to the number of search points and the inter-view modes tested during the rate-distortion optimization. In this paper, a solution that exploits the available depth map data, together with the multi-view geometry, is proposed to identify a better DE search area; such that it allows a reduction in its search points. It also exploits the number of different depth levels present within the current macro-block to determine which modes can be used for DE to further reduce its computations. Simulation results demonstrate that this can save up to 95% of the encoding time, with little influence on the coding efficiency of the texture and the depth map multi-view video coding. This makes 3D video coding more practical for any consumer devices, which tend to have limited computational power.","0098-3063;1558-4127","","10.1109/TCE.2014.6780928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6780928","3D video coding;fast disparity estimation;fast mode selection;disparity estimation;multi-view video coding","Encoding;Estimation;Video coding;Vectors;Three-dimensional displays;Voltage control;Computational efficiency","image texture;video coding","3D video coding complexity reduction;disparity estimation;interview redundancy;image texture;image depth;multiview video;optimal mode;disparity vector;vector estimation;depth map data;multiview geometry;search point reduction","","4","","36","","","","","IEEE","IEEE Journals & Magazines"
"Coded Modulation for Fiber-Optic Networks: Toward better tradeoff between signal processing complexity and optical transparent reach","L. Beygi; E. Agrell; J. M. Kahn; M. Karlsson","Department of Signals and Systems, Chalmers University of Technology, Gothenburg, 412 96, SWEDEN; Department of Signals and Systems, Chalmers University of Technology, Gothenburg, 412 96, SWEDEN; Department of Electrical Engineering, Stanford University, Stanford, California 94305 USA; Dept of Microtechnology and Nanoscience, Chalmers University of Technology, Gothenburg, 412 96, SWEDEN","IEEE Signal Processing Magazine","","2014","31","2","93","103","In this tutorial, we study the joint design of forward error correction (FEC) and modulation for fiber-optic communications. To this end, we use an information-theoretic design framework to investigate coded modulation (CM) techniques for standard additive white Gaussian noise (AWGN) channels and fiber-optic channels. This design guideline helps us provide a comprehensive overview of the CM schemes in the literature. Then, by invoking recent advances in optical channel modeling for nondispersion-managed links, we discuss two-dimensional (2-D) and four-dimensional (4-D) CM schemes. Moreover, we discuss the electronic computational complexity and hardware constraints of CM schemes for optical communications. Finally, we address CM schemes with signal shaping and rate-adaptation capabilities to accommodate the data transmission scheme to optical links with different signal qualities.","1053-5888;1558-0792","","10.1109/MSP.2013.2290805","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6739216","","Decoding;Optical fiber polarization;Optical fiber networks;Optical fiber dispersion;Optical fiber amplifiers;Modulation;Digital signal processing","AWGN;forward error correction;modulation coding;optical fibre dispersion;optical fibre networks","nondispersion managed links;optical channel modeling;fiber optic channels;AWGN channel;standard additive white Gaussian noise;information theoretic design;fiber optic communication;FEC;forward error correction;optical transparent reach;signal processing complexity;fiber optic network;coded modulation","","19","","30","","","","","IEEE","IEEE Journals & Magazines"
"Code Constructions for Distributed Storage With Low Repair Bandwidth and Low Repair Complexity","S. Kumar; A. Graell i Amat; I. Andriyanova; F. Brännström; E. Rosnes","Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; ETIS-UMR8051 Group, ENSEA/University of Cergy-Pontoise/CNRS, Cergy, France; Department of Electrical Engineering, Chalmers University of Technology, Gothenburg, Sweden; Simula@UiB, Bergen, Norway","IEEE Transactions on Communications","","2018","66","12","5847","5860","We present the construction of a family of erasure correcting codes for distributed storage which achieve low repair bandwidth and complexity at the expense of a lower fault tolerance. The construction is based on two classes of codes, where the primary goal of the first class of codes is to provide fault tolerance, while the second class aims at reducing the repair bandwidth and repair complexity. The repair procedure is a two-step procedure where parts of the failed node are repaired in the first step using the first code. The downloaded symbols during the first step are cached in the memory and used to repair the remaining erased data symbols at minimal additional read cost during the second step. The first class of codes is based on maximum distance separable (MDS) codes modified using piggybacks, while the second class is designed to reduce the number of additional symbols that need to be downloaded to repair the remaining erased symbols. We numerically show that the proposed codes achieve better repair bandwidth compared to MDS codes, codes constructed using piggybacks, and local reconstruction/Pyramid codes, while a better repair complexity is achieved when compared to MDS, Zigzag, Pyramid codes, and codes constructed using piggybacks.","0090-6778;1558-0857","","10.1109/TCOMM.2018.2858765","Norges Forskningsråd; Simula@UiB; Vetenskapsrådet; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418386","Code concatenation;codes for distributed storage;piggybacking;repair bandwidth;repair complexity","Maintenance engineering;Bandwidth;Complexity theory;Fault tolerant systems;Storage management;Systematics","block codes;error correction codes;fault tolerance;forward error correction;linear codes;storage management","code constructions;distributed storage;low repair bandwidth;low repair complexity;erasure correcting codes;repair procedure;maximum distance separable codes;MDS codes;fault tolerance;erased data symbols;local reconstruction codes;Pyramid codes","","","","30","","","","","IEEE","IEEE Journals & Magazines"
"Reduced-complexity decoding of high-rate LDPC codes over partial-response channels","Z. Qin; K. Cai; S. Zhang","Data Storage Institute, Singapore, 117608; Data Storage Institute, Singapore, 117608; Data Storage Institute, Singapore, 117608","2010 IEEE International Conference on Communication Systems","","2010","","","777","781","In this paper, we consider high-rate low-density parity-check (LDPC) codes for magnetic recording systems. We propose an efficient approach to reduce the computational complexity of the sum-product algorithm (SPA) for LDPC decoding by restricting the check operation to a small subset of input messages. Furthermore, the first-order MacLaurin Series is used to simplify the core operation and an attenuation factor is introduced to improve bit-error-rate (BER) performance by scaling down log-likelihood ratio (LLR) reliabilities in the decoding process. Simulation results show that the proposed decoder can obtain a noticeable performance gain over the conventional SPA at high signal-to-noise ratios (SNR) when used in turbo equalization schemes; while achieving a significantly lower computational complexity.","","978-1-4244-7006-8978-1-4244-7004-4978-1-4244-7005","10.1109/ICCS.2010.5685882","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5685882","Low-Density Parity-Check (LDPC) Codes;Sum-Product Algorithm (SPA);Turbo Equalization","Decoding;Attenuation;Bit error rate;Iterative decoding;Signal to noise ratio;Approximation methods","channel coding;computational complexity;decoding;error statistics;magnetic recording;parity check codes","reduced-complexity decoding;high-rate LDPC codes;partial-response channels;high-rate low-density parity-check codes;magnetic recording systems;computational complexity;sum-product algorithm;SPA;LDPC decoding;first-order MacLaurin series;attenuation factor;bit-error-rate;BER performance;log-likelihood ratio reliability;decoding process;signal-to-noise ratios;turbo equalization schemes","","1","","14","","","","","IEEE","IEEE Conferences"
"A performance and complexity optimization of joint code and frame synchronization for DVB-S2/RCS mobile","G. Gabelli; C. Palestini; S. Cioni; A. Vanelli-Coralli; G. E. Corazza","University of Bologna, DEIS-ARCES, Viale Risorgimento, 2 - 40136 Bologna, Italy; University of Bologna, DEIS-ARCES, Viale Risorgimento, 2 - 40136 Bologna, Italy; University of Bologna, DEIS-ARCES, Viale Risorgimento, 2 - 40136 Bologna, Italy; University of Bologna, DEIS-ARCES, Viale Risorgimento, 2 - 40136 Bologna, Italy; University of Bologna, DEIS-ARCES, Viale Risorgimento, 2 - 40136 Bologna, Italy","2010 5th Advanced Satellite Multimedia Systems Conference and the 11th Signal Processing for Space Communications Workshop","","2010","","","190","197","This paper analyzes the possibility to perform joint frame synchronization and code acquisition for the DVB-RCS mobile system. Since the standard foresees the adoption of Direct Sequence Spread Spectrum techniques, novel detectors able to cope with the scenario at hand are proposed. Comparison in terms of Mean Acquisition Time and complexity are carried out, and the best solution in terms of performance / complexity tradeoff is identified.","2329-7093;2326-5949","978-1-4244-6833-1978-1-4244-6831-7978-1-4244-6832","10.1109/ASMS-SPSC.2010.5586906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5586906","","Digital video broadcasting;Detectors;Synchronization;AWGN;Correlation;Complexity theory;Satellites","codes;digital video broadcasting;mobile satellite communication;spread spectrum communication;synchronisation","performance optimization;complexity optimization;joint code-frame synchronization;DVB-RCS mobile system;code acquisition;direct sequence spread spectrum techniques;mean acquisition time","","","","10","","","","","IEEE","IEEE Conferences"
"Analysis of H.265/HEVC, H.264 and VP9 coding efficiency based on video content complexity","L. Mengzhe; J. Xiuhua; L. Xiaohua","School of Information Engineering, Communication University of China, Beijing, China; School of Information Engineering, Communication University of China, Beijing, China; School of Information Engineering, Communication University of China, Beijing, China","2015 IEEE International Conference on Computer and Communications (ICCC)","","2015","","","420","424","The main goal of this paper is to find the relationship between video content complexity and the objective quality, further to compare H.265/HEVC, H.264/AVC, and VP9 encoders in three modes based on complexity of the video content. There are three evaluation metrics: Peak Signal to Noise Ratio (PSNR), structural similarity (SSIM), and encoding time, which were introduced to compare the three video encoders comprehensively. Each video sequence has 250 frames, and their resolution is 1920×1080, differing in spatial and temporal complexity. An extensive range of video content complexity were selected in the video sequences. Objective quality evaluation based on the calculation of spatio-temporal activities is tested to validate its suitability as video content complexity indicator in quality assessment. According to the analysis of experiment results, all the encoded modes present an obvious same trend. On a fixed video resolution and bit rate, with the video complexity increasing, the objective quality is decreasing and the encoding times is increasing. It was also concluded that the objective quality of X265-slow is far better than other encoder modes of X264 and VP9. In terms of encoding time, the encoding time of H264 was shown to be less than both HEVC and VP9.","","978-1-4673-8126-0978-1-4673-8125-3978-1-4673-8124","10.1109/CompComm.2015.7387608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387608","video encoder;complexity of video content;objective quality;encoding time","Encoding;Complexity theory;Streaming media;Measurement;Video coding;Silicon;Video sequences","video coding","H.265 coding efficiency;HEVC coding efficiency;H.264 coding efficiency;VP9 coding efficiency;video content complexity;Peak Signal to Noise Ratio;PSNR;structural similarity;SSIM;encoding time;video quality evaluation;spatio temporal activities;video resolution;video complexity","","7","","10","","","","","IEEE","IEEE Conferences"
"Model-based complexity-aware coding for multiview video plus depth","V. Nguyen; M. N. Do","Advanced Digital Sciences Center (ADSC), Singapore; University of Illinois at Urbana-Champaign, IL, USA","2013 IEEE International Conference on Image Processing","","2013","","","2192","2196","This paper presents an efficient complexity-aware coding method for the popular 3-D video format, multiview video plus depth (MVD). Specifically, we propose a complexity-rate-distortion (C-R-D) model for the synthesized view. The proposed C-R-D model is then utilized to efficiently allocate the scarce computational resource for the texture and depth encoders to achieve the optimal quality of the synthesized view. Experimental results show the effectiveness of the proposed model and complexity allocation solution. Compared with the straightforward complexity allocation method with a fixed 1:1 ratio, the proposed method provided 0.2-0.9 dB gain in the peak-signal-to-noise ratio (PSNR) of the synthesized view quality.","1522-4880;2381-8549","978-1-4799-2341","10.1109/ICIP.2013.6738452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738452","3D video;complexity-aware;distortion model;resource allocation","","distortion;image texture;video coding","model-based complexity-aware coding;3D video format;multiview video plus depth;MVD;complexity-rate-distortion model;C-R-D model;synthesized view optimal quality;computational resource allocation;texture encoders;depth encoders;complexity allocation solution;peak-signal-to-noise ratio;PSNR","","","","13","","","","","IEEE","IEEE Conferences"
"Low complexity distributed video coding using compressed sensing","S. Roohi; M. Noorhosseini; J. Zamani; H. S. Rad","Dept. of Computer Arts, Islamic Art University of Tabriz, Tabriz; Dept. of Computer Engineering and Information Technology, Amirkabir University of Technology, Tehran, Iran; Dept. of Biomedical Engineering, Amirkabir University of Technology, Tehran, Iran; Dept. of Medical Physics and Biomedical Engineering Tehran University of Medical Science (TUMS), And Research Center for Science and Technology in Medicine (RCSTIM), Imam Khomeini Hospital Complex, Keshavarz Blvd Tehran, Iran","2013 8th Iranian Conference on Machine Vision and Image Processing (MVIP)","","2013","","","53","57","Compressive sensing (CS) is an efficient method to reconstruct sparse images with under-sampled data. In this method sensing and coding steps integrated to a one-step, low-complexity measurement acquisition system. In this paper, we use a Non-linear Conjugate Gradient (NLCG) algorithm to significantly increase the quality of reconstructed frames of video sequences. Our proposed framework divides sequence of a video to several groups of pictures (GOPs), where each GOP consisting of one key frame followed by two non-key frames. CS is then applied on each key and non-key frame with different sampling rates. For reconstruction final frames, NLCG algorithm was performed on each key frame with acceptable fidelity. To achieve desired quality on low-rate sampled non-key frames, NLCG modified using side information (SI) obtained from last two successive reconstructed key frames. Based on some performance measures such as SNR, PSNR, SSIM and RSE, our implementation results indicate that employing NLCG with Gaussian sampling matrix outperforms other methods in quality measures.","2166-6784;2166-6776","978-1-4673-6184-2978-1-4673-6182","10.1109/IranianMVIP.2013.6779949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779949","compressed sensing (CS);distributed video coding (DVC);sparse reconstruction;nonlinear conjugate gradient (NLCG)","Integrated circuits;Machine vision;Artificial intelligence","compressed sensing;conjugate gradient methods;image reconstruction;sparse matrices;video coding","low complexity distributed video coding;compressed sensing;nonlinear conjugate gradient algorithm;NLCG;groups of pictures;GOP;PSNR;SSIM;RSE;Gaussian sampling matrix","","2","","18","","","","","IEEE","IEEE Conferences"
"Low complexity technique exploiting 2D source correlation using single parity check codes","M. A. M. Izhar; H. M. Kaidi; R. A. Dziyauddin; N. Ahmad","UTM-MIMOS Center of Excellence, Universiti Teknologi Malaysia (UTM) 81310 Skudai, Johor, Malaysia; UTM-MIMOS Center of Excellence, Universiti Teknologi Malaysia (UTM) 81310 Skudai, Johor, Malaysia; UTM-MIMOS Center of Excellence, Universiti Teknologi Malaysia (UTM) 81310 Skudai, Johor, Malaysia; UTM-MIMOS Center of Excellence, Universiti Teknologi Malaysia (UTM) 81310 Skudai, Johor, Malaysia","2014 IEEE 2nd International Symposium on Telecommunication Technologies (ISTT)","","2014","","","115","120","This paper proposes a low complexity joint source-channel coding (JSCC) technique that can well exploit two-dimensional (2D) source correlation using turbo single parity check (SPC) codes. Based on similar structure of the product accumulate code, a rate-1 inner code is serially concatenated to the turbo SPC code structure in order to overcome the high error floor issue of turbo SPC codes. The 2D source correlation, horizontal and vertical directions of the source are exploited during the decoding process. In order to exploit the source statistics (i.e. transition probabilities), a decoding algorithm based on modified check operation is proposed. This modified check operation has significantly lower computational complexity than the previous technique that based on modified Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm. Simulation results reveal that the proposed technique that based on modified check operation achieves about 4.5 times lower computational complexity than the technique that based on modified BCJR algorithm. In addition to the lower computational complexity, the proposed JSCC technique can achieve optimal performance, similar with the technique that based on modified BCJR algorithm. The interesting features of low computational complexity and high performance of the proposed technique is important in designing JSCC system exploiting higher dimensional source correlation.","","978-1-4799-5982-2978-1-4799-5981","10.1109/ISTT.2014.7238188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238188","","Decoding;Correlation;Computational complexity;Turbo codes;Bit error rate;Iterative decoding","combined source-channel coding;computational complexity;correlation theory;decoding;parity check codes;statistical analysis;turbo codes","2D source correlation;low complexity joint source-channel coding technique;low complexity JSCC technique;turbo single parity check code;turbo SPC code structure;product accumulate code;decoding process;source statistics;modified BCJR algorithm;modified Bahl-Cocke-Jelinek-Raviv algorithm;low computational complexity","","1","","18","","","","","IEEE","IEEE Conferences"
"Low Complexity and Provably Efficient Algorithm for Joint Inter and Intrasession Network Coding in Wireless Networks","A. Khreishah; I. Khalil; J. Wu","Temple University, Philadelphia; United Arab Emirates University, Al Ain; Temple University, Philadelphia","IEEE Transactions on Parallel and Distributed Systems","","2013","24","10","2015","2024","The performance of wireless networks can be enhanced by performing network coding on the intermediate relay nodes. To enhance the throughput of large wireless networks, we can decompose them into a superposition of simple relay networks called two-hop relay networks. Previously, the capacity region of two-hop relay networks with multiple unicast sessions and limited feedback was characterized where packet erasure channels are used. A near-optimal coding scheme that exploits the broadcast nature and the diversity of the wireless links was proposed. However, the complexity of the scheme is exponential in terms of the number of sessions, as it requires the knowledge of the packets that are received by any subset of the receivers. In this paper, we provide a polynomial time coding scheme and characterize its performance using linear equations. The coding scheme uses random network coding to carefully mix intra and intersession network coding and makes a linear, not exponential, number of decisions. For two-hop relay networks with two sessions, we provide an optimal coding scheme that does not require the knowledge of the channel conditions. We also provide a linear programming formulation that uses our two-hop relay network results as a building block in large lossy multihop networks.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2012.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244791","Network coding;lossy wireless networks;two-hop relay networks;capacity;fairness","Relays;Encoding;Network coding;Wireless networks;Joints;Throughput;Complexity theory","linear programming;network coding;packet radio networks;polynomials;radio links;relay networks (telecommunication)","provably efficient algorithm;low complexity algorithm;joint intersession network coding;joint intrasession network coding;wireless networks;intermediate relay nodes;multiple unicast sessions;two-hop relay networks;packet erasure channels;near-optimal coding scheme;broadcast nature;wireless links;polynomial time coding scheme;linear equations;optimal coding scheme;building block;large lossy multihop networks;linear programming formulation","","6","","27","","","","","IEEE","IEEE Journals & Magazines"
"Low-Complexity Image and Video Coding Based on an Approximate Discrete Tchebichef Transform","P. A. M. Oliveira; R. J. Cintra; F. M. Bayer; S. Kulasekera; A. Madanayake","Departamento de Estatística, Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil; Departamento de Estatística, Signal Processing Group, Universidade Federal de Pernambuco, Recife, Brazil; Departamento de Estatística and Laboratório de Ciências Espaciais de Santa Maria (LACESM), Universidade Federal de Santa Maria, Santa Maria, Brazil; Department of Electrical and Computer Engineering, University of Akron, Akron, OH, USA; Department of Electrical and Computer Engineering, University of Akron, Akron, OH, USA","IEEE Transactions on Circuits and Systems for Video Technology","","2017","27","5","1066","1076","The usage of linear transformations has great relevance for data decorrelation applications, like image and video compression. In that sense, the discrete Tchebichef transform (DTT) possesses useful coding and decorrelation properties. The DTT transform kernel does not depend on the input data and fast algorithms can be developed to real-time applications. However, the DTT fast algorithm presented in literature possess high computational complexity. In this paper, we introduce a new low-complexity approximation for the DTT. The fast algorithm of the proposed transform is multiplication free and requires a reduced number of additions and bit-shifting operations. Image and video compression simulations in popular standards show good performance of the proposed transform. Regarding hardware resource consumption for FPGA shows a 43.1% reduction in configurable logic blocks and ASIC place and route realization shows a 57.7% reduction in the area-time figure compared with the 2D version of the exact DTT.","1051-8215;1558-2205","","10.1109/TCSVT.2016.2515378","Brazilian Agencies, such as Conselho Nacional de Desenvolvimento Científico e Tecnológico, Fundação do Amparo a Ciência e Tecnologia, and Fundação de Amparo à Pesquisa do Estado do Rio Grande do Sul; University of Akron, Akron, OH, USA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373604","Approximate transforms;discrete Tchebichef transform (DTT);fast algorithms;image and video coding","Approximation methods;Image coding;Complexity theory;Discrete cosine transforms;Approximation algorithms;Video coding","application specific integrated circuits;discrete transforms;field programmable gate arrays;video coding","low-complexity image coding;low-complexity video coding;approximate discrete Tchebichef transform;linear transformations;DTT transform kernel;image compression;video compression;hardware resource consumption;FPGA;configurable logic blocks;ASIC place and route realization","","9","","73","","","","","IEEE","IEEE Journals & Magazines"
"CTU-Level Complexity Control for High Efficiency Video Coding","J. Zhang; S. Kwong; T. Zhao; Z. Pan","Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; College of Physics and Information Engineering, Fuzhou University, Fuzhou, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Multimedia","","2018","20","1","29","44","Among the existing video-related applications, a large proportion have requirements for the scalability of the video coding complexity, such as live video chatting and video coding on power-limited mobile devices. Hence, the complexity control algorithms, which aim to make an effective and flexible tradeoff between coding complexity and rate-distortion (RD) performance, have a great practical value. In this paper, a novel complexity control scheme for high efficiency video coding (HEVC) is proposed by dynamically adjusting the depth range for each coding tree unit (CTU). To control the complexity accurately, a statistical model is proposed to estimate the coding complexity of each CTU. Then the complexity budget is allocated to each CTU proportionally to its estimated complexity. At last, the depth range is optimized for each CTU based on the allocated complexity and the probability that contains the actual maximum depth. Our method works well even if the ratio of target complexity to full complexity drops to 40%. The experimental results show that our proposed method outperforms other four state-of-the-art methods in terms of the RD performance, and has superior complexity control accuracy and complexity control stability compared with other one-pass complexity control strategies.","1520-9210;1941-0077","","10.1109/TMM.2017.2723238","Natural Science Foundation of China; Hong Kong RGC General Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967832","Coding tree unit (CTU);complexity allocation;complexity control;high efficiency video coding (HEVC)","Complexity theory;Encoding;Video coding;Streaming media;Copper;Resource management;Optimization methods","video coding","CTU-level complexity control;high efficiency video coding;video coding complexity;live video chatting;complexity control algorithms;rate-distortion performance;depth range;coding tree unit;complexity budget;estimated complexity;allocated complexity;target complexity;superior complexity control accuracy;complexity control stability;one-pass complexity control strategies;video-related applications;complexity control scheme;RD performance","","6","","38","Traditional","","","","IEEE","IEEE Journals & Magazines"
"Complexity-scalable video coding and power-rate-distortion modeling forwireless video chat applications","S. Chuah; Z. Chen; Y. Tan","School of EEE, Nanyang Tech. Univ., Singapore; School of EEE, Nanyang Tech. Univ., Singapore; Wuhan Univ., China","2013 Visual Communications and Image Processing (VCIP)","","2013","","","1","6","Wireless video chat is a power-consuming and of high bitrate application. To prolong the operational lifetime, optimal rate and power allocations in joint coding and transmission are necessary. We exploit low motion and high inter-frame correlation in video chats to determine a complexity-scalable video coding adaptation which is Pareto optimal. We propose a model that describes power-rate-distortion (PRD) characteristic of the complexity scalable video coding more accurately. As video contents are non-stationary, we formulate an online algorithm for the model's parameters updates. We demonstrate that the PRD model with online updates can be applied to solve the rate and power allocation problem optimally in joint coding and transmission. Simulation results confirm that the model describes PRD characteristics more accurately via online recursive updates. The resource allocation scheme which is based on the PRD model yields better video quality than recent methods in a resource-constrained wireless video chat application.","","978-1-4799-0290-3978-1-4799-0288","10.1109/VCIP.2013.6706331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706331","complexity scalability;video coding;rate and power allocation;wireless video chats","Encoding;Complexity theory;Video coding;Resource management;Wireless communication;Adaptation models;Joints","resource allocation;video coding;video communication","complexity scalable video coding;power rate distortion modeling;wireless video chat applications;high bitrate application;operational lifetime;optimal rate;power allocations;joint coding and transmission;low motion correlation;high interframe correlation;Pareto optimal;online recursive updates;resource allocation scheme;video quality","","","","9","","","","","IEEE","IEEE Conferences"
"Low Complexity Demapping Algorithms for Multilevel Codes","G. Gul; A. Vargas; W. H. Gerstacker; M. Breiling","Signal Processing Group, Institute of Telecommunications, Technische Universitat Darmstadt, 64283 Darmstadt, Germany; Fraunhofer Institute for Integrated Circuits (IIS), Am Wolfsmantel 33, D-91058 Erlangen, Germany; Universitat Erlangen-Nurnberg, Chair of Mobile Communications, Cauerstras e 7, D-91058 Erlangen, Germany; Fraunhofer Institute for Integrated Circuits (IIS), Am Wolfsmantel 33, D-91058 Erlangen, Germany","IEEE Transactions on Communications","","2011","59","4","998","1008","In order to reduce the computational complexity of maximum-likelihood symbol estimation (MLSE) demapping of multilevel codes which is based on block partitioning and which produces soft input for a multistage decoding (MSD) process, three different demapping algorithms are proposed. It is theoretically proven that the proposed algorithms can reduce exponentially increasing computational complexity of the MLSE demapping algorithm to a constant complexity (neglecting comparisons). It is shown by extensive simulations for AWGN and Rayleigh fading channels that the proposed low complexity demapping algorithms can achieve near MLSE performance.","0090-6778;1558-0857","","10.1109/TCOMM.2011.020411.090497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5715838","Multilevel codes;multistage decoding;maximum likelihood symbol estimation;log likelihood ratio","Complexity theory;Decoding;Modulation;Maximum likelihood estimation;Approximation methods;Encoding;Approximation algorithms","AWGN;codes;computational complexity;maximum likelihood estimation;Rayleigh channels","low complexity demapping algorithms;multilevel codes;computational complexity;maximum likelihood symbol estimation;AWGN;Rayleigh fading channels","","7","","14","","","","","IEEE","IEEE Journals & Magazines"
"Decoding complexity of irregular LDGM-LDPC codes over the BISOM channels","M. Raina; P. Spasojević","WINLAB, Rutgers University, North Brunswick, NJ, 08901, USA; WINLAB, Rutgers University, North Brunswick, NJ, 08901, USA","2010 44th Annual Conference on Information Sciences and Systems (CISS)","","2010","","","1","6","An irregular LDGM-LDPC code is studied as a sub-code of an LDPC code with some randomly punctured output-bits. It is shown that the LDGM-LDPC codes achieve rates arbitrarily close to the channel-capacity of the binary-input symmetric-output memoryless (BISOM) channel with bounded complexity. The measure of complexity is the average-degree (per information-bit) of the check-nodes for the factor-graph of the code. A lower-bound on the average degree of the check-nodes of the irregular LDGM-LDPC codes is obtained. The bound does not depend on the decoder used at the receiver. The stability condition for decoding the irregular LDGM-LDPC codes over the binary-erasure channel (BEC) under iterative-decoding with message-passing is described.","","978-1-4244-7417-2978-1-4244-7416","10.1109/CISS.2010.5464840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5464840","Bounded-complexity-codes;iterative-decoding;capacity-approaching-codes","Parity check codes;Iterative decoding;Stability;Capacity planning;Memoryless systems;Channel capacity","channel coding;communication complexity;graph theory;iterative decoding;parity check codes","decoding complexity;irregular LDGM-LDPC codes;BISOM channels;randomly punctured output-bits;channel capacity;binary-input symmetric-output memoryless channel;bounded complexity;complexity measure;factor graph;lower bound;stability condition;binary-erasure channel;iterative decoding;message passing","","1","","16","","","","","IEEE","IEEE Conferences"
"Interleaved LDPC codes, reduced-complexity inner decoder and an iterative decoder for the Davey-MacKay construction","Xiaopeng Jiao; M. A. Armand","Communications and Information Engineering Group, Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576; Communications and Information Engineering Group, Department of Electrical and Computer Engineering, National University of Singapore, Singapore 117576","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","742","746","The inner decoder of the Davey-MacKay (DM) construction for combating insertions, deletions and substitution errors, has high complexity and produces bursts of output likelihoods of greatest uncertainty in the vicinity of insertions and deletions. We therefore propose (i) a lookup-table-based implementation of the inner decoder to reduce its complexity, (ii) the use of interleaved LDPC codes as outer codes in the DM construction to spread the uncertain likelihoods produced by the inner decoder over several constituent LDPC codewords. Simulation results show that the proposed lookup table approach reduces the complexity of the inner decoder considerably while a significant improvement in frame error rate (FER) performance can be obtained with small interleaving depths. Our lookup table approach culminates in an iterative decoding scheme which yields improved FER performance over its non-iterative counterparts, yet with only a modest increase in decoding complexity, when the insertion/deletion probability is small.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6034232","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034232","","Decoding;Iterative decoding;Complexity theory;Delta modulation;Synchronization","interleaved codes;iterative decoding;parity check codes;probability;table lookup","interleaved LDPC codes;reduced-complexity inner decoder;iterative decoder;Davey-MacKay construction;lookup-table;uncertain likelihoods;LDPC codewords;frame error rate;decoding complexity;insertion/deletion probability","","4","","8","","","","","IEEE","IEEE Conferences"
"Integrating Sparsity into Fulcrum Codes: Investigating Throughput, Complexity and Overhead","V. Nguyen; G. T. Nguyen; F. Gabriel; D. E. Lucani; F. H. P. Fitzek","NA; NA; NA; NA; NA","2018 IEEE International Conference on Communications Workshops (ICC Workshops)","","2018","","","1","6","Real-world communication systems consist of heterogeneous devices and network nodes with diverse computation capabilities. Even though it can improve the system's throughput and end-to-end delay, the use of a network code such as RLNC in communication networks may be inefficient. The coding complexity of using a single finite field along the communication path can become too computationally demanding for some nodes and eventually degrade the end-to-end performance. Fulcrum network codes employ a combination of a large and a small finite field in the encoding process and subsequently allows intermediate and end nodes to select, depending on their computation power, the field size they want to operate. However, it is still unclear how to reduce Fulcrum's decoding complexity especially for a high generation size. In this paper, we integrate sparsity at the encoding process, meaning to reduce the number of non-zero coefficients, resulting in three Fulcrum variations and conduct a thorough performance evaluation. Our simulation results show that sparsity significantly improves Fulcrum codes, increasing encoding and decoding speeds by 20x and 1.8, respectively, while maintaining a low overhead. This gains are on top of the gains provided by Fulcrum codes over RLNC.","2474-9133","978-1-5386-4328-0978-1-5386-4329","10.1109/ICCW.2018.8403538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8403538","","Encoding;Decoding;Throughput;Receivers;Computational complexity;Network coding","5G mobile communication;decoding;linear codes;network coding;random codes;synchronisation","Fulcrum network codes;encoding process;Fulcrum's decoding complexity;real-world communication systems;heterogeneous devices;network nodes;end-to-end delay;random linear network coding;RLNC","","2","","12","","","","","IEEE","IEEE Conferences"
"Low-complexity fixed-to-fixed joint source-channel coding","I. E. Bocharova; A. Guillén i Fabregas; B. D. Kudryashov; A. Martinez; A. T. Campo; G. Vazquez-Vilar","St. Petersburg Univ. of Information Technologies; Universitat Pompeu Fabra; St. Petersburg Univ. of Information Technologies; Universitat Pompeu Fabra; Universitat Pompeu Fabra; Universitat Pompeu Fabra","2014 8th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)","","2014","","","132","136","A source-channel coding scheme in which source messages are assigned to two classes and encoded using a channel code that depends on the class index is studied. A low-complexity implementation with two quasi-cyclic LDPC codes with belief-propagation decoding achieves a better frame error rate than optimized separate coding. The coding gain obtained by simulation is consistent with the theoretical gain.","2165-4719;2165-4700","978-1-4799-5985","10.1109/ISTC.2014.6955100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955100","","Decoding;Channel coding;Iterative decoding;Joints;Complexity theory","combined source-channel coding;computational complexity;cyclic codes;error statistics;parity check codes","joint source-channel coding scheme;fixed-to-fixed codes;coding gain;belief-propagation decoding;quasicyclic LDPC codes;low-complexity implementation;source messages","","1","","14","","","","","IEEE","IEEE Conferences"
"Frame Complexity Guided Lagrange Multiplier Selection for H.264 Intra-Frame Coding","C. Pang; O. C. Au; J. Dai; F. Zou","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Signal Processing Letters","","2011","18","12","733","736","Rate-distortion (R-D) optimized mode decision plays an essential role in H.264/AVC encoding. Among all the possible coding modes, it aims to select the one which has the best trade-off between bitrate and compression distortion. Specifically, this tradeoff is tuned through the choice of the Lagrange multiplier. However, in the H.264 reference software, the value of Lagrange multiplier is only determined by the quantization stepsize, with no consideration of the characteristics of the input signal. In this paper, we address the problem of optimal Lagrange determination for intra-frames in a signal dependent environment, and a novel frame complexity guided Lagrange multiplier selection method is introduced. Our contributions are two-fold. First, we propose a novel signal dependent R-D model which uses the average input frame gradient to measure the signal complexity. Second, based on the proposed signal dependent R-D model, we derive the closed-form solution of the optimal Lagrange multiplier. Experimental results suggest that up to 8.18% bitrate reduction can be obtained with negligible additional computation complexity.","1070-9908;1558-2361","","10.1109/LSP.2011.2172940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062565","Frame complexity;Lagrange multiplier;mode decision;<formula formulatype=""inline""><tex Notation=""TeX"">${\rm R}$</tex></formula>-<formula formulatype=""inline""><tex Notation=""TeX"">${\rm D}$</tex></formula> optimization","Encoding;Lagrangian functions;MPEG 4 Standard;Quantization;Video coding;Video sequences","computational complexity;optimisation;quantisation (signal);video coding","H.264 intra-frame coding;rate-distortion optimized mode decision;R-D optimized mode decision;H.264/AVC encoding;coding modes;compression distortion;H.264 reference software;quantization stepsize;optimal Lagrange determination;signal dependent environment;frame complexity guided Lagrange multiplier selection method;signal dependent R-D model;average input frame gradient;signal complexity;closed-form solution;optimal Lagrange multiplier;bitrate reduction;negligible additional computation complexity","","4","","14","","","","","IEEE","IEEE Journals & Magazines"
"Low complexity algorithm approaching the ML decoding of binary LDPC codes","I. E. Bocharova; B. D. Kudryashov; V. Skachek; Y. Yakimenka","Department of Information Systems, Petersburg University of Information Technologies, Mechanics and Optics, 197101, Russia; Department of Information Systems, Petersburg University of Information Technologies, Mechanics and Optics, 197101, Russia; University of Tartu, 50409, Estonia; University of Tartu, 50409, Estonia","2016 IEEE International Symposium on Information Theory (ISIT)","","2016","","","2704","2708","A novel method for decoding of low-density parity-check codes on the AWGN channel is presented. In the proposed method, first, a standard belief-propagation decoder is applied, then a certain number of positions is erased using a combination of a reliability criterion and a set of masks. A list erasure decoder is then applied to the resulting word. The performance of the proposed method is analyzed mathematically and demonstrated by simulations.","2157-8117","978-1-5090-1806-2978-1-5090-1807","10.1109/ISIT.2016.7541790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7541790","","Maximum likelihood decoding;Parity check codes;Reliability;Light emitting diodes;AWGN channels;Algorithm design and analysis","AWGN channels;parity check codes;telecommunication network reliability","ML decoding;low density parity check codes;binary LDPC codes;AWGN channel;beliefpropagation decoder;erasure decoder","","6","","14","","","","","IEEE","IEEE Conferences"
"On Low-Complexity Decoding of Product Codes for High-Throughput Fiber-Optic Systems","A. Sheikh; A. Graell i Amat; G. Liva; C. Hager; H. D. Pfister","Department of Electrical Engineering, Chalmers University of Technology, Sweden; Department of Electrical Engineering, Chalmers University of Technology, Sweden; Institute of Communications and Navigation of the German Aerospace Center (DLR), Germany; Department of Electrical Engineering, Chalmers University of Technology, Sweden; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA","2018 IEEE 10th International Symposium on Turbo Codes & Iterative Information Processing (ISTC)","","2018","","","1","5","We study low-complexity iterative decoding algorithms for product codes. We revisit two algorithms recently proposed by the authors based on bounded distance decoding (BDD) of the component codes that improve the performance of conventional iterative BDD (iBDD). We then propose a novel decoding algorithm that is based on generalized minimum distance decoding of the component codes. The proposed algorithm closes over 50% of the performance gap between iBDD and turbo product decoding (TPD) based on the Chase-Pyndiah algorithm at a bit error rate of 10-5. Moreover, the algorithm only leads to a limited increase in complexity with respect to iBDD and has significantly lower complexity than TPD. The studied algorithms are particularly interesting for high-throughput fiberoptic communications.","2165-4719;2165-4700","978-1-5386-7048-4978-1-5386-7047-7978-1-5386-7049","10.1109/ISTC.2018.8625279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625279","","Iterative decoding;Decoding;Reliability;Complexity theory;Product codes;Iterative algorithms;Encoding","computational complexity;decoding;error statistics;iterative decoding;product codes;turbo codes","low-complexity decoding;product codes;high-throughput fiber-optic systems;component codes;conventional iterative BDD;iBDD;novel decoding algorithm;generalized minimum distance decoding;bounded distance decoding;turbo product decoding;Chase-Pyndiah algorithm;decoding algorithms","","2","","23","","","","","IEEE","IEEE Conferences"
"Reducing the Complexity of Orthogonal Code Based Synthetic Aperture Ultrasound System","M. Yang; S. Wei; C. Chakrabarti","NA; NA; NA","2012 IEEE Workshop on Signal Processing Systems","","2012","","","270","275","In this paper, we propose several techniques to reduce the computation complexity of orthogonal chirp and orthogonal Golay code based synthetic aperture ultrasound systems. First, we minimize the complexity in terms of number of effective multiplications by choosing the system parameters for a specified SNR gain. The proposed method helps in reducing the complexity by about 20. Next, we consider a system that compensates for body motion. We reduce the complexity of motion compensation first by doing the calculations in polar domain, and second, by making use of the correlation in motion velocity field in a small neighborhood. These approximations reduce the computation complexity by3000 with minimal performance penalty.","2162-3570;2162-3562;2162-3562","978-1-4673-2987-3978-1-4673-2986-6978-0-7695-4856","10.1109/SiPS.2012.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363267","Synthetic aperture ultrasound;orthogonal coded excitation;Golay code;chirp;motion compensation","Complexity theory;Chirp;Signal to noise ratio;Gain;Motion compensation;Delay;Ultrasonic imaging","biomedical ultrasonics;computational complexity;Golay codes;image coding;medical image processing;motion compensation;orthogonal codes;ultrasonic imaging","computation complexity reduction;orthogonal code-based synthetic aperture ultrasound system;orthogonal chirp;orthogonal Golay code;specified SNR gain;body motion;motion compensation;polar domain;motion velocity field","","","","10","","","","","IEEE","IEEE Conferences"
"Performance Improvement of Accumulate-Repeat-Accumulate Codes with Bounded Complexity","W. Saad; S. M. Ibraheem; M. M. A. Elrazzak; S. S. Eldin; A. E. Aboelazm","NA; NA; NA; NA; NA","2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)","","2015","","","1","5","In this paper, we introduce a new family of codes named as systematic spatially-coupled accumulate repeat accumulate SC-ARA codes based on chain of interconnected proto-graphs over the binary erasure channels (BEC). It has been observed that Spatially-coupled low density parity check (SC-LDPC) codes are able to approach the capacity universally across the binary-input memoryless (BMS) channels. A density evolution (DE) analysis is provided for this class of codes which categorized by bounded density and/or complexity per information bit. Simulation results show that the spatially coupling of ensembles of ARA codes, with bounded density and/or complexity, drives the message-passing belief propagation decoding threshold (BP) to be closed to the maximum a posterior (MAP) threshold of the underlying codes over the BEC.","","978-1-4799-8091-8978-1-4799-8090","10.1109/VTCFall.2015.7391080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391080","","Parity check codes;Complexity theory;Decoding;Couplings;Encoding;Systematics;Repeaters","channel coding;parity check codes","accumulate repeat accumulate codes;bounded complexity;performance improvement;systematic spatially coupled accumulate;SC-ARA codes;interconnected protographs;binary erasure channels;BEC;spatially coupled low density parity check;SC-LDPC codes;binary input memoryless;BMS channels;density evolution analysis;DE analysis;bounded density;message passing belief propagation decoding threshold;BP;maximum a posterior;MAP threshold","","","","16","","","","","IEEE","IEEE Conferences"
"Low-Complexity Generator Polynomial Search for Turbo Trellis-Coded Spatial Modulation Using Symbol-based EXIT Charts","T. Hara; K. Ishibashi; S. X. Ng; L. Hanzo","Advanced Wireless & Communication Research Center (AWCC), The University of Electro-Communications, Tokyo, 182-8585; Advanced Wireless & Communication Research Center (AWCC), The University of Electro-Communications, Tokyo, 182-8585; Advanced Wireless & Communication Research Center (AWCC), The University of Electro-Communications, Tokyo, 182-8585; Advanced Wireless & Communication Research Center (AWCC), The University of Electro-Communications, Tokyo, 182-8585","2018 IEEE 10th International Symposium on Turbo Codes & Iterative Information Processing (ISTC)","","2018","","","1","5","A low-complexity generator polynomial (GP) search method is proposed for spatial modulation relying on turbo trellis coding (SM-TTC), which achieves a high coding gain over independent and identically distributed (i.i.d.) fading channels and is robust against spatial correlation among receiving antenna elements. The proposed GP search method invokes symbol-based extrinsic information transfer (EXIT) charts and does not need to execute iterative decoding. It is demonstrated that our SM-TTC scheme with the best GP obtained by the proposed method outperforms one with conventional codes.","2165-4719;2165-4700","978-1-5386-7048-4978-1-5386-7047-7978-1-5386-7049","10.1109/ISTC.2018.8625283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625283","","Modulation;Correlation;Linear antenna arrays;Iterative decoding;Transmitters;Search methods","channel coding;convolutional codes;fading channels;iterative decoding;receiving antennas;trellis coded modulation;turbo codes","turbo trellis-coded spatial modulation;symbol-based EXIT charts;low-complexity generator polynomial search method;turbo trellis coding;spatial correlation;GP search method;symbol-based extrinsic information transfer charts;SM-TTC scheme;coding gain;receiving antenna elements","","","","17","","","","","IEEE","IEEE Conferences"
"Low-complexity segmented CRC-aided SC stack decoder for polar codes","W. Song; H. Zhou; Y. Zhao; S. Zhang; X. You; C. Zhang","National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; Intel Collaborative Research Institutes on Mobile Networking and Computing, Intel Labs, Shanghai, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing, China","2016 50th Asilomar Conference on Signals, Systems and Computers","","2016","","","1189","1193","In order to improve the decoding performance of successive cancellation (SC) decoder for polar code, SC stack (SCS) polar decoder has been proposed by existing literatures. Compared with SCL decoder, SCS decoder successfully reduces the decoding complexity especially at high SNR while keeping the same decoding performance. However in correlated channels at low SNR, SCS decoder suffers a lot from the overwhelming complexity due to frequent backtracking operations. Aim to lower such complexity as well as the required storage resources of SCS decoder, this paper proposes a low complexity SCS polar decoder based on segmented CRC scheme. Employing the segmented CRC as an early termination scheme, the proposed SCS decoder can successfully reduce the complexity especially at low SNR without any performance degradation. Numerical results have shown that for (1024, 512) polar code, the proposed approach achieves more than 34.4% complexity reduction and 29.7% resource occupation reduction compared to the conventional CRC-aided SCS (CA-SCS) decoder at SNR of 1.5 dB.","","978-1-5386-3954-2978-1-5386-3952-8978-1-5386-3955","10.1109/ACSSC.2016.7869560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869560","Polar codes;segmented CRC;stack decoding","Maximum likelihood decoding;Signal to noise ratio;Time complexity;Reliability;Detectors","cyclic redundancy check codes;decoding;numerical analysis","successive cancellation decoder;CRC-aided SC stack decoder;polar codes;SC stack polar decoder;SCS polar decoder;segmented CRC scheme","","4","","8","","","","","IEEE","IEEE Conferences"
"Performance and complexity of tunable sparse network coding with gradual growing tuning functions over wireless networks","P. Garrido; C. W. Sørensen; D. E. Lucani; R. Agüero","University of Cantabria, Santander, Spain; Department of Electronic Systems, Aalborg University, Denmark; Department of Electronic Systems, Aalborg University, Denmark; University of Cantabria, Santander, Spain","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","","2016","","","1","6","Random Linear Network Coding (RLNC) has been shown to be a technique with several benefits, in particular when applied over wireless mesh networks, since it provides robustness against packet losses. On the other hand, Tunable Sparse Network Coding (TSNC) is a promising concept, which leverages a trade-off between computational complexity and goodput. An optimal density tuning function has not been found yet, due to the lack of a closed-form expression that links density, performance and computational cost. In addition, it would be difficult to implement, due to the feedback delay. In this work we propose two novel tuning functions with a lower computational cost, which do not highly increase the overhead in terms of the transmission of linear dependent packets compared with RLNC and previous proposals. Furthermore, we also broaden previous studies of TSNC techniques, by means of an extensive simulation campaign carried out using the ns-3 simulator. This brings the possibility of assessing their performance over more realistic scenarios, e.g considering MAC effects and delays. We exploit this implementation to analyze the impact of the feedback sent by the decoder. The results, compared to RLNC, show a reduction of 3.5 times in the number of operations without jeopardizing the network performance, in terms of goodput, even when we consider the delay effect on the feedback sent by the decoder.","2166-9589","978-1-5090-3254-9978-1-5090-3255","10.1109/PIMRC.2016.7794915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794915","Random Linear Coding;Sparse Matrices;Simulation;Wireless Networks;TSNC","Decoding;Encoding;Wireless networks;Network coding;Tuning;Computational complexity","computational complexity;linear codes;network coding;radio networks;random codes","wireless networks;random linear network coding;wireless mesh networks;tunable sparse network coding;computational complexity;closed-form expression;ns-3 simulator;MAC","","2","","20","","","","","IEEE","IEEE Conferences"
"A low-complexity parallel-friendly rate control algorithm for ultra-low delay high definition video coding","S. Sanz-Rodríguez; T. Mayer; M. Álvarez-Mesa; T. Schierl","Image Communication Group, Technische Universität Berlin, Germany; Image Communication Group, Technische Universität Berlin, Germany; Embedded Systems Architectures, Technische Universität Berlin, Germany; Multimedia Communications, Fraunhofer HHI, Berlin, Germany","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","","2013","","","1","4","Ultra-low delay high definition (HD) video coding applications such as video conferencing demand, first, low-complexity video encoders able to support multi-core framework for parallel processing and, second, rate control algorithms (RCAs) for successful video content delivering under delay constraints. In this paper a low-complexity parallel-friendly RCA is proposed for HD video conferencing. Specifically, it has been implemented on an optimized H.264/Scalable Video Coding (SVC) encoder, providing excellent performance in terms of buffer control, while achieving acceptable quality of compressed video under the imposed delay constraints.","","978-1-4799-1604","10.1109/ICMEW.2013.6618275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618275","Video coding;high definition;ultra-low delay;rate control;parallel processing;low-complexity","Encoding;Bit rate;Streaming media;Video coding;Real-time systems;Parallel processing;Complexity theory","parallel processing;video coding","buffer control;SVC encoder;H.264/scalable video coding encoder;HD video conferencing;low-complexity parallel-friendly RCA;ultra-low delay high definition video coding;low-complexity parallel-friendly rate control algorithm","","6","","11","","","","","IEEE","IEEE Conferences"
"A novel high-throughput, low-complexity bit-flipping decoder for LDPC codes","K. Le; F. Ghaffari; D. Declercq; B. Vasic; C. Winstead","ETIS, UMR-8051, Université Paris Sein, Université de Cergy-Pontoise, ENSEA, CNRS, France; ETIS, UMR-8051, Université Paris Sein, Université de Cergy-Pontoise, ENSEA, CNRS, France; ETIS, UMR-8051, Université Paris Sein, Université de Cergy-Pontoise, ENSEA, CNRS, France; Department of Electrical and Computer Engineering, University of Arizona, Tucson, AZ, USA; Department of Electrical and Computer Engineering, Utah State University, UT, USA","2017 International Conference on Advanced Technologies for Communications (ATC)","","2017","","","126","131","This paper presents a new high-throughput, low-complexity Bit Flipping (BF) decoder for Low-Density Parity-Check (LDPC) codes on the Binary Symmetric Channel (BSC), called Probabilistic Parallel Bit Flipping (PPBF). The advantage of PPBF comes from the fact that, no global operation is required during the decoding process and from that, all of the computations could be parallelized and localized at the computing units. Also in PPBF, the probabilistic feature in flipping the Variable Node (VN) is incorporated for all satisfaction level of its CN neighbors. This type of probabilistic incorporation makes PPBF more dynamic to correct some error patterns which are unsolvable by other BF decoders. PPBF offers a faster decoding process with an equivalent error correction performance to the Probabilistic Gradient Descent Bit Flipping (PGDBF) decoder, which is better than all so-far introduced BF decoders in BSC channel. A hardware implementation architecture of PPBF is also presented in this paper with detailed circuits for the probabilistic signal generator and processing units. The implementation of PPBF on FPGA confirms that, the PPBF complexity is much lower than that of the PGDBF and even lower than the one of the deterministic Gradient Descent Bit Flipping (GDBF) decoder. The good decoding performance along with the high throughput and low complexity lead PPBF decoder to become a brilliant candidate for the next generation of communication and storage standards.","2162-1039","978-1-5386-2898-0978-1-5386-2899","10.1109/ATC.2017.8167601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8167601","","Decoding;Probabilistic logic;Iterative decoding;Throughput;Oscillators;Error correction","decoding;error correction;field programmable gate arrays;gradient methods;parity check codes;probability","novel high-throughput;low-complexity bit-flipping decoder;LDPC codes;probabilistic feature;probabilistic incorporation;BF decoders;faster decoding process;equivalent error correction performance;Probabilistic Gradient Descent Bit Flipping decoder;probabilistic signal generator;processing units;PPBF complexity;deterministic Gradient Descent Bit Flipping decoder;low complexity lead PPBF decoder;Probabilistic Parallel Bit Flipping;decoding performance;Low-Density Parity Check codes","","4","","14","","","","","IEEE","IEEE Conferences"
"Low-complexity detection and decoding scheme for LDPC-coded MLC NAND flash memory","X. Lin; G. Han; S. Ouyang; Y. Li; Y. Fang","School of Information Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou 510006, China; School of Information Engineering, Guangdong University of Technology, Guangzhou 510006, China; National Mobile Communications Research Laboratory, Southeast University, Nanjing 210096, China; School of Information Engineering, Guangdong University of Technology, Guangzhou 510006, China","China Communications","","2018","15","6","58","67","With the development of manufacture technology, the multi-level cell (MLC) technique dramatically increases the storage density of NAND flash memory. As the result, cell-to-cell interference (CCI) becomes more serious and hence causes an increase in the raw bit error rate of data stored in the cells. Recently, low-density parity-check (LDPC) codes have appeared to be a promising solution to combat the interference of MLC NAND flash memory. However, the decoding complexity of the sum-product algorithm (SPA) is extremely high. In this paper, to improve the accuracy of the log likelihood ratio (LLR) information of each bit in each NAND flash memory cell, we adopt a non-uniform detection (N-UD) which uses the average maximum mutual information to determine the value of the soft-decision reference voltages. Furthermore, with an aim to reduce the decoding complexity and improve the decoding performance, we propose a modified soft reliability-based iterative majority-logic decoding (MSRBI-MLGD) algorithm, which uses a non-uniform quantizer based on power function to decode LDPC codes. Simulation results show that our design can offer a desirable trade-off between the performance and complexity for high-column-weight LDPC-coded MLC NAND flash memory.","1673-5447","","10.1109/CC.2018.8398504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8398504","Cell-to-cell interference (CCI);LDPC codes;MLC NAND flash memory;non-uniform detection (N-UD);modified soft reliability-based iterative majority-logic decoding (MSRBI-MLGD) algorithm","Threshold voltage;Flash memories;Complexity theory;Iterative decoding;Probability density function;Interference","circuit reliability;error statistics;flash memories;interference (signal);iterative decoding;NAND circuits;parity check codes;quantisation (signal)","multilevel cell technique;cell-to-cell interference;low-density parity-check codes;decoding complexity;NAND flash memory cell;majority-logic decoding algorithm;low-complexity detection scheme;LDPC codes;low-complexity decoding scheme;manufacture technology;CCI;bit error rate;sum-product algorithm;SPA;log likelihood ratio;LLR;nonuniform detection;N-UD;average maximum mutual information;soft-decision reference voltages;modified soft reliability-based iterative majority-logic decoding algorithm;MSRBI-MLGD algorithm;nonuniform quantizer;power function;high-column-weight LDPC-coded MLC NAND flash memory","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Low Complexity Secure Code (LCSC) Design for Big Data in Cloud Storage Systems","M. K. Kiskani; H. R. Sadjadpour; M. R. Rahimi; F. Etemadieh","NA; NA; NA; NA","2018 IEEE International Conference on Communications (ICC)","","2018","","","1","7","In the era of big data, reducing the computational complexity of servers in data centers will be an important goal. We propose Low Complexity Secure Codes (LCSCs) that are specifically designed to provide information theoretic security in cloud distributed storage systems. Unlike traditional coding schemes that are designed for error correction capabilities, these codes are only designed to provide security with low decoding complexity. These sparse codes are able to provide (asymptotic) perfect secrecy similar to Shannon cipher. The simultaneous promise of low decoding complexity and perfect secrecy make these codes very desirable for cloud storage systems with large amount of data. The design is particularly suitable for large size archival data such as movies and pictures. The complexity of these codes are compared with traditional encryption techniques.","1938-1883","978-1-5386-3180-5978-1-5386-3181","10.1109/ICC.2018.8422283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8422283","","Cloud computing;Protocols;Complexity theory;Channel coding;Encryption","Big Data;computational complexity;decoding;telecommunication security","Low complexity secure code design;big data;cloud storage systems;computational complexity;data centers;information theoretic security;cloud distributed storage systems;low decoding complexity;sparse codes","","1","","37","","","","","IEEE","IEEE Conferences"
"Very large scale integration (VLSI) implementation of low-complexity variable block size motion estimation for H.264/AVC coding","S. -. Hsia; P. -. Hong","Department of Electronics Engineering, National Kaohsiung First University of Science and Technology; Department of Computer and Communication Engineering, National Kaohsiung First University of Science and Technology","IET Circuits, Devices & Systems","","2010","4","5","414","424","This study presents a fast algorithm and its very large scale integration (VLSI) design to implement the variable block size motion estimation. The fast algorithm is proposed with a hardware-oriented concept for regular VLSI design. Simulations show that the proposed algorithm can reduce about 90% motion searching time, whereas PSNR only decreases about 0.02 dB on average. Based on the fast algorithm, VLSI architecture is designed with parallel structure and pipeline timing schedule to achieve high throughput rate for the HDTV system. The chip can compute 41 vectors for various block size during 24-240 cycles as using only 96 processing elements. Comparisons with contemporary VLSI architectures, this chip can offer higher processing speed, wider searching range and lower circuit complexity.","1751-858X;1751-8598","","10.1049/iet-cds.2009.0200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5567025","","","circuit complexity;high definition television;logic design;motion estimation;pipeline processing;video coding;VLSI","low-complexity variable block size motion estimation;H.264/AVC coding;VLSI design;hardware-oriented concept;motion searching time;VLSI architecture;parallel structure;pipeline timing schedule;throughput rate;HDTV system;circuit complexity","","5","","","","","","","IET","IET Journals & Magazines"
"The complexity of sphere decoding perfect codes under a vanishing gap to ML performance","J. Jaldén; P. Elia","ACCESS Linnaeus Center - Signal Processing Lab, KTH Royal Institute of Technology, Stockholm, Sweden; Mobile Communications Department, EURECOM, Sophia Antipolis, France","2011 IEEE International Symposium on Information Theory Proceedings","","2011","","","2836","2840","We consider the complexity of the sphere decoding (SD) algorithm when decoding a class of full rate space-time block codes that are optimal, over the quasi-static MIMO channel, with respect to the diversity-multiplexing tradeoff (DMT). Towards this we introduce the SD complexity exponent which represents the high signal-to-noise ratio (SNR) exponent of the tightest run-time complexity constraints that can be imposed on the SD algorithm while maintaining arbitrarily close to maximum likelihood (ML) performance. Similar to the DMT exposition, our approach naturally captures the dependence of the SD algorithm's computational complexity on the codeword density, code size and channel randomness, and provides simple closed form solutions in terms of the system dimensions and the multiplexing gain.","2157-8117;2157-8095;2157-8095","978-1-4577-0595-3978-1-4577-0596-0978-1-4577-0594","10.1109/ISIT.2011.6034092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034092","","Complexity theory;Multiplexing;Signal to noise ratio;Lattices;Maximum likelihood decoding;Upper bound","computational complexity;maximum likelihood decoding;space-time block codes","sphere decoding perfect codes;vanishing gap;full rate space-time block codes;quasistatic MIMO channel;diversity-multiplexing tradeoff;sphere decoding complexity exponent;signal-to-noise ratio;run-time complexity constraints;maximum likelihood performance;computational complexity;codeword density;code size;channel randomness;simple closed form solutions;system dimensions;multiplexing gain;DMT exposition","","2","","14","","","","","IEEE","IEEE Conferences"
"Low complexity video coding based on spatial resolution adaptation","M. Afonso; F. Zhang; A. Katsenou; D. Agrafiotis; D. Bull","Department of Electrical and Electronic Engineering, University of Bristol, BS8 1UB, UK; Department of Electrical and Electronic Engineering, University of Bristol, BS8 1UB, UK; Department of Electrical and Electronic Engineering, University of Bristol, BS8 1UB, UK; Department of Electrical and Electronic Engineering, University of Bristol, BS8 1UB, UK; Department of Electrical and Electronic Engineering, University of Bristol, BS8 1UB, UK","2017 IEEE International Conference on Image Processing (ICIP)","","2017","","","3011","3015","In this paper, a novel spatial resolution adaptation approach for video compression is proposed. Its ability to dynamically apply downsampling to frames exhibiting low spatial detail delivers improved rate distortion performance, together with a reduction in computational complexity of the encoding process. This method is based on an experimental investigation of the dependence between the QP threshold, which determines when to encode lower resolution frames, and the distortion obtained after downsampling/upsampling. The proposed approach is integrated with the High Efficiency Video Coding (HEVC) reference codec for intra coding, and evaluated on 15 high-resolution test sequences with varying levels of spatial detail. The results show a promising average bitrate savings of approximately 4% (B-D measurements), and significant complexity reduction (29% on average).","2381-8549","978-1-5090-2175-8978-1-5090-2174-1978-1-5090-2176","10.1109/ICIP.2017.8296835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296835","Video compression;spatial resolution adaptation;image scaling;HEVC","Encoding;Spatial resolution;High definition video;Complexity theory;Distortion;Image reconstruction","computational complexity;data compression;image resolution;video codecs;video coding","low complexity video coding;video compression;computational complexity;encoding process;High Efficiency Video Coding reference codec;intra coding;spatial resolution adaptation","","3","","21","","","","","IEEE","IEEE Conferences"
"Applying Supervised Learning to the Static Prediction of Locality-Pattern Complexity in Scientific Code","N. Alsaedi; S. Carr; A. Fong","NA; NA; NA","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","","2018","","","995","1000","On modern computer systems, the performance of an application depends largely on its locality. Current compiler static locality analysis has limited applicability due to limited run-time information. By instrumenting and running programs, training-based locality analysis is able to predict the locality of an application based on the size of the input data accurately; however, it is costly in terms of time and space. In this paper, we combine source-code analysis with training-based locality analysis to construct a supervised-learning model parameterized only by the source code properties. This model is the first to be able to predict the upper bound of data reuse change (locality pattern complexity) at compile time for loop nests in array-based programs without the need to instrument and run the program. The result is the ability to predict how virtual memory usage grows as a function of the input size efficiently. We have evaluated our model using array-based code as input to a variety of classification algorithms. These algorithms include Naive Bayes, Decision tree, and Support Vector Machine (SVM). Our experiments show that SVM outperforms the other classifiers with 97% precision, a 97% true positive rate and a 1% false positive rate. We are able to predict the growth rate of memory usage in unseen scientific code accurately without the need to instrument and run the program. This work represents a significant step in developing an accurate static memory usage predictor for use in Virtual Machines (VMs) in cloud data centers.","","978-1-5386-6805-4978-1-5386-6806","10.1109/ICMLA.2018.00162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614187","data locality, virtual memory, reuse distance function, locality-pattern complexity, supervised learning","Complexity theory;Predictive models;Histograms;Arrays;Prediction algorithms;Support vector machines;Analytical models","Bayes methods;cloud computing;computational complexity;decision trees;learning (artificial intelligence);pattern classification;program compilers;support vector machines;virtual machines","cloud data centers;virtual machines;SVM;support vector machine;Decision tree;Naive Bayes;classification algorithms;virtual memory usage;static memory usage predictor;compiler static locality analysis;locality-pattern complexity static prediction;supervised learning;scientific code;array-based code;array-based programs;locality pattern complexity;data reuse change;source code properties;supervised-learning model;source-code analysis;training-based locality analysis;run-time information;modern computer systems","","","","29","","","","","IEEE","IEEE Conferences"
"Syntax, Predicates, Idioms - What Really Affects Code Complexity?","S. Ajami; Y. Woodbridge; D. G. Feitelson","NA; NA; NA","2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)","","2017","","","66","76","Program comprehension concerns the ability to understand code written by others. But not all code is the same. We use an experimental platform fashioned as an online game-like environment to measure how quickly and accurately 222 professional programmers can interpret code snippets with similar functionality but different structures. The results indicate, inter alia, that 'for' loops are significantly harder than 'if's, that some but not all negations make a predicate harder, and that loops counting down are slightly harder than loops counting up. This demonstrates how the effect of syntactic structures, different ways to express predicates, and the use of known idioms can be measured empirically, and that syntactic structures are not necessarily the most important factor. By amassing many more empirical results like these it may be possible to derive better code complexity metrics than we have today.","","978-1-5386-0535-6978-1-5386-0536","10.1109/ICPC.2017.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961505","Code complexity;program understanding;gamification","Complexity theory;Compounds;Syntactics;Programming;Context;Time measurement","computational linguistics;professional aspects;program control structures;program interpreters;reverse engineering;software metrics","program comprehension;online game-like environment;professional programmers;code snippets;code interpretation;loops;syntactic structures;predicates;idioms;code complexity metrics","","1","","44","","","","","IEEE","IEEE Conferences"
"Research track: Session I — Program comprehension","","","2012 28th IEEE International Conference on Software Maintenance (ICSM)","","2012","","","5","5","Start of the above-titled section of the conference proceedings record.","1063-6773","978-1-4673-2312-3978-1-4673-2313","10.1109/ICSM.2012.6405246","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405246","","","","","","","","","","","","","IEEE","IEEE Conferences"
"18th IEEE International Conference on Program Comprehension [Cover art]","","","2010 IEEE 18th International Conference on Program Comprehension","","2010","","","C1","C1","Presents the cover of the 2010 IEEE 18th International Conference on Program Comprehension proceedings.","1092-8138;1092-8138","978-1-4244-7603-9978-1-4244-7604-6978-0-7695-4113","10.1109/ICPC.2010.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5521784","","","","","","","","","","","","","IEEE","IEEE Conferences"
