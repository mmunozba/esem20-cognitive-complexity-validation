BibliographyType,ISBN,Identifier,Author,Title,Journal,Volume,Number,Month,Pages,Year,Address,Note,URL,Booktitle,Chapter,Edition,Series,Editor,Publisher,ReportType,Howpublished,Institution,Organizations,School,Annote,Custom1,Custom2,Custom3,Custom4,Custom5
7,"","TARJAN20141","Tarjan, Laslo; Šenk, Ivana; Tegeltija, Srdjan; Stankovski, Stevan; Ostojic, Gordana","A readability analysis for QR code application in a traceability system","Computers and Electronics in Agriculture",109,,"","1 - 11",2014,"","","http://www.sciencedirect.com/science/article/pii/S0168169914002142","","","","","","","","","","","","","Traceability of data through transformation stages of each individual food product, starting from raw products and to the final product, as well as printing the key data on the product package, adds to the consumers’ trust in product quality. For each food product, it is necessary to track data starting from the stage of raw products farming, through food processing, transport, warehousing, to retailing and reaching the end consumer. In order to allow insight to the key data to the user (mostly end consumer), this paper suggests recording the data on the product package in the form of a quick response two-dimensional barcode (QR code) in key points of the product’s life cycle. For efficient functioning of the proposed system, it is essential to ensure fast and reliable operation through proper placement of the QR code on the package during production, and fast and easy data reading by the product consumer. This paper presents the results of a readability analysis of QR code of variable contents, size and data error correction level, which are read by smartphones running an Android platform. The experiments were performed with various types of base material on which the code was printed. Furthermore, QR code readability analysis was conducted in the case when there is a geometric deformation of the code. Based on the detailed analysis of the collected data, it can be concluded that QR code readability is not directly influenced by the number of coded characters, or by the error correction level, but only by the size of modules that constitute the code. Furthermore, the results show that the change of the base material does not influence the read time, but influences the code readability. The paper further presents an example of the proposed traceability system, where the QR codes are used for data tracking and tracing for fruit yogurts, based on the recommendations gained through the readability analysis. This traceability system concept is universal and can be used for various products with slight modifications.","","Traceability, QR code, Barcode, Mobile phone, Readability","",""
7,"","THOMPSON2019100873","Thompson, Jack; Sutton, Katherine; Kuo, Tony","The added value of establishing a lexicon to help inform, compare, and better understand the implementation of policy, systems, and environmental change strategies in Supplemental Nutrition Assistance Program Education","Preventive Medicine Reports",14,,"","100873",2019,"","","http://www.sciencedirect.com/science/article/pii/S2211335519300555","","","","","","","","","","","","","Categorization of terms/concepts/constructs that allows for better understanding and comparison of public health interventions is often lacking in program implementation and evaluation. A classification system such as a lexicon, when used appropriately, can help address this need. The present narrative describes a lexicon of policy, systems, and environmental change strategies (PSEs) that was developed and prototyped to aid local implementation of Supplemental Nutrition Assistance Program Education (SNAP-Ed) interventions in obesity prevention. The lexicon was reviewed and refined by a panel of experts who provided iterative feedback on the system's scope and utility. To develop the lexicon, a team from the local health department: (i) conducted an inventory (community context scan) of SNAP-Ed PSEs implemented in Los Angeles County during 2010–2015; (ii) assessed commonalities among PSEs that were translated into “index factors” to contextualize terms/concepts/constructs relevant to SNAP-Ed services planning; and (iii) convened a panel of experts to review and test the classification system for quality and usability. In the latter activity, the panel reviewed the terms/concepts/constructs within the context of two geographical areas and by the selected PSEs. The final version of the lexicon organized the terms/concepts/constructs of the local SNAP-Ed PSEs into overarching categories, so they can be compared/assessed by type, content, and/or impact. The goal of the project was to create a classification system that can help facilitate meaningful communications among program implementers, evaluators, and community stakeholders. The lexicon has practical implications and potential applications for other jurisdictions interested in reducing obesity rates through SNAP-Ed PSEs.","","Obesity prevention, Public health, Policy, systems and environmental change, Lexicon, Program implementation, Program evaluation","",""
7,"","LISTON20151014","Liston, J.; Geary, T.","Evaluating a Guidance Counsellor Education Programme: The Methodological Complexities","Procedia - Social and Behavioral Sciences",191,,"","1014 - 1018",2015,"","The Proceedings of 6th World Conference on educational Sciences","http://www.sciencedirect.com/science/article/pii/S1877042815029171","","","","","","","","","","","","","The focus of this paper is on the methodological considerations of a PhD study which evaluated a guidance counsellor education programme,offered by a University in Ireland. The programme is offered as a two-year part-time course, which is delivered over four semesters and two summer schools.Good practice and areas for improvement were identified, acknowledged and affirmed throughout the study. However in doing so a number of methodological complexities were considered. The model selected for evaluating the programme negotiated the sensitive nature and the contextual dimensions of guidance counselling while also ensuring a depth in findings. Underlying the term ‘evaluation’ in the context of the study was the motivation to seek knowledge, explore and illuminate new learning in the area of guidance counsellor education. Therefore the study can be described as principally an evaluation with illuminative and exploratory characteristics.This paper explores the strengths and limitations of the methodological approach adopted.","","Guidance Counsellor Education, Programme Evaluation, Guidance Counsellor Personal Development, The Irish Perspective.","",""
7,"","HENSEL2018105","Hensel, Jera; Giesl, Jürgen; Frohn, Florian; Ströder, Thomas","Termination and complexity analysis for programs with bitvector arithmetic by symbolic execution","Journal of Logical and Algebraic Methods in Programming",97,,"","105 - 130",2018,"","","http://www.sciencedirect.com/science/article/pii/S2352220817300524","","","","","","","","","","","","","In earlier work, we developed an approach for automated termination analysis of C programs with explicit pointer arithmetic, which is based on symbolic execution. However, similar to many other termination techniques, this approach assumed the program variables to range over mathematical integers instead of bitvectors. This eases mathematical reasoning but is unsound in general. In this paper, we extend our approach in order to handle fixed-width bitvector integers. Thus, we present the first technique for termination analysis of C programs that covers both byte-accurate pointer arithmetic and bit-precise modeling of integers. Moreover, we show that our approach can also be used to analyze the runtime complexity of bitvector programs. We implemented our contributions in the automated termination prover AProVE and evaluate its power by extensive experiments.","","Termination analysis, Bitvectors, Symbolic execution, , Runtime complexity","",""
7,"","PADRO20131072","Padró, Carles; Vázquez, Leonor; Yang, An","Finding lower bounds on the complexity of secret sharing schemes by linear programming","Discrete Applied Mathematics",161,7,"","1072 - 1084",2013,"","","http://www.sciencedirect.com/science/article/pii/S0166218X12004003","","","","","","","","","","","","","Optimizing the maximum, or average, length of the shares in relation to the length of the secret for every given access structure is a difficult and long-standing open problem in cryptology. Most of the known lower bounds on these parameters have been obtained by implicitly or explicitly using that every secret sharing scheme defines a polymatroid related to the access structure. The best bounds that can be obtained by this combinatorial method can be determined by using linear programming, and this can be effectively done for access structures on a small number of participants. By applying this linear programming approach, we improve some of the known lower bounds for the access structures on five participants and the graph access structures on six participants for which these parameters were still undetermined. Nevertheless, the lower bounds that are obtained by this combinatorial method are not tight in general. For some access structures, they can be improved by adding to the linear program non-Shannon information inequalities as new constraints. We obtain in this way new separation results for some graph access structures on eight participants and for some ports of non-representable matroids. Finally, we prove that, for two access structures on five participants, the combinatorial lower bound cannot be attained by any linear secret sharing scheme.","","Secret sharing, Linear programming, Polymatroid, Non-Shannon information inequalities","",""
7,"","LAW2018110","Law, Mark; Russo, Alessandra; Broda, Krysia","The complexity and generality of learning answer set programs","Artificial Intelligence",259,,"","110 - 146",2018,"","","http://www.sciencedirect.com/science/article/pii/S000437021830105X","","","","","","","","","","","","","Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.","","Non-monotonic logic-based learning, Answer Set Programming, Complexity of non-monotonic learning","",""
7,"","DUTTON20191126","Dutton, Spencer; Marnay, Chris; Feng, Wei; Robinson, Matthew; Mammoli, Andrea","Moore vs. Murphy: Tradeoffs between complexity and reliability in distributed energy system scheduling using software-as-a-service","Applied Energy",238,,"","1126 - 1137",2019,"","","http://www.sciencedirect.com/science/article/pii/S0306261919300650","","","","","","","","","","","","","Software-based optimization of building control strategies, including scheduling, has the potential to improve the performance of existing complex heating, ventilation, and air conditioning (HVAC), storage, and other systems—especially if temporally variable energy production, such as solar thermal or photovoltaics, is included. If reductions in energy bills can be achieved using optimized control strategies that take advantage of cost-saving opportunities, such as time-of-use pricing, the additional bill savings can cover further efficiency investment costs. As computer processing becomes cheaper over time (Moore’s Law), opportunities to perform complex control optimization become more abundant, and these can be performed remotely as software-as-a-service (SaaS). However, by “perfecting” our control strategies, we run an increased risk that when something unexpected happens (Murphy’s Law), the consequences of failure are greater. This study used simulation to explore the potential benefits of HVAC schedule optimization, delivery, and implementation using a SaaS paradigm, at various levels of complexity. Implementing optimal schedules in a model of an efficient building’s HVAC system, the study predicts energy cost savings of up to 10% compared to the naïve reference control strategy. Optimizing more system control variables increases the potential energy cost savings; however, these savings could be compromised by failures in communication inherent in delivering schedules via SaaS. The additional cost of energy resulting from the risk of increased demand charges generally increased with increased communication failure to a much larger extent than the risk of increased energy use charges. This work suggests that moderate improvements in performance, achieved at low cost by simple means, may be more effective than highly optimized schemes, which are more susceptible to failure due to their dependence on complex interactions between systems.","","Distributed energy resources, Control optimization, Software-as-a-service, HVAC complexity, microgrids","",""
7,"","HANG2017116","Hang, Bo; Kang, Changqing; Wang, Yi","A low computational complexity bandwidth extension method for mobile audio coding","Computers & Electrical Engineering",60,,"","116 - 125",2017,"","","http://www.sciencedirect.com/science/article/pii/S0045790617312557","","","","","","","","","","","","","Mobile devices always demand low computational complexity because computing resources, such as processor and battery, in mobile are always limited. The bandwidth extension (BWE) algorithm in mobile audio codec standard of China was proposed to improve audio quality in mobile communication. But the computational complexity of the algorithm is too high to implement in mobile devices. By analyzing the BWE algorithm, we discover that the main reason of high computational complexity is the frequently usage of time-frequency transformation. Then a low computational complexity scheme is proposed, which include algorithm optimization and code optimization. The experiment results show that computation time consumption ratio of BWE module in encoder and decoder are decreased by 4.5 and 14.3 percentage points respectively, without reducing the overall audio codec subjective quality, which is be conductive to the algorithm implement in mobile audio field.","","Mobile audio, Bandwidth extension, Computational complexity, Time-frequency transformation","",""
7,"","PONCE20121170","Ponce, Héctor R.; López, Mario J.; Mayer, Richard E.","Instructional effectiveness of a computer-supported program for teaching reading comprehension strategies","Computers & Education",59,4,"","1170 - 1183",2012,"","","http://www.sciencedirect.com/science/article/pii/S0360131512001340","","","","","","","","","","","","","This article examines the effectiveness of a computer-based instructional program (e-PELS) aimed at direct instruction in a collection of reading comprehension strategies. In e-PELS, students learn to highlight and outline expository passages based on various types of text structures (such as comparison or cause-and-effect) as well as to paraphrase, self-question, and summarize. The study involved 1041 fourth-grade elementary students from 21 schools distributed in three regions in central Chile. Participant teachers integrated this program into the Spanish language curriculum, instructing their students during thirty sessions of 90 min each during one school semester. Pretest-to-posttest gains in reading comprehension scores were significantly greater for students instructed with this program than for students who received traditional instruction (d = .5), with particularly strong effects for lower-achieving students (d = .7). The findings support the efficacy of direct instruction in specific learning strategies in a computer-based environment.","","Learning strategies, Subject areas, Elementary education, Interactive learning environments, Classroom teaching, Reading comprehension","",""
7,"","DURISIC20131275","Durisic, Darko; Nilsson, Martin; Staron, Miroslaw; Hansson, Jörgen","Measuring the impact of changes to the complexity and coupling properties of automotive software systems","Journal of Systems and Software",86,5,"","1275 - 1293",2013,"","","http://www.sciencedirect.com/science/article/pii/S0164121212003330","","","","","","","","","","","","","Background
 In the past few decades exponential increase in the amount of software used in cars has been recorded together with enhanced requirements for functional safety of their embedded software. As the evolution of software systems in cars often entails changes to software architecture, it is important to be able to monitor their impact.
 Method
 We conducted a case study on a distributed software system in cars at Volvo Car Corporation with the goal to develop, apply and evaluate measures of complexity and coupling which could support software architects in monitoring changes.
 Results
 The results showed that two metrics – structural complexity and coupling measures – can guide architectural work and turn attention of architects to most complex subsystems. The results were confirmed by monitoring a complete electrical system of a vehicle under two releases.
 Conclusion
 By applying the metrics after each significant change in the architecture, it is possible to verify that certain quality attributes have not deteriorated and to identify new testing areas. Using these metrics increases the product quality with respect to stability, reliability, and maintainability and also has potential to reduce long-term software development/maintenance costs.","","Automotive software system, Quality metric, Architectural change, Maintainability, Complexity, Coupling","",""
7,"","CHATTERJI2016128","Chatterji, Madhabi","Causal inferences on the effectiveness of complex social programs: Navigating assumptions, sources of complexity and evaluation design challenges","Evaluation and Program Planning",59,,"","128 - 140",2016,"","","http://www.sciencedirect.com/science/article/pii/S0149718916301094","","","","","","","","","","","","","This paper explores avenues for navigating evaluation design challenges posed by complex social programs (CSPs) and their environments when conducting studies that call for generalizable, causal inferences on the intervention’s effectiveness. A definition is provided of a CSP drawing on examples from different fields, and an evaluation case is analyzed in depth to derive seven (7) major sources of complexity that typify CSPs, threatening assumptions of textbook-recommended experimental designs for performing impact evaluations. Theoretically-supported, alternative methodological strategies are discussed to navigate assumptions and counter the design challenges posed by the complex configurations and ecology of CSPs. Specific recommendations include: sequential refinement of the evaluation design through systems thinking, systems-informed logic modeling; and use of extended term, mixed methods (ETMM) approaches with exploratory and confirmatory phases of the evaluation. In the proposed approach, logic models are refined through direct induction and interactions with stakeholders. To better guide assumption evaluation, question-framing, and selection of appropriate methodological strategies, a multiphase evaluation design is recommended.","","Impact evaluations, Experimental designs, Mixed methods, Causal inferences, Complex social programs","",""
7,"","CAPELLINI20151339","Capellini, Simone Aparecida; de Almeida Rodrigues Pinto, Cataryne; Cunha, Vera Lúcia Orlandi","Reading Comprehension Intervention Program for Teachers from 3rd Grade’ Students","Procedia - Social and Behavioral Sciences",174,,"","1339 - 1345",2015,"","International Conference on New Horizons in Education, INTE 2014, 25-27 June 2014, Paris, France","http://www.sciencedirect.com/science/article/pii/S1877042815008083","","","","","","","","","","","","","This study aimed to verify the effectiveness of Reading comprehension intervention program for teachers from 3rd grade'students. 4 teachers of 3rd grade from public school were divided: GI - two teachers submitted to intervention program and GII - two teachers not submitted to intervention program. Reading Comprehension Assessment Protocol was applied in all students of these teachers in pre and post-intervention. The results revealed a difference between the performance of the situations of pre and post-testing for GI. The intervention program was effectiveness for teachers to teach narrative and expository texts comprehension for students.","","Learning. Reading, Intervention, Reading Comprehension.","",""
7,"","GUZZARDOTAMARGO2016138","Tamargo, Rosa E. Guzzardo; Kroff, Jorge R. Valdés; Dussias, Paola E.","Examining the relationship between comprehension and production processes in code-switched language","Journal of Memory and Language",89,,"","138 - 161",2016,"","Speaking and Listening: Relationships Between Language Production and Comprehension","http://www.sciencedirect.com/science/article/pii/S0749596X15001461","","","","","","","","","","","","","We employ code-switching (the alternation of two languages in bilingual communication) to test the hypothesis, derived from experience-based models of processing (e.g., Boland, Tanenhaus, Carlson, & Garnsey, 1989; Gennari & MacDonald, 2009), that bilinguals are sensitive to the combinatorial distributional patterns derived from production and that they use this information to guide processing during the comprehension of code-switched sentences. An analysis of spontaneous bilingual speech confirmed the existence of production asymmetries involving two auxiliary+participle phrases in Spanish–English code-switches. A subsequent eye-tracking study with two groups of bilingual code-switchers examined the consequences of the differences in distributional patterns found in the corpus study for comprehension. Participants’ comprehension costs mirrored the production patterns found in the corpus study. Findings are discussed in terms of the constraints that may be responsible for the distributional patterns in code-switching production and are situated within recent proposals of the links between production and comprehension.","","Bilingualism, Code-switching, Experience-based processing, Eye-tracking, Spanish–English","",""
7,"","FARINA2018147","Farina, Almo","Ecoacoustic codes and ecological complexity","Biosystems",164,,"","147 - 154",2018,"","Code Biology","http://www.sciencedirect.com/science/article/pii/S0303264717302228","","","","","","","","","","","","","Multi-layer communication and sensing network assures the exchange of relevant information between animals and their umwelten, imparting complexity to the ecological systems. Individual soniferous species, the acoustic community, and soundscape are the three main operational levels that comprise this multi-layer network. Acoustic adaptation and acoustic niche are two more important mechanisms that regulate the acoustic performances at the first level while the acoustic community model explains the complexity of the interspecific acoustic network at the second level. Acoustic habitat and ecoacoustic events are two of the most relevant mechanisms that operate at the third level. The exchange of ecoacoustic information on each of these levels is assured by ecoacoustic codes. At the level of individual sonifeorus species, a dyadic intraspecific exchange of information is established between an emitter and a receiver. Ecoacoustic codes discriminate, identify, and label specific signals that pertain to the theme, variation, motif repetition, and intensity of signals. At the acoustic community level, a voluntarily or involuntarily communication is established between networks of interspecific emitters and receivers. Ecoacoustic codes at this level transmit information (e.g., recognition of predators, location of food sources, availability and location of refuges) between one species and the acoustically interacting community and impart cohesion to interspecific assemblages. At the soundscape level, acoustic information is transferred from a mosaic of geophonies, biophonies, and technophonies to different species that discriminate meaningful ecoacoustic events and their temporal dynamics during habitat selection processes. Ecoacoustic codes at this level operate on a limited set of signals from the environmental acoustic dynamic that are heterogeneous in time and space, and these codes are interpreted differently according to the species during habitat selection and the completion of phenological cycles. The process of ecoacoustic coding can be interpreted according to the eco-field theory, which describes the procedures utilized by a receiver to intercept and optimize acoustic information. The acoustic codes may be detected and identified using mathematical models that simulate their performances. The Acoustic Complexity Indices are an appropriate tool to investigate the acoustic codes in action on all three levels. Ecoacoustic codes are powerful agencies used by sound-adapted species to cope with environmental novelties, and their efficiency may represent an important divide between whether a species perpetuates or becomes extinct.","","Ecological complexity, Soundscape, Ecoacoustic codes, Eco-fields, Acoustic complexity index","",""
7,"","RANDO2012148","Rando, Oliver J.","Combinatorial complexity in chromatin structure and function: revisiting the histone code","Current Opinion in Genetics & Development",22,2,"","148 - 155",2012,"","Genome architecture and expression","http://www.sciencedirect.com/science/article/pii/S0959437X1200024X","","","","","","","","","","","","","Covalent modifications of histone proteins play key roles in transcription, DNA repair, recombination, and other such processes. Over a hundred histone modifications have been described, and a popular idea in the field is that the function of a single histone mark cannot be understood without understanding its combinatorial co-occurrence with other marks, an idea generally called the ‘histone code hypothesis.’ This idea is hotly debated, with increasing biochemical evidence for chromatin regulatory factors that bind to specific histone modification combinations, but functional and localization studies finding minimal combinatorial complexity in histone modification patterns. This review will focus on these contrasting results, and will briefly touch on possible ways to reconcile these conflicting views.","","","",""
7,"","LEWIS2015155","Lewis, Ashley G.; Bastiaansen, Marcel","A predictive coding framework for rapid neural dynamics during sentence-level language comprehension","Cortex",68,,"","155 - 168",2015,"","Special issue: Prediction in speech and language processing","http://www.sciencedirect.com/science/article/pii/S0010945215000714","","","","","","","","","","","","","There is a growing literature investigating the relationship between oscillatory neural dynamics measured using electroencephalography (EEG) and/or magnetoencephalography (MEG), and sentence-level language comprehension. Recent proposals have suggested a strong link between predictive coding accounts of the hierarchical flow of information in the brain, and oscillatory neural dynamics in the beta and gamma frequency ranges. We propose that findings relating beta and gamma oscillations to sentence-level language comprehension might be unified under such a predictive coding account. Our suggestion is that oscillatory activity in the beta frequency range may reflect both the active maintenance of the current network configuration responsible for representing the sentence-level meaning under construction, and the top-down propagation of predictions to hierarchically lower processing levels based on that representation. In addition, we suggest that oscillatory activity in the low and middle gamma range reflect the matching of top-down predictions with bottom-up linguistic input, while evoked high gamma might reflect the propagation of bottom-up prediction errors to higher levels of the processing hierarchy. We also discuss some of the implications of this predictive coding framework, and we outline ideas for how these might be tested experimentally.","","Language comprehension, Neural oscillations, Beta, Gamma, Predictive coding","",""
7,"","HANNEMANNTAMAS20131575","Hannemann-Tamás, Ralf; Gábor, Attila; Szederkényi, Gábor; Hangos, Katalin M.","Model complexity reduction of chemical reaction networks using mixed-integer quadratic programming","Computers & Mathematics with Applications",65,10,"","1575 - 1595",2013,"","Grasping Complexity","http://www.sciencedirect.com/science/article/pii/S0898122112006852","","","","","","","","","","","","","The model complexity reduction problem of large chemical reaction networks under isobaric and isothermal conditions is considered. With a given detailed kinetic mechanism and measured data of the key species over a finite time horizon, the complexity reduction is formulated in the form of a mixed-integer quadratic optimization problem where the objective function is derived from the parametric sensitivity matrix. The proposed method sequentially eliminates reactions from the mechanism and simultaneously tunes the remaining parameters until the pre-specified tolerance limit in the species concentration space is reached. The computational efficiency and numerical stability of the optimization are improved by a pre-reduction step followed by suitable scaling and initial conditioning of the Hessian involved. The proposed complexity reduction method is illustrated using three well-known case studies taken from reaction kinetics literature.","","Chemical reaction networks, Model reduction, Mixed-integer quadratic programming, Sensitivity analysis","",""
7,"","KAHLER20161595","Kahler, Christopher W.; Caswell, Amy J.; Laws, M. Barton; Walthers, Justin; Magill, Molly; Mastroleo, Nadine R.; Howe, Chanelle J.; Souza, Timothy; Wilson, Ira; Bryant, Kendall; Monti, Peter M.","Using topic coding to understand the nature of change language in a motivational intervention to reduce alcohol and sex risk behaviors in emergency department patients","Patient Education and Counseling",99,10,"","1595 - 1602",2016,"","","http://www.sciencedirect.com/science/article/pii/S0738399116301902","","","","","","","","","","","","","Objective
 To elucidate patient language that supports changing a health behavior (change talk) or sustaining the behavior (sustain talk).
 Methods
 We developed a novel coding system to characterize topics of patient speech in a motivational intervention targeting alcohol and HIV/sexual risk in 90 Emergency Department patients. We further coded patient language as change or sustain talk.
 Results
 For both alcohol and sex, discussions focusing on benefits of behavior change or change planning were most likely to involve change talk, and these topics comprised a large portion of all change talk. Greater discussion of barriers and facilitators of change also was associated with more change talk. For alcohol use, benefits of drinking behavior was the most common topic of sustain talk. For sex risk, benefits of sexual behavior were rarely discussed, and sustain talk centered more on patterns and contexts, negations of drawbacks, and drawbacks of sexual risk behavior change.
 Conclusions
 Topic coding provided unique insights into the content of patient change and sustain talk.
 Practice implications
 Patients are most likely to voice change talk when conversation focuses on behavior change rather than ongoing behavior. Interventions addressing multiple health behaviors should address the unique motivations for maintaining specific risky behaviors.","","Alcohol, Sex risk, HIV, Motivational interviewing, Dialogue coding","",""
7,"","SINGH2018160","Singh, Ravinder; O’Farrell, Timothy; Biagi, Mauro; David, John P. R.","Coded color shift keying with frequency domain equalization for low complexity energy efficient indoor visible light communications","Physical Communication",31,,"","160 - 168",2018,"","","http://www.sciencedirect.com/science/article/pii/S1874490717306067","","","","","","","","","","","","","The adaptive tri-chromatic color shift keying (CSK) modulation standardized in IEEE 802.15.7 and the advanced quad-chromatic CSK are unable to utilize their entire spectral efficiency range over dispersive indoor VLC channels and incur large power penalties without the use of channel equalization and forward error correction. This degrades the energy efficiency and limits the throughput capability of CSK schemes to approximately half of the data-rates specified in the IEEE standard for multimedia services in wireless personal area networks. To comply with the desire for low latency and minimal implementation complexity in the CSK standardization framework, we consider an industry standard binary convolutional code and frequency domain equalization (FDE) which provide one-shot data packet processing. Our results show that when operating over hybrid indoor visible light communication (VLC) links, the FDE based rate-adaptive coded modulation CSK scheme achieves a significant 11 dB SNR gain over an uncoded, unequalized system.","","Visible light communications, Color shift keying, IEEE 802.15.7, Channel coding, Frequency domain equalization, Indoor VLC channels","",""
7,"","ELGEZAWI2017161","Elgezawi, Moataz; Hassan, Khalid; Alagl, Adel; Al-Thobity, Ahmad M.; Al-Mutairi, Basel; Al-Houtan, Thamir; Sadaf, Shazia","Complexity of comprehensive care treatments in undergraduate dental programs: The benefits of observing and assisting experienced faculty members","The Saudi Dental Journal",29,4,"","161 - 166",2017,"","","http://www.sciencedirect.com/science/article/pii/S101390521730055X","","","","","","","","","","","","","Objective
 To improve the confidence of the final year dental students in completing occlusal and oral rehabilitation of patients, with complexities beyond their scope, based on full analysis of the biomechanical and esthetic considerations of each case.
 Material & methods
 Two comprehensive patient situations presenting with special difficulties including extensive, reduced vertical dimension of occlusion, limited interocclusal space and maxillary alveolar bone for implant insertion necessitating bone augmentation and a sinus lift surgery was managed by two students at our institute. Procedures like surgical crown lengthening, sinus lifting, and bone augmentation were performed by senior faculty with the respective two students’ assisting as well as following up at the healing phase and reporting progress of healing and any possible complications to the supervisor. Students’ reported significant improvement in decision making skills; time management; interpersonal skills, management of cases in an evidence –based interdisciplinary approach as well as increase in their confidence in managing complex cases independently. Follow up with both cases showed optimum outcome and patients’ satisfaction.
 Results
 Students’ reported significant improvement in decision making skills; time management; interpersonal skills, management of cases in an evidence –based interdisciplinary approach as well as increase in their confidence in managing complex cases independently. Follow up with both cases showed optimum outcome and patients’ satisfaction.
 Conclusions
 Exposing students to manage complex oral rehabilitation including procedures like sinus lifting and bone augmentation, through an evidence-based interdisciplinary approach during the undergraduate comprehensive clinical dentistry course enhances their confidence and clinical acumen as an independent practitioner.","","Comprehensive dentistry, Case report, Occlusal and oral rehabilitation, Interdisciplinary management, Undergraduate student","",""
7,"","ELANGO2016161","Elango, G. Arul; Sudha, G. F.","Design of complete software GPS signal simulator with low complexity and precise multipath channel model","Journal of Electrical Systems and Information Technology",3,2,"","161 - 180",2016,"","","http://www.sciencedirect.com/science/article/pii/S2314717216300320","","","","","","","","","","","","","The need for GPS data simulators have become important due to the tremendous growth in the design of versatile GPS receivers. Commercial hardware and software based GPS simulators are expensive and time consuming. In this work, a low cost simple novel GPS L1 signal simulator is designed for testing and evaluating the performance of software GPS receiver in a laboratory environment. A typical real time paradigm, similar to actual satellite derived GPS signal is created on a computer generated scenario. In this paper, a GPS software simulator is proposed that may offer a lot of analysis and testing flexibility to the researchers and developers as it is totally software based primarily running on a laptop/personal computer without the requirement of any hardware. The proposed GPS simulator allows provision for re-configurability and test repeatability and is developed in VC++ platform to minimize the simulation time. It also incorporates Rayleigh multipath channel fading model under non-line of sight (NLOS) conditions. In this work, to efficiently design the simulator, several Rayleigh fading models viz. Inverse Discrete Fourier Transform (IDFT), Filtering White Gaussian Noise (FWFN) and modified Sum of Sinusoidal (SOS) simulators are tested and compared in terms of accuracy of its first and second order statistical metrics, execution time and the later one is found to be as the best appropriate Rayleigh multipath model suitable for incorporating with GPS simulator. The fading model written in ‘MATLAB’ engine has been linked with software GPS simulator module enable to test GPS receiver’s functionality in different fading environments.","","GPS signal simulator, Rayleigh fading, Non line of sight (NLOS), Sum of sinusoids (SOS)","",""
7,"","COTRONEO2013163","Cotroneo, Domenico; Natella, Roberto; Pietrantuono, Roberto","Predicting aging-related bugs using software complexity metrics","Performance Evaluation",70,3,"","163 - 178",2013,"","Special Issue on Software Aging and Rejuvenation","http://www.sciencedirect.com/science/article/pii/S0166531612000946","","","","","","","","","","","","","Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs.","","Software aging, Fault prediction, Software complexity metrics, Aging-related bugs","",""
7,"","LIU2015165","Liu, Xiying; Zhao, Shancheng; Ma, Xiao","A low complexity detection/decoding algorithm for NB-LDPC coded PRCPM system","Physical Communication",17,,"","165 - 171",2015,"","","http://www.sciencedirect.com/science/article/pii/S1874490715000464","","","","","","","","","","","","","This paper studies the combination of non-binary low-density parity-check (NB-LDPC) codes and M-ary partial response continuous phase modulation (PRCPM). A low-complexity joint detection/decoding algorithm is proposed, which is referred to as the Max-Log-MAP/X-EMS algorithm. In this joint algorithm, the CPM detector is implemented by the Max-Log-MAP algorithm while the LDPC decoder is implemented by the extended min-sum (EMS) algorithms. Three kinds of EMS algorithms, including D-EMS, T-EMS, and M-EMS algorithms, are compared, which are referred to as X-EMS algorithm for convenience. Simulation results show that the Max-Log-MAP/X-EMS algorithm performs as well as the traditional iterative detection/decoding algorithm based on the BCJR algorithm and the q-ary sum–product algorithm, but with lower complexity. In addition, comparison of the proposed NB-LDPC coded PRCPM system with the eBCH coded PRCPM system is given, which shows the performance advantages of our system.","","BCJR algorithm, Iterative decoding, NB-LDPC codes, PRCPM, -EMS algorithms","",""
7,"","THALMANN2014172","Thalmann, Stefan; Bachlechner, Daniel; Demetz, Lukas; Manhart, Markus","Complexity is dead, long live complexity! How software can help service providers manage security and compliance","Computers & Security",45,,"","172 - 185",2014,"","","http://www.sciencedirect.com/science/article/pii/S0167404814000935","","","","","","","","","","","","","Service providers expected to see a simplification regarding security and compliance management as standards and best practice were applied to complex information technology (IT) outsourcing arrangements. However, security and compliance management became even more complex and is presenting greater challenges to service providers than ever before. In this article, we focus on the work practices of service providers dealing with complex and transitory security requirements and distributed IT infrastructures. Based on the results of semi-structured interviews followed by a think-aloud study, we first describe specific requirements to be met by software supporting security and compliance management in complex IT outsourcing arrangements, and discuss the extent to which existing software already meets them. We show that existing software, which is primarily designed for in-house settings, fails to meet requirements of complex IT outsourcing arrangements such as (1) the use of standardized and formal descriptions of security requirements and configurations, (2) the definition of a interface allowing to exchange messages and to delegate tasks, (3) the provision of mechanisms for designing and implementing a configuration for specific security requirements across organizational boundaries, (4) the provision of mechanisms for verifying and approving the enforcement of these security requirements, and (5) the provision of mechanisms for searching and browsing security requirements, configurations and links between them. We then propose a software architecture that claims to be capable of meeting those requirements and outline how this claim was evaluated by means of another think-aloud study in which potential end users were asked to perform a series of tasks using a prototypical implementation of the architecture. The results of the evaluation confirm that the software meets the described requirements and suggests that it facilitates the management of security and compliance in complex IT outsourcing arrangements.","","Security management, Compliance, Service provision, Software architecture, Outsourcing","",""
7,"","STUMM2018174","Stumm, Christopher; Szielasko, Klaus; Granath, Tim; Stauch, Claudia; Mandel, Karl","Raspberry-like supraparticles from nanoparticle building-blocks as code-objects for hidden signatures readable by terahertz rays","Materials Today Communications",16,,"","174 - 177",2018,"","","http://www.sciencedirect.com/science/article/pii/S2352492818301764","","","","","","","","","","","","","Supraparticles, i.e., raspberry-like microparticles which are composed of nanoparticles (iron oxide), reveal specific interaction properties with terahertz (THz) rays. Depending on the density of the clustering of the nanoparticles within the raspberry-like supraparticle, characteristic THz components are altered upon transmission. The clustering can be adjusted upon supraparticle assembly via modification of the nanoparticles’ surfaces. By employing very densely and very loosely clustered supraparticles, a graphical coding system can be developed which allows creating signatures that are hidden in the bulk of a material (an object) and are easily and unambiguously decodable with THz rays.","","supraparticles, codes, markers, anti-counterfeit, terahertz","",""
7,"","URQUIZAFUENTES2013178","Urquiza-Fuentes, Jaime; Velázquez-Iturbide, J. Ángel","Toward the effective use of educational program animations: The roles of student's engagement and topic complexity","Computers & Education",67,,"","178 - 192",2013,"","","http://www.sciencedirect.com/science/article/pii/S0360131513000523","","","","","","","","","","","","","Programming is one of the most complex subjects in computer science degrees. Program visualization is one of the approaches adopted to make programming concepts more accessible to students. In this work we study the educational impact of an active and highly engaging approach, namely the construction of program animations by students. We systematically compared this approach with two instructional scenarios, based on viewing animations and on the traditional instruction without systematic use of animations. A general conclusion of this work is that animations actually improve learning in terms of some educational aspects: short-term and long-term knowledge acquisition, and drop-out rates. Short-term improvements depend on the complexity level of the topic: while there is no impact for simple topics, there is a learning improvement in complex topics using the viewing and constructing approaches, and there is a learning improvement for highly complex topics using the viewing approach. In the long-term, drop-out rates were significantly decreased for students involved in the two most engaging approaches. In addition, both animation viewing and animation construction improved students' passing-rate in the term exam. Nevertheless, we were unable to prove in the long term that students involved in construction tasks yielded higher grades than those involved in viewing tasks.","","Evaluation of CAL systems, Interactive learning environments, Multimedia/hypermedia systems, Programming and programming languages, Simulations","",""
7,"","CHAN20151859","Chan, Tiffany L.; Perlmutter, Monica S.; Andrews, Melva; Sunness, Janet S.; Goldstein, Judith E.; Massof, Robert W.","Equating Visual Function Scales to Facilitate Reporting of Medicare Functional G-Code Severity/Complexity Modifiers for Low-Vision Patients","Archives of Physical Medicine and Rehabilitation",96,10,"","1859 - 1865",2015,"","","http://www.sciencedirect.com/science/article/pii/S0003999315005420","","","","","","","","","","","","","Objective
 To present a method of estimating and equating scales across functional assessment instruments that appropriately represents changes in a patient's functional ability and can be meaningfully mapped to changes in Medicare G-code severity modifiers.
 Design
 Previously published measures of patients' overall visual ability, estimated from low-vision patient responses to 7 different visual function rating scale questionnaires, are equated and mapped onto Medicare G-code severity modifiers.
 Setting
 Outpatient low-vision rehabilitation clinics.
 Participants
 The analyses presented in this article were performed on raw or summarized low-vision patient ratings of visual function questionnaire (VFQ) items obtained from previously published research studies.
 Interventions
 Previously published visual ability measures from Rasch analysis of low-vision patient ratings of items in different VFQs (National Eye Institute Visual Functioning Questionnaire, Index of Visual Functioning, Activities of Daily Vision Scale, Visual Activities Questionnaire) were equated with the Activity Inventory (AI) scale. The 39 items in the Self-Report Assessment of Functional Visual Performance (SRAFVP) and the 48 items in the Veterans Affairs Low Vision Visual Functioning Questionnaire (VA LV VFQ) were paired with similar items in the AI in order to equate the scales.
 Main Outcome Measures
 Tests using different observation methods and indicators cannot be directly compared on the same scale. All test results would have to be transformed to measures of the same functional ability variable on a common scale as described here, before a single measure could be estimated from the multiple measures.
 Results
 Bivariate regression analysis was performed to linearly transform the SRAFVP and VA LV VFQ item measures to the AI item measure scale. The nonlinear relationship between person measures of visual ability on a logit scale and item response raw scores was approximated with a logistic function, and the 2 regression coefficients were estimated for each of the 7 VFQs. These coefficients can be used with the logistic function to estimate functional ability on the same interval scale for each VFQ and for transforming raw VFQ responses to Medicare's G-code severity modifier categories.
 Conclusions
 The principle of using equated interval scales allows for comparison across measurement instruments of low-vision functional status and outcomes, but can be applied to any area of rehabilitation.","","Medicare, Occupational therapy, Outcome assessment (health care), Rehabilitation, Vision, low","",""
7,"","KUHN2013193","Kuhn, Kenneth D.","Ground delay program planning: Delay, equity, and computational complexity","Transportation Research Part C: Emerging Technologies",35,,"","193 - 203",2013,"","","http://www.sciencedirect.com/science/article/pii/S0968090X13001617","","","","","","","","","","","","","The Federal Aviation Administration, in consultation with air carriers, manages Ground Delay Programs, delaying aircraft scheduled to land at capacity constrained airports prior to takeoff to increase the safety and efficiency of air travel. Prior research optimizes Ground Delay Program planning to minimize either delay or a weighted combination of delay and measures of inequity, a key concern in practice. Such approaches have several shortcomings including an inability to find all Pareto-optimal policies and a reliance on (one or many) models relating fundamentally incompatible objectives. This article introduces several two-phase approaches to Ground Delay Program planning that address the problems of weighted sum methods while managing computational burdens, another key concern in practice. A computational study demonstrates the benefits of the new approaches on realistic problem instances.","","Air traffic control, Ground delay program, Delay, Equity, Bi-objective optimization, Bi-criteria optimization, Multi-objective optimization, Multi-criteria optimization","",""
7,"","BLONDEEL20141971","Blondeel, Marjon; Schockaert, Steven; Vermeir, Dirk; Cock, Martine De","Complexity of fuzzy answer set programming under Łukasiewicz semantics","International Journal of Approximate Reasoning",55,9,"","1971 - 2003",2014,"","Weighted Logics for Artificial Intelligence","http://www.sciencedirect.com/science/article/pii/S0888613X13002338","","","","","","","","","","","","","Fuzzy answer set programming (FASP) is a generalization of answer set programming (ASP) in which propositions are allowed to be graded. Little is known about the computational complexity of FASP and almost no techniques are available to compute the answer sets of a FASP program. In this paper, we analyze the computational complexity of FASP under Łukasiewicz semantics. In particular we show that the complexity of the main reasoning tasks is located at the first level of the polynomial hierarchy, even for disjunctive FASP programs for which reasoning is classically located at the second level. Moreover, we show a reduction from reasoning with such FASP programs to bilevel linear programming, thus opening the door to practical applications. For definite FASP programs we can show P-membership. Surprisingly, when allowing disjunctions to occur in the body of rules – a syntactic generalization which does not affect the expressivity of ASP in the classical case – the picture changes drastically. In particular, reasoning tasks are then located at the second level of the polynomial hierarchy, while for simple FASP programs, we can only show that the unique answer set can be found in pseudo-polynomial time. Moreover, the connection to an existing open problem about integer equations suggests that the problem of fully characterizing the complexity of FASP in this more general setting is not likely to have an easy solution.","","Answer set programming, Łukasiewicz logic, Computational complexity","",""
7,"","GU20152","Gu, Zhouye; Zheng, Jianhua; Ling, Nam; Zhang, Philipp","Low complexity Bi-Partition mode selection for 3D video depth intra coding","Displays",40,,"","2 - 8",2015,"","Next Generation TV Systems and Technologies","http://www.sciencedirect.com/science/article/pii/S0141938215000931","","","","","","","","","","","","","This paper proposes a fast mode decision algorithm for 3D High Efficiency Video Coding (3D-HEVC) depth intra coding. In the current 3D-HEVC design, it is observed that for most of the cases, full rate-distortion (RD) cost search of Bi-Partition mode could be skipped since most coding units (CUs) of depth map are very flat or smooth while Bi-Partition modes are designed for CUs with edge or sharp transition. Using the rough RD cost value calculated by HEVC Rough Mode Decision as a selection threshold, we propose a fast Bi-Partition modes selection algorithm to speed up the encoding process. The test result for the proposed fast algorithm reports a 34.4% encoding time saving with a 0.3% bitrate increase on synthesized views for the All-Intra test case and negligible impact under the random access test case. Moreover, by simply varying the selection threshold, we can make a tradeoff between encoding time saving and bitrate loss based on the requirement of different applications.","","Video coding, 3D-HEVC, Bi-Partition mode, Depth map, Intra coding, Low complexity","",""
7,"","TOROPOV201420","Toropov, Andrey A.; Toropova, Alla P.; Raska, Ivan; Leszczynska, Danuta; Leszczynski, Jerzy","Comprehension of drug toxicity: Software and databases","Computers in Biology and Medicine",45,,"","20 - 25",2014,"","","http://www.sciencedirect.com/science/article/pii/S0010482513003429","","","","","","","","","","","","","Quantitative structure–property/activity relationships (QSPRs/QSARs) are a tool (in silico) to rapidly predict various endpoints in general, and drug toxicity in particular. However, this dynamic evolution of experimental data (expansion of existing experimental data on drugs toxicity) leads to the problem of critical estimation of the data. The carcinogenicity, mutagenicity, liver effects and cardiac toxicity should be evaluated as the most important aspects of the drug toxicity. The toxicity is a multidimensional phenomenon. It is apparent that the main reasons for the increase in applications of in silico prediction of toxicity include the following: (i) the need to reduce animal testing; (ii) computational models provide reliable toxicity prediction; (iii) development of legislation that is related to use of new substances; (iv) filling data gaps; (v) reduction of cost and time; (vi) designing of new compounds; (vii) advancement of understanding of biology and chemistry. This mini-review provides analysis of existing databases and software which are necessary for use of robust computational assessments and robust prediction of potential drug toxicities by means of in silico methods.",""," toxicology, methods, QSAR, Computational toxicology, Drug toxicity","",""
7,"","MACKOWIAK2018206","Maćkowiak, Michał; Nawrocki, Jerzy; Ochodek, Mirosław","On some end-user programming constructs and their understandability","Journal of Systems and Software",142,,"","206 - 222",2018,"","","http://www.sciencedirect.com/science/article/pii/S0164121218300633","","","","","","","","","","","","","Context: End-user programming is becoming more and more important. However, existing programming paradigms and the languages based on them seem far-removed from what end-user programmers would need, especially in the area of Management Information. Objective: To evaluate the understandability of a set of programming constructs based on the spreadsheet metaphor from the point of view of end-user programmers in the context of Management Information. The examined set comprises single assignment with exemplary computations, data-driven iterations, selection by colours, and read-write heads (we refer to this as Board Programming). Method:A series of experiments was performed with students of Management Engineering, split into an experimental group and a control group. Each participant was given a piece of code expressed either with the proposed programming construct (experimental group) or its classical counterpart (control group). Their task was to predict the results. For the purpose of evaluation, the FACT indicators of understandability (First attempt failure rate, Attempt number, Cancellation ratio, prediction Time) were proposed and measured. Results:Three of the four examined features, i.e. single assignment with exemplary computations, data-driven iterations, and read-write heads, proved to increase understandability of the chosen programs with regard to three out of the four FACT indicators, and these results were statistically significant. Selection by colours was not as effective as expected: the FACT indicator values were improved by that feature, but the difference was not statistically significant. Conclusions:The described programming constructs appear to be an interesting option when designing an end-user programming language for domain experts in the field of Management Information.","","End-user programming, Code understanding, Spreadsheets","",""
7,"","FERRER20132125","Ferrer, Javier; Chicano, Francisco; Alba, Enrique","Estimating software testing complexity","Information and Software Technology",55,12,"","2125 - 2139",2013,"","","http://www.sciencedirect.com/science/article/pii/S0950584913001535","","","","","","","","","","","","","Context
 Complexity measures provide us some information about software artifacts. A measure of the difficulty of testing a piece of code could be very useful to take control about the test phase.
 Objective
 The aim in this paper is the definition of a new measure of the difficulty for a computer to generate test cases, we call it Branch Coverage Expectation (BCE). We also analyze the most common complexity measures and the most important features of a program. With this analysis we are trying to discover whether there exists a relationship between them and the code coverage of an automatically generated test suite.
 Method
 The definition of this measure is based on a Markov model of the program. This model is used not only to compute the BCE, but also to provide an estimation of the number of test cases needed to reach a given coverage level in the program. In order to check our proposal, we perform a theoretical validation and we carry out an empirical validation study using 2600 test programs.
 Results
 The results show that the previously existing measures are not so useful to estimate the difficulty of testing a program, because they are not highly correlated with the code coverage. Our proposed measure is much more correlated with the code coverage than the existing complexity measures.
 Conclusion
 The high correlation of our measure with the code coverage suggests that the BCE measure is a very promising way of measuring the difficulty to automatically test a program. Our proposed measure is useful for predicting the behavior of an automatic test case generator.","","Evolutionary testing, Complexity, Branch coverage, Search based software engineering, Evolutionary algorithms, Testability","",""
7,"","TIWARI201722","Tiwari, Saurabh; Gupta, Atul","Investigating comprehension and learnability aspects of use cases for software specification problems","Information and Software Technology",91,,"","22 - 43",2017,"","","http://www.sciencedirect.com/science/article/pii/S0950584917304329","","","","","","","","","","","","","Context: Availability of multiple use case templates to document software requirements inevitably requires their characterization in terms of their relevance, usefulness, and the degree of the formality of the expressions. Objective: This paper reports two experimental studies that separately investigate two usability aspects, namely the comprehension and the learnability of use case templates for software specification problems. Method: We judged the comprehension aspect by evaluating the subjects’ understanding of the requirements, specified in eight different use case templates, and the ease with which the changes were made by them in the requirement specifications. The learnability aspect was judged by assessing the completeness, the correctness, and the redundancy of the use case specifications developed by the subjects using these eight use case templates for three software specification problems. Results: Our results suggested that the Kettenis’s use case template was found to be significantly more understandable, and the templates by Tiwari, Yue and Somé were found to be significantly more flexible to adapt to the changes. On the learnability aspect, the way we formulated it, we found different templates to be more complete (Kettenis), correct (Somé), and non-redundant (Tiwari). Conclusion: The specifications documented using a more detailed use case template with an intermediate degree of formality can be more comprehensible and flexible to adapt to the required changes to be made in the specification. A more formal template seems to enhance the learnability as well.","","Use cases, Use case templates, Usability aspects, , , Software specification problem, Experimental study","",""
7,"","AMANI2014223","Amani, Elie; Djouani, Karim; Kurien, Anish","Low Complexity Decoding of the 4×4 Perfect Space-time Block Code","Procedia Computer Science",32,,"","223 - 228",2014,"","The 5th International Conference on Ambient Systems, Networks and Technologies (ANT-2014), the 4th International Conference on Sustainable Energy Information Technology (SEIT-2014)","http://www.sciencedirect.com/science/article/pii/S1877050914006188","","","","","","","","","","","","","The 4x4 perfect space-time block code (STBC) is one type in a family of perfect STBCs that have full rate, full diversity, a non- vanishing constant minimum determinant that improves spectral efficiency, uniform average transmitted energy per antenna, and good shaping. These codes suffer very high complexity maximum likelihood (ML) decoding. The exhaustive ML decoding of the 4x4 perfect STBC for instance has complexity of O(N16), with N being the size of the underlying QAM constellation. This paper suggests a fast decoding algorithm for the 4x4 perfect STBC that reduces decoding complexity from O(N16) to O(N8). The algorithm is based on conditional minimization of the ML decision metric with respect to one subset of symbols given another. This low complexity decoding approach is essentially ML because it suffers no loss in symbol error rate (SER) performance when compared to the exhaustive ML decoding.","","Low complexity decoding, maximum likelihood decoding, multiple-input multiple-output (MIMO), perfect space-time block code.","",""
7,"","RENIERS2014224","Reniers, Dennie; Voinea, Lucian; Ersoy, Ozan; Telea, Alexandru","The Solid* toolset for software visual analytics of program structure and metrics comprehension: From research prototype to product","Science of Computer Programming",79,,"","224 - 240",2014,"","Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)","http://www.sciencedirect.com/science/article/pii/S016764231200086X","","","","","","","","","","","","","Software visual analytics (SVA) tools combine static program analysis and fact extraction with information visualization to support program comprehension. However, building efficient and effective SVA tools is highly challenging, as it involves extensive software development in program analysis, graphics, information visualization, and interaction. We present a SVA toolset for software maintenance, and detail two of its components which target software structure, metrics and code duplication. We illustrate the toolset’s usage for constructing software visualizations with examples in education, research, and industrial contexts. We discuss the design evolution from research prototypes to integrated, scalable, and easy-to-use products, and present several guidelines for the development of efficient and effective SVA solutions.","","Software visualization, Static analysis, Visual tool design","",""
7,"","GRAVINO201523","Gravino, Carmine; Scanniello, Giuseppe; Tortora, Genoveffa","Source-code comprehension tasks supported by UML design models: Results from a controlled experiment and a differentiated replication","Journal of Visual Languages & Computing",28,,"","23 - 38",2015,"","","http://www.sciencedirect.com/science/article/pii/S1045926X14001591","","","","","","","","","","","","","Objective: The main objective is to investigate whether the comprehension of object-oriented source-code increases when it is added with UML class and sequence diagrams produced in the software design phase. Methods: We conducted a controlled experiment and a differentiated replication with young software maintainers. In particular, groups of Bachelor and Master students were involved. Results: The results show that more experienced participants better comprehend source-code when added with UML design models. An average improvement (or benefit) of circa 12% was achieved when the participants accomplished the comprehension task with UML class and sequence diagrams. The results of an analysis on the time to accomplish comprehension tasks showed that less experienced participants significantly spent more time when comprehending source-code with UML design models. This kind of participants spent on average 44.8% of the time to accomplish the same task with source-code alone. Implications: It is useless to give UML design models to comprehend source-code in case maintainers are not adequately experienced with the UML. Furthermore, the less the experience of participants, the more the time to accomplish a comprehension task with UML diagram is.","","Design models, Controlled experiment, Source-code comprehension","",""
7,"","MUNOZ201524","Muñoz, Raúl González; Shehab, Essam; Weinitzke, Martin; Bence, Rachel; Fowler, Chris; Tothill, Stephen; Baguley, Paul","Key Challenges in Software Application Complexity and Obsolescence Management within Aerospace Industry","Procedia CIRP",37,,"","24 - 29",2015,"","CIRPe 2015 - Understanding the life cycle implications of manufacturing","http://www.sciencedirect.com/science/article/pii/S2212827115008537","","","","","","","","","","","","","Software applications are becoming highly critical in aircraft development lifecycle. They provide the capability and functionalities required to design, manufacture and support aerospace assets of increasing complexity. As such criticality rises, there is a need to implement monitoring and management techniques on software application portfolios, in order to optimise service performance. These methodologies will support industry activities, avoiding operation disruption and mitigating the systemic risks that come with a higher complexity, as well as controlling obsolescence issues over time. This paper seeks to understand the key challenges faced when monitoring software lifecycle and obsolescence over large software application portfolios in aerospace industry. The paper describes results from a series of interviews and workshops with industry experts, as well as meetings with academic experts and literature reviews, to provide a discerning sight on the challenges faced when managing large application portfolios, in terms of complexity and obsolescence.","","Software Obsolescence, System Complexity, Software Lifecycle, Aerospace","",""
7,"","FITTKAU2017259","Fittkau, Florian; Krause, Alexander; Hasselbring, Wilhelm","Software landscape and application visualization for system comprehension with ExplorViz","Information and Software Technology",87,,"","259 - 277",2017,"","","http://www.sciencedirect.com/science/article/pii/S0950584916301185","","","","","","","","","","","","","Context: The number of software applications deployed in organizations is constantly increasing. Those applications – often several hundreds – form large software landscapes. Objective: The comprehension of such landscapes and their applications is often impeded by, for instance, architectural erosion, personnel turnover, or changing requirements. Therefore, an efficient and effective way to comprehend such software landscapes is required. Method: In our ExplorViz visualization, we introduce hierarchical abstractions aiming at solving system comprehension tasks fast and accurately for large software landscapes. Besides hierarchical visualization on the landscape level, ExplorViz provides multi-level visualization from the landscape to the level of individual applications. The 3D application-level visualization is empirically evaluated with a comparison to the Extravis approach, with physical models and in virtual reality. To evaluate ExplorViz, we conducted four controlled experiments. We provide packages containing all our experimental data to facilitate the verifiability, reproducibility, and further extensibility of our results. Results: We observed a statistical significant increase in task correctness of the hierarchical visualization compared to the flat visualization. The time spent did not show any significant differences. For the comparison with Extravis, we observed that solving program comprehension tasks using ExplorViz leads to a significant increase in correctness and in less or similar time spent. The physical models improved the team-based program comprehension process for specific tasks by initiating gesture-based interaction, but not for all tasks. The participants of our virtual reality experiment with ExplorViz rated the realized gestures for translation, rotation, and selection as highly usable. However, our zooming gesture was less favored. Conclusion: The results backup our claim that our hierarchical and multi-level approach enhances the current state of the art in landscape and application visualization for better software system comprehension, including new forms of interaction with physical models and virtual reality.","","Software visualization, Dynamic analysis, System comprehension","",""
7,"","WEIJIA201626","Weijia, Lei; Shengnan, Chen","Performance analysis of complexity reduction BP decoding of rateless codes by deleting low reliable symbols","The Journal of China Universities of Posts and Telecommunications",23,5,"","26 - 31",2016,"","","http://www.sciencedirect.com/science/article/pii/S1005888516600547","","","","","","","","","","","","","This paper investigates the performance of the method used to reduce the decoding complexity of rateless codes through the deletion of the received symbols with low reliability. In the decoder, the received symbols whose absolute value of logarithm likelihood ratio (LLR) is lower than the threshold are removed, together with their corresponding edges, and thus not involved in the decoding process. The relationship between the deletion probability and the likelihood ratio deletion threshold is derived. The average mutual information per received symbol is analyzed in the case of deletion. The required number of symbols for the decoder to keep the same performance as regular decoding decreases since the average mutual information per symbol increases with the deletion, thus reducing the decoding complexity. This paper analyzes the reduction of decoding computations and the consequent transmission efficiency loss from the perspective of mutual information. The simulation results of decoding performance are consistent with those of the theoretical analysis, which show that the method can effectively reduce the decoding complexity at the cost of a slight loss of transmission efficiency.","","rateless codes, symbols deletion, logarithm likelihood ratio, average mutual information, decoding complexity","",""
7,"","ERACAR20122697","Eracar, Yönet A.; Kokar, Mieczyslaw M.","Self-control of the time complexity of a constraint satisfaction problem solver program","Journal of Systems and Software",85,12,"","2697 - 2706",2012,"","Self-Adaptive Systems","http://www.sciencedirect.com/science/article/pii/S0164121212001501","","","","","","","","","","","","","This paper presents the self-controlling software paradigm and reports on its use to control the branch and bound based constraint satisfaction problem solving algorithm. In this paradigm, an algorithm is first conceptualized as a dynamical system and then a feedback control loop is added to control its behavior. The loop includes a Quality of Service component that assesses the performance of the algorithm during its run time and a controller that adjusts the parameters of the algorithm in order to achieve the control goal. Although other approaches – generally termed as “self-*” – make use of control loops, this use is limited to the structure of the software system, rather than to its behavior and its dynamics. This paper advocates the analysis of dynamics of any program with control loops. The self-controlling software paradigm is evaluated on two different NP-hard constraint satisfaction and optimization problems. The results of the evaluation show an improvement in the performance due to the added control loop for both of the tested constraint satisfaction problems.","","Self-controlling software, Constraint satisfaction problem, Branch and bound algorithm, PID controller, Job scheduling problem, Fixture design problem","",""
7,"","FABER2011278","Faber, Wolfgang; Pfeifer, Gerald; Leone, Nicola","Semantics and complexity of recursive aggregates in answer set programming","Artificial Intelligence",175,1,"","278 - 298",2011,"","John McCarthy's Legacy","http://www.sciencedirect.com/science/article/pii/S000437021000038X","","","","","","","","","","","","","The addition of aggregates has been one of the most relevant enhancements to the language of answer set programming (ASP). They strengthen the modelling power of ASP in terms of natural and concise problem representations. Previous semantic definitions typically agree in the case of non-recursive aggregates, but the picture is less clear for aggregates involved in recursion. Some proposals explicitly avoid recursive aggregates, most others differ, and many of them do not satisfy desirable criteria, such as minimality or coincidence with answer sets in the aggregate-free case. In this paper we define a semantics for programs with arbitrary aggregates (including monotone, antimonotone, and nonmonotone aggregates) in the full ASP language allowing also for disjunction in the head (disjunctive logic programming — DLP). This semantics is a genuine generalization of the answer set semantics for DLP, it is defined by a natural variant of the Gelfond–Lifschitz transformation, and treats aggregate and non-aggregate literals in a uniform way. This novel transformation is interesting per se also in the aggregate-free case, since it is simpler than the original transformation and does not need to differentiate between positive and negative literals. We prove that our semantics guarantees the minimality (and therefore the incomparability) of answer sets, and we demonstrate that it coincides with the standard answer set semantics on aggregate-free programs. Moreover, we carry out an in-depth study of the computational complexity of the language. The analysis pays particular attention to the impact of syntactical restrictions on programs in the form of limited use of aggregates, disjunction, and negation. While the addition of aggregates does not affect the complexity of the full DLP language, it turns out that their presence does increase the complexity of normal (i.e., non-disjunctive) ASP programs up to the second level of the polynomial hierarchy. However, we show that there are large classes of aggregates the addition of which does not cause any complexity gap even for normal programs, including the fragment allowing for arbitrary monotone, arbitrary antimonotone, and stratified (i.e., non-recursive) nonmonotone aggregates. The analysis provides some useful indications on the possibility to implement aggregates in existing reasoning engines.","","Nonmonotonic reasoning, Answer set programming, Aggregates, Computational complexity","",""
7,"","BUJAKI201928","Bujaki, Merridee; Lento, Camillo; Sayed, Naqi","Utilizing professional accounting concepts to understand and respond to academic dishonesty in accounting programs","Journal of Accounting Education",47,,"","28 - 47",2019,"","","http://www.sciencedirect.com/science/article/pii/S0748575118301350","","","","","","","","","","","","","We apply professional accounting concepts to academic fraud in accounting education. First, we use the fraud triangle to understand professors’ perceptions of academic dishonesty and find two components to each fraud triangle corner. Specifically, the attitude and pressure corners have elements of faculty and student agency, while the opportunity corner is within a professor’s control. Second, risk mapping reveals plagiarism and exam cheating as more impactful than assessment protocols. We also find that faculty efforts to control academic dishonesty are mostly well-directed; however, there are opportunities to employ both preventive and detective controls more frequently.","","Fraud triangle, Risk management techniques, Academic dishonesty, Controls for academic dishonesty","",""
7,"","ALAOUI2018284","Alaoui, M. S. El Kasmi; Nouh, S.; Marzak, A.","A low complexity soft decision decoder for linear block codes","Procedia Computer Science",127,,"","284 - 292",2018,"","PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017","http://www.sciencedirect.com/science/article/pii/S1877050918301364","","","","","","","","","","","","","The Chase-2 decoding algorithm is an efficient soft input hard output decoder that uses a list of most likely error patterns. The main idea behind this decoder is to use a hard decision decoder HD in 2t times, where t is the error correcting capability of the used code. The complexity of Chase-2 decoder is then of order 2t.O(HD). The choice of the HD decoder impacts considerably the complexity. In this paper, we present the Chase-HSDec decoder where we have integrated the low complexity hard decision decoder HSDec that we have recently developed as component decoder. Given its low temporal complexity comparing to their competitors, the use of HSDec in the Chasing technique, decrease the temporal complexity. Chase-HSDec is applied to decode some BCH and QR codes; the simulation results show that the proposed solution yield to good error correcting performances. The comparisons between the proposed decoder and some competitors have shown that it exceeds them. The main advantage of this decoder is that it guarantees the same performances of Chase-Berlekamp Massey decoder with reduced temporal complexity. Contrary to the Chase-Berlekamp Massey decoder which is generally used for BCH codes; it is possible to apply Chase-HSDec on other linear codes defined with their generator matrix.","","Error correcting codes, Soft Input Hard Output decoder, HSDec, Chase-2 algorithm","",""
7,"","ABDULRAHMAN2014286","Abdul-Rahman, Siti-Soraya; du Boulay, Benedict","Learning programming via worked-examples: Relation of learning styles to cognitive load","Computers in Human Behavior",30,,"","286 - 298",2014,"","","http://www.sciencedirect.com/science/article/pii/S074756321300335X","","","","","","","","","","","","","This paper describes an experiment that compared learners with contrasting learning styles, Active vs. Reflective, using three different strategies for learning programming via worked-examples: Paired-method, Structure-emphasising, and Completion. The quality of the learners’ acquired cognitive schemata was assessed in terms of their post-test performance. The experiment investigated variations in learners’ cognitive load, taking both the learning strategies and the learners’ learning styles into account. Overall, the results of the experiment were inconsistent. In comparing the effects of the strategies during the learning phase, the study found significant differences in cognitive load. Unexpectedly, no differences were then detected either in cognitive load or in performance during the post-test (post-test). In comparing the effects of the learning styles during the learning phase and the transfer phase, medium effect sizes suggested that learning style may have had an effect on cognitive load. However, no significant difference was observed in performance during the post-test.","","Cognitive load, Learning styles, Worked-example strategies, Programming","",""
7,"","CIMINI201929","Cimini, Gionata; Bemporad, Alberto","Complexity and convergence certification of a block principal pivoting method for box-constrained quadratic programs","Automatica",100,,"","29 - 37",2019,"","","http://www.sciencedirect.com/science/article/pii/S0005109818305247","","","","","","","","","","","","","Active-set (AS) methods for quadratic programming (QP) are particularly suitable for real-time optimization, as they provide a high-quality solution in a finite number of iterations. However, they add or remove one constraint at each iteration, which makes them inefficient when the number of constraints and variables grows. Block principal pivoting (BPP) methods perform instead multiple changes in the working set in a single iteration, resulting in a much faster execution time. The infeasible primal–dual method proposed by Kunisch and Rendl (KR) (Kunisch and Rendl, 2003) is a BPP method for box-constrained QP that is particularly attractive when reducing the time for finding an accurate solution is crucial, such as in linear model predictive control (MPC) applications. However, the method is guaranteed to converge only under very restrictive sufficient conditions, and tight bounds on the worst-case complexity are not available. For a given set of box-constrained QP’s that depend on a vector of parameters, such as those that arise in linear MPC, this paper proposes an algorithm that computes offline the exact number of iterations and flops needed by the KR method in the worst-case, and the region of the parameter space for which the method converges or is proved to cycle.","","Quadratic programming, Active-set methods, Block principal pivoting methods, Linear model predictive control, Complexity certification","",""
7,"","JUN20183","Jun, Dongsan","A fast coding unit mode decision method based on the mode inheritance of upper coding unit for low-complexity compression of smart contents","Displays",55,,"","3 - 9",2018,"","Advances in Smart Content-Oriented Display Technology","http://www.sciencedirect.com/science/article/pii/S0141938218300891","","","","","","","","","","","","","In combination with Internet-of-things (IoT) technology, realistic media will continue to evolve with more smart and intelligent media services via interactions between users and displays. Therefore, it is important to efficiently compress smart contents on low-power and low-complexity hand-held displays. High efficiency video coding (HEVC) is the latest video coding technology that can achieve excellent coding performance, but at the cost of tremendously increased complexity due to the use of newly adopted video coding tools. Most previous fast encoding methods were mainly investigated in terms of spatio-temporal correlation between the current coding unit (CU) and neighboring CUs. As HEVC has a recursive quad-tree structure, the correlation between different CU depths can be higher than the spatio-temporal correlation. In this study, a fast CU mode decision method is proposed for estimating the mode of a current CU to be encoded either as intra or inter mode prior to encoding a current CU. The method uses the information of the best upper CU mode, where an upper CU indicates a larger block than a current CU in the quad-tree structure. The experimental results show that the encoding process of the proposed method is faster than that of a previous method with an unnoticeable coding loss.","","Video coding, HEVC, Complexity reduction","",""
7,"","OHARE201830","O’Hare, Liam; Stark, Patrick; McConnellogue, Sheila; Lloyd, Katrina; Cockerill, Maria; Biggart, Andy","Protocol: A cluster randomised controlled trial of Reciprocal Reading: A teacher training comprehension programme","International Journal of Educational Research",92,,"","30 - 42",2018,"","","http://www.sciencedirect.com/science/article/pii/S0883035518306530","","","","","","","","","","","","","This paper presents the research protocol for a pragmatic RCT of the Reciprocal Reading programme. Reciprocal Reading (RR) is a workforce development programme that supports practicing Teachers/Teaching Assistants develop and deliver comprehension instruction in mainstream UK settings for pupils aged 8–11 years. The protocol outlines a research design that will assess whether the RR programme can improve a number of specific outcomes in intervention group pupils, in a sample of schools experiencing higher than average levels of disadvantage. The primary outcome for analysis is reading comprehension with secondary outcomes of overall literacy, reading accuracy and comprehension pre-cursors at the child and school level. The study will also include a process evaluation using qualitative data and quantitative implementation data.","","Reciprocal Reading, Comprehension, Primary school, Teacher training, Literacy, RCT","",""
7,"","KAUR201931","Kaur, Loveleen; Mishra, Ashutosh","Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe","Information and Software Technology",106,,"","31 - 48",2019,"","","http://www.sciencedirect.com/science/article/pii/S0950584918301903","","","","","","","","","","","","","Context
 It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
 Objective
 This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
 Method
 For multiple successive releases of two Java-based software projects, where the source code of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's source code Java files. We construct eight datasets and build predictive models using statistical analysis and machine learning techniques.
 Results
 The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change.","","Cognitive complexity, Software change, Software metrics, Logistic regression analysis, Machine learning","",""
7,"","ZHANG2013313","Zhang, Yan; Surisetty, Sheela; Scaffidi, Christopher","Assisting comprehension of animation programs through interactive code visualization","Journal of Visual Languages & Computing",24,5,"","313 - 326",2013,"","","http://www.sciencedirect.com/science/article/pii/S1045926X13000402","","","","","","","","","","","","","Visual languages have been widely used to help people create animation programs. However, current programming environments lack features supporting efficient code exploration and program comprehension, particularly for understanding relationships among parts of animation programs. In this paper, we present novel interactive visualizations aimed at helping people to understand animation programs. We conducted an empirical study to evaluate the impact of these visualizations on programmer comprehension of the code, showing that our approach enabled programmers to comprehend more information with less effort and in less time. This result is potentially significant because it demonstrates an approach for helping users to explore and understand animation code. We anticipate that this approach could be applied in a wide variety of animation programming tools, which could ease common animation programming tasks that require understanding code.","","Animation programming, Visualization, Novice programmers","",""
7,"","FANG201634","Fang, Jiunn-Tsair; Chen, Zong-Yi; Lai, Chang-Rui; Chang, Pao-Chi","Computational complexity allocation and control for inter-coding of high efficiency video coding with fast coding unit split decision","Journal of Visual Communication and Image Representation",40,,"","34 - 41",2016,"","","http://www.sciencedirect.com/science/article/pii/S1047320316300955","","","","","","","","","","","","","HEVC provides the quadtree structure of the coding unit (CU) with four coding-tree depths to facilitate high coding efficiency. However, compared with previous standards, the HEVC encoder increases computational complexity considerably, thus making it inappropriate for applications in power-constrained devices. This study therefore proposes a computational complexity allocation and control method for the low-delay P-frame configuration of the HEVC encoder. The complexity allocation includes the group of pictures (GOP) layer, the frame layer, and the CU layer in the HEVC encoder. Each layer involved uses individual method to distribute the complexity. In particular, motion vector estimation information is applied for CU complexity allocation and depth split determination. The total computational complexity can thus be reduced to 80% and 60% or even lower. Experiment results revealed that the average BD-PSNR exhibited a decrease of approximately 0.1dB and a BD-bitrate increment of 2% when the target complexity was reduced to 60%.","","High-efficiency video coding (HEVC), Complexity allocation, Complexity control, Coding unit, Motion vector","",""
7,"","JABALLAH201834","Jaballah, Sami; Larabi, Mohamed-Chaker; Tahar, Jamel Belhadj","Low complexity intra prediction mode decision for 3D-HEVC depth coding","Signal Processing: Image Communication",67,,"","34 - 47",2018,"","","http://www.sciencedirect.com/science/article/pii/S0923596518304703","","","","","","","","","","","","","The 3D High Efficiency Video Coding (3D-HEVC) is the latest 3D extension of the HEVC video coding standard. It supports multi-view videos plus depth (MVD), which is a sophisticated format for enhanced 3D content. Depth modeling modes (DMM) are adopted in the 3D-HEVC for better sharp edge encoding. However, employing DMM causes important increase of computational complexity. In this paper, we propose a three steps scheme for fast intra depth coding in 3D-HEVC. Based on the kirsch edge detection algorithm, a fast angular mode decision algorithm is performed. In order to avoid unnecessary evaluation of the DMMs, a new early termination approach (ET) based on the smoothness of the processed block is adopted. Besides, based on the enhanced shuffled frog leaping algorithm (E-SFLA), a new heuristic method to search the optimal wedgelet pattern for DMM1 Depth mode is introduced. The proposed algorithm is implemented on the top of four different versions of HTM. It outperforms HTM-16.0, the latest used version, and four different work from the literature, in terms of encoding time with similar coding and quality efficiency.","","3D high-efficiency video coding (3D-HEVC), Intra prediction, Heuristic, Wedgelet, Low complexity","",""
7,"","YEH2018342","Yeh, Chia-Hung; Tseng, Wen-Yu; Kang, Li-Wei; Lee, Cheng-Wei; Muchtar, Kahlil; Chen, Mei-Juan","Coding unit complexity-based predictions of coding unit depth and prediction unit mode for efficient HEVC-to-SHVC transcoding with quality scalability","Journal of Visual Communication and Image Representation",55,,"","342 - 351",2018,"","","http://www.sciencedirect.com/science/article/pii/S1047320318301330","","","","","","","","","","","","","To support good video quality of experiences in heterogeneous environments, transcoding an existed HEVC (high efficiency video coding) video bitstream to a SHVC (scalability extension of HEVC) bitstream with quality scalability is highly required. A straightforward way is to first fully decode the input HEVC bitstream and then fully re-encode it with the SHVC encoder, which requires a tremendous computational complexity. To solve the problem, in this paper, a coding unit complexity (CUC)-based prediction method for predictions of CU (coding unit) depth and PU (prediction unit) mode for efficient HEVC-to-SHVC transcoding with quality scalability is proposed to significantly reduce the transcoding complexity. The proposed method contains two prediction techniques, including (i) early termination and (ii) adaptive confidence interval, and predicts the CU depth and PU mode relying on the decoded information from the input HEVC bitstream. Experimental results have shown that the proposed method significantly outperforms the traditional HEVC-to-SHVC method by 74.14% on average in reductions of encoding time for SHVC enhancement layer.","","HEVC (high efficiency video coding), SHVC (scalability extension of HEVC), Video transcoding, Scalable video coding, Early termination, Coding unit complexity","",""
7,"","SMITH2011355","Smith, Jason McC.","The Pattern Instance Notation: A simple hierarchical visual notation for the dynamic visualization and comprehension of software patterns","Journal of Visual Languages & Computing",22,5,"","355 - 374",2011,"","","http://www.sciencedirect.com/science/article/pii/S1045926X11000188","","","","","","","","","","","","","Design patterns are a common tool for developers and architects to understand and reason about a software system. Visualization techniques for patterns tend to be either highly theoretical in nature or based on a structural view of a system's implementation. The Pattern Instance Notation is a simple notation technique for visualizing design patterns and other abstractions of software engineering. While based on a formal representation of design patterns, PIN is a tool for comprehension or reasoning which requires no formal training or study, and it is suitable for the programmer or designer without a theoretical background. PIN is hierarchical in nature and compactly encapsulates abstractions that may be spread widely across a system in a concise graphical format, while allowing for repeated unveiling of deeper layers of complexity and interaction on demand. It is designed to be used in either a dynamic visualization tool, or as a static representation for documentation and as a teaching aid.","","Design patterns, Visualization, Education, Comprehension","",""
7,"","VIJAYANAGAR2014361","Vijayanagar, Krishna Rao; Kim, Joohee; Lee, Yunsik; Kim, Jong-bok","Low complexity distributed video coding","Journal of Visual Communication and Image Representation",25,2,"","361 - 372",2014,"","","http://www.sciencedirect.com/science/article/pii/S1047320313002204","","","","","","","","","","","","","Context
 Conventional video encoding is a computationally intensive process that requires a lot of computing resources, power and memory. Such codecs cannot be deployed in remote sensors that are constrained in terms of power, memory and computational capabilities. For such applications, distributed video coding might hold the answer.
 Objective
 In this paper, we propose a distributed video coding (DVC) architecture that adheres to the principles of DVC by shifting the computational complexity from the encoder to the decoder and caters to low-motion scenarios like video conferencing and surveillance of hallways and buildings.
 Method
 The architecture presented is block-based and introduces a simple yet effective classification scheme that aims at maximizing the use of skip blocks to exploit temporal correlation between consecutive frames. In addition to the skip blocks, a dynamic GOP size control algorithm is proposed that instantaneously alters the GOP size in response to the video statistics without causing any latency and without the need to buffer additional frames at the encoder. To facilitate real-time video delivery and consumption, iterative channel codes like low density parity check codes and turbo codes are not used and in their place a Bose–Chaudhuri–Hocquenghem (BCH) code with encoder rate control is used.
 Results
 In spite of reducing the complexity and eliminating the feedback channel, the proposed architecture can match and even surpass the performance of current DVC systems making it a viable solution as a codec for low-motion scenarios.
 Conclusion
 We conclude that the proposed architecture is a suitable solution for applications that require real-time, low bit rate video transmission but have constrained resources and cannot support the complex conventional video encoding solutions.
 Practical implications
 The practical implications of the proposed DVC architecture include deployment in remote video sensors like hallway and building surveillance, video conferencing, video sensors that are deployed in remote regions (wildlife surveillance applications), and capsule endoscopy.","","Distributed video coding, Wyner–Ziv, GOP size control, Video surveillance, Encoder rate control, BCH code, Unidirectional, Low-complexity","",""
7,"","WU2015367","Wu, Yating; Zhu, Y. S.; Leung, S. H.; Wong, W. K.; Wang, Tao","Low-complexity code tracking loop with multipath diversity for GNSS over multipath fading channels","Acta Astronautica",115,,"","367 - 375",2015,"","","http://www.sciencedirect.com/science/article/pii/S0094576515002325","","","","","","","","","","","","","A low-complexity noncoherent code tracking loop is proposed for global navigation satellite systems (GNSS) over multipath fading channels. Instead of trying to remove the multipath interference from the received signal or utilizing complex signal processing techniques such as maximum-likelihood estimation of the delay parameters, the proposed scheme collects useful component from the multipath signal by introducing assistant signals into the local reference signals. Cross-correlation from neighboring paths is combined constructively and forms an enhanced effective discriminator characteristic. Multipath diversity is thereby achieved with low computational complexity. The S-curve of the new loop is shown to be bias-free and odd-symmetric. Mean square tracking errors obtained by both linear and nonlinear analyses are used to assess the loop׳s tracking performance in terms of accuracy and robustness.","","Global navigation satellite system (GNSS), Code tracking loop, Multipath channels","",""
7,"","MOONS2013368","Moons, Jan; Backer, Carlos De","The design and pilot evaluation of an interactive learning environment for introductory programming influenced by cognitive load theory and constructivism","Computers & Education",60,1,"","368 - 384",2013,"","","http://www.sciencedirect.com/science/article/pii/S0360131512001959","","","","","","","","","","","","","This article presents the architecture and evaluation of a novel environment for programming education. The design of this programming environment, and the way it is used in class, is based on the findings of constructivist and cognitivist learning paradigms. The environment is evaluated based on qualitative student and teacher evaluations and experiments performed over a three year timespan. As the findings show, the students and teachers see the environment and the way it is used as an invaluable part of their education, and the experiments show that the environment can help with understanding programming concepts that most students consider very difficult.","","Architectures for educational technology system, Interactive learning environments, Programming and programming languages","",""
7,"","FEREE201541","Férée, Hugo; Hainry, Emmanuel; Hoyrup, Mathieu; Péchoux, Romain","Characterizing polynomial time complexity of stream programs using interpretations","Theoretical Computer Science",585,,"","41 - 54",2015,"","Developments in Implicit Complexity","http://www.sciencedirect.com/science/article/pii/S0304397515002017","","","","","","","","","","","","","This paper provides a criterion based on interpretation methods on term rewrite systems in order to characterize the polynomial time complexity of second order functionals. For that purpose it introduces a first order functional stream language that allows the programmer to implement second order functionals. This characterization is extended through the use of exp-poly interpretations as an attempt to capture the class of Basic Feasible Functionals (bff). Moreover, these results are adapted to provide a new characterization of polynomial time complexity in computable analysis. These characterizations give a new insight on the relations between the complexity of functional stream programs and the classes of functions computed by Oracle Turing Machine, where oracles are treated as inputs.","","Stream programs, Type-2 functionals, Interpretations, Polynomial time, Basic feasible functionals, Computable analysis, Rewriting","",""
7,"","DEWI2017415","Dewi, Renny Sari; Subriadi, Apol Pribadi; Sholiq","A Modification Complexity Factor in Function Points Method for Software Cost Estimation Towards Public Service Application","Procedia Computer Science",124,,"","415 - 422",2017,"","4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia","http://www.sciencedirect.com/science/article/pii/S187705091732940X","","","","","","","","","","","","","Lately, the stages of planning software development projects have begun to consider the scientific side. As an intangible outcome, the budgeting must also be done in a transparent and accountable manner. The authors use Function Points (FP) method approach on the basis of 4 main reasons for estimating the effort and cost of software development (4 public service applications as research object). In this study, there are two core phases, first, elaborating complexity factors based on other method references (i.e Use Case Points) and mapping of non-functional requirements on Term of Reference. Furthermore, the second phase is to calculate and compare the estimated effort and cost if the original FP method before and after modified on the complexity factor. We conclude there is a difference of 7.19 percent (equivalent to IDR 13,567,631) between FP method calculations using and not using modified complexity adjustment factors.","","Software Estimation, Software Size, Software Cost, Function Points, Public Service Application","",""
7,"","BIAGI2010434","Biagi, Laura; Cioni, Giovanni; Fogassi, Leonardo; Guzzetta, Andrea; Tosetti, Michela","Anterior intraparietal cortex codes complexity of observed hand movements","Brain Research Bulletin",81,4,"","434 - 440",2010,"","","http://www.sciencedirect.com/science/article/pii/S0361923009003840","","","","","","","","","","","","","Human and monkey studies clearly show that the anterior intraparietal area (AIP) is crucial for hand-related visuomotor transformations. Human AIP activates also during observation of hand actions, involving it in the mirror system. It is not known, however, whether its activation can also reflect a difference in the complexity of the observed action. In the present study we used functional magnetic resonance imaging (fMRI) to explore the activation of human area AIP during the observation of complex object-manipulation tasks (e.g. inserting a key in a lock and turning it) as compared to simple tasks (whole hand grasping of an object) executed with the left and the right hand in a first person perspective. The results show that, in general, both complex and simple tasks produced an activation of the fronto-parietal mirror system and that the activity of AIP in each hemisphere was higher during observation of the contralateral hand (hand identity effect). A Region-Of-Interest (ROI) analysis of the parietal activations responding to hand identity showed that each AIP was more active during the observation of complex with respect to simple tasks. In the right AIP this effect was stronger during observation of the contralateral hand, in the left AIP was strong during observation of both hands. This complexity-related property was not observed in the other activated areas. These findings support the concept that the observation of motor acts retrieves the internal representation of those same acts in the observer's motor system (direct-matching hypothesis based on the mirror neuron mechanism).","","Action observation, Mirror neuron, Manipulation, Parietal cortex, Hand identity, Grasping","",""
7,"","ZAMAN2019444","Zaman, Umer; Jabbar, Zulaikha; Nawaz, Shahid; Abbas, Mazhar","Understanding the soft side of software projects: An empirical study on the interactive effects of social skills and political skills on complexity – performance relationship","International Journal of Project Management",37,3,"","444 - 460",2019,"","","http://www.sciencedirect.com/science/article/pii/S0263786318306379","","","","","","","","","","","","","While prior research considers project complexity as a double-edged sword, researchers and practitioners still remain unclear whether project complexity serves as productive or counterproductive ingredient for project performance. Our research brings clarity on the dynamic nature of complexity-performance relationship by integrating social exchange theory with recent developments in project management research to develop and test a novel framework involving interactive roles of social skills and political skills in software-projects. Regardless of calls for further empirical studies, researchers have predominantly neglected the fundamental role of human efforts and human interaction in outlining performance particularly in complex projects. Drawing on a survey based sample of 242 project managers and use of variance based structural equation modeling, the findings illuminate theoretical and practical contributions in better understanding complexities in software-projects performance. In addition, prioritizing human-centric factors i.e. social skills and political skills in supporting complexity- performance relationship further enhances contributions of this research.","","Project complexity, Project performance, Social skills, Political skills, Structural equation Modeling, SMART PLS","",""
7,"","ZHANG201746","Zhang, Lihua; Li, Rong; He, Junyi; Yang, Qiuping; Wu, Yanan; Huang, Jingshan; Wu, Bin","Co-expression analysis among microRNAs, long non-coding RNAs, and messenger RNAs to understand the pathogenesis and progression of diabetic kidney disease at the genetic level","Methods",124,,"","46 - 56",2017,"","Integrative Analysis of Omics Data","http://www.sciencedirect.com/science/article/pii/S1046202317300464","","","","","","","","","","","","","Diabetic kidney disease (DKD) is a serious disease that presents a major health problem worldwide. There is a desperate need to explore novel biomarkers to further facilitate the early diagnosis and effective treatment in DKD patients, thus preventing them from developing end-stage renal disease (ESRD). However, most regulation mechanisms at the genetic level in DKD still remain unclear. In this paper, we describe our innovative methodologies that integrate biological, computational, and statistical approaches to investigate important roles performed by regulations among microRNAs (miRs), long non-coding RNAs (lncRNAs), and messenger RNAs (mRNAs) in DKD. We conducted fully transparent, rigorously designed experiments. Our robust and reproducible results identified hsa-miR-223-3p as a candidate novel biomarker performing important roles in DKD disease process.","","MicroRNA (miR), Long non-coding RNA (lncRNA), Diabetes, Diabetic kidney disease (DKD), Co-expression analysis, Biomarker","",""
7,"","ZHOU201746","Zhou, Mingliang; Zhang, Yongfei; Li, Bo; Hu, Hai-Miao","Complexity-based intra frame rate control by jointing inter-frame correlation for high efficiency video coding","Journal of Visual Communication and Image Representation",42,,"","46 - 64",2017,"","","http://www.sciencedirect.com/science/article/pii/S1047320316302395","","","","","","","","","","","","","Rate control is of great significance for the High Efficiency Video Coding (HEVC). Due to the high efficiency and low complexity, the R-lambda model has been applied to the HEVC as the default rate control algorithm. However, the video content complexity, which can help improve the code efficiency and rate control performance, is not fully considered in the R-lambda model. To address this problem, an intra-frame rate control algorithm, which aims to provide improved and smooth video quality, is developed in this paper by jointly taking into consideration the frame-level content complexity between the encoded intra frames and the encoded inter frame, as well as the CTU-level complexity among different CTUs in texture–different regions for intra-frame. Firstly, in order to improve the rate control efficiency, this paper introduces a new prediction measure of content complexity for CTUs of intra-frame by jointly considering the inter-frame correlations between encoding intra frame and previous encoded inter frames as well as correlations between encoding intra frame and previous encoded intra frame. Secondly, a frame-level complexity-based bit-allocation-balancing method, by jointly considering the inter-frame correlation between intra frame and previous encoded inter frame, is brought up so that the smoothness of the visual quality can be improved between adjacent inter- and intra-frames. Thirdly, a new region-division and complexity-based CTU-level bit allocation method is developed to improve the objective quality and to reduce PSNR fluctuation among CTUs in intra-frame. Intheend, related model parameters are updated during the encoding process to increase rate control accuracy. As a result, as can be seen from the extensive experimental results that compared with the state-of-the-art schemes, the video quality can be significantly improved. More specifically, up to 10.5% and on average 5.2% BD-Rate reduction was achieved compared to HM16.0 and up to 2.7% and an average of 2.0% BD-Rate reduction was achieved compared tostate-of-the-art algorithm. Besides, a superior performance in enhancing the smoothness of quality can be achieved, which outperforms the state-of-the-art algorithms in term of flicker measurement, frame and CTU-wise PSNR, as well as buffer fullness.","","HEVC, Complexity, Intra-frame, R-lambda model, Region-based, Quality smoothness","",""
7,"","DICKIN2015465","Dickin, Katherine L.; Larios, Flor; Parra, Pilar A.","Cognitive Interviewing to Enhance Comprehension and Accuracy of Responses to a Spanish-Language Nutrition Program Evaluation Tool","Journal of Nutrition Education and Behavior",47,5,"","465 - 471.e1",2015,"","","http://www.sciencedirect.com/science/article/pii/S1499404615005448","","","","","","","","","","","","","Objective
 To cognitively test a Spanish translation of a questionnaire evaluating parent and child food and activity behaviors and assess accuracy of understanding and ease of answering.
 Methods
 Iterative rounds of cognitive interviewing, qualitative analysis, and revision were conducted with 19 low-income, native Spanish-speaking mothers of children aged 3–11 years, in 5 communities in New York. Key informant interviews were conducted with 2 Spanish-speaking nutrition educators experienced with the questionnaire.
 Results
 Based on responses, improvements were made to (1) ensure clear and familiar wording, (2) clarify time frames for specifying the frequency of behaviors, and (3) express constructs not amenable to direct translation or for which meanings differed by country of origin. Cognitive interviewing results also informed improvements to the English language version.
 Conclusions and Implications
 Even after translation by native speakers, in-depth cognitive interviewing is needed to ensure that questionnaires are understood as intended by low-literacy, immigrant populations and to facilitate collection of valid evaluation data.","","low-income families, Hispanic, questionnaire development, nutrition education","",""
7,"","HAO2012465","Hao, Wu; Guoqing, Wu","On the Concept of Trusted Computing and Software Watermarking: A Computational Complexity Treatise","Physics Procedia",25,,"","465 - 474",2012,"","International Conference on Solid State Devices and Materials Science, April 1-2, 2012, Macao","http://www.sciencedirect.com/science/article/pii/S1875389212005287","","","","","","","","","","","","","Trusted computing is a new requirement of information security, after the successful modern cryptography. Although there are many developments of methods and techniques to address on the problems of trusted computing, the lack of rigorous theoretic treatment makes it hard to give further mathematical treatment. In this paper, we develop theory and concepts of trusted computing founded on computational complexity, which analogies to modern cryptography and find close connections between trusted computing and one way functions, one of the pillars of the modern cryptography. After that, we reveal that trusted computing has a subtle but deep relation with software watermarking, that concept can be found on the concept of trusted computing.","","Trusted computing, Trust measurement, one way function, software watermarking","",""
7,"","CHEN20164758","Chen, Ming; Yang, Yongshuang; Zhang, Qiuwen; Zhao, Xiaoxin; Huang, Xinpeng; Gan, Yong","Low complexity depth mode decision for HEVC-based 3D video coding","Optik",127,11,"","4758 - 4767",2016,"","","http://www.sciencedirect.com/science/article/pii/S003040261630047X","","","","","","","","","","","","","The emerging international standard high efficiency video coding (HEVC) based 3D video coding (3D-HEVC) was recently standardized by the Joint Collaborative Team on 3D video coding (JCT-3V) as an extension of HEVC for coding the multi-view video plus depth content. In test model of 3D-HEVC, new depth inter and intra modes including disparity estimation (DE), depth modeling modes (DMM) and region boundary Chain coding (RBC) are exploited for depth map coding. These new inter and intra modes achieve the highest possible coding efficiency, but it results in extremely large encoding time which obstructs 3D-HEVC from practical application. In this paper, we propose a low complexity depth mode decision algorithm for both inter and intra prediction to reduce the computational complexity of 3D-HEVC. The basic idea of the proposed algorithm is to utilize the depth map changes characteristic to predict the current depth treeblock prediction mode and early skip unnecessary inter and intra modes, such as DE, DMM and RBC. Experimental results show that the proposed low complexity algorithm can reduce the encoding time by up to 35% for 3D-HEVC mode decision, with negligible loss of rate-distortion (RD) performance.","","3D-HEVC, Depth map coding, Mode decision","",""
7,"","OUDJANI2018484","Oudjani, Brahim; Tebbikh, Hicham; Doghmane, Noureddine","Modification of extrinsic information for parallel concatenated Gallager/Convolutional code to improve performance/complexity trade-offs","AEU - International Journal of Electronics and Communications",83,,"","484 - 491",2018,"","","http://www.sciencedirect.com/science/article/pii/S1434841117309883","","","","","","","","","","","","","To benefit the properties of both Low-Density Parity-Check (LDPC) and Turbo Convolutional Codes (TCC), we propose a practical concatenated Gallager/Convolutional code in a turbo coding way. The modified code creates a balance between the advantages and the disadvantages of LDPC and TCC in terms of the overall complexity and latency. This will be done through two different component SISO decoders; LDPC and convolutional code of the same rate 1/2 without interleaver. Since the two SISO decoders are different in nature, they exchange extrinsic information that will be easily adapted to each other. The study of computation complexity and decoding performance over an AWGN channel indicates that such approach leads to excellent performance because of several factors. The proposed approach achieves a trade-off between waterfall and error floor regions. It reduces complexity decoding compared to TCC and 3D - TCC. It provides a better coding gain over LDPC and PCGC (Parallel Concatenated Gallager Codes). These features will ensure optimal outcomes and cost-performance ratio, and thus, this trend can be the best choice for today's communication systems.","","Computation complexity, Convolutional code, Extrinsic information, LDPC, Parallel concatenation, Turbo code","",""
7,"","MOHAMED2013497","Mohamed, Noraini; Sulaiman, Raja Fitriyah Raja; Endut, Wan Rohana Wan","The Use of Cyclomatic Complexity Metrics in Programming Performance's Assessment","Procedia - Social and Behavioral Sciences",90,,"","497 - 503",2013,"","6th International Conference on University Learning and Teaching (InCULT 2012)","http://www.sciencedirect.com/science/article/pii/S1877042813020077","","","","","","","","","","","","","Programming course is usually difficult and complex as taken from a student's point of view. Complexity of a programming code may be determined by the number of linearly independent path of its source code. This paper will be discussing the consistencies of the complexity of the code supposedly produced by students based on the programming questions found in the programming assessment throughout a semester of studies. The comparisons of the level of complexity and the students’ grade for various assessments materials are done. Based on this a relationship is observed and the conclusion is drawn on whether the consistencies in producing a certain pattern of expected complexity of a programming code has an effect on students performance.","","Cyclomatic Complexity, programming, performance assessment","",""
7,"","VAHIDOV201750","Vahidov, Rustam; Saade, Raafat; Yu, Bo","The effects of interplay between negotiation tactics and task complexity in software agent to human negotiations","Electronic Commerce Research and Applications",26,,"","50 - 61",2017,"","","http://www.sciencedirect.com/science/article/pii/S1567422317300789","","","","","","","","","","","","","Modern networked business environment enables design of flexible and effective mechanisms of exchange between economic parties. Online negotiations allow geographically and temporally separated participants to engage in exchange of offers in search for acceptable agreements. The digital medium enables development of software agents, which can assist with negotiation tasks while saving time and human effort. The current paper investigates the prospects of utilizing software agents in negotiations with the human counterparts. It presents the findings from experiment where human subjects acted as buyers negotiating with software agent sellers over a mobile phone plan. An electronic negotiation system incorporating software agents was used in the experiment. The agents employed various concession-making schedules while engaging in negotiation tasks involving one of two complexity levels. Negotiation task complexity was manipulated using different number of issues involved in the negotiations. Subjects were recruited among university students. Negotiations between the subjects and agents took place during a two-day period in an asynchronous mode through the web. The findings suggest that interaction between negotiation task complexity and negotiation tactic has significant effects on negotiation outcomes and subjective assessments by the human participants. In particular, task complexity had a higher impact on the agreement rate when agents employed a competitive tactic vs. when they used a conceding one.","","Concession-making, Electronic negotiations, Experimental studies, Mechanism design, Multi-issue negotiations, Negotiations, Software agents","",""
5,"","HANNA2017515","Hanna, R.; Karlan, D.","Chapter 7 - Designing Social Protection Programs: Using Theory and Experimentation to Understand How to Help Combat Poverty","",2,,"","515 - 553",2017,"","","http://www.sciencedirect.com/science/article/pii/S2214658X16300022","Handbook of Economic Field Experiments","","","Handbook of Economic Field Experiments","Banerjee, Abhijit Vinayak; Duflo, Esther","North-Holland","","","","","","","“Antipoverty” programs come in many varieties, ranging from multifaceted, complex programs to more simple cash transfers. Articulating and understanding the root problem motivating government and nongovernmental organization intervention are critical for choosing among many antipoverty policies or combinations thereof. Policies should differ depending on whether the underlying problem is about uninsured shocks, liquidity constraints, information failures, or some combination of all of the above. Experimental designs and thoughtful data collection can help diagnose the root problems better, thus providing better predictions for what antipoverty programs to employ in specific conditions and contexts. However, the more complex theories are likewise more challenging to test, requiring larger samples, and often more nuanced experimental designs, as well as detailed data on many aspects of household and community behavior and outcomes. We provide guidance on these design and testing issues for social protection programs, from how to target programs, to who should implement the program, and to whether and what conditions to require for program participation. In short, careful experimentation-designed testing can help provide a stronger conceptual understanding of why programs do or not work, thereby allowing one to ultimately make stronger policy prescriptions that further the goal of poverty reduction.","","Social protection, Development, Antipoverty, O10, O12, H53","",""
7,"","DASILVA2015527","da Silva, Ivonei Freitas; da Mota Silveira Neto, Paulo Anselmo; O’Leary, Pádraig; de Almeida, Eduardo Santana; de Lemos Meira, Silvio Romero","Using a multi-method approach to understand Agile software product lines","Information and Software Technology",57,,"","527 - 542",2015,"","","http://www.sciencedirect.com/science/article/pii/S0950584914001438","","","","","","","","","","","","","Context
 Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
 Objective
 This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
 Method
 Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
 Results
 This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
 Conclusion
 Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies.","","Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion","",""
7,"","YUSHCHENKO2017538","Yushchenko, Alisa; Patel, Martin Kumar","Cost-effectiveness of energy efficiency programs: How to better understand and improve from multiple stakeholder perspectives?","Energy Policy",108,,"","538 - 550",2017,"","","http://www.sciencedirect.com/science/article/pii/S0301421517303695","","","","","","","","","","","","","Cost-effectiveness analysis is one of the core elements of energy efficiency program evaluation, having important impact on energy policy, program design and budget allocation. In this context two questions arise. How to evaluate cost-effectiveness of energy efficiency programs in a more comprehensive manner? And how to improve the programs’ cost-effectiveness? We evaluate the cost-effectiveness of energy efficiency programs in Switzerland, based on three electricity-saving programs in Geneva. We review the existing practices of cost-effectiveness analysis and propose a somewhat modified methodology that allows considering perspectives of the following stakeholders: program participants, energy consumers, program administrator, utility, geographic jurisdiction territory and society as a whole. We also analyze the cost breakdown of energy efficiency programs and its evolution over time. We find that energy efficiency programs can contribute to energy efficiency target achievement while having positive economic and social impacts, including increased GDP and employment. However, energy efficiency programs can potentially lead to increased energy tariffs and higher costs for utilities. As policy recommendations for increasing cost-effectiveness of energy efficiency programs we propose a larger scale of energy efficiency programs, focus on education and training as well as on the development of long-term relationships with program participants and contractors.","","Energy efficiency programs, Cost-effectiveness, Stakeholders, GDP, Employment","",""
7,"","GUO201355","Yi-jun, G. U. O.; Jian-jun, H. A. O.; Guang-xin, Y. U. E.","Reduce the decoding complexity: segment linear network coding","The Journal of China Universities of Posts and Telecommunications",20,6,"","55 - 61",2013,"","","http://www.sciencedirect.com/science/article/pii/S1005888513601090","","","","","","","","","","","","","The throughput gain obtained by linear network coding (LNC) grows as the generation size increases, while the decoding complexity also grows exponentially. High decoding complexity makes the decoder to be the bottleneck for high speed and large data transmissions. In order to reduce the decoding complexity of network coding, a segment linear network coding (SLNC) scheme is proposed. SLNC provides a general coding structure for the generation-based network coding. By dividing a generation into several segments and restraining the coding coefficients of the symbols within the same segment, SLNC splits a high-rank matrix inversion into several low-rank matrix inversions, therefore reduces the decoding complexity dramatically. In addition, two coefficient selection strategies are proposed for both centrally controlled networks and distributed networks respectively. The theoretical analysis and simulation results prove that SLNC achieves a fairly low decoding complexity at a cost of rarely few extra transmissions.","","SLNC, generation-based network coding, random LNC, decoding complexity, extra transmission cost","",""
7,"","QIAN201756","Qian, Jianping; Du, Xiaowei; Zhang, Baoyan; Fan, Beilei; Yang, Xinting","Optimization of QR code readability in movement state using response surface methodology for implementing continuous chain traceability","Computers and Electronics in Agriculture",139,,"","56 - 64",2017,"","","http://www.sciencedirect.com/science/article/pii/S0168169916302095","","","","","","","","","","","","","Logistics and storage is the main processing for agro-food supply chain. Because of disconnection information between the two processing, it is difficult to trace continuously. An intelligent conveyer belt provides an effective method to associate storage and logistics by QR code scanning and information recording. Improving the QR code readability in movement state is the core of implementing continuous chain traceability with this belt. In this paper, a intelligent conveyer belt including automatic conveyer unit, barcode scanning unit, fault remove unit and control display unit was designed. Four factors affected QR readability were selected and the value range was confirmed, which was reading distance, code size, coded characters and belt moving speed. Based on the belt, an Central Composite Inscribed (CCI) experiment of four factors with five levels was designed using Response Surface Methodology (RSM) to obtain the optimal reading parameters. The result shows that the main factors of reading distance, belt moving speed and the interaction between reading distance and code size have the significant effect on QR code readability. Under the optimization condition of 141.45mm reading distance, 34.58mm code size, 100 bytes coded characters and 2.98m/min belt moving speed, the average value of QR code readability was 95%. With the optimization parameters, the intelligent conveyer belt was used in an apple marketing enterprise. The result shows that the continuous traceability between storage and logistic can be implemented with the extended breadth, deepened depth and improved precision.","","QR code, Traceability, Response surface methodology, Barcode readability, Optimization","",""
7,"","BAILLOT201656","Baillot, Patrick; Lago, Ugo Dal","Higher-order interpretations and program complexity","Information and Computation",248,,"","56 - 81",2016,"","Development on Implicit Computational Complexity (DICE 2013)","http://www.sciencedirect.com/science/article/pii/S0890540115001388","","","","","","","","","","","","","Polynomial interpretations and their generalizations like quasi-interpretations have been used in the setting of first-order functional languages to design criteria ensuring statically some complexity bounds on programs [10]. This fits in the area of implicit computational complexity, which aims at giving machine-free characterizations of complexity classes. In this paper, we extend this approach to the higher-order setting. For that we consider simply-typed term rewriting systems [35], we define higher-order polynomial interpretations for them, and we give a criterion ensuring that a program can be executed in polynomial time. In order to obtain a criterion flexible enough to validate interesting programs using higher-order primitives, we introduce a notion of polynomial quasi-interpretations, coupled with a simple termination criterion based on linear types and path-like orders.","","Implicit computational complexity, Term rewriting systems, Type systems, Lambda-calculus","",""
7,"","KANG2012569","Kang, Li-Wei; Lu, Chun-Shien; Lin, Chih-Yang","Low-complexity video coding via power–rate–distortion optimization","Journal of Visual Communication and Image Representation",23,3,"","569 - 585",2012,"","","http://www.sciencedirect.com/science/article/pii/S1047320312000326","","","","","","","","","","","","","Wireless multimedia sensor networks (WMSNs) have been potentially applicable for several emerging applications. The resources, i.e., power and bandwidth available to visual sensors in a WMSN are, however, very limited. Hence, it is important but challenging to achieve efficient resource allocation and optimal video data compression while maximizing the overall network lifetime. In this paper, a power–rate–distortion (PRD) optimized resource-scalable low-complexity multiview video encoding scheme is proposed. In our video encoder, both the temporal and interview information can be exploited based on the comparisons of extracted media hashes without performing motion and disparity estimations, which are known to be time-consuming. We present a PRD model to characterize the relationship between the available resources and the RD performance of our encoder. More specifically, an RD function in terms of the percentages for different coding modes of blocks and the target bit rate under the available resource constraints is derived for optimal coding mode decision. The major goal here is to design a PRD model to optimize a “motion estimation-free” low-complexity video encoder for applications with resource-limited devices, instead of designing a general-purpose video codec to compete compression performance against current compression standards (e.g., H.264/AVC). Analytic results verify the accuracy of our PRD model, which can provide a theoretical guideline for performance optimization under limited resource constraints. Simulation results on joint RD performance and power consumption (measured in terms of encoding time) demonstrate the applicability of our video coding scheme for WMSNs.","","Hash, Low-complexity video coding, Motion estimation, Rate–distortion, Power–rate–distortion optimization, Multiview video coding, Wireless multimedia sensor networks, Low-power and power-aware video coding","",""
7,"","LEUTHOLD201157","Leuthold, Hartmut; Schröter, Hannes","Motor programming of finger sequences of different complexity","Biological Psychology",86,1,"","57 - 64",2011,"","","http://www.sciencedirect.com/science/article/pii/S030105111000270X","","","","","","","","","","","","","In a response precuing task, we used behavioral and electrophysiological measures – Contingent Negative Variation (CNV) and the readiness potential (RP) – to investigate the programming of three-element response sequences of different complexity. Precuing effects on foreperiod CNV and RT indicated the use of advance information about response hand and response sequence. Crucially, with advance information about both hand and sequence, heterogeneous response sequences (e.g., 1→2→2) elicited larger foreperiod CNV activity over medial motor areas than homogeneous response sequences (e.g., 1→2→3), whereas CNV activity over lateral motor areas was not influenced by sequence complexity. It was only before response execution that lateral but not medial RP activity was stronger for heterogeneous than homogeneous response sequences. Both behavioral and electrophysiological measures indicated finger-order dependent influences on the duration of on-line response programming during response execution.","","Response sequence complexity, Response precuing, Motor programming, Contingent negative variation, Readiness potential","",""
7,"","REIN201558","Rein, Stephan A.; Fitzek, Frank H. P.; Gühmann, Clemens; Sikora, Thomas","Evaluation of the wavelet image two-line coder: A low complexity scheme for image compression","Signal Processing: Image Communication",37,,"","58 - 74",2015,"","","http://www.sciencedirect.com/science/article/pii/S0923596515001150","","","","","","","","","","","","","This paper introduces the wavelet image two-line (Wi2l) coding algorithm for low complexity compression of images. The algorithm recursively encodes an image backwards reading only two lines of a wavelet subband, which are read in blocks of 512 bytes from flash memory. It thus only requires very little memory, i.e., a memory array for two wavelet subband lines, an array to store intermediate tree level data, and an array for writing binary data. A picture of 256×256 pixels would require 1152 bytes of memory. Computation time for the coding is derived analytically and measured on a real system. The times on a low-cost microcontroller for 256×256 grayscale pictures are measured as 0.25–0.6s for encoding and 0.22–0.77s for decoding. The algorithm can thus realize a low complexity system for compression of images when combined with a customized scheme for the wavelet transform; low complexity here refers to low memory, minimum write access to flash memory, usage of integer operations only, and low conceptual complexity (ease of implementation). As demonstrated in this paper, a compression performance similar to JPEG 2000 and the more recent Google WebP picture compression is achieved. The compression system uses flash memory (SD or MMC card) and a small camera sensor thus building an image communication system. It is also suitable for mobile devices or satellite communication. The underlying C source code is made publicly available.","","Wavelet image two-line coder (Wi2l), Set-partitioning in hierarchical trees (SPIHT), Backward coding of wavelet trees (BCWT), Fractional wavelet filter, Fixed-point wavelet transform, Camera sensor node","",""
7,"","HENDRAWAN2015597","Hendrawan, Rully Agus; Maruyama, Katsuhisa","Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension","Procedia Computer Science",72,,"","597 - 604",2015,"","The Third Information Systems International Conference 2015","http://www.sciencedirect.com/science/article/pii/S1877050915036297","","","","","","","","","","","","","By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration.","","source code visualization, program comprehension, logical coupling, particle swarm optimization, association mining","",""
7,"","MI201860","Mi, Qing; Keung, Jacky; Xiao, Yan; Mensah, Solomon; Gao, Yujin","Improving code readability classification using convolutional neural networks","Information and Software Technology",104,,"","60 - 71",2018,"","","http://www.sciencedirect.com/science/article/pii/S0950584918301496","","","","","","","","","","","","","Context
 Code readability classification (which refers to classification of a piece of source code as either readable or unreadable) has attracted increasing concern in academia and industry. To construct accurate classification models, previous studies depended mainly upon handcrafted features. However, the manual feature engineering process is usually labor-intensive and can capture only partial information about the source code, which is likely to limit the model performance.
 Objective
 To improve code readability classification, we propose the use of Convolutional Neural Networks (ConvNets).
 Method
 We first introduce a representation strategy (with different granularities) to transform source codes into integer matrices as the input to ConvNets. We then propose DeepCRM, a deep learning-based model for code readability classification. DeepCRM consists of three separate ConvNets with identical architectures that are trained on data preprocessed in different ways. We evaluate our approach against five state-of-the-art code readability models.
 Results
 The experimental results show that DeepCRM can outperform previous approaches. The improvement in accuracy ranges from 2.4% to 17.2%.
 Conclusions
 By eliminating the need for manual feature engineering, DeepCRM provides a relatively improved performance, confirming the efficacy of deep learning techniques in the task of code readability classification.","","Code readability, Convolutional Neural Network, Deep learning, Program comprehension, Empirical software engineering, Open source software","",""
7,"","CAI201862","Cai, Ruizhe; Ren, Ao; Soundarajan, Sucheta; Wang, Yanzhi","A low-computation-complexity, energy-efficient, and high-performance linear program solver based on primal–dual interior point method using memristor crossbars","Nano Communication Networks",18,,"","62 - 71",2018,"","","http://www.sciencedirect.com/science/article/pii/S1878778917301394","","","","","","","","","","","","","Linear programming is required in a wide variety of application including routing, scheduling, and various optimization problems. The primal–dual interior point (PDIP) method is state-of-the-art algorithm for solving linear programs, and can be decomposed to matrix–vector multiplication and solving systems of linear equations, both of which can be conducted by the emerging memristor crossbar technique in O(1) time complexity in the analog domain. This work is the first to apply memristor crossbar for linear program solving based on the PDIP method, which has been reformulated for memristor crossbars to compute in the analog domain. The proposed linear program solver can overcome limitations of memristor crossbars such as supporting only non-negative coefficients, and has been extended for higher scalability. The proposed solver is iterative and achieves O(N) computation complexity in each iteration. Experimental results demonstrate that reliable performance with high accuracy can be achieved under process variations.","","Memristor, Memristor crossbar, Linear programming, Primal–dual interior point method","",""
7,"","TOREZZAN2016643","Torezzan, Cristiano; Panek, Luciano; Firer, Marcelo","A low complexity coding and decoding strategy for the quadratic Gaussian CEO problem","Journal of the Franklin Institute",353,3,"","643 - 656",2016,"","","http://www.sciencedirect.com/science/article/pii/S0016003215004548","","","","","","","","","","","","","We consider the quadratic Gaussian CEO problem, where the goal is to estimate a measure based on several Gaussian noisy observations which must be encoded and sent to a centralized receiver using limited transmission rate. For real applications, besides minimizing the average distortion, given the transmission rate, it is important to take into account memory and processing constraints. Considering these motivations, we present a low complexity coding and decoding strategy, which exploits the correlation between the measurements to reduce the number of bits to be transmitted by refining the output of the quantization stage. The CEO makes an estimate using a decoder based on a process similar to majority voting. We derive explicit expression for the CEO׳s error probability and compare numerical simulations with known achievability results and bounds.","","","",""
7,"","BLONDINMASSE20116455","Massé, A. Blondin; Brlek, S.; Labbé, S.; Vuillon, L.","Palindromic complexity of codings of rotations","Theoretical Computer Science",412,46,"","6455 - 6463",2011,"","","http://www.sciencedirect.com/science/article/pii/S0304397511006682","","","","","","","","","","","","","We study the palindromic complexity of infinite words obtained by coding rotations on partitions of the unit circle by inspecting the return words. The main result is that every coding of rotations on two intervals is full, that is, it realizes the maximal palindromic complexity. As a byproduct, a slight improvement about return words in codings of rotations is obtained: every factor of a coding of rotations on two intervals has at most 4 complete return words, where the bound is realized only for a finite number of factors. We also provide a combinatorial proof for the special case of complementary-symmetric Rote sequences by considering both palindromes and antipalindromes occurring in it.","","Codings of rotations, Sturmian, Rote, Return words, Full words","",""
7,"","PAN201671","Pan, Xiaolong; Liu, Bo; Zheng, Jianglong; Tian, Qinghua","Low complexity Reed–Solomon-based low-density parity-check design for software defined optical transmission system based on adaptive puncturing decoding algorithm","Optics Communications",372,,"","71 - 75",2016,"","","http://www.sciencedirect.com/science/article/pii/S0030401816302577","","","","","","","","","","","","","We propose and demonstrate a low complexity Reed–Solomon-based low-density parity-check (RS-LDPC) code with adaptive puncturing decoding algorithm for elastic optical transmission system. Partial received codes and the relevant column in parity-check matrix can be punctured to reduce the calculation complexity by adaptive parity-check matrix during decoding process. The results show that the complexity of the proposed decoding algorithm is reduced by 30% compared with the regular RS-LDPC system. The optimized code rate of the RS-LDPC code can be obtained after five times iteration.","","Adaptive decoding algorithm, RS-LDPC codes, Low complexity, Optical transmission system","",""
7,"","SHARMA2012716","Sharma, Ashish; Kushwaha, Dharmender Singh","Estimation of Software Development Effort from Requirements Based Complexity","Procedia Technology",4,,"","716 - 722",2012,"","2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012","http://www.sciencedirect.com/science/article/pii/S2212017312003957","","","","","","","","","","","","","Software development effort estimation deals with predicting the effort required to develop quality software. The efficient software development requires accurate estimates, because inappropriate estimates causes’ trouble during the implementation of software processes. Hence, this paper aims to propose a measure for the estimation of software development effort (SDE) on the basis of requirement based complexity of yet to be developed software. The requirement based complexity has its basis on software requirements specification (SRS) of the proposed software, in order to carry out a systematic and accurate estimation of SDE. For validation purpose, the proposed SDE measure is also categorically compared with various other established SDE estimation practices proposed in the past like algorithmic models, function point count, use case point and lines of code (LOC). The result obtained validates the claim that the proposed SDE measure is systematic, comprehensive one and compares well with other prevalent SDE measures. Hence, it is even more useful because the complexity and SDE estimates are obtained at very early stage of software development life cycle (SDLC) as compared to other software development effort practices proposed in the past.","","Improved requirement based complexity, Requirement based development effort function point, IEEE-830:1998, Requirement based development effort, Software Requirement Specification (SRS)","",""
7,"","KURMYSHEV2011718","Kurmyshev, E. V.; Guillen-Bonilla, J. T.","Complexity reduced coding of binary pattern units in image classification","Optics and Lasers in Engineering",49,6,"","718 - 722",2011,"","Research in Optics and Photonics at CIO, Mexico","http://www.sciencedirect.com/science/article/pii/S0143816610002952","","","","","","","","","","","","","The ability to simulate and control complex physical situations in real time is an important element of many engineering and robotics applications, including pattern recognition and image classification. One of the ways to meet specific requirements of a process is a reduction of computational complexity of algorithms. In this work we propose a new coding of binary pattern units (BPU) that reduces the time and spatial complexity of algorithms of image classification significantly. We apply this coding to a particular but important case of the coordinated clusters representation (CCR) of images. This algorithm reduces the dimension of the CCR feature space and, as a consequence, the time and space complexity of the CCR based methods of image classification, exponentially. In addition, the new coding preserves all the fundamental properties of the CCR that are successfully used in the recognition, classification and segmentation of texture images. The same approach to the coding of BPUs can be used in the Local Binary Pattern (LBP) method. In order to evaluate the reduction of time and space complexity, we did an experiment on multiclass classification of images using the “traditional” and the new coding of the CCR. This test showed very effective reduction of the computing time and required computer memory with the use of the new coding of BPUs of the CCR, retaining 100% or a little less efficiency of classification at the time.","","Computational complexity, Pattern recognition, Classification of images, Fast coordinated clusters representation","",""
7,"","ZARNOWIECKI201872","Zarnowiecki, Dorota; Nguyen, Ha; Hampton, Catherine; Boffa, John; Segal, Leonie","The Australian Nurse-Family Partnership Program for aboriginal mothers and babies: Describing client complexity and implications for program delivery","Midwifery",65,,"","72 - 81",2018,"","","http://www.sciencedirect.com/science/article/pii/S0266613818301906","","","","","","","","","","","","","Context
 The Australian Nurse-Family Partnership Program is a home visiting program for Aboriginal mothers and infants (pregnancy to child's second birthday) adapted from the US Nurse Family Partnership program. It aims to improve outcomes for Australian Aboriginal mothers and babies, and disrupt intergenerational cycles of poor health and social and economic disadvantage. The aim of this study was to describe the complexity of Program clients in the Central Australian family partnership program, understand how client complexity affects program delivery and the implications for desirable program modification.
 Methods
 Australian Nurse-Family Partnership Program data collected using standardised data forms by nurses during pregnancy home visits (n = 276 clients from 2009 to 2015) were used to describe client complexity and adversity in relation to demographic and economic characteristics, mental health and personal safety. Semi-structured interviews with 11 Australian Nurse-Family Partnership Program staff and key stakeholders explored in more depth the nature of client adversity and how this affected Program delivery.
 Findings
 Most clients were described as “complicated” being exposed to extreme poverty (66% on welfare), living with insecure housing, many experiencing domestic violence (almost one third experiencing 2 + episodes of violence in 12 months). Sixty-six percent of clients had experienced four or more adversities. These adversities were found challenging for Program delivery. For example, housing conditions mean that around half of all ‘home visits’ could not be conducted in the home (held instead in staff cars or community locations) and together with exposure to violence undermined client capacity to translate program learnings into action. Crises with the basics of living regularly intruded into the delivery of program content, and low client literacy meant written hand-outs were unhelpful for many, requiring the development of pictorial-based program materials. Adversity increased the time needed to deliver program content.
 Conclusions
 Modifications to the Australian Nurse-Family Partnership Program model to reflect the specific complexities and adversities faced by the client populations is important for effective service delivery and to maximise the chance of meeting program goals of improving the health and well-being of Australian Aboriginal mothers and their infants.","","Nurse home visiting, Nurse-Family Partnership, Aboriginal health, Maternal child health service","",""
7,"","HAINRY201878","Hainry, Emmanuel; Péchoux, Romain","A type-based complexity analysis of Object Oriented programs","Information and Computation",261,,"","78 - 115",2018,"","Developments in Implicit Computational Complexity (DICE) 2014 and 2015","http://www.sciencedirect.com/science/article/pii/S0890540118300786","","","","","","","","","","","","","A type system is introduced for a generic Object Oriented programming language in order to infer resource upper bounds. A sound and complete characterization of the set of polynomial time computable functions is obtained. As a consequence, the heap-space and the stack-space requirements of typed programs are also bounded polynomially. This type system is inspired by previous works on Implicit Computational Complexity, using tiering and non-interference techniques. The presented methodology has several advantages. First, it provides explicit big O polynomial upper bounds to the programmer, hence its use could allow the programmer to avoid memory errors. Second, type checking is decidable in polynomial time. Last, it has a good expressivity since it analyzes most object oriented features like inheritance, overload, override and recursion. Moreover it can deal with loops guarded by objects and can also be extended to statements that alter the control flow like break or return.","","Object Oriented Program, Type system, Complexity, Polynomial time","",""
7,"","MOSHTARI20138","Moshtari, Sara; Sami, Ashkan; Azimi, Mahdi","Using complexity metrics to improve software security","Computer Fraud & Security",2013,5,"","8 - 17",2013,"","","http://www.sciencedirect.com/science/article/pii/S1361372313700459","","","","","","","","","","","","","Information technology is quickly spreading across critical infrastructures and software has become an inevitable part of industries and organisations. At the same time, many cyberthreats are the result of poor software coding. Stuxnet, which was the most powerful cyber-weapon used against industrial control systems, exploited zero-day vulnerabilities in Microsoft Windows.1 The US Department of Homeland Security (DHS) also announced that software vulnerabilities are among the three most common cyber-security vulnerabilities in Industrial Control Systems (ICSs).2 Therefore, improving software security has an important role in increasing the security level of computer-based systems. Software vulnerability prediction is a tedious task, so automating vulnerability prediction would save a lot of time and resources. One recently used methodology in vulnerability prediction is based on automatic fault prediction using software metrics. Here, Sara Moshtari, Ashkan Sami and Mahdi Azimi of Shiraz University, Iran build on previous studies by providing more complete vulnerability information. They show what can be achieved using different classification techniques and more complete vulnerability information.","","","",""
7,"","HEERING201582","Heering, Jan","Generative software complexity and software understanding","Science of Computer Programming",97,,"","82 - 85",2015,"","Special Issue on New Ideas and Emerging Results in Understanding Software","http://www.sciencedirect.com/science/article/pii/S0167642313003018","","","","","","","","","","","","","Taking generative software development as our point of departure, we introduce generative software complexity as a measure for quantifying the structural complexity of software. After explaining that it is the same as Kolmogorov complexity, we discuss its merits from the viewpoint of software understanding. The results obtained are in many ways unsatisfactory, but sufficiently intriguing to warrant further work.","","Structural complexity, Software generation, Software understanding, Kolmogorov complexity","",""
7,"","STEFIK2011820","Stefik, Andreas; Hundhausen, Christopher; Patterson, Robert","An empirical investigation into the design of auditory cues to enhance computer program comprehension","International Journal of Human-Computer Studies",69,12,"","820 - 838",2011,"","","http://www.sciencedirect.com/science/article/pii/S1071581911000814","","","","","","","","","","","","","Decades of research have led to notable improvements in the representations used to aid human comprehension of computer programs. Much of this research has focused on visual representations, which leaves open the question of how best to design auditory representations of computer programs. While this question has particular relevance for visually impaired programmers, sighted programmers might also benefit from enhanced auditory representations of their programs. In order to investigate this question empirically, first, we introduce artifact encoding, a novel approach to rigorously measuring the comprehensibility of auditory representations of computer programs. Using this approach as a foundation, we present an experimental study that compared the comprehensibility of two alternative auditory program representations: one with lexical scoping cues that convey the nesting level of program statements, and another without such scoping cues. The results of our first experiment validate both artifact encoding and the scoping cues we used. To see whether auditory cues validated through our paradigm can aid program comprehension in a realistic task scenario, we experimentally compared programmers' ability to debug programs using three alternative environments: (1) an auditory execution environment with our empirically derived auditory cues; (2) an auditory execution environment with the current state-of-the-art auditory cues generated by a screen reader running on top of Microsoft Visual Studio; and (3) a visual version of the execution environment. The results of our second experiment showed that our comprehensible auditory cues are significantly better than the state-of-the-art, affording human performance approaching the effectiveness of visual representations within the statistical margin of error. This research contributes a novel methodology and foundational empirical data that can guide the design of effective auditory representations of computer programs.","","Auditory programming, Programming, Debugging, Program comprehension","",""
7,"","HUDRY201983","Hudry, Olivier; Lobstein, Antoine","Unique (optimal) solutions: Complexity results for identifying and locating–dominating codes","Theoretical Computer Science",767,,"","83 - 102",2019,"","","http://www.sciencedirect.com/science/article/pii/S0304397518306054","","","","","","","","","","","","","We investigate the complexity of four decision problems dealing with the uniqueness of a solution in a graph: “Uniqueness of an r-Locating–Dominating Code with bounded size” (U-LDCr), “Uniqueness of an Optimal r-Locating–Dominating Code” (U-OLDCr), “Uniqueness of an r-Identifying Code with bounded size” (U-IdCr), “Uniqueness of an Optimal r-Identifying Code” (U-OIdCr), for any fixed integer r≥1. In particular, we describe a polynomial reduction from “Unique Satisfiability of a Boolean formula” (U-SAT) to U-OLDCr, and from U-SAT to U-OIdCr; for U-LDCr and U-IdCr, we can do even better and prove that their complexity is the same as that of U-SAT, up to polynomials. Consequently, all these problems are NP-hard, and U-LDCr and U-IdCr belong to the class DP.","","Complexity theory, Graph theory, Uniqueness of solution, Polynomial reduction, Locating–dominating codes, Identifying codes","",""
7,"","KANDJANI201586","Kandjani, Hadi; Tavana, Madjid; Bernus, Peter; Wen, Lian; Mohtarami, Amir","Using extended Axiomatic Design theory to reduce complexities in Global Software Development projects","Computers in Industry",67,,"","86 - 96",2015,"","","http://www.sciencedirect.com/science/article/pii/S0166361514001869","","","","","","","","","","","","","Global Software Development (GSD) projects could be best understood as intrinsically complex adaptive living systems: they cannot purely be considered as ‘designed systems’, as deliberate design/control episodes and processes (using ‘software engineering’ models) are intermixed with emergent change episodes and processes (that may perhaps be explained by models). Therefore to understand GSD projects as complex systems we need to combine the state-of-the-art of GSD research, as addressed in the software engineering discipline, with results of other disciplines that study complexity (e.g. Enterprise Architecture, Complexity and Information Theory, Axiomatic Design theory). In this paper we study the complexity of GSD projects and propose an upper bound estimation of Kolmogorov complexity (KC) to estimate the information content (as a complexity measure) of project plans. We demonstrate using two hypothetical examples how good and bad project plans compare with respect to complexity, and propose the application of extended Axiomatic Design (AD) theory to reduce the complexity of GSD projects in the project planning stage, as well as to keep this complexity as low as possible during the project execution stage.","","Global Software Development, Complexity, Extended Axiomatic Design theory, Kolmogorov complexity","",""
7,"","DOUTHWAITE201788","Douthwaite, Boru; Hoffecker, Elizabeth","Towards a complexity-aware theory of change for participatory research programs working within agricultural innovation systems","Agricultural Systems",155,,"","88 - 102",2017,"","","http://www.sciencedirect.com/science/article/pii/S0308521X17303190","","","","","","","","","","","","","Agricultural innovation systems (AIS) are increasingly recognized as complex adaptive systems in which interventions cannot be expected to create predictable, linear impacts. Nevertheless, the logic models and theory of change (ToC) used by standard-setting international agricultural research agencies and donors assume that agricultural research will create impact through a predictable linear adoption pathway which largely ignores the complexity dynamics of AIS, and which misses important alternate pathways through which agricultural research can improve system performance and generate sustainable development impact. Despite a growing body of literature calling for more dynamic, flexible and “complexity-aware” approaches to monitoring and evaluation, few concrete examples exist of ToC that takes complexity dynamics within AIS into account, or provide guidance on how such theories could be developed. This paper addresses this gap by presenting an example of how an empirically-grounded, complexity-aware ToC can be developed and what such a model might look like in the context of a particular type of program intervention. Two detailed case studies are presented from an agricultural research program which was explicitly seeking to work in a “complexity-aware” way within aquatic agricultural systems in Zambia and the Philippines. Through an analysis of the outcomes of these interventions, the pathways through which they began to produce impacts, and the causal factors at play, we derive a “complexity-aware” ToC to model how the cases worked. This middle-range model, as well as an overarching model that we derive from it, offer an alternate narrative of how development change can be produced in agricultural systems, one which aligns with insights from complexity science and which, we argue, more closely represents the ways in which many research for development interventions work in practice. The nested ToC offers a starting point for asking a different set of evaluation and research questions which may be more relevant to participatory research efforts working from within a complexity-aware, agricultural innovation systems perspective.","","Theory of change, Complexity, Agricultural innovation systems, Participatory action research, Empowerment, Evaluation","",""
7,"","DVORAK201291","Dvořák, V.","On the Complexity and Optimization of Branching Programs for Decision Diagram Machines","IFAC Proceedings Volumes",45,7,"","91 - 96",2012,"","11th IFAC,IEEE International Conference on Programmable Devices and Embedded Systems","http://www.sciencedirect.com/science/article/pii/S1474667015350692","","","","","","","","","","","","","Decision Diagram Machines (DDMs) are special purpose processors that evaluate decision diagrams. First, this paper derives upper bounds on the cost of multi-terminal binary decision diagrams (MTBDDs) for multiple-output logic functions. From these bounds we can estimate the size of branching programs running on various DDMs. Second, optimization of heterogeneous branching programs is undertaken that makes the area-time trade-off between the amount of memory required for a branching program and its execution time. As a case study, optimal architectures of branching programs are found for a set of benchmark tasks. Beside DDMs, the technique can also be used for micro-controllers with a support for multi-way branching running logic-intensive embedded firmware.","","Boolean functions, multi-terminal binary decision diagrams MTBDDs, branching programs, MTBDD complexity, decision diagram machines DDMs","",""
7,"","HEINTZ201392","Heintz, Joos; Kuijpers, Bart; Paredes, Andrés Rojas","Software Engineering and complexity in effective Algebraic Geometry","Journal of Complexity",29,1,"","92 - 138",2013,"","","http://www.sciencedirect.com/science/article/pii/S0885064X1200043X","","","","","","","","","","","","","One may represent polynomials not only by their coefficients but also by arithmetic circuits which evaluate them. This idea allowed in the past fifteen years considerable complexity progress in effective polynomial equation solving. We present a circuit based computation model which captures all known symbolic elimination algorithms in effective Algebraic Geometry and exhibit a class of simple elimination problems which require exponential size circuits to be solved in this model. This implies that the known, circuit based elimination algorithms are already optimal.","","Robust parameterized arithmetic circuit, Isoparametric routine, Branching parsimonious algorithm, Flat family of zero dimensional elimination problems","",""
7,"","STRANOVSKA2014936","Stranovská, Eva; Munková, Daša; Munk, Michal","Dynamics of Reading Comprehension Skills in Linguistic Intervention Programme","Procedia - Social and Behavioral Sciences",149,,"","936 - 942",2014,"","LUMEN 2014 - From Theory to Inquiry in Social Sciences, Iasi, Romania, 10-12 April 2014","http://www.sciencedirect.com/science/article/pii/S1877042814050174","","","","","","","","","","","","","The aim of this study is an examination of dynamics of reading comprehension in a foreign language through the linguistic intervention programme. The point of that was to examine an influence of the linguistic intervention programme on reading comprehension skills. The linguistic intervention programme represents a method of active social learning, autonomous learning and a set of strategies and specific methods of foreign language learning. Two hundred and twenty one university students took part in this experiment, where we used as research methods - an observation and a test of foreign-language proficiency focusing on reading comprehension skills (English, German). The results have shown interesting findings in the direction of supporting or obstructing variable on the process of reading comprehension in a foreign language.","","Linguistic intervention programme, intervention, reading comprehension skills, foreign language;","",""
7,"","SLOWACK201094","Slowack, Jürgen; Škorupa, Jozef; Mys, Stefaan; Lambert, Peter; Grecos, Christos; de Walle, Rik Van","Flexible distribution of complexity by hybrid predictive-distributed video coding","Signal Processing: Image Communication",25,2,"","94 - 110",2010,"","","http://www.sciencedirect.com/science/article/pii/S0923596509001374","","","","","","","","","","","","","There is currently limited flexibility for distributing complexity in a video coding system. While rate-distortion-complexity (RDC) optimization techniques have been proposed for conventional predictive video coding with encoder-side motion estimation, they fail to offer true flexible distribution of complexity between encoder and decoder since the encoder is assumed to have always more computational resources available than the decoder. On the other hand, distributed video coding solutions with decoder-side motion estimation have been proposed, but hardly any RDC optimized systems have been developed. To offer more flexibility for video applications involving multi-tasking or battery-constrained devices, in this paper, we propose a codec combining predictive video coding concepts and techniques from distributed video coding and show the flexibility of this method in distributing complexity. We propose several modes to code frames, and provide complexity analysis illustrating encoder and decoder computational complexity for each mode. Rate distortion results for each mode indicate that the coding efficiency is similar. We describe a method to choose which mode to use for coding each inter frame, taking into account encoder and decoder complexity constraints, and illustrate how complexity is distributed more flexibly.","","Distributed video coding, Hybrid video coding, Wyner-Ziv coding","",""
7,"","NOVAKOVIC2012959","Novakovic, B.; Saffery, R.","The ever growing complexity of placental epigenetics – Role in adverse pregnancy outcomes and fetal programming","Placenta",33,12,"","959 - 970",2012,"","","http://www.sciencedirect.com/science/article/pii/S0143400412003669","","","","","","","","","","","","","As the primary interface between maternal and fetal circulations, the placenta is subject to a myriad of environmental exposures with the capacity to alter placental function and fetal development. Many of these effects are likely to be mediated by epigenetic (‘above DNA’) change, which is also in turn regulated by maternal and fetal genetic factors. Linking specific environmental exposures, genetic, and epigenetic variation to maternal and fetal outcomes may provide valuable mechanistic insights into the role of placental dysfunction in pregnancy-associated disease and later health. The complexities are manifold but are rapidly being overcome by technological advances and emerging analytical approaches. Although focussing on recent genome-scale and gene-specific DNA methylation studies in the human placenta, this review also discusses the potential of a future broader exploration of combined environmental, genetic and epigenomic approaches, encompassing higher order epigenetic modifications, for unravelling the molecular mechanisms underlying gene-environment interaction at the fetomaternal interface.","","Epigenetics, DNA methylation, Environment, Diet, Placenta, Preeclampsia, Birth-weight","",""
7,"","DURAND2016R587","Durand, Pierre M.; Sym, Stuart; Michod, Richard E.","Programmed Cell Death and Complexity in Microbial Systems","Current Biology",26,13,"","R587 - R593",2016,"","","http://www.sciencedirect.com/science/article/pii/S0960982216305541","","","","","","","","","","","","","Programmed cell death (PCD) is central to organism development and for a long time was considered a hallmark of multicellularity. Its discovery, therefore, in unicellular organisms presents compelling questions. Why did PCD evolve? What is its ecological effect on communities? To answer these questions, one is compelled to consider the impacts of PCD beyond the cell, for death obviously lowers the fitness of the cell. Here, we examine the ecological effects of PCD in different microbial scenarios and conclude that PCD can increase biological complexity. In mixed microbial communities, the mode of death affects the microenvironment, impacting the interactions between taxa. Where the population comprises groups of relatives, death has a more explicit effect. Death by lysis or other means can be harmful, while PCD can evolve by providing advantages to relatives. The synchronization of death between individuals suggests a group level property is being maintained and the mode of death also appears to have had an impact during the origin of multicellularity. PCD can result in the export of fitness from the cell to the group level via re-usable resources and PCD may also provide a mechanism for how groups beget new groups comprising kin. Furthermore, PCD is a means for solving a central problem of group living — the toxic effects of death — by making resources in dying cells beneficial to others. What emerges from the data reviewed here is that while PCD carries an obvious cost to the cell, it can be a driver of complexity in microbial communities.","","","",""
7,"","HUNG20121128","Hung, Chao-Hsiung; Hang, Hsueh-Ming","A reduced-complexity image coding scheme using decision-directed wavelet-based contourlet transform","Journal of Visual Communication and Image Representation",23,7,"","1128 - 1143",2012,"","","http://www.sciencedirect.com/science/article/pii/S1047320312001046","","","","","","","","","","","","","Recently the wavelet-based contourlet transform (WBCT) is adopted for image coding because it matches better image textures of different orientations. However, its computational complexity is very high. In this paper, we propose three tools to enhance the WBCT coding scheme, in particular, on reducing its computational complexity. First, we propose short-length 2-D filters for directional transform. Second, the directional transform is applied to only a few selected subbands and the selection is done by a mean-shift-based decision procedure. Third, we fine-tune the context tables used by the arithmetic coder in WBCT coding to improve coding efficiency and to reduce computation. Simulations show that, at comparable coded image quality, the proposed scheme saves over 92% computing time of the original WBCT scheme. Comparing to the conventional 2-D wavelet coding schemes, it produces clearly better subjective image quality.","","Contourlet transform, Wavelet-based contourlet transform, Bit-plane coding, Directional filter bank, Directional transform, Wavelet transform, Image coding, Computational complexity reduction, Adaptive Directional Transform","",""
7,"","LEE20121179","Lee, Jin Young; Wey, Ho-Cheon; Park, Du-Sik","A high performance and low complexity sampling-based intra coding method for H.264/AVC","Journal of Visual Communication and Image Representation",23,8,"","1179 - 1188",2012,"","","http://www.sciencedirect.com/science/article/pii/S1047320312001290","","","","","","","","","","","","","The H.264/AVC video coding standard can achieves higher compression performance than previous video coding standards, such as MPEG-2, MPEG-4, and H.263. Especially, in order to obtain the high coding performance in intra pictures, the H.264/AVC encoder employs various directional spatial prediction modes and the rate-distortion (RD) optimization technique inducing high computational complexity. For further improvement in the coding performance with the low computational complexity, we introduce a sampling-based intra coding method. The proposed method generates two sub-images, which are defined as a sampled sub-image and a prediction error sub-image in this paper, from an original image through horizontal or vertical sampling and prediction processes, and then each sub-image is encoded with different intra prediction modes, quantization parameters, and scanning patterns. Experimental results demonstrate that the proposed method significantly improves the intra coding performance and reduces the encoding complexity with the smaller number of the RD cost calculation process.","","H.264/AVC, Sampling-based intra coding, Sampled sub-image, Prediction error sub-image, Intra prediction mode, Rate-distortion (RD) optimization, Intra mode decision, Video coding","",""
7,"","SUN20121244","Sun, Wei; Huang, Guo H.; Lv, Ying; Li, Gongchen","Waste management under multiple complexities: Inexact piecewise-linearization-based fuzzy flexible programming","Waste Management",32,6,"","1244 - 1257",2012,"","","http://www.sciencedirect.com/science/article/pii/S0956053X12000335","","","","","","","","","","","","","To tackle nonlinear economies-of-scale (EOS) effects in interval-parameter constraints for a representative waste management problem, an inexact piecewise-linearization-based fuzzy flexible programming (IPFP) model is developed. In IPFP, interval parameters for waste amounts and transportation/operation costs can be quantified; aspiration levels for net system costs, as well as tolerance intervals for both capacities of waste treatment facilities and waste generation rates can be reflected; and the nonlinear EOS effects transformed from objective function to constraints can be approximated. An interactive algorithm is proposed for solving the IPFP model, which in nature is an interval-parameter mixed-integer quadratically constrained programming model. To demonstrate the IPFP’s advantages, two alternative models are developed to compare their performances. One is a conventional linear-regression-based inexact fuzzy programming model (IPFP2) and the other is an IPFP model with all right-hand-sides of fussy constraints being the corresponding interval numbers (IPFP3). The comparison results between IPFP and IPFP2 indicate that the optimized waste amounts would have the similar patterns in both models. However, when dealing with EOS effects in constraints, the IPFP2 may underestimate the net system costs while the IPFP can estimate the costs more accurately. The comparison results between IPFP and IPFP3 indicate that their solutions would be significantly different. The decreased system uncertainties in IPFP’s solutions demonstrate its effectiveness for providing more satisfactory interval solutions than IPFP3. Following its first application to waste management, the IPFP can be potentially applied to other environmental problems under multiple complexities.","","Piecewise linearization, Fuzzy flexible programming, Interval nonlinear programming, Economies of scale","",""
7,"","MCVICAR20101277","McVicar, Tim R.; Niel, Tom G. Van; Li, LingTao; Wen, ZhongMing; Yang, QinKe; Li, Rui; Jiao, Feng","Parsimoniously modelling perennial vegetation suitability and identifying priority areas to support China's re-vegetation program in the Loess Plateau: Matching model complexity to data availability","Forest Ecology and Management",259,7,"","1277 - 1290",2010,"","Managing landscapes at multiple scales for sustainability of ecosystem functions","http://www.sciencedirect.com/science/article/pii/S0378112709003338","","","","","","","","","","","","","The Chinese Central government's policy to re-vegetate large areas of the Loess Plateau is currently being rapidly implemented at the provincial, prefecture, county, township, and village levels of government. Managers at these five levels of government need access to information to assist them to plan the land use change prior to performing on-ground activities. To this end, the suitability of 38 predominately native species in the 113,000km2 Coarse Sandy Hilly Catchments (CSHC) has been mapped at a 100m resolution. In this data-sparse region, this was achieved by using a five-variable spatial overlay approach as we were able to readily access the required environmental variables and rule-set defining the species’ requirements (or tolerances). As the rules did not consider optimal growth they were possibly ‘too inclusive’, so the spatial extent of areas suggested for re-planting was refined by defining ‘target areas’ for trees, shrubs and grasses based on precipitation, aspect, landform, and slope. In the land-use planning criteria developed here we suggest that hill-slopes and gullies with slopes greater than or equal to 15° (defined from a 100m resolution DEM) be left for natural succession. Due to lateral flow of water, sediment and nutrients from these steep slope and gullies, further prioritising re-vegetation target areas to the zone adjacent to and down slope from these steep portions of the landscape reduces sediment entering the river network with a minimal decrease of regional stream flow. These two functions (mapping species suitability and locating where priority and target re-vegetation activities should be undertaken) are available at a 100m resolution for the entire CSHC by accessing a bilingual decision support tool called ReVegIH (Re-Vegetation Impacts on Hydrology). Finally, an ecohydrological model was used to simulate changes in average annual stream flow originating from the CSHC based on implementing the ‘target’ and ‘priority’ area re-vegetation activities within the constraints of two land limits.","","Ecohydrology, Scenario planning, Grain-for-Green Project, Loess Plateau, Vegetation suitability mapping, Re-vegetation, Afforestation, Land-use planning, Soil erosion, Water yield, Stream flow, Environmental management","",""
7,"","KEELING20131345","Keeling, Debbie Isobel; Daryanto, Ahmad; de Ruyter, Ko; Wetzels, Martin","Take it or leave it: Using regulatory fit theory to understand reward redemption in channel reward programs","Industrial Marketing Management",42,8,"","1345 - 1356",2013,"","","http://www.sciencedirect.com/science/article/pii/S0019850113001351","","","","","","","","","","","","","Channel Reward Programs (CRPs) facilitate relationship management within reseller networks in distribution channels, yet a persistent problem is that rewards are not seen as valuable, which can reduce program investment. By applying Regulatory Fit theory, to understand how to sustain goal orientation (promotion or prevention) and stimulate task engagement through a match with the manner of goal pursuit, this study demonstrates that the presentation style suppliers adopt influences resellers' perceptions of reward value and their rate of point redemption with respect to CRPs. Two field studies demonstrate the mechanisms driving this effect. First, fit effects result from the interaction between reward type and presentation format (i.e., verbal vs. numerical) and affect perceived reward values and investment decisions. Second, cognitive engagement and “feeling right” about reward redemption mediate the effects of fit on investment opportunity evaluations. In turn, the findings demonstrate that CRP efficacy can be enhanced by stimulating regulatory orientations that match the presentation formats of the reward and that the dual affective–cognitive processes affect probabilistic judgments of rewards. This additional mechanism can further stimulate resellers' engagement with and investment in CRPs within complex decision-making contexts.","","Regulatory fit, Regulatory focus, Feeling right, Cognitive engagement, Loyalty programs","",""
7,"","LEBEDEV2014142","Lebedev, Alexander; Olmos, J. J. Vegas; Pang, Xiaodan; Monroy, Idelfonso Tafur; Larsen, Knud J.; Forchhammer, Søren","Low complexity source and channel coding for mm-wave hybrid fiber-wireless links","Optics Communications",318,,"","142 - 146",2014,"","","http://www.sciencedirect.com/science/article/pii/S0030401813011668","","","","","","","","","","","","","We report on the performance of channel and source coding applied for an experimentally realized hybrid fiber-wireless W-band link. Error control coding performance is presented for a wireless propagation distance of 3 m and 20km fiber transmission. We report on peak signal-to-noise ratio performance of several encoded high-definition video sequences constrained by the channel bitrate and the packet size. We argue that light video compression and low complexity channel coding for the W-band fiber-wireless link enable low-delay multiple channel 1080p wireless HD video transmission.","","Fiber optics, H.264/AVC, Millimeter wave transmission, Optical communication, Radio over Fiber, W-band wireless","",""
7,"","ARDESTANI2011143","Ardestani, Majid R.; Shirazi, Ali Asghar Beheshti; Hashemi, Mahmoud Reza","Low-complexity unbalanced multiple description coding based on balanced clusters for adaptive peer-to-peer video streaming","Signal Processing: Image Communication",26,3,"","143 - 161",2011,"","","http://www.sciencedirect.com/science/article/pii/S092359651100004X","","","","","","","","","","","","","Multiple description scalable coding based on T+2D wavelet decomposition structure is highly flexible for peer-to-peer (P2P) video streaming. Finding the optimal truncation point of each wavelet-decomposed code block (CB) within each description is an NP-hard problem (Akyol et al., 2007 [1]). For P2P video streaming, it is necessary to implement an efficient multiple description encoder with three attributes; “adaptive” (due to the time-varying capacity of the P2P network links and nodes), “low-complexity” (because of the low processing power of the receiving peer) with arbitrarily “unbalanced descriptions” (because of the unequal capacities of the different sending peers). To design a multiple description encoder with the above mentioned features, we propose a simple clustering algorithm for partitioning the CBs into a limited number of clusters. This simple and efficient clustering algorithm significantly reduces the size of redundancy-rate assignment matrix, such that one can find the optimal channel-aware cluster-level redundancy-rate assignment matrix using a low-complexity full search approach. This approach improves the decoding quality compared to the co-echelon adaptive frameworks (Akyol et al., 2007 [1]; Tillo et al., 2007 [8]) in which a non-optimal heuristic rate assignment pattern is used. Especially for the unbalanced P2P scenario (which is the usual case), the performance gain of the proposed approach over the one presented in Tillo et al. (2007) [8] (generating only balanced descriptions) is significant (0.95–3.0dB). In addition, the proposed clustering approach may be analytically represented by closed-form relations for low-complexity computation of the optimal encoding parameters. Our complexity analysis shows that the proposed approach requires 52–96% less computations compared to the framework in Akyol et al. (2007) [1]. Therefore, an efficient real-time post-encoding adaptation mechanism may be realized. The simulation results demonstrate that the adaptive proposed framework outperforms the approach presented in Akyol et al. (2007) [1] by (0.26–0.95dB) and the non-adaptive multiple description coding by (1.1–2.3dB).","","Scalable video coding, Multiple description coding, Motion compensated temporal filtering, Channel-aware redundancy-rate allocation, Clustering","",""
7,"","EHLERT20111629","Ehlert, Bryan A.; Durham, Christopher A.; Parker, Frank M.; Bogey, William M.; Powell, Charles S.; Stoner, Michael C.","Impact of operative indication and surgical complexity on outcomes after thoracic endovascular aortic repair at National Surgical Quality Improvement Program Centers","Journal of Vascular Surgery",54,6,"","1629 - 1636.e1",2011,"","","http://www.sciencedirect.com/science/article/pii/S0741521411015710","","","","","","","","","","","","","Introduction
 Thoracic endovascular aortic repair (TEVAR) devices are increasingly being utilized to treat aortic pathologies outside of the original Food & Drug Administration (FDA) approval for nonruptured descending thoracic aorta aneurysms (DTAs). The objective of this study was to evaluate the outcomes of patients undergoing TEVAR, elucidating the role of surgical and pathologic variables on morbidity and mortality.
 Methods
 National Surgical Quality Improvement Program (NSQIP) data were reviewed for all patients undergoing endovascular thoracic aorta repair from 2005 to 2007. The patients' operative indication and surgical complexity were used to divide them into study and control populations. Comorbid profiles were assessed utilizing a modified Charlson Comorbidity Index (CCI). Thirty-day occurrences of mortality and serious adverse events (SAEs) were used as study endpoints. Univariate and multivariate models were created using demographic and clinical variables to assess for significant differences in endpoints (P ≤ .05).
 Results
 A total of 440 patients undergoing TEVAR were identified. When evaluating patients based on operative indication, the ruptured population had increased mortality and SAE rates compared to the nonruptured DTA population (22.6% vs 6.2%;P < .01 and 35.5% vs 9.1%;P < .01, respectively). Further analysis by surgical complexity revealed increased mortality and SAE rates when comparing the brachiocephalic aortic debranching population to the noncovered left subclavian artery population (23.1% vs 6.5%; P = .02 and 30.8% vs 9.1%; P < .01, respectively). Multivariate analysis demonstrated that operative indication was not a correlate of mortality or SAEs (odds ratio [OR], 0.95; P = .92 and OR, 1.42; P = .39, respectively); however, brachiocephalic aortic debranching exhibited a deleterious effect on mortality (OR, 8.75; P < .01) and SAE rate (OR, 6.67; P = .01).
 Conclusion
 The operative indication for a TEVAR procedure was not found to be a predictor of poor patient outcome. Surgical complexity, specifically the need for brachiocephalic aortic debranching and aortoiliac conduit, was shown to influence the occurrence of SAEs in a multivariate model. Comparative data, such as these, illustrate real-world outcomes of patients undergoing TEVAR outside of the original FDA-approved indications. This information is of paramount importance to various stakeholders, including third-party payers, the device industry, regulatory agencies, surgeons, and their patients.","","","",""
7,"","BLANCH20121684","Blanch, Sílvia; Duran, David; Flores, Marta; Valdebenito, Vanessa","The Effects of a Peer Tutoring Programme to Improve the Reading Comprehension Competence Involving Primary Students at School and their Families at Home","Procedia - Social and Behavioral Sciences",46,,"","1684 - 1688",2012,"","4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain","http://www.sciencedirect.com/science/article/pii/S1877042812014905","","","","","","","","","","","","","This paper presents on the effects of an educational programme involving peer tutoring at school and family tutoring at home on child reading comprehension achievement. We drew upon a sample of 303 students and 223 family tutors. The methodology combined a quasi-experimental design and a qualitative analysis of texts. Background variables were collected by means of student and parent questionnaires and also teacher and family interviews. An analysis of the family tutoring interactions was also monitored. Overall, the study reveals the effectiveness of peer learning to improve reading comprehension skills and the potential of family involvement for the development of academic skills.","","Peer tutoring, Reading comprehension, Family involvement, Literacy skills","",""
7,"","HSU2018185","Hsu, Chi-Ching; Yu-Heng, Wu; Menolascina, Filippo; Nordling, Torbjörn E. M.","Modelling of the GAL1 Genetic Circuit in Yeast Using Three Equations⁎⁎Chi-Ching Hsu, Yu-Heng Wu, and Torbjörn Nordling was in part supported by the Ministry of Science and Technology, Taiwan (grant 105-2218-E-006-016-MY2 and 105-2911-I-006-518), and the Innovation and Technology Program (I-Dream), National Applied Research Laboratories (NARLabs), Taiwan. Filippo Menolascina was in part supported by the Royal Society of Edinburgh, U.K. We sincerely thank Diego di Bernardo and Gianfranco Fiore for providing their data, Matlab code, and helping us understand their analysis published in Fiore et al. (2013).","IFAC-PapersOnLine",51,18,"","185 - 190",2018,"","10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018","http://www.sciencedirect.com/science/article/pii/S2405896318319785","","","","","","","","","","","","","Synthetic gene circuits can be used to modify and control existing biological processes and thus e.g. increase drug yields. Currently their use is hampered by the, largely, trial and error approach used to design them. Lack of reliable quantitative dynamical models of genetic circuits e.g. prevents the use of well established control design methods. We aim toward creation of a pipeline for automated closed-loop identification of dynamic models of synthetically engineered genetic circuits in microorganisms. As a step towards this aim, we here study modelling of the input-output behaviour of the yGIL337 strain of S. cerevisiae. In this strain expression of the fluorescent reporter can be turned on by growing the yeast in galactose and off by glucose. We perform parameter estimation on a system of three ordinary differential equations of Michaelis-Menten type based on in vivo data from a microfluidic experiment by Fiore et al. (2013) after redoing the data preprocessing. The parameter estimation is done using AMIGO2–a state of the art Matlab toolbox for iterative identification of dynamical models. We show that the goodness-of-fit of our model is comparable to the five models proposed by Fiore et al. and we hypothesise that the system is an adaptive feedback system.","","synthetic biology, systems biology, genetic circuit, system identification, parameter estimation","",""
7,"","MCDONALD2014195","McDonald, Jo; Steelman, Karen L.; Veth, Peter; Mackey, Jeremy; Loewen, Josh; Thurber, Casey R.; Guilderson, T. P.","Results from the first intensive dating program for pigment art in the Australian arid zone: insights into recent social complexity","Journal of Archaeological Science",46,,"","195 - 204",2014,"","","http://www.sciencedirect.com/science/article/pii/S0305440314000910","","","","","","","","","","","","","The Canning Stock Route Project (Rock Art and Jukurrpa) has yielded the first radiocarbon dates for rock paintings in the Western Desert of Australia. We report on the results of a large-scale project to directly-date both charcoal and inorganic-pigmented pictographs using plasma oxidation combined with accelerator mass spectrometry. This project has yielded the largest number of art dates from any region in the world: one site alone has produced 12 art dates (from 30 collected samples). Our work advances the testing of the dating method through the systematic use of replicates and explores the methodological implications of dating very small samples (10–40 μg carbon). Thirty-six radiocarbon age determinations range from 3000 years ago to Modern. The results contribute to an understanding of art production in the Australian arid zone during a period of extreme cultural dynamism. We have demonstrated for the first time that significant late Holocene changes in discard rates of artefacts and technological organization of the extractive technologies of implements such seed-grinders is matched by a very high level of stylistic heterogeneity in the art – which has been systematically dated within and between dialect groups.","","Rock art, Radiocarbon dating, Plasma oxidation, Accelerator mass spectrometry, Arid zone, Pigment","",""
7,"","SIDEBOTHAM2015201","Sidebotham, M.; Fenwick, J.; Carter, A.; Gamble, J.","Using the five senses of success framework to understand the experiences of midwifery students enroled in an undergraduate degree program","Midwifery",31,1,"","201 - 207",2015,"","","http://www.sciencedirect.com/science/article/pii/S0266613814002071","","","","","","","","","","","","","Background
 developing a student׳s sense of capability, purpose, resourcefulness, identity and connectedness (five-senses of success) are key factors that may be important in predicting student satisfaction and progression within their university program.
 Aim
 the study aimed to examine the expectations and experiences of second and third year midwifery students enroled in a Bachelor of Midwifery program and identify barriers and enablers to success.
 Method
 a descriptive exploratory qualitative design was used. Fifty-six students enroled in either year 2 or 3 of the Bachelor of Midwifery program in SE Queensland participated in an anonymous survey using open-ended questions. In addition, 16 students participated in two year-level focus groups. Template analysis, using the Five Senses Framework, was used to analyse the data set.
 Findings
 early exposure to ‘hands on’ clinical midwifery practice as well as continuity of care experiences provided students with an opportunity to link theory to practice and increased their perception of capability as they transitioned through the program. Student׳s sense of identity, purpose, resourcefulness, and capability was strongly influenced by the programs embedded meta-values, including a ‘woman centred’ approach. In addition, a student׳s ability to form strong positive relationships with women, peers, lecturers and supportive clinicians was central to developing connections and ultimately a sense of success. A sense of connection not only fostered an ongoing belief that challenges could be overcome but that students׳ themselves could initiate or influence change.
 Conclusions
 the five senses framework provided a useful lens through which to analyse the student experience. Key factors to student satisfaction and retention within a Bachelor of Midwifery program include: a clearly articulated midwifery philosophy, strategies to promote student connectedness including the use of social media, and further development of clinician׳s skills in preceptorship, clinical teaching and facilitation. Program delivery methods and student support systems should be designed to enable maximum flexibility to promote capability and resourcefulness and embed sense of purpose and identity early in the program.","","Midwifery, Students, Undergraduate, Expectations, Experiences, Five senses of success model","",""
7,"","XIA20172169","Xia, L.; Najafi, E.; Bergveld, H. J.; Donkers, M. C. F.","A Computationally Efficient Implementation of an Electrochemistry-Based Model for Lithium-Ion Batteries**This work has received financial support from the Horizon 2020 programme of the European Union under the grant ‘Integrated Components for Complexity Control in affordable electrified cars (3Ccar-662192)’ and under the grant ‘Electric Vehicle Enhanced Range, Lifetime And Safety Through INGenious battery manage- ment (EVERLASTING-713771)’.","IFAC-PapersOnLine",50,1,"","2169 - 2174",2017,"","20th IFAC World Congress","http://www.sciencedirect.com/science/article/pii/S2405896317305931","","","","","","","","","","","","","Lithium-ion batteries are commonly employed in various applications owing to high energy density and long service life. Lithium-ion battery models are used for analysing batteries and enabling power control in applications. The Doyle-Fuller-Newman (DFN) model is a popular electrochemistry-based lithium-ion battery model which represents solid-state and electrolyte diffusion dynamics and accurately predicts the current/voltage response. However, implementation of the full DFN model requires significant computation time. This paper proposes a computationally efficient implementation of the full DFN battery model, which is convenient for real-time applications. The proposed implementation is based on spatial and temporal discretisation of the governing partial differential equations and a particular numerical method for solving the resulting discretised model equations, which is based on a damped Newton’s method. In a simulation study, the numerical efficiency of the proposed implementation is shown.","","Lithium-ion battery, electrochemistry-based model, partial differential equations, numerical methods","",""
7,"","WEHRENS2012274","Wehrens, Rik; Bal, Roland","Health programs struggling with complexity: A case study of the Dutch ‘PreCare’ project","Social Science & Medicine",75,2,"","274 - 282",2012,"","","http://www.sciencedirect.com/science/article/pii/S0277953612002742","","","","","","","","","","","","","This article aims to understand the effects of rationalized health programs (the basic components of which are efficiency, calculability, predictability and control) on local practices. We discuss how a successful U.S. intervention in preventive youth health care (the Nurse Family Partnership) has been translated and adapted within a Dutch setting. The Dutch version of the program is called ‘PreCare’. The empirical analysis highlights the effects of rationalized health programs on local practices, in terms of the amount of work required, how local practices are disciplined, how these programs (re)draw boundaries, the ‘travel expenditures’ involved (and developed ‘coping strategies’), and how local practices (try to) reshape the program. Our empirical analysis builds on a combination of qualitative methods. We conducted 16 semi-structured interviews with 19 people involved in the PreCare program. The majority of the interviews were conducted between July and November 2008. We also conducted an analysis of relevant documents related to the PreCare intervention and protocol. Furthermore, we observed at several meetings, including case conferences and management intervision meetings. The article makes a theoretical and practical contribution to the field. Theoretically, we show how the rationalization process is linked to a broader development of quantification and how both developments are based on a particularly modern ontology and epistemology in which what is considered ‘real’ and ‘knowledgeable’ becomes closely tied to what is measurable. The article offers a different conceptualization of rationalized health programs, one that acknowledges the need to standardize some elements, but also recognizes the need to be open and flexible toward local practices. We specifically focus on the tools that are able to deal with both the need to standardize and the need to be open toward local practices. We suggest that ‘(re)writing devices’ are a fruitful category of tools for this purpose.","","Rationalization in health care, Health promotion programs, Quantification, Standardization, (re)writing devices, Complexities in health care","",""
7,"","BERRY2011284","Berry, Jay G.; Agrawal, Rishi; Kuo, Dennis Z.; Cohen, Eyal; Risko, Wanessa; Hall, Matt; Casey, Patrick; Gordon, John; Srivastava, Rajendu","Characteristics of Hospitalizations for Patients Who Use a Structured Clinical Care Program for Children with Medical Complexity","The Journal of Pediatrics",159,2,"","284 - 290",2011,"","","http://www.sciencedirect.com/science/article/pii/S0022347611001466","","","","","","","","","","","","","Objective
 To describe the characteristics of hospitalizations for patients who use clinical programs that provide care coordination for children with multiple, chronic medical conditions.
 Study design
 Retrospective analysis of 1083 patients hospitalized between June 2006 and July 2008 who used a structured, pediatric complex-care clinical program within 4 children’s hospitals. Chronic diagnosis prevalence (ie, technology assistance, neurologic impairment, and other complex chronic conditions), inpatient resource utilization (ie, length of stay, 30-day readmission), and reasons for hospitalization were assessed across the programs.
 Results
 Over the 2-year study period, complex-care program patients experienced a mean of 3.1 ± 2.8 admissions, a mean length of hospital stay per admission of 12.2 ± 25.5 days, and a 30-day hospital readmission rate of 25.4%. Neurologic impairment (57%) and presence of a gastrostomy tube (56%) were the most common clinical characteristics of program patients. Notable reasons for admission included major surgery (47.1%), medical technology malfunction (9.0%), seizure (6.4%), aspiration pneumonia (3.9%), vomiting/feeding difficulties (3.4%), and asthma (1.8%).
 Conclusions
 Hospitalized patients who used a structured clinical program for children with medical complexity experienced lengthy hospitalizations with high early readmission rates. Reducing hospital readmission may be one potential strategy for decreasing inpatient expenditures in this group of children with high resource utilization.","","","",""
7,"","KUO2016291","Kuo, Dennis Z.; Berry, Jay G.; Glader, Laurie; Morin, Melinda J.; Johaningsmeir, Sarah; Gordon, John","Health Services and Health Care Needs Fulfilled by Structured Clinical Programs for Children with Medical Complexity","The Journal of Pediatrics",169,,"","291 - 296.e1",2016,"","","http://www.sciencedirect.com/science/article/pii/S0022347615011695","","","","","","","","","","","","","Objective
 To describe family-reported health service needs of children with medical complexity (CMC) and to assess which needs are more often addressed in a tertiary care center-based structured clinical program for CMC.
 Study design
 Mailed survey to families of CMC enrolled in a structured-care program providing care coordination and oversight at 1 of 3 children's hospitals. Outcomes included receipt of 14 specific health service needs. Paired t tests compared unmet health care needs prior to and following program enrollment.
 Results
 Four hundred forty-one of 968 (46%) surveys were returned and analyzed. Respondents reported their children had a mean age of 7 (SD 5) years. A majority of respondents reported the child had developmental delay (79%) and feeding difficulties (64%). Of the respondents, 56% regarded the primary care provider as the primary point of contact for medical issues. Respondents reported an increase in meeting all 14 health services needs after enrollment in a tertiary care center-based structured clinical program, including primary care checkups (82% vs 96%), therapies (78% vs 91%), mental health care (34% vs 58%), respite care (56% vs 75%), and referrals (51% vs 83%) (all P < .001).
 Conclusions
 Tertiary care center-based structured clinical care programs for CMC may address and fulfill a broad range of health service needs that are not met in the primary care setting.","","","",""
7,"","OLDLAND2014319","Oldland, Elizabeth; Driscoll, Andrea; Currey, Judy","High complexity chronic heart failure management programmes: Programme characteristics and 12 month patient outcomes","Collegian",21,4,"","319 - 326",2014,"","","http://www.sciencedirect.com/science/article/pii/S1322769613000851","","","","","","","","","","","","","Summary
 Chronic heart failure management programmes (CHF-MPs) have been developed to improve, clinical outcomes in response to the high burden of disease from chronic heart failure (CHF). Programmes vary in model, duration, complexity of interventions and incorporation of evidence-based guidelines for programme delivery. Few studies have explored patient outcomes at 12 months from enrolment in a CHF. The aim of the current study was to explore the characteristics and clinical outcomes of patients enrolled in four high complexity CHF-MPs at 12 months after initial enrolment. A secondary aim was to explore the adoption of key evidence-based CHF management strategies in these programmes. After ethics approval, a multisite mixed methods design was implemented incorporating survey and chart audit. Programme characteristics and interventions used in four CHF-MPs were surveyed in Stage 1. Stage 2 involved a chart audit of patients enrolled in the programmes (N=135) on or after the 1/1/07. Primary endpoints were all-cause hospitalisation and/or mortality at 12 months. Data were analysed using descriptive and inferential statistics. All programmes implemented a high complexity of evidence-based interventions consistent with national guidelines. However, documentation of New York Heart Association functional class was rare; limiting quantifiable evaluation of response to therapy throughout programme enrolment. The majority of patients (73%) had severe systolic heart failure with high co-morbidities reflected in a mean Charlson's total co-morbidity score of 3 (±2.1). The high rate of baseline evidence-based, pharmacotherapy (beta-blocker: 86%, n=112 and ACE inhibitor: 76%, n=103) was maintained at 12 months (71% and 84% respectively). At 12 months all cause hospitalisation and/or mortality was 57% (n=77). The CHF-MPs in this study implemented complex evidence-based interventions resulting in high rates of key medication prescription. However, despite the implementation of several evidence-based interventions, over a period of 12 months, more than half of the patients were rehospitalised or died.","","Heart failure management programmes, Chronic heart failure, Evidence-based interventions, Complexity","",""
7,"","ZENIL2014341","Zenil, Hector; Soler-Toscano, Fernando; Dingle, Kamaludin; Louis, Ard A.","Correlation of automorphism group size and topological properties with program-size complexity evaluations of graphs and complex networks","Physica A: Statistical Mechanics and its Applications",404,,"","341 - 358",2014,"","","http://www.sciencedirect.com/science/article/pii/S0378437114001691","","","","","","","","","","","","","We show that numerical approximations of Kolmogorov complexity (K) of graphs and networks capture some group-theoretic and topological properties of empirical networks, ranging from metabolic to social networks, and of small synthetic networks that we have produced. That K and the size of the group of automorphisms of a graph are correlated opens up interesting connections to problems in computational geometry, and thus connects several measures and concepts from complexity science. We derive these results via two different Kolmogorov complexity approximation methods applied to the adjacency matrices of the graphs and networks. The methods used are the traditional lossless compression approach to Kolmogorov complexity, and a normalised version of a Block Decomposition Method (BDM) based on algorithmic probability theory.","","Kolmogorov complexity, Complex networks, Graph automorphisms, Algorithmic probability, Compressibility, Network biology","",""
7,"","ESMAEILI2010360","Esmaeili, M.; Tadayon, M. H.; Gulliver, T. A.","Low-complexity girth-8 high-rate moderate length QC LDPC codes","AEU - International Journal of Electronics and Communications",64,4,"","360 - 365",2010,"","","http://www.sciencedirect.com/science/article/pii/S1434841109000715","","","","","","","","","","","","","A low-complexity method for constructing a large class of high-rate girth-8 quasi-cyclic low-density parity-check (LDPC) codes with moderate length 1000<n<10000 is presented. This method is based on combining configurations with incidence matrices composed of cyclic permutation matrices and Tanner graph girth at least 8. Shortened lattice codes of length n<380 and girth 8 are employed as component codes. From the performance perspective, the resulting codes are comparable to their counterparts such as random-like and shortened array LDPC codes.","","LDPC codes, Combined codes, Lattice codes","",""
7,"","LINDQVIST2016392","Lindqvist, Johan; Bånkestad, Daniel; Carstensen, Anna-Maria; Lundin, Björn; Wik, Torsten","Complexity of Chlorophyll Fluorescence Dynamic Response as an Indicator of Excessive Light Intensity**This work was funded by the Mistra Innovation research program, within the Swedish Foundation for Strategic Environmental Research (Mistra).","IFAC-PapersOnLine",49,16,"","392 - 397",2016,"","5th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2016","http://www.sciencedirect.com/science/article/pii/S2405896316316354","","","","","","","","","","","","","Abstract:
 The controllability of LED lighting systems for greenhouses and plant factories offers a possibility for light induced diagnose of plant status. Here, a novel method for proximal remote detection of plant light tolerance is investigated. The method is based on an identification of a transfer function model for the measured chlorophyll fluorescence response to a small step variation in blue LED light. It is postulated that the least required model order decreases as the plants become light stressed due to saturation effects at excess light conditions. We apply this method to basil and lettuce plants under different background light intensities, and the results are compared to measured effective quantum yield (y(II)), relative electron transport rate through PSII (ETR(II)) and non-photochemical quenching (NPQ), all reflecting the photosynthetic performance. For both species it is indeed found that the required model order decreases with increasing background light intensity at the same time as the measured reference parameters indicates a decreased photosynthetic efficiency. It is suggested that the light intensity should be such that the chlorophyll fluorescence response requires a model order of 3 or higher to avoid ineffective irradiation of the plants.","","Photosynthesis, fluorescence, plant sensing, step function responses, dynamic behaviour, system identification, method development, feedback control","",""
7,"","JIANG2013415","Jiang, Dan; Guo, Jichang; Wu, Xiaojia","Low-complexity distributed multi-view video coding for wireless video sensor networks based on compressive sensing theory","Neurocomputing",120,,"","415 - 421",2013,"","Image Feature Detection and Description","http://www.sciencedirect.com/science/article/pii/S0925231213002919","","","","","","","","","","","","","Sparsity is an attractive feature of images. Images can be efficiently represented using a few significant coefficients and sparse reconstructed from a small set of random linear measurements by utilizing the sparse feature in compressive sensing theory. Storage and transmission of multi-view video sequences involve large volumes of redundant data. These data can be efficiently compressed with techniques which encode the signals independently and decode them jointly. By integrating the respective characteristics of compressive sensing and distributed source coding, we propose a novel multi-view video coding approach for use in resource limited devices such as wireless video sensor networks. The proposed approach can explore the sparsity of video images, allow for low complexity encoder and the exploitation of inter-camera correlation without communications among cameras. Simulation results show the proposed framework outperforms the baseline compressive sensing-based scheme of intra frame coding by 3–5dB. Compared with conventional H.264 or DVC scheme, the proposed frameworks simple while the quality of reconstructed image and compressibility are kept.","","Sparsity, Sparse reconstruction, Multi-view video, Compressive sensing, Wireless video sensor networks, Distributed source coding","",""
7,"","RAIJMAKERS201642","Raijmakers, L. H. J.; Shivakumar, K. M.; Donkers, M. C. F.; Lammers, M. J. G.; Bergveld, H. J.","Crosstalk Interferences on Impedance Measurements in Battery Packs**This work has received financial support from the Dutch Ministry of Economic Affairs under the grant A green Deal in Energy Materials (ADEM) and from the Horizon 2020 programme of the European Union under the grant Integrated Components for Complexity Control in affordable electrified cars (3Ccar-662192).","IFAC-PapersOnLine",49,11,"","42 - 47",2016,"","8th IFAC Symposium on Advances in Automotive Control AAC 2016","http://www.sciencedirect.com/science/article/pii/S2405896316313283","","","","","","","","","","","","","In order to provide the required power and energy for e.g. automotive applications, a multitude of cells is assembled into a battery pack. For safety and control purposes it is of interest to equip every single cell with an Electrochemical Impedance Spectroscopy (EIS) measurement system. However, performing EIS measurements simultaneously on each cell in a battery pack introduces crosstalk interferences in surrounding cells. This causes EIS measurements in battery packs to be inaccurate. An experimental investigation on a battery pack showed that crosstalk is a linear phenomenon which is dependent on the measurement frequency, the relative position of the cells and the inter-cell spacing. Based on the experimental results and a proposed two-coil model with inductive coupling, a transfer-function description has been developed in order to simulate the crosstalk behavior. This model can be used as a supporting tool in the development of ElS-based measurement systems in battery packs.","","Crosstalk, Batteries, Electrochemical Impedance Spectroscopy, Electromagnetic interferences, Battery packs, Crosstalk modeling","",""
7,"","LILJAANDERSSON2012453","Andersson, Petra Lilja; Edberg, Anna-Karin","Swedish nursing students' experience of aspects important for their learning process and their ability to handle the complexity of the nursing degree program","Nurse Education Today",32,4,"","453 - 457",2012,"","","http://www.sciencedirect.com/science/article/pii/S0260691711001237","","","","","","","","","","","","","Summary
 The aim of the study was to explore nursing students' experiences of aspects important for their learning process and their ability to handle the complexity of the nursing degree program. The study was longitudinal and qualitative based on interviews with nursing students, six women and two men aged 20–36, during their three years of education. In all, seven patterns were found embracing aspects of importance for the students' learning: Having a clear goal, being able to re-evaluate one's ideas, being acknowledged, when the abstract becomes tangible, using one's own experiences as a tool for learning, hovering between closeness and distance regarding one's future profession and handling theory and practice in relation to one another. The results show the importance of providing clinical courses, strongly connected to the theoretical parts of the program and to use reflection and experience-based learning in the nursing program.","","Education, Nursing students, Qualitative methods, Interview","",""
7,"","FOUCAUD201548","Foucaud, Florent","Decision and approximation complexity for identifying codes and locating-dominating sets in restricted graph classes","Journal of Discrete Algorithms",31,,"","48 - 68",2015,"","24th International Workshop on Combinatorial Algorithms (IWOCA 2013)","http://www.sciencedirect.com/science/article/pii/S1570866714000653","","","","","","","","","","","","","An identifying code is a subset of vertices of a graph with the property that each vertex is uniquely determined (identified) by its nonempty neighbourhood within the identifying code. When only vertices out of the code are asked to be identified, we get the related concept of a locating-dominating set. These notions are closely related to a number of similar and well-studied concepts such as the one of a test cover. In this paper, we study the decision problems Identifying Code and Locating-Dominating Set (which consist in deciding whether a given graph admits an identifying code or a locating-dominating set, respectively, with a given size) and their minimization variants Minimum Identifying Code and Minimum Locating-Dominating Set. These problems are known to be NP-hard, even when the input graph belongs to a number of specific graph classes such as planar bipartite graphs. Moreover, it is known that they are approximable within a logarithmic factor, but hard to approximate within any sub-logarithmic factor. We extend the latter result to the case where the input graph is bipartite, split or co-bipartite: both problems remain hard in these cases. Among other results, we also show that for bipartite graphs of bounded maximum degree (at least 3), the two problems are hard to approximate within some constant factor, a question which was open. We summarize all known results in the area, and we compare them to the ones for the related problem Dominating Set. In particular, our work exhibits important graph classes for which Dominating Set is efficiently solvable, but Identifying Code and Locating-Dominating Set are hard (whereas in all previous works, their complexity was the same). We also introduce graph classes for which the converse holds, and for which the complexities of Identifying Code and Locating-Dominating Set differ.","","Test cover, Separating system, Identifying code, Locating-dominating set, NP-completeness, Approximation","",""
7,"","HOANGVAN201551","Van, Xiem Hoang; Ascenso, João; Pereira, Fernando","HEVC backward compatible scalability: A low encoding complexity distributed video coding based approach","Signal Processing: Image Communication",33,,"","51 - 70",2015,"","","http://www.sciencedirect.com/science/article/pii/S0923596515000235","","","","","","","","","","","","","The growing heterogeneity and dynamic nature of the networks, terminals, and usage environments has boosted the need for powerful scalable video coding engines able to efficiently adapt to changing consumption conditions. Some emerging applications such as video surveillance, visual sensor networks, and remote space transmission require scalable coding solutions, which are not only compression efficient but also provide low encoding complexity and high error resilience. Following the call for scalable extensions of the emerging High Efficiency Video Coding (HEVC) standard, targeting the so-called Scalable HEVC (SHVC) standard, this paper proposes a novel scalable video coding solution offering quality scalability by combining the predictive and distributed video coding approaches by means of an HEVC compliant base layer and distributed coding enhancement layers. The proposed Distributed Scalable Video Coding (DSVC) solution follows an Intra-encoding with Inter-decoding approach to provide low encoding complexity and error robustness while achieving high compression. Towards this objective, this paper adopts a novel coding architecture and proposes several novel enhancement layers coding tools, notably for side information creation, correlation modeling, and coding mode selection. The experimental results reveal that the DSVC RD performance outperforms the relevant alternative coding solutions, notably by up to 33.22% and 11.28% BD-Rate gains regarding the relevant HEVC-Simulcast and SHVC-Intra-benchmarks, while achieving a lower encoding complexity.","","Scalable video coding, Distributed video coding, HEVC standard, Quality scalability, Side information creation, Correlation modeling","",""
7,"","DENG2012522","Deng, Zhi-Pin; Chan, Yui-Lam; Jia, Ke-Bin; Fu, Chang-Hong; Siu, Wan-Chi","Iterative search strategy with selective bi-directional prediction for low complexity multiview video coding","Journal of Visual Communication and Image Representation",23,3,"","522 - 534",2012,"","","http://www.sciencedirect.com/science/article/pii/S1047320312000296","","","","","","","","","","","","","The multiview video coding (MVC) extension of H.264/AVC is the emerging standard for compression of impressive 3D and free-viewpoint video. The coding structure in MVC adopts motion and disparity estimation to exploit temporal and inter-view dependencies in MVC. It results in a considerable increase in encoding complexity. Most of the computational burden comes from uni-directional and bi-directional prediction. In this paper, an iterative search strategy is designed to speed up the uni-directional prediction in MVC. It can work with an adaptive search range adjustment through a confidence measure of a loop constraint to obtain both motion and disparity vectors jointly. Furthermore, a selective bi-directional prediction algorithm is proposed to enhance the coding performance by analyzing the statistical characteristics of bi-directional prediction in MVC. Experimental results demonstrate that, by using the proposed fast search, the temporal and inter-view redundancies of multiview video can be eliminated sufficiently with low complexity.","","3DTV, Multiview video coding, Disparity estimation, Motion estimation, Bi-directional prediction, Hierarchical B picture, Adaptive search range, JMVC","",""
7,"","ALARFAJ2012612","Alarfaj, Abeer; Alshumaimeri, Yousif","The effect of a suggested training program on reading speed and comprehension of Saudi female university students","Procedia - Social and Behavioral Sciences",31,,"","612 - 628",2012,"","World Conference on Learning, Teaching & Administration - 2011","http://www.sciencedirect.com/science/article/pii/S1877042811030436","","","","","","","","","","","","","The aim of this study was to design and test the efficacy of a training program to increase the reading speed and comprehension of female students at King Saud University. Over the course of six weeks, the program consisted of 12 training sessions, 60minutes each. Results indicated that (a) a six-week period is appropriate for speed-reading and comprehension training; (b) the program has a significant impact on increasing reading-speed; and (c) the program has a significant impact on comprehension (literal, deductive, and critical reasoning). The proposed training program, using active participation, is recommended to increase students’ reading speed.","","Reading speed, reading comprehension, training programs, reading skill, university students","",""
7,"","KARIOTOGLOU201663","Kariotoglou, Nikolaos; Margellos, Kostas; Lygeros, John","On the computational complexity and generalization properties of multi-stage and stage-wise coupled scenario programs","Systems & Control Letters",94,,"","63 - 69",2016,"","","http://www.sciencedirect.com/science/article/pii/S0167691116300494","","","","","","","","","","","","","We discuss the computational complexity and feasibility properties of scenario sampling techniques for uncertain optimization programs. We propose an alternative way of dealing with a special class of stage-wise coupled programs and compare it with existing methods in the literature in terms of feasibility and computational complexity. We identify trade-offs between different methods depending on the problem structure and the desired probability of constraint satisfaction. To illustrate our results, an example from the area of approximate dynamic programming is considered.","","Scenario approach, Randomized optimization, Uncertain systems, Approximate dynamic programming","",""
7,"","GEBAUER201364","Gebauer, Sandra Kristina; Zaunbauer, Anna C. M.; Möller, Jens","Cross-language transfer in English immersion programs in Germany: Reading comprehension and reading fluency","Contemporary Educational Psychology",38,1,"","64 - 74",2013,"","","http://www.sciencedirect.com/science/article/pii/S0361476X12000483","","","","","","","","","","","","","Cross-language effects on reading skills are of particular interest in the context of foreign language immersion programs. Although there is an extensive literature on cross-language effects on reading in general, research focusing on immersion students and including different dimensions of reading acquisition such as reading fluency and reading comprehension is scarce. This study therefore investigated cross-language transfer between first-language (L1) and second-language (L2) reading fluency and reading comprehension in a group of 220 German elementary school students who were enrolled in English partial immersion programs. Students were tested in grades 3 and 4. Structural equation modeling was used to examine cross-language transfer in a cross-lagged panel design. Results showed moderate cross-language paths when controlling for autoregressive effects. These findings are in line with previous results showing reciprocal transfer effects between L1 and L2 reading comprehension and reading fluency. In addition, the overall dominance of paths from L2 to L1 over paths from L1 to L2 suggests immersion-specific relations that may be attributable to the plentiful opportunities for academic reading in the L2 at school. Hence, skills necessary for successful reading can evidently be acquired in an L2 context and transferred to the L1. These findings underline the importance of cross-language transfer between reading skills in immersion programs.","","Cross-language transfer, Reading comprehension, Elementary school, Immersion education, Second language acquisition","",""
7,"","TANAKA201465","Tanaka, Hiroshi","Toward Project and Program Management Paradigm in the Space of Complexity: A Case Study of Mega and Complex Oil and Gas Development and Infrastructure Projects","Procedia - Social and Behavioral Sciences",119,,"","65 - 74",2014,"","Selected papers from the 27th IPMA (International Project Management Association), World Congress, Dubrovnik, Croatia, 2013","http://www.sciencedirect.com/science/article/pii/S1877042814021016","","","","","","","","","","","","","The author published his research paper at 26th International Project Management Association World Congress in which he argued that the monodukuri industry, or broadly hard systems project industry, is being affected either positively or negatively by a variety of complexity categorized by P.E.S.T.L.E. (political, economic, social, technological, legal and environmental) factors and proposed a conceptual model of an enterprise viability system reinforced by meta program management. This paper is based on the author's continuing meta program management research and contextual analysis of the project industry, traces how the typical events discussed under each of the PESTLE factor categories have behaved thereafter to confirm the validity of impact descriptions, and presents a case analysis of current mega oil and gas development and complex infrastructure projects for dominant characteristics of project operations. Then new thoughts of project and program management in the space of complexity of the project industry are proposed as the first step to build a new management paradigm, which has been qualitatively induced by the cases under study and are deriving from existing research results on complex projects. The new thoughts include meta program management to balance multi objectives; knowledge and stakeholder integration to create complex projects; finance planning and structuring as an essential ingredient of materializing complex projects; management of extreme projects; and contingent risk management.","","Project industry, complex projects, meta program management, knowledge integration, new paradigm of project management","",""
7,"","ALRWELE201571","Alrwele, Noura Shabak","Efficacy of a Training Program in Some Self-Managed Strategies on Developing English Language Reading Comprehension and Reader's Self-Perception of 11th Grade Female Students","Procedia - Social and Behavioral Sciences",177,,"","71 - 76",2015,"","First Global Conference on Contemporary Issues in Education (GLOBE-EDU 2014) 12-14 July 2014, Las Vegas, USA","http://www.sciencedirect.com/science/article/pii/S1877042815016912","","","","","","","","","","","","","The purpose of this study was to investigate the efficacy of a self-managed training program on developing the accuracy in responding to text-explicit, text-implicit questions, and reader's self-perception of 11th grade female students learning English as a foreign language, and to determine if significant differences existed between the research groups. A quasi- experimental design was used. Participants assigned to experimental group were (24) while (23) students were assigned to the control group that received traditional instruction. A program consisting of 18 sessions was designed and implemented. Data were collected using a reading comprehension test and the “Reader Self Perception Scale” (RSPS/2), each instrument was administered as Pre- and Post-test. Findings revealed that students in the experimental group performed better than those in the control group during the post-test on both text explicit, text implicit reading comprehension questions, and also on the Reader Self- Perception Scale (RSPS/2). statistically significant differences (P<.05) were found between the experimental and control group regarding their scores in text explicit, text implicit reading comprehension, and overall reader self- perception mean scores. Based on the findings of the study, the research came out of several recommendation.","","Self-Managed Strategies, Reading Comprehension, Reader's Self-Perception.","",""
7,"","SHAHABINEJAD2013738","Shahabinejad, M.; Talebi, S.; Shahabinejad, M.","Space–time–frequency coding over quasi-static frequency-selective channels with linear complexity for the ML receiver","Scientia Iranica",20,3,"","738 - 745",2013,"","","http://www.sciencedirect.com/science/article/pii/S1026309813000321","","","","","","","","","","","","","The achievement of our previously proposed space–time coding algorithm entitled full-rate linear-receiver space–time block code (FRLR STBC) has motivated us to propose, in this paper, a new class of high-rate space–time–frequency block codes (STFBCs) over frequency-selective Rayleigh fading channels. We have called these codes FRLR STFBCs with interleaving (FRLR STFBCs-I). FRLR STFBCs-I could achieve a full-diversity property over quasi-static channels. Simulation results also verify that the proposed schemes exhibit proper performances in comparison with the recently proposed STFBCs. The most outstanding characteristic of the newly introduced high-rate codes is the linear complexity of the maximum likelihood (ML) receiver which makes possible a fast and economical decoding process.","","Wireless communication, Multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO–OFDM), Quasi-static channels, Space–time–frequency coding, Fading channels","",""
7,"","JETZEK201689","Jetzek, Thorhildur","Managing complexity across multiple dimensions of liquid open data: The case of the Danish Basic Data Program","Government Information Quarterly",33,1,"","89 - 104",2016,"","","http://www.sciencedirect.com/science/article/pii/S0740624X15300186","","","","","","","","","","","","","Current literature on open government data has uncovered a wide range of challenges related to these important initiatives. The problems encountered include: insufficient data quality and interoperability, problems regarding governance and motivation, lack of capabilities, and heterogeneous political and ideological agendas. A common open data infrastructure might resolve some of these problems, however, implementing such an infrastructure is a highly complex task. This longitudinal case study of the Danish Basic Data Program (BDP) is intended to improve our understanding of the challenges related to providing open access to government data through open data infrastructure. The BDP aims to improve the quality of selected government data, make them more coherent, and improve accessibility through the implementation of a common data distribution platform. The program is expected to increase government efficiency and stimulate innovation. This case study describes the evolution of the BDP and identifies the main structural elements of an open data infrastructure. Data analysis uncovered four tensions, which are identified as key challenges of an open data infrastructure implementation. These tensions are presented with four suggested governance strategies that were used in the BDP case. The main contribution of the paper is a process model where the main phases and mechanisms of an open data infrastructure implementation, use and impacts are identified and explained.","","Open data, Open data infrastructure, Liquid open data, System-of-systems governance, Value generating mechanisms","",""
7,"","KENDIG2015e94","Kendig, Claire; Tyson, Anna; Young, Sven; Mabedi, Charles; Cairns, Bruce; Charles, Anthony","The Effect of a New Surgery Residency Program on Case Volume and Case Complexity in a Sub-Saharan African Hospital","Journal of Surgical Education",72,4,"","e94 - e99",2015,"","","http://www.sciencedirect.com/science/article/pii/S1931720414002694","","","","","","","","","","","","","Background
 Improved access to surgical care could prevent a significant burden of disease and disability-adjusted life years, and workforce shortages are the biggest obstacle to surgical care. To address this shortage, a 5-year surgical residency program was established at Kamuzu Central Hospital (KCH) in July 2009. As the residency enters its fourth year, we hypothesized that the initiation of a general surgical residency program would result in an increase in the overall case volume and complexity at KCH.
 Methods
 We conducted a retrospective analysis of operated cases at KCH during the 3 years before and the third year after the implementation of the KCH surgical residency program, from July 2006 to July 2009 and the calendar year 2012, respectively.
 Results
 During the 3 years before the initiation of the surgical residency, an average of 2317 operations were performed per year, whereas in 2012, 2773 operations were performed, representing a 20% increase. Before residency, an average of 1191 major operations per year were performed, and in 2012, 1501 major operations were performed, representing a 26% increase.
 Conclusion
 Our study demonstrates that operative case volume and complexity increase following the initiation of a surgical residency program in a sub-Saharan tertiary hospital. We believe that by building on established partnerships and emphasizing education, research, and clinical care, we can start to tackle the issues of surgical access and care.","","Global Health, Global Surgery, Surgical Education in Sub Saharan Africa, Surgical Workforce, Patient Care, Medical Knowledge, Practice-Based Learning and Improvement, Systems-Based Practice and Professionalism","",""
