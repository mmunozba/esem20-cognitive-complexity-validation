@article{TOREZZAN2016643,
title = "A low complexity coding and decoding strategy for the quadratic Gaussian CEO problem",
journal = "Journal of the Franklin Institute",
volume = "353",
number = "3",
pages = "643 - 656",
year = "2016",
issn = "0016-0032",
doi = "https://doi.org/10.1016/j.jfranklin.2015.12.011",
url = "http://www.sciencedirect.com/science/article/pii/S0016003215004548",
author = "Cristiano Torezzan and Luciano Panek and Marcelo Firer",
abstract = "We consider the quadratic Gaussian CEO problem, where the goal is to estimate a measure based on several Gaussian noisy observations which must be encoded and sent to a centralized receiver using limited transmission rate. For real applications, besides minimizing the average distortion, given the transmission rate, it is important to take into account memory and processing constraints. Considering these motivations, we present a low complexity coding and decoding strategy, which exploits the correlation between the measurements to reduce the number of bits to be transmitted by refining the output of the quantization stage. The CEO makes an estimate using a decoder based on a process similar to majority voting. We derive explicit expression for the CEO׳s error probability and compare numerical simulations with known achievability results and bounds."
}
@article{DICKIN2015465,
title = "Cognitive Interviewing to Enhance Comprehension and Accuracy of Responses to a Spanish-Language Nutrition Program Evaluation Tool",
journal = "Journal of Nutrition Education and Behavior",
volume = "47",
number = "5",
pages = "465 - 471.e1",
year = "2015",
issn = "1499-4046",
doi = "https://doi.org/10.1016/j.jneb.2015.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S1499404615005448",
author = "Katherine L. Dickin and Flor Larios and Pilar A. Parra",
keywords = "low-income families, Hispanic, questionnaire development, nutrition education",
abstract = "Objective
To cognitively test a Spanish translation of a questionnaire evaluating parent and child food and activity behaviors and assess accuracy of understanding and ease of answering.
Methods
Iterative rounds of cognitive interviewing, qualitative analysis, and revision were conducted with 19 low-income, native Spanish-speaking mothers of children aged 3–11 years, in 5 communities in New York. Key informant interviews were conducted with 2 Spanish-speaking nutrition educators experienced with the questionnaire.
Results
Based on responses, improvements were made to (1) ensure clear and familiar wording, (2) clarify time frames for specifying the frequency of behaviors, and (3) express constructs not amenable to direct translation or for which meanings differed by country of origin. Cognitive interviewing results also informed improvements to the English language version.
Conclusions and Implications
Even after translation by native speakers, in-depth cognitive interviewing is needed to ensure that questionnaires are understood as intended by low-literacy, immigrant populations and to facilitate collection of valid evaluation data."
}
@article{JABALLAH201834,
title = "Low complexity intra prediction mode decision for 3D-HEVC depth coding",
journal = "Signal Processing: Image Communication",
volume = "67",
pages = "34 - 47",
year = "2018",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2018.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S0923596518304703",
author = "Sami Jaballah and Mohamed-Chaker Larabi and Jamel Belhadj Tahar",
keywords = "3D high-efficiency video coding (3D-HEVC), Intra prediction, Heuristic, Wedgelet, Low complexity",
abstract = "The 3D High Efficiency Video Coding (3D-HEVC) is the latest 3D extension of the HEVC video coding standard. It supports multi-view videos plus depth (MVD), which is a sophisticated format for enhanced 3D content. Depth modeling modes (DMM) are adopted in the 3D-HEVC for better sharp edge encoding. However, employing DMM causes important increase of computational complexity. In this paper, we propose a three steps scheme for fast intra depth coding in 3D-HEVC. Based on the kirsch edge detection algorithm, a fast angular mode decision algorithm is performed. In order to avoid unnecessary evaluation of the DMMs, a new early termination approach (ET) based on the smoothness of the processed block is adopted. Besides, based on the enhanced shuffled frog leaping algorithm (E-SFLA), a new heuristic method to search the optimal wedgelet pattern for DMM1 Depth mode is introduced. The proposed algorithm is implemented on the top of four different versions of HTM. It outperforms HTM-16.0, the latest used version, and four different work from the literature, in terms of encoding time with similar coding and quality efficiency."
}
@article{GRAVINO201523,
title = "Source-code comprehension tasks supported by UML design models: Results from a controlled experiment and a differentiated replication",
journal = "Journal of Visual Languages & Computing",
volume = "28",
pages = "23 - 38",
year = "2015",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2014.12.004",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X14001591",
author = "Carmine Gravino and Giuseppe Scanniello and Genoveffa Tortora",
keywords = "Design models, Controlled experiment, Source-code comprehension",
abstract = "Objective: The main objective is to investigate whether the comprehension of object-oriented source-code increases when it is added with UML class and sequence diagrams produced in the software design phase. Methods: We conducted a controlled experiment and a differentiated replication with young software maintainers. In particular, groups of Bachelor and Master students were involved. Results: The results show that more experienced participants better comprehend source-code when added with UML design models. An average improvement (or benefit) of circa 12% was achieved when the participants accomplished the comprehension task with UML class and sequence diagrams. The results of an analysis on the time to accomplish comprehension tasks showed that less experienced participants significantly spent more time when comprehending source-code with UML design models. This kind of participants spent on average 44.8% of the time to accomplish the same task with source-code alone. Implications: It is useless to give UML design models to comprehend source-code in case maintainers are not adequately experienced with the UML. Furthermore, the less the experience of participants, the more the time to accomplish a comprehension task with UML diagram is."
}
@article{KURMYSHEV2011718,
title = "Complexity reduced coding of binary pattern units in image classification",
journal = "Optics and Lasers in Engineering",
volume = "49",
number = "6",
pages = "718 - 722",
year = "2011",
note = "Research in Optics and Photonics at CIO, Mexico",
issn = "0143-8166",
doi = "https://doi.org/10.1016/j.optlaseng.2010.12.014",
url = "http://www.sciencedirect.com/science/article/pii/S0143816610002952",
author = "E.V. Kurmyshev and J.T. Guillen-Bonilla",
keywords = "Computational complexity, Pattern recognition, Classification of images, Fast coordinated clusters representation",
abstract = "The ability to simulate and control complex physical situations in real time is an important element of many engineering and robotics applications, including pattern recognition and image classification. One of the ways to meet specific requirements of a process is a reduction of computational complexity of algorithms. In this work we propose a new coding of binary pattern units (BPU) that reduces the time and spatial complexity of algorithms of image classification significantly. We apply this coding to a particular but important case of the coordinated clusters representation (CCR) of images. This algorithm reduces the dimension of the CCR feature space and, as a consequence, the time and space complexity of the CCR based methods of image classification, exponentially. In addition, the new coding preserves all the fundamental properties of the CCR that are successfully used in the recognition, classification and segmentation of texture images. The same approach to the coding of BPUs can be used in the Local Binary Pattern (LBP) method. In order to evaluate the reduction of time and space complexity, we did an experiment on multiclass classification of images using the “traditional” and the new coding of the CCR. This test showed very effective reduction of the computing time and required computer memory with the use of the new coding of BPUs of the CCR, retaining 100% or a little less efficiency of classification at the time."
}
@article{LAW2018110,
title = "The complexity and generality of learning answer set programs",
journal = "Artificial Intelligence",
volume = "259",
pages = "110 - 146",
year = "2018",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2018.03.005",
url = "http://www.sciencedirect.com/science/article/pii/S000437021830105X",
author = "Mark Law and Alessandra Russo and Krysia Broda",
keywords = "Non-monotonic logic-based learning, Answer Set Programming, Complexity of non-monotonic learning",
abstract = "Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks."
}
@article{LEUTHOLD201157,
title = "Motor programming of finger sequences of different complexity",
journal = "Biological Psychology",
volume = "86",
number = "1",
pages = "57 - 64",
year = "2011",
issn = "0301-0511",
doi = "https://doi.org/10.1016/j.biopsycho.2010.10.007",
url = "http://www.sciencedirect.com/science/article/pii/S030105111000270X",
author = "Hartmut Leuthold and Hannes Schröter",
keywords = "Response sequence complexity, Response precuing, Motor programming, Contingent negative variation, Readiness potential",
abstract = "In a response precuing task, we used behavioral and electrophysiological measures – Contingent Negative Variation (CNV) and the readiness potential (RP) – to investigate the programming of three-element response sequences of different complexity. Precuing effects on foreperiod CNV and RT indicated the use of advance information about response hand and response sequence. Crucially, with advance information about both hand and sequence, heterogeneous response sequences (e.g., 1→2→2) elicited larger foreperiod CNV activity over medial motor areas than homogeneous response sequences (e.g., 1→2→3), whereas CNV activity over lateral motor areas was not influenced by sequence complexity. It was only before response execution that lateral but not medial RP activity was stronger for heterogeneous than homogeneous response sequences. Both behavioral and electrophysiological measures indicated finger-order dependent influences on the duration of on-line response programming during response execution."
}
@article{TARJAN20141,
title = "A readability analysis for QR code application in a traceability system",
journal = "Computers and Electronics in Agriculture",
volume = "109",
pages = "1 - 11",
year = "2014",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2014.08.015",
url = "http://www.sciencedirect.com/science/article/pii/S0168169914002142",
author = "Laslo Tarjan and Ivana Šenk and Srdjan Tegeltija and Stevan Stankovski and Gordana Ostojic",
keywords = "Traceability, QR code, Barcode, Mobile phone, Readability",
abstract = "Traceability of data through transformation stages of each individual food product, starting from raw products and to the final product, as well as printing the key data on the product package, adds to the consumers’ trust in product quality. For each food product, it is necessary to track data starting from the stage of raw products farming, through food processing, transport, warehousing, to retailing and reaching the end consumer. In order to allow insight to the key data to the user (mostly end consumer), this paper suggests recording the data on the product package in the form of a quick response two-dimensional barcode (QR code) in key points of the product’s life cycle. For efficient functioning of the proposed system, it is essential to ensure fast and reliable operation through proper placement of the QR code on the package during production, and fast and easy data reading by the product consumer. This paper presents the results of a readability analysis of QR code of variable contents, size and data error correction level, which are read by smartphones running an Android platform. The experiments were performed with various types of base material on which the code was printed. Furthermore, QR code readability analysis was conducted in the case when there is a geometric deformation of the code. Based on the detailed analysis of the collected data, it can be concluded that QR code readability is not directly influenced by the number of coded characters, or by the error correction level, but only by the size of modules that constitute the code. Furthermore, the results show that the change of the base material does not influence the read time, but influences the code readability. The paper further presents an example of the proposed traceability system, where the QR codes are used for data tracking and tracing for fruit yogurts, based on the recommendations gained through the readability analysis. This traceability system concept is universal and can be used for various products with slight modifications."
}
@article{DURAND2016R587,
title = "Programmed Cell Death and Complexity in Microbial Systems",
journal = "Current Biology",
volume = "26",
number = "13",
pages = "R587 - R593",
year = "2016",
issn = "0960-9822",
doi = "https://doi.org/10.1016/j.cub.2016.05.057",
url = "http://www.sciencedirect.com/science/article/pii/S0960982216305541",
author = "Pierre M. Durand and Stuart Sym and Richard E. Michod",
abstract = "Programmed cell death (PCD) is central to organism development and for a long time was considered a hallmark of multicellularity. Its discovery, therefore, in unicellular organisms presents compelling questions. Why did PCD evolve? What is its ecological effect on communities? To answer these questions, one is compelled to consider the impacts of PCD beyond the cell, for death obviously lowers the fitness of the cell. Here, we examine the ecological effects of PCD in different microbial scenarios and conclude that PCD can increase biological complexity. In mixed microbial communities, the mode of death affects the microenvironment, impacting the interactions between taxa. Where the population comprises groups of relatives, death has a more explicit effect. Death by lysis or other means can be harmful, while PCD can evolve by providing advantages to relatives. The synchronization of death between individuals suggests a group level property is being maintained and the mode of death also appears to have had an impact during the origin of multicellularity. PCD can result in the export of fitness from the cell to the group level via re-usable resources and PCD may also provide a mechanism for how groups beget new groups comprising kin. Furthermore, PCD is a means for solving a central problem of group living — the toxic effects of death — by making resources in dying cells beneficial to others. What emerges from the data reviewed here is that while PCD carries an obvious cost to the cell, it can be a driver of complexity in microbial communities."
}
@article{GUZZARDOTAMARGO2016138,
title = "Examining the relationship between comprehension and production processes in code-switched language",
journal = "Journal of Memory and Language",
volume = "89",
pages = "138 - 161",
year = "2016",
note = "Speaking and Listening: Relationships Between Language Production and Comprehension",
issn = "0749-596X",
doi = "https://doi.org/10.1016/j.jml.2015.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S0749596X15001461",
author = "Rosa E. Guzzardo Tamargo and Jorge R. Valdés Kroff and Paola E. Dussias",
keywords = "Bilingualism, Code-switching, Experience-based processing, Eye-tracking, Spanish–English",
abstract = "We employ code-switching (the alternation of two languages in bilingual communication) to test the hypothesis, derived from experience-based models of processing (e.g., Boland, Tanenhaus, Carlson, & Garnsey, 1989; Gennari & MacDonald, 2009), that bilinguals are sensitive to the combinatorial distributional patterns derived from production and that they use this information to guide processing during the comprehension of code-switched sentences. An analysis of spontaneous bilingual speech confirmed the existence of production asymmetries involving two auxiliary+participle phrases in Spanish–English code-switches. A subsequent eye-tracking study with two groups of bilingual code-switchers examined the consequences of the differences in distributional patterns found in the corpus study for comprehension. Participants’ comprehension costs mirrored the production patterns found in the corpus study. Findings are discussed in terms of the constraints that may be responsible for the distributional patterns in code-switching production and are situated within recent proposals of the links between production and comprehension."
}
@article{ZHOU201746,
title = "Complexity-based intra frame rate control by jointing inter-frame correlation for high efficiency video coding",
journal = "Journal of Visual Communication and Image Representation",
volume = "42",
pages = "46 - 64",
year = "2017",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2016.11.013",
url = "http://www.sciencedirect.com/science/article/pii/S1047320316302395",
author = "Mingliang Zhou and Yongfei Zhang and Bo Li and Hai-Miao Hu",
keywords = "HEVC, Complexity, Intra-frame, R-lambda model, Region-based, Quality smoothness",
abstract = "Rate control is of great significance for the High Efficiency Video Coding (HEVC). Due to the high efficiency and low complexity, the R-lambda model has been applied to the HEVC as the default rate control algorithm. However, the video content complexity, which can help improve the code efficiency and rate control performance, is not fully considered in the R-lambda model. To address this problem, an intra-frame rate control algorithm, which aims to provide improved and smooth video quality, is developed in this paper by jointly taking into consideration the frame-level content complexity between the encoded intra frames and the encoded inter frame, as well as the CTU-level complexity among different CTUs in texture–different regions for intra-frame. Firstly, in order to improve the rate control efficiency, this paper introduces a new prediction measure of content complexity for CTUs of intra-frame by jointly considering the inter-frame correlations between encoding intra frame and previous encoded inter frames as well as correlations between encoding intra frame and previous encoded intra frame. Secondly, a frame-level complexity-based bit-allocation-balancing method, by jointly considering the inter-frame correlation between intra frame and previous encoded inter frame, is brought up so that the smoothness of the visual quality can be improved between adjacent inter- and intra-frames. Thirdly, a new region-division and complexity-based CTU-level bit allocation method is developed to improve the objective quality and to reduce PSNR fluctuation among CTUs in intra-frame. Intheend, related model parameters are updated during the encoding process to increase rate control accuracy. As a result, as can be seen from the extensive experimental results that compared with the state-of-the-art schemes, the video quality can be significantly improved. More specifically, up to 10.5% and on average 5.2% BD-Rate reduction was achieved compared to HM16.0 and up to 2.7% and an average of 2.0% BD-Rate reduction was achieved compared tostate-of-the-art algorithm. Besides, a superior performance in enhancing the smoothness of quality can be achieved, which outperforms the state-of-the-art algorithms in term of flicker measurement, frame and CTU-wise PSNR, as well as buffer fullness."
}
@article{SHARMA2012716,
title = "Estimation of Software Development Effort from Requirements Based Complexity",
journal = "Procedia Technology",
volume = "4",
pages = "716 - 722",
year = "2012",
note = "2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012",
issn = "2212-0173",
doi = "https://doi.org/10.1016/j.protcy.2012.05.116",
url = "http://www.sciencedirect.com/science/article/pii/S2212017312003957",
author = "Ashish Sharma and Dharmender Singh Kushwaha",
keywords = "Improved requirement based complexity, Requirement based development effort function point, IEEE-830:1998, Requirement based development effort, Software Requirement Specification (SRS)",
abstract = "Software development effort estimation deals with predicting the effort required to develop quality software. The efficient software development requires accurate estimates, because inappropriate estimates causes’ trouble during the implementation of software processes. Hence, this paper aims to propose a measure for the estimation of software development effort (SDE) on the basis of requirement based complexity of yet to be developed software. The requirement based complexity has its basis on software requirements specification (SRS) of the proposed software, in order to carry out a systematic and accurate estimation of SDE. For validation purpose, the proposed SDE measure is also categorically compared with various other established SDE estimation practices proposed in the past like algorithmic models, function point count, use case point and lines of code (LOC). The result obtained validates the claim that the proposed SDE measure is systematic, comprehensive one and compares well with other prevalent SDE measures. Hence, it is even more useful because the complexity and SDE estimates are obtained at very early stage of software development life cycle (SDLC) as compared to other software development effort practices proposed in the past."
}
@article{HANNEMANNTAMAS20131575,
title = "Model complexity reduction of chemical reaction networks using mixed-integer quadratic programming",
journal = "Computers & Mathematics with Applications",
volume = "65",
number = "10",
pages = "1575 - 1595",
year = "2013",
note = "Grasping Complexity",
issn = "0898-1221",
doi = "https://doi.org/10.1016/j.camwa.2012.11.024",
url = "http://www.sciencedirect.com/science/article/pii/S0898122112006852",
author = "Ralf Hannemann-Tamás and Attila Gábor and Gábor Szederkényi and Katalin M. Hangos",
keywords = "Chemical reaction networks, Model reduction, Mixed-integer quadratic programming, Sensitivity analysis",
abstract = "The model complexity reduction problem of large chemical reaction networks under isobaric and isothermal conditions is considered. With a given detailed kinetic mechanism and measured data of the key species over a finite time horizon, the complexity reduction is formulated in the form of a mixed-integer quadratic optimization problem where the objective function is derived from the parametric sensitivity matrix. The proposed method sequentially eliminates reactions from the mechanism and simultaneously tunes the remaining parameters until the pre-specified tolerance limit in the species concentration space is reached. The computational efficiency and numerical stability of the optimization are improved by a pre-reduction step followed by suitable scaling and initial conditioning of the Hessian involved. The proposed complexity reduction method is illustrated using three well-known case studies taken from reaction kinetics literature."
}
@article{TOROPOV201420,
title = "Comprehension of drug toxicity: Software and databases",
journal = "Computers in Biology and Medicine",
volume = "45",
pages = "20 - 25",
year = "2014",
issn = "0010-4825",
doi = "https://doi.org/10.1016/j.compbiomed.2013.11.013",
url = "http://www.sciencedirect.com/science/article/pii/S0010482513003429",
author = "Andrey A. Toropov and Alla P. Toropova and Ivan Raska and Danuta Leszczynska and Jerzy Leszczynski",
keywords = " toxicology,  methods, QSAR, Computational toxicology, Drug toxicity",
abstract = "Quantitative structure–property/activity relationships (QSPRs/QSARs) are a tool (in silico) to rapidly predict various endpoints in general, and drug toxicity in particular. However, this dynamic evolution of experimental data (expansion of existing experimental data on drugs toxicity) leads to the problem of critical estimation of the data. The carcinogenicity, mutagenicity, liver effects and cardiac toxicity should be evaluated as the most important aspects of the drug toxicity. The toxicity is a multidimensional phenomenon. It is apparent that the main reasons for the increase in applications of in silico prediction of toxicity include the following: (i) the need to reduce animal testing; (ii) computational models provide reliable toxicity prediction; (iii) development of legislation that is related to use of new substances; (iv) filling data gaps; (v) reduction of cost and time; (vi) designing of new compounds; (vii) advancement of understanding of biology and chemistry. This mini-review provides analysis of existing databases and software which are necessary for use of robust computational assessments and robust prediction of potential drug toxicities by means of in silico methods."
}
@article{FERRER20132125,
title = "Estimating software testing complexity",
journal = "Information and Software Technology",
volume = "55",
number = "12",
pages = "2125 - 2139",
year = "2013",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2013.07.007",
url = "http://www.sciencedirect.com/science/article/pii/S0950584913001535",
author = "Javier Ferrer and Francisco Chicano and Enrique Alba",
keywords = "Evolutionary testing, Complexity, Branch coverage, Search based software engineering, Evolutionary algorithms, Testability",
abstract = "Context
Complexity measures provide us some information about software artifacts. A measure of the difficulty of testing a piece of code could be very useful to take control about the test phase.
Objective
The aim in this paper is the definition of a new measure of the difficulty for a computer to generate test cases, we call it Branch Coverage Expectation (BCE). We also analyze the most common complexity measures and the most important features of a program. With this analysis we are trying to discover whether there exists a relationship between them and the code coverage of an automatically generated test suite.
Method
The definition of this measure is based on a Markov model of the program. This model is used not only to compute the BCE, but also to provide an estimation of the number of test cases needed to reach a given coverage level in the program. In order to check our proposal, we perform a theoretical validation and we carry out an empirical validation study using 2600 test programs.
Results
The results show that the previously existing measures are not so useful to estimate the difficulty of testing a program, because they are not highly correlated with the code coverage. Our proposed measure is much more correlated with the code coverage than the existing complexity measures.
Conclusion
The high correlation of our measure with the code coverage suggests that the BCE measure is a very promising way of measuring the difficulty to automatically test a program. Our proposed measure is useful for predicting the behavior of an automatic test case generator."
}
@article{DEWI2017415,
title = "A Modification Complexity Factor in Function Points Method for Software Cost Estimation Towards Public Service Application",
journal = "Procedia Computer Science",
volume = "124",
pages = "415 - 422",
year = "2017",
note = "4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2017.12.172",
url = "http://www.sciencedirect.com/science/article/pii/S187705091732940X",
author = "Renny Sari Dewi and Apol Pribadi Subriadi and  Sholiq",
keywords = "Software Estimation, Software Size, Software Cost, Function Points, Public Service Application",
abstract = "Lately, the stages of planning software development projects have begun to consider the scientific side. As an intangible outcome, the budgeting must also be done in a transparent and accountable manner. The authors use Function Points (FP) method approach on the basis of 4 main reasons for estimating the effort and cost of software development (4 public service applications as research object). In this study, there are two core phases, first, elaborating complexity factors based on other method references (i.e Use Case Points) and mapping of non-functional requirements on Term of Reference. Furthermore, the second phase is to calculate and compare the estimated effort and cost if the original FP method before and after modified on the complexity factor. We conclude there is a difference of 7.19 percent (equivalent to IDR 13,567,631) between FP method calculations using and not using modified complexity adjustment factors."
}
@article{DUTTON20191126,
title = "Moore vs. Murphy: Tradeoffs between complexity and reliability in distributed energy system scheduling using software-as-a-service",
journal = "Applied Energy",
volume = "238",
pages = "1126 - 1137",
year = "2019",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2019.01.067",
url = "http://www.sciencedirect.com/science/article/pii/S0306261919300650",
author = "Spencer Dutton and Chris Marnay and Wei Feng and Matthew Robinson and Andrea Mammoli",
keywords = "Distributed energy resources, Control optimization, Software-as-a-service, HVAC complexity, microgrids",
abstract = "Software-based optimization of building control strategies, including scheduling, has the potential to improve the performance of existing complex heating, ventilation, and air conditioning (HVAC), storage, and other systems—especially if temporally variable energy production, such as solar thermal or photovoltaics, is included. If reductions in energy bills can be achieved using optimized control strategies that take advantage of cost-saving opportunities, such as time-of-use pricing, the additional bill savings can cover further efficiency investment costs. As computer processing becomes cheaper over time (Moore’s Law), opportunities to perform complex control optimization become more abundant, and these can be performed remotely as software-as-a-service (SaaS). However, by “perfecting” our control strategies, we run an increased risk that when something unexpected happens (Murphy’s Law), the consequences of failure are greater. This study used simulation to explore the potential benefits of HVAC schedule optimization, delivery, and implementation using a SaaS paradigm, at various levels of complexity. Implementing optimal schedules in a model of an efficient building’s HVAC system, the study predicts energy cost savings of up to 10% compared to the naïve reference control strategy. Optimizing more system control variables increases the potential energy cost savings; however, these savings could be compromised by failures in communication inherent in delivering schedules via SaaS. The additional cost of energy resulting from the risk of increased demand charges generally increased with increased communication failure to a much larger extent than the risk of increased energy use charges. This work suggests that moderate improvements in performance, achieved at low cost by simple means, may be more effective than highly optimized schemes, which are more susceptible to failure due to their dependence on complex interactions between systems."
}
@article{LISTON20151014,
title = "Evaluating a Guidance Counsellor Education Programme: The Methodological Complexities",
journal = "Procedia - Social and Behavioral Sciences",
volume = "191",
pages = "1014 - 1018",
year = "2015",
note = "The Proceedings of 6th World Conference on educational Sciences",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2015.04.650",
url = "http://www.sciencedirect.com/science/article/pii/S1877042815029171",
author = "J. Liston and T. Geary",
keywords = "Guidance Counsellor Education, Programme Evaluation, Guidance Counsellor Personal Development, The Irish Perspective.",
abstract = "The focus of this paper is on the methodological considerations of a PhD study which evaluated a guidance counsellor education programme,offered by a University in Ireland. The programme is offered as a two-year part-time course, which is delivered over four semesters and two summer schools.Good practice and areas for improvement were identified, acknowledged and affirmed throughout the study. However in doing so a number of methodological complexities were considered. The model selected for evaluating the programme negotiated the sensitive nature and the contextual dimensions of guidance counselling while also ensuring a depth in findings. Underlying the term ‘evaluation’ in the context of the study was the motivation to seek knowledge, explore and illuminate new learning in the area of guidance counsellor education. Therefore the study can be described as principally an evaluation with illuminative and exploratory characteristics.This paper explores the strengths and limitations of the methodological approach adopted."
}
@article{KANDJANI201586,
title = "Using extended Axiomatic Design theory to reduce complexities in Global Software Development projects",
journal = "Computers in Industry",
volume = "67",
pages = "86 - 96",
year = "2015",
issn = "0166-3615",
doi = "https://doi.org/10.1016/j.compind.2014.10.008",
url = "http://www.sciencedirect.com/science/article/pii/S0166361514001869",
author = "Hadi Kandjani and Madjid Tavana and Peter Bernus and Lian Wen and Amir Mohtarami",
keywords = "Global Software Development, Complexity, Extended Axiomatic Design theory, Kolmogorov complexity",
abstract = "Global Software Development (GSD) projects could be best understood as intrinsically complex adaptive living systems: they cannot purely be considered as ‘designed systems’, as deliberate design/control episodes and processes (using ‘software engineering’ models) are intermixed with emergent change episodes and processes (that may perhaps be explained by models). Therefore to understand GSD projects as complex systems we need to combine the state-of-the-art of GSD research, as addressed in the software engineering discipline, with results of other disciplines that study complexity (e.g. Enterprise Architecture, Complexity and Information Theory, Axiomatic Design theory). In this paper we study the complexity of GSD projects and propose an upper bound estimation of Kolmogorov complexity (KC) to estimate the information content (as a complexity measure) of project plans. We demonstrate using two hypothetical examples how good and bad project plans compare with respect to complexity, and propose the application of extended Axiomatic Design (AD) theory to reduce the complexity of GSD projects in the project planning stage, as well as to keep this complexity as low as possible during the project execution stage."
}
@article{COTRONEO2013163,
title = "Predicting aging-related bugs using software complexity metrics",
journal = "Performance Evaluation",
volume = "70",
number = "3",
pages = "163 - 178",
year = "2013",
note = "Special Issue on Software Aging and Rejuvenation",
issn = "0166-5316",
doi = "https://doi.org/10.1016/j.peva.2012.09.004",
url = "http://www.sciencedirect.com/science/article/pii/S0166531612000946",
author = "Domenico Cotroneo and Roberto Natella and Roberto Pietrantuono",
keywords = "Software aging, Fault prediction, Software complexity metrics, Aging-related bugs",
abstract = "Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs."
}
@article{ZARNOWIECKI201872,
title = "The Australian Nurse-Family Partnership Program for aboriginal mothers and babies: Describing client complexity and implications for program delivery",
journal = "Midwifery",
volume = "65",
pages = "72 - 81",
year = "2018",
issn = "0266-6138",
doi = "https://doi.org/10.1016/j.midw.2018.06.019",
url = "http://www.sciencedirect.com/science/article/pii/S0266613818301906",
author = "Dorota Zarnowiecki and Ha Nguyen and  Catherine Hampton and John Boffa and Leonie Segal",
keywords = "Nurse home visiting, Nurse-Family Partnership, Aboriginal health, Maternal child health service",
abstract = "Context
The Australian Nurse-Family Partnership Program is a home visiting program for Aboriginal mothers and infants (pregnancy to child's second birthday) adapted from the US Nurse Family Partnership program. It aims to improve outcomes for Australian Aboriginal mothers and babies, and disrupt intergenerational cycles of poor health and social and economic disadvantage. The aim of this study was to describe the complexity of Program clients in the Central Australian family partnership program, understand how client complexity affects program delivery and the implications for desirable program modification.
Methods
Australian Nurse-Family Partnership Program data collected using standardised data forms by nurses during pregnancy home visits (n = 276 clients from 2009 to 2015) were used to describe client complexity and adversity in relation to demographic and economic characteristics, mental health and personal safety. Semi-structured interviews with 11 Australian Nurse-Family Partnership Program staff and key stakeholders explored in more depth the nature of client adversity and how this affected Program delivery.
Findings
Most clients were described as “complicated” being exposed to extreme poverty (66% on welfare), living with insecure housing, many experiencing domestic violence (almost one third experiencing 2 + episodes of violence in 12 months). Sixty-six percent of clients had experienced four or more adversities. These adversities were found challenging for Program delivery. For example, housing conditions mean that around half of all ‘home visits’ could not be conducted in the home (held instead in staff cars or community locations) and together with exposure to violence undermined client capacity to translate program learnings into action. Crises with the basics of living regularly intruded into the delivery of program content, and low client literacy meant written hand-outs were unhelpful for many, requiring the development of pictorial-based program materials. Adversity increased the time needed to deliver program content.
Conclusions
Modifications to the Australian Nurse-Family Partnership Program model to reflect the specific complexities and adversities faced by the client populations is important for effective service delivery and to maximise the chance of meeting program goals of improving the health and well-being of Australian Aboriginal mothers and their infants."
}
@article{ZAMAN2019444,
title = "Understanding the soft side of software projects: An empirical study on the interactive effects of social skills and political skills on complexity – performance relationship",
journal = "International Journal of Project Management",
volume = "37",
number = "3",
pages = "444 - 460",
year = "2019",
issn = "0263-7863",
doi = "https://doi.org/10.1016/j.ijproman.2019.01.015",
url = "http://www.sciencedirect.com/science/article/pii/S0263786318306379",
author = "Umer Zaman and Zulaikha Jabbar and Shahid Nawaz and Mazhar Abbas",
keywords = "Project complexity, Project performance, Social skills, Political skills, Structural equation Modeling, SMART PLS",
abstract = "While prior research considers project complexity as a double-edged sword, researchers and practitioners still remain unclear whether project complexity serves as productive or counterproductive ingredient for project performance. Our research brings clarity on the dynamic nature of complexity-performance relationship by integrating social exchange theory with recent developments in project management research to develop and test a novel framework involving interactive roles of social skills and political skills in software-projects. Regardless of calls for further empirical studies, researchers have predominantly neglected the fundamental role of human efforts and human interaction in outlining performance particularly in complex projects. Drawing on a survey based sample of 242 project managers and use of variance based structural equation modeling, the findings illuminate theoretical and practical contributions in better understanding complexities in software-projects performance. In addition, prioritizing human-centric factors i.e. social skills and political skills in supporting complexity- performance relationship further enhances contributions of this research."
}
@article{KUHN2013193,
title = "Ground delay program planning: Delay, equity, and computational complexity",
journal = "Transportation Research Part C: Emerging Technologies",
volume = "35",
pages = "193 - 203",
year = "2013",
issn = "0968-090X",
doi = "https://doi.org/10.1016/j.trc.2013.07.008",
url = "http://www.sciencedirect.com/science/article/pii/S0968090X13001617",
author = "Kenneth D. Kuhn",
keywords = "Air traffic control, Ground delay program, Delay, Equity, Bi-objective optimization, Bi-criteria optimization, Multi-objective optimization, Multi-criteria optimization",
abstract = "The Federal Aviation Administration, in consultation with air carriers, manages Ground Delay Programs, delaying aircraft scheduled to land at capacity constrained airports prior to takeoff to increase the safety and efficiency of air travel. Prior research optimizes Ground Delay Program planning to minimize either delay or a weighted combination of delay and measures of inequity, a key concern in practice. Such approaches have several shortcomings including an inability to find all Pareto-optimal policies and a reliance on (one or many) models relating fundamentally incompatible objectives. This article introduces several two-phase approaches to Ground Delay Program planning that address the problems of weighted sum methods while managing computational burdens, another key concern in practice. A computational study demonstrates the benefits of the new approaches on realistic problem instances."
}
@article{HEINTZ201392,
title = "Software Engineering and complexity in effective Algebraic Geometry",
journal = "Journal of Complexity",
volume = "29",
number = "1",
pages = "92 - 138",
year = "2013",
issn = "0885-064X",
doi = "https://doi.org/10.1016/j.jco.2012.04.005",
url = "http://www.sciencedirect.com/science/article/pii/S0885064X1200043X",
author = "Joos Heintz and Bart Kuijpers and Andrés Rojas Paredes",
keywords = "Robust parameterized arithmetic circuit, Isoparametric routine, Branching parsimonious algorithm, Flat family of zero dimensional elimination problems",
abstract = "One may represent polynomials not only by their coefficients but also by arithmetic circuits which evaluate them. This idea allowed in the past fifteen years considerable complexity progress in effective polynomial equation solving. We present a circuit based computation model which captures all known symbolic elimination algorithms in effective Algebraic Geometry and exhibit a class of simple elimination problems which require exponential size circuits to be solved in this model. This implies that the known, circuit based elimination algorithms are already optimal."
}
@article{CAPELLINI20151339,
title = "Reading Comprehension Intervention Program for Teachers from 3rd Grade’ Students",
journal = "Procedia - Social and Behavioral Sciences",
volume = "174",
pages = "1339 - 1345",
year = "2015",
note = "International Conference on New Horizons in Education, INTE 2014, 25-27 June 2014, Paris, France",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2015.01.756",
url = "http://www.sciencedirect.com/science/article/pii/S1877042815008083",
author = "Simone Aparecida Capellini and Cataryne de Almeida Rodrigues Pinto and Vera Lúcia Orlandi Cunha",
keywords = "Learning. Reading, Intervention, Reading Comprehension.",
abstract = "This study aimed to verify the effectiveness of Reading comprehension intervention program for teachers from 3rd grade'students. 4 teachers of 3rd grade from public school were divided: GI - two teachers submitted to intervention program and GII - two teachers not submitted to intervention program. Reading Comprehension Assessment Protocol was applied in all students of these teachers in pre and post-intervention. The results revealed a difference between the performance of the situations of pre and post-testing for GI. The intervention program was effectiveness for teachers to teach narrative and expository texts comprehension for students."
}
@article{NOVAKOVIC2012959,
title = "The ever growing complexity of placental epigenetics – Role in adverse pregnancy outcomes and fetal programming",
journal = "Placenta",
volume = "33",
number = "12",
pages = "959 - 970",
year = "2012",
issn = "0143-4004",
doi = "https://doi.org/10.1016/j.placenta.2012.10.003",
url = "http://www.sciencedirect.com/science/article/pii/S0143400412003669",
author = "B. Novakovic and R. Saffery",
keywords = "Epigenetics, DNA methylation, Environment, Diet, Placenta, Preeclampsia, Birth-weight",
abstract = "As the primary interface between maternal and fetal circulations, the placenta is subject to a myriad of environmental exposures with the capacity to alter placental function and fetal development. Many of these effects are likely to be mediated by epigenetic (‘above DNA’) change, which is also in turn regulated by maternal and fetal genetic factors. Linking specific environmental exposures, genetic, and epigenetic variation to maternal and fetal outcomes may provide valuable mechanistic insights into the role of placental dysfunction in pregnancy-associated disease and later health. The complexities are manifold but are rapidly being overcome by technological advances and emerging analytical approaches. Although focussing on recent genome-scale and gene-specific DNA methylation studies in the human placenta, this review also discusses the potential of a future broader exploration of combined environmental, genetic and epigenomic approaches, encompassing higher order epigenetic modifications, for unravelling the molecular mechanisms underlying gene-environment interaction at the fetomaternal interface."
}
@article{MOONS2013368,
title = "The design and pilot evaluation of an interactive learning environment for introductory programming influenced by cognitive load theory and constructivism",
journal = "Computers & Education",
volume = "60",
number = "1",
pages = "368 - 384",
year = "2013",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2012.08.009",
url = "http://www.sciencedirect.com/science/article/pii/S0360131512001959",
author = "Jan Moons and Carlos De Backer",
keywords = "Architectures for educational technology system, Interactive learning environments, Programming and programming languages",
abstract = "This article presents the architecture and evaluation of a novel environment for programming education. The design of this programming environment, and the way it is used in class, is based on the findings of constructivist and cognitivist learning paradigms. The environment is evaluated based on qualitative student and teacher evaluations and experiments performed over a three year timespan. As the findings show, the students and teachers see the environment and the way it is used as an invaluable part of their education, and the experiments show that the environment can help with understanding programming concepts that most students consider very difficult."
}
@article{OUDJANI2018484,
title = "Modification of extrinsic information for parallel concatenated Gallager/Convolutional code to improve performance/complexity trade-offs",
journal = "AEU - International Journal of Electronics and Communications",
volume = "83",
pages = "484 - 491",
year = "2018",
issn = "1434-8411",
doi = "https://doi.org/10.1016/j.aeue.2017.10.033",
url = "http://www.sciencedirect.com/science/article/pii/S1434841117309883",
author = "Brahim Oudjani and Hicham Tebbikh and Noureddine Doghmane",
keywords = "Computation complexity, Convolutional code, Extrinsic information, LDPC, Parallel concatenation, Turbo code",
abstract = "To benefit the properties of both Low-Density Parity-Check (LDPC) and Turbo Convolutional Codes (TCC), we propose a practical concatenated Gallager/Convolutional code in a turbo coding way. The modified code creates a balance between the advantages and the disadvantages of LDPC and TCC in terms of the overall complexity and latency. This will be done through two different component SISO decoders; LDPC and convolutional code of the same rate 1/2 without interleaver. Since the two SISO decoders are different in nature, they exchange extrinsic information that will be easily adapted to each other. The study of computation complexity and decoding performance over an AWGN channel indicates that such approach leads to excellent performance because of several factors. The proposed approach achieves a trade-off between waterfall and error floor regions. It reduces complexity decoding compared to TCC and 3D - TCC. It provides a better coding gain over LDPC and PCGC (Parallel Concatenated Gallager Codes). These features will ensure optimal outcomes and cost-performance ratio, and thus, this trend can be the best choice for today's communication systems."
}
@article{AMANI2014223,
title = "Low Complexity Decoding of the 4×4 Perfect Space-time Block Code",
journal = "Procedia Computer Science",
volume = "32",
pages = "223 - 228",
year = "2014",
note = "The 5th International Conference on Ambient Systems, Networks and Technologies (ANT-2014), the 4th International Conference on Sustainable Energy Information Technology (SEIT-2014)",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2014.05.418",
url = "http://www.sciencedirect.com/science/article/pii/S1877050914006188",
author = "Elie Amani and Karim Djouani and Anish Kurien",
keywords = "Low complexity decoding, maximum likelihood decoding, multiple-input multiple-output (MIMO), perfect space-time block code.",
abstract = "The 4x4 perfect space-time block code (STBC) is one type in a family of perfect STBCs that have full rate, full diversity, a non- vanishing constant minimum determinant that improves spectral efficiency, uniform average transmitted energy per antenna, and good shaping. These codes suffer very high complexity maximum likelihood (ML) decoding. The exhaustive ML decoding of the 4x4 perfect STBC for instance has complexity of O(N16), with N being the size of the underlying QAM constellation. This paper suggests a fast decoding algorithm for the 4x4 perfect STBC that reduces decoding complexity from O(N16) to O(N8). The algorithm is based on conditional minimization of the ML decision metric with respect to one subset of symbols given another. This low complexity decoding approach is essentially ML because it suffers no loss in symbol error rate (SER) performance when compared to the exhaustive ML decoding."
}
@article{KANG2012569,
title = "Low-complexity video coding via power–rate–distortion optimization",
journal = "Journal of Visual Communication and Image Representation",
volume = "23",
number = "3",
pages = "569 - 585",
year = "2012",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2012.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S1047320312000326",
author = "Li-Wei Kang and Chun-Shien Lu and Chih-Yang Lin",
keywords = "Hash, Low-complexity video coding, Motion estimation, Rate–distortion, Power–rate–distortion optimization, Multiview video coding, Wireless multimedia sensor networks, Low-power and power-aware video coding",
abstract = "Wireless multimedia sensor networks (WMSNs) have been potentially applicable for several emerging applications. The resources, i.e., power and bandwidth available to visual sensors in a WMSN are, however, very limited. Hence, it is important but challenging to achieve efficient resource allocation and optimal video data compression while maximizing the overall network lifetime. In this paper, a power–rate–distortion (PRD) optimized resource-scalable low-complexity multiview video encoding scheme is proposed. In our video encoder, both the temporal and interview information can be exploited based on the comparisons of extracted media hashes without performing motion and disparity estimations, which are known to be time-consuming. We present a PRD model to characterize the relationship between the available resources and the RD performance of our encoder. More specifically, an RD function in terms of the percentages for different coding modes of blocks and the target bit rate under the available resource constraints is derived for optimal coding mode decision. The major goal here is to design a PRD model to optimize a “motion estimation-free” low-complexity video encoder for applications with resource-limited devices, instead of designing a general-purpose video codec to compete compression performance against current compression standards (e.g., H.264/AVC). Analytic results verify the accuracy of our PRD model, which can provide a theoretical guideline for performance optimization under limited resource constraints. Simulation results on joint RD performance and power consumption (measured in terms of encoding time) demonstrate the applicability of our video coding scheme for WMSNs."
}
@article{ERACAR20122697,
title = "Self-control of the time complexity of a constraint satisfaction problem solver program",
journal = "Journal of Systems and Software",
volume = "85",
number = "12",
pages = "2697 - 2706",
year = "2012",
note = "Self-Adaptive Systems",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2012.05.060",
url = "http://www.sciencedirect.com/science/article/pii/S0164121212001501",
author = "Yönet A. Eracar and Mieczyslaw M. Kokar",
keywords = "Self-controlling software, Constraint satisfaction problem, Branch and bound algorithm, PID controller, Job scheduling problem, Fixture design problem",
abstract = "This paper presents the self-controlling software paradigm and reports on its use to control the branch and bound based constraint satisfaction problem solving algorithm. In this paradigm, an algorithm is first conceptualized as a dynamical system and then a feedback control loop is added to control its behavior. The loop includes a Quality of Service component that assesses the performance of the algorithm during its run time and a controller that adjusts the parameters of the algorithm in order to achieve the control goal. Although other approaches – generally termed as “self-*” – make use of control loops, this use is limited to the structure of the software system, rather than to its behavior and its dynamics. This paper advocates the analysis of dynamics of any program with control loops. The self-controlling software paradigm is evaluated on two different NP-hard constraint satisfaction and optimization problems. The results of the evaluation show an improvement in the performance due to the added control loop for both of the tested constraint satisfaction problems."
}
@article{ZHANG201746,
title = "Co-expression analysis among microRNAs, long non-coding RNAs, and messenger RNAs to understand the pathogenesis and progression of diabetic kidney disease at the genetic level",
journal = "Methods",
volume = "124",
pages = "46 - 56",
year = "2017",
note = "Integrative Analysis of Omics Data",
issn = "1046-2023",
doi = "https://doi.org/10.1016/j.ymeth.2017.05.023",
url = "http://www.sciencedirect.com/science/article/pii/S1046202317300464",
author = "Lihua Zhang and Rong Li and Junyi He and Qiuping Yang and Yanan Wu and Jingshan Huang and Bin Wu",
keywords = "MicroRNA (miR), Long non-coding RNA (lncRNA), Diabetes, Diabetic kidney disease (DKD), Co-expression analysis, Biomarker",
abstract = "Diabetic kidney disease (DKD) is a serious disease that presents a major health problem worldwide. There is a desperate need to explore novel biomarkers to further facilitate the early diagnosis and effective treatment in DKD patients, thus preventing them from developing end-stage renal disease (ESRD). However, most regulation mechanisms at the genetic level in DKD still remain unclear. In this paper, we describe our innovative methodologies that integrate biological, computational, and statistical approaches to investigate important roles performed by regulations among microRNAs (miRs), long non-coding RNAs (lncRNAs), and messenger RNAs (mRNAs) in DKD. We conducted fully transparent, rigorously designed experiments. Our robust and reproducible results identified hsa-miR-223-3p as a candidate novel biomarker performing important roles in DKD disease process."
}
@article{CIMINI201929,
title = "Complexity and convergence certification of a block principal pivoting method for box-constrained quadratic programs",
journal = "Automatica",
volume = "100",
pages = "29 - 37",
year = "2019",
issn = "0005-1098",
doi = "https://doi.org/10.1016/j.automatica.2018.10.050",
url = "http://www.sciencedirect.com/science/article/pii/S0005109818305247",
author = "Gionata Cimini and Alberto Bemporad",
keywords = "Quadratic programming, Active-set methods, Block principal pivoting methods, Linear model predictive control, Complexity certification",
abstract = "Active-set (AS) methods for quadratic programming (QP) are particularly suitable for real-time optimization, as they provide a high-quality solution in a finite number of iterations. However, they add or remove one constraint at each iteration, which makes them inefficient when the number of constraints and variables grows. Block principal pivoting (BPP) methods perform instead multiple changes in the working set in a single iteration, resulting in a much faster execution time. The infeasible primal–dual method proposed by Kunisch and Rendl (KR) (Kunisch and Rendl, 2003) is a BPP method for box-constrained QP that is particularly attractive when reducing the time for finding an accurate solution is crucial, such as in linear model predictive control (MPC) applications. However, the method is guaranteed to converge only under very restrictive sufficient conditions, and tight bounds on the worst-case complexity are not available. For a given set of box-constrained QP’s that depend on a vector of parameters, such as those that arise in linear MPC, this paper proposes an algorithm that computes offline the exact number of iterations and flops needed by the KR method in the worst-case, and the region of the parameter space for which the method converges or is proved to cycle."
}
@article{CHEN20164758,
title = "Low complexity depth mode decision for HEVC-based 3D video coding",
journal = "Optik",
volume = "127",
number = "11",
pages = "4758 - 4767",
year = "2016",
issn = "0030-4026",
doi = "https://doi.org/10.1016/j.ijleo.2016.01.204",
url = "http://www.sciencedirect.com/science/article/pii/S003040261630047X",
author = "Ming Chen and Yongshuang Yang and Qiuwen Zhang and Xiaoxin Zhao and Xinpeng Huang and Yong Gan",
keywords = "3D-HEVC, Depth map coding, Mode decision",
abstract = "The emerging international standard high efficiency video coding (HEVC) based 3D video coding (3D-HEVC) was recently standardized by the Joint Collaborative Team on 3D video coding (JCT-3V) as an extension of HEVC for coding the multi-view video plus depth content. In test model of 3D-HEVC, new depth inter and intra modes including disparity estimation (DE), depth modeling modes (DMM) and region boundary Chain coding (RBC) are exploited for depth map coding. These new inter and intra modes achieve the highest possible coding efficiency, but it results in extremely large encoding time which obstructs 3D-HEVC from practical application. In this paper, we propose a low complexity depth mode decision algorithm for both inter and intra prediction to reduce the computational complexity of 3D-HEVC. The basic idea of the proposed algorithm is to utilize the depth map changes characteristic to predict the current depth treeblock prediction mode and early skip unnecessary inter and intra modes, such as DE, DMM and RBC. Experimental results show that the proposed low complexity algorithm can reduce the encoding time by up to 35% for 3D-HEVC mode decision, with negligible loss of rate-distortion (RD) performance."
}
@article{PADRO20131072,
title = "Finding lower bounds on the complexity of secret sharing schemes by linear programming",
journal = "Discrete Applied Mathematics",
volume = "161",
number = "7",
pages = "1072 - 1084",
year = "2013",
issn = "0166-218X",
doi = "https://doi.org/10.1016/j.dam.2012.10.020",
url = "http://www.sciencedirect.com/science/article/pii/S0166218X12004003",
author = "Carles Padró and Leonor Vázquez and An Yang",
keywords = "Secret sharing, Linear programming, Polymatroid, Non-Shannon information inequalities",
abstract = "Optimizing the maximum, or average, length of the shares in relation to the length of the secret for every given access structure is a difficult and long-standing open problem in cryptology. Most of the known lower bounds on these parameters have been obtained by implicitly or explicitly using that every secret sharing scheme defines a polymatroid related to the access structure. The best bounds that can be obtained by this combinatorial method can be determined by using linear programming, and this can be effectively done for access structures on a small number of participants. By applying this linear programming approach, we improve some of the known lower bounds for the access structures on five participants and the graph access structures on six participants for which these parameters were still undetermined. Nevertheless, the lower bounds that are obtained by this combinatorial method are not tight in general. For some access structures, they can be improved by adding to the linear program non-Shannon information inequalities as new constraints. We obtain in this way new separation results for some graph access structures on eight participants and for some ports of non-representable matroids. Finally, we prove that, for two access structures on five participants, the combinatorial lower bound cannot be attained by any linear secret sharing scheme."
}
@article{THALMANN2014172,
title = "Complexity is dead, long live complexity! How software can help service providers manage security and compliance",
journal = "Computers & Security",
volume = "45",
pages = "172 - 185",
year = "2014",
issn = "0167-4048",
doi = "https://doi.org/10.1016/j.cose.2014.05.012",
url = "http://www.sciencedirect.com/science/article/pii/S0167404814000935",
author = "Stefan Thalmann and Daniel Bachlechner and Lukas Demetz and Markus Manhart",
keywords = "Security management, Compliance, Service provision, Software architecture, Outsourcing",
abstract = "Service providers expected to see a simplification regarding security and compliance management as standards and best practice were applied to complex information technology (IT) outsourcing arrangements. However, security and compliance management became even more complex and is presenting greater challenges to service providers than ever before. In this article, we focus on the work practices of service providers dealing with complex and transitory security requirements and distributed IT infrastructures. Based on the results of semi-structured interviews followed by a think-aloud study, we first describe specific requirements to be met by software supporting security and compliance management in complex IT outsourcing arrangements, and discuss the extent to which existing software already meets them. We show that existing software, which is primarily designed for in-house settings, fails to meet requirements of complex IT outsourcing arrangements such as (1) the use of standardized and formal descriptions of security requirements and configurations, (2) the definition of a interface allowing to exchange messages and to delegate tasks, (3) the provision of mechanisms for designing and implementing a configuration for specific security requirements across organizational boundaries, (4) the provision of mechanisms for verifying and approving the enforcement of these security requirements, and (5) the provision of mechanisms for searching and browsing security requirements, configurations and links between them. We then propose a software architecture that claims to be capable of meeting those requirements and outline how this claim was evaluated by means of another think-aloud study in which potential end users were asked to perform a series of tasks using a prototypical implementation of the architecture. The results of the evaluation confirm that the software meets the described requirements and suggests that it facilitates the management of security and compliance in complex IT outsourcing arrangements."
}
@article{GUO201355,
title = "Reduce the decoding complexity: segment linear network coding",
journal = "The Journal of China Universities of Posts and Telecommunications",
volume = "20",
number = "6",
pages = "55 - 61",
year = "2013",
issn = "1005-8885",
doi = "https://doi.org/10.1016/S1005-8885(13)60109-0",
url = "http://www.sciencedirect.com/science/article/pii/S1005888513601090",
author = "Yi-jun GUO and Jian-jun HAO and Guang-xin YUE",
keywords = "SLNC, generation-based network coding, random LNC, decoding complexity, extra transmission cost",
abstract = "The throughput gain obtained by linear network coding (LNC) grows as the generation size increases, while the decoding complexity also grows exponentially. High decoding complexity makes the decoder to be the bottleneck for high speed and large data transmissions. In order to reduce the decoding complexity of network coding, a segment linear network coding (SLNC) scheme is proposed. SLNC provides a general coding structure for the generation-based network coding. By dividing a generation into several segments and restraining the coding coefficients of the symbols within the same segment, SLNC splits a high-rank matrix inversion into several low-rank matrix inversions, therefore reduces the decoding complexity dramatically. In addition, two coefficient selection strategies are proposed for both centrally controlled networks and distributed networks respectively. The theoretical analysis and simulation results prove that SLNC achieves a fairly low decoding complexity at a cost of rarely few extra transmissions."
}
@article{QIAN201756,
title = "Optimization of QR code readability in movement state using response surface methodology for implementing continuous chain traceability",
journal = "Computers and Electronics in Agriculture",
volume = "139",
pages = "56 - 64",
year = "2017",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2017.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S0168169916302095",
author = "Jianping Qian and Xiaowei Du and Baoyan Zhang and Beilei Fan and Xinting Yang",
keywords = "QR code, Traceability, Response surface methodology, Barcode readability, Optimization",
abstract = "Logistics and storage is the main processing for agro-food supply chain. Because of disconnection information between the two processing, it is difficult to trace continuously. An intelligent conveyer belt provides an effective method to associate storage and logistics by QR code scanning and information recording. Improving the QR code readability in movement state is the core of implementing continuous chain traceability with this belt. In this paper, a intelligent conveyer belt including automatic conveyer unit, barcode scanning unit, fault remove unit and control display unit was designed. Four factors affected QR readability were selected and the value range was confirmed, which was reading distance, code size, coded characters and belt moving speed. Based on the belt, an Central Composite Inscribed (CCI) experiment of four factors with five levels was designed using Response Surface Methodology (RSM) to obtain the optimal reading parameters. The result shows that the main factors of reading distance, belt moving speed and the interaction between reading distance and code size have the significant effect on QR code readability. Under the optimization condition of 141.45mm reading distance, 34.58mm code size, 100 bytes coded characters and 2.98m/min belt moving speed, the average value of QR code readability was 95%. With the optimization parameters, the intelligent conveyer belt was used in an apple marketing enterprise. The result shows that the continuous traceability between storage and logistic can be implemented with the extended breadth, deepened depth and improved precision."
}
@article{ELGEZAWI2017161,
title = "Complexity of comprehensive care treatments in undergraduate dental programs: The benefits of observing and assisting experienced faculty members",
journal = "The Saudi Dental Journal",
volume = "29",
number = "4",
pages = "161 - 166",
year = "2017",
issn = "1013-9052",
doi = "https://doi.org/10.1016/j.sdentj.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S101390521730055X",
author = "Moataz Elgezawi and Khalid Hassan and Adel Alagl and Ahmad M. Al-Thobity and Basel Al-Mutairi and Thamir Al-Houtan and Shazia Sadaf",
keywords = "Comprehensive dentistry, Case report, Occlusal and oral rehabilitation, Interdisciplinary management, Undergraduate student",
abstract = "Objective
To improve the confidence of the final year dental students in completing occlusal and oral rehabilitation of patients, with complexities beyond their scope, based on full analysis of the biomechanical and esthetic considerations of each case.
Material & methods
Two comprehensive patient situations presenting with special difficulties including extensive, reduced vertical dimension of occlusion, limited interocclusal space and maxillary alveolar bone for implant insertion necessitating bone augmentation and a sinus lift surgery was managed by two students at our institute. Procedures like surgical crown lengthening, sinus lifting, and bone augmentation were performed by senior faculty with the respective two students’ assisting as well as following up at the healing phase and reporting progress of healing and any possible complications to the supervisor. Students’ reported significant improvement in decision making skills; time management; interpersonal skills, management of cases in an evidence –based interdisciplinary approach as well as increase in their confidence in managing complex cases independently. Follow up with both cases showed optimum outcome and patients’ satisfaction.
Results
Students’ reported significant improvement in decision making skills; time management; interpersonal skills, management of cases in an evidence –based interdisciplinary approach as well as increase in their confidence in managing complex cases independently. Follow up with both cases showed optimum outcome and patients’ satisfaction.
Conclusions
Exposing students to manage complex oral rehabilitation including procedures like sinus lifting and bone augmentation, through an evidence-based interdisciplinary approach during the undergraduate comprehensive clinical dentistry course enhances their confidence and clinical acumen as an independent practitioner."
}
@article{JUN20183,
title = "A fast coding unit mode decision method based on the mode inheritance of upper coding unit for low-complexity compression of smart contents",
journal = "Displays",
volume = "55",
pages = "3 - 9",
year = "2018",
note = "Advances in Smart Content-Oriented Display Technology",
issn = "0141-9382",
doi = "https://doi.org/10.1016/j.displa.2018.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0141938218300891",
author = "Dongsan Jun",
keywords = "Video coding, HEVC, Complexity reduction",
abstract = "In combination with Internet-of-things (IoT) technology, realistic media will continue to evolve with more smart and intelligent media services via interactions between users and displays. Therefore, it is important to efficiently compress smart contents on low-power and low-complexity hand-held displays. High efficiency video coding (HEVC) is the latest video coding technology that can achieve excellent coding performance, but at the cost of tremendously increased complexity due to the use of newly adopted video coding tools. Most previous fast encoding methods were mainly investigated in terms of spatio-temporal correlation between the current coding unit (CU) and neighboring CUs. As HEVC has a recursive quad-tree structure, the correlation between different CU depths can be higher than the spatio-temporal correlation. In this study, a fast CU mode decision method is proposed for estimating the mode of a current CU to be encoded either as intra or inter mode prior to encoding a current CU. The method uses the information of the best upper CU mode, where an upper CU indicates a larger block than a current CU in the quad-tree structure. The experimental results show that the encoding process of the proposed method is faster than that of a previous method with an unnoticeable coding loss."
}
@article{OHARE201830,
title = "Protocol: A cluster randomised controlled trial of Reciprocal Reading: A teacher training comprehension programme",
journal = "International Journal of Educational Research",
volume = "92",
pages = "30 - 42",
year = "2018",
issn = "0883-0355",
doi = "https://doi.org/10.1016/j.ijer.2018.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S0883035518306530",
author = "Liam O’Hare and Patrick Stark and Sheila McConnellogue and Katrina Lloyd and Maria Cockerill and Andy Biggart",
keywords = "Reciprocal Reading, Comprehension, Primary school, Teacher training, Literacy, RCT",
abstract = "This paper presents the research protocol for a pragmatic RCT of the Reciprocal Reading programme. Reciprocal Reading (RR) is a workforce development programme that supports practicing Teachers/Teaching Assistants develop and deliver comprehension instruction in mainstream UK settings for pupils aged 8–11 years. The protocol outlines a research design that will assess whether the RR programme can improve a number of specific outcomes in intervention group pupils, in a sample of schools experiencing higher than average levels of disadvantage. The primary outcome for analysis is reading comprehension with secondary outcomes of overall literacy, reading accuracy and comprehension pre-cursors at the child and school level. The study will also include a process evaluation using qualitative data and quantitative implementation data."
}
@article{FITTKAU2017259,
title = "Software landscape and application visualization for system comprehension with ExplorViz",
journal = "Information and Software Technology",
volume = "87",
pages = "259 - 277",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916301185",
author = "Florian Fittkau and Alexander Krause and Wilhelm Hasselbring",
keywords = "Software visualization, Dynamic analysis, System comprehension",
abstract = "Context: The number of software applications deployed in organizations is constantly increasing. Those applications – often several hundreds – form large software landscapes. Objective: The comprehension of such landscapes and their applications is often impeded by, for instance, architectural erosion, personnel turnover, or changing requirements. Therefore, an efficient and effective way to comprehend such software landscapes is required. Method: In our ExplorViz visualization, we introduce hierarchical abstractions aiming at solving system comprehension tasks fast and accurately for large software landscapes. Besides hierarchical visualization on the landscape level, ExplorViz provides multi-level visualization from the landscape to the level of individual applications. The 3D application-level visualization is empirically evaluated with a comparison to the Extravis approach, with physical models and in virtual reality. To evaluate ExplorViz, we conducted four controlled experiments. We provide packages containing all our experimental data to facilitate the verifiability, reproducibility, and further extensibility of our results. Results: We observed a statistical significant increase in task correctness of the hierarchical visualization compared to the flat visualization. The time spent did not show any significant differences. For the comparison with Extravis, we observed that solving program comprehension tasks using ExplorViz leads to a significant increase in correctness and in less or similar time spent. The physical models improved the team-based program comprehension process for specific tasks by initiating gesture-based interaction, but not for all tasks. The participants of our virtual reality experiment with ExplorViz rated the realized gestures for translation, rotation, and selection as highly usable. However, our zooming gesture was less favored. Conclusion: The results backup our claim that our hierarchical and multi-level approach enhances the current state of the art in landscape and application visualization for better software system comprehension, including new forms of interaction with physical models and virtual reality."
}
@article{HENDRAWAN2015597,
title = "Visualizing Time-based Weighted Coupling Using Particle Swarm Optimization to Aid Program Comprehension",
journal = "Procedia Computer Science",
volume = "72",
pages = "597 - 604",
year = "2015",
note = "The Third Information Systems International Conference 2015",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2015.12.168",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915036297",
author = "Rully Agus Hendrawan and Katsuhisa Maruyama",
keywords = "source code visualization, program comprehension, logical coupling, particle swarm optimization, association mining",
abstract = "By knowing software coupling, developers can get better view of the software quality and improve their productivity in development and maintenance. This paper presents a method to visualize coupling network that are often very complex, using heuristic approach based on particle swarming optimization. Each node is placed randomly and assigned with initial speed. Node that are coupled together will be attracted each other and trying to get closer until they reach a particular distance. This distance is determined from the coupling value of two nodes. A closely related nodes will move closer until reaching a short distance. On each iteration, node position is dynamically updated based on attraction and repulsive force around them. Thus, gradually forming a near best solution of logical coupling graph. The coupling values are measured by mining the association rule from changes history. A software development project sometimes can be very active, updates happen within minutes. Sometimes it becomes slow with weekly or even monthly updates. Time-based weighted analysis method was used to accommodate these time sensitive situations. A co-change in a short duration will be weighted more than co-changes that happen in longer duration."
}
@article{RENIERS2014224,
title = "The Solid* toolset for software visual analytics of program structure and metrics comprehension: From research prototype to product",
journal = "Science of Computer Programming",
volume = "79",
pages = "224 - 240",
year = "2014",
note = "Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S016764231200086X",
author = "Dennie Reniers and Lucian Voinea and Ozan Ersoy and Alexandru Telea",
keywords = "Software visualization, Static analysis, Visual tool design",
abstract = "Software visual analytics (SVA) tools combine static program analysis and fact extraction with information visualization to support program comprehension. However, building efficient and effective SVA tools is highly challenging, as it involves extensive software development in program analysis, graphics, information visualization, and interaction. We present a SVA toolset for software maintenance, and detail two of its components which target software structure, metrics and code duplication. We illustrate the toolset’s usage for constructing software visualizations with examples in education, research, and industrial contexts. We discuss the design evolution from research prototypes to integrated, scalable, and easy-to-use products, and present several guidelines for the development of efficient and effective SVA solutions."
}
@article{HAINRY201878,
title = "A type-based complexity analysis of Object Oriented programs",
journal = "Information and Computation",
volume = "261",
pages = "78 - 115",
year = "2018",
note = "Developments in Implicit Computational Complexity (DICE) 2014 and 2015",
issn = "0890-5401",
doi = "https://doi.org/10.1016/j.ic.2018.05.006",
url = "http://www.sciencedirect.com/science/article/pii/S0890540118300786",
author = "Emmanuel Hainry and Romain Péchoux",
keywords = "Object Oriented Program, Type system, Complexity, Polynomial time",
abstract = "A type system is introduced for a generic Object Oriented programming language in order to infer resource upper bounds. A sound and complete characterization of the set of polynomial time computable functions is obtained. As a consequence, the heap-space and the stack-space requirements of typed programs are also bounded polynomially. This type system is inspired by previous works on Implicit Computational Complexity, using tiering and non-interference techniques. The presented methodology has several advantages. First, it provides explicit big O polynomial upper bounds to the programmer, hence its use could allow the programmer to avoid memory errors. Second, type checking is decidable in polynomial time. Last, it has a good expressivity since it analyzes most object oriented features like inheritance, overload, override and recursion. Moreover it can deal with loops guarded by objects and can also be extended to statements that alter the control flow like break or return."
}
@article{SINGH2018160,
title = "Coded color shift keying with frequency domain equalization for low complexity energy efficient indoor visible light communications",
journal = "Physical Communication",
volume = "31",
pages = "160 - 168",
year = "2018",
issn = "1874-4907",
doi = "https://doi.org/10.1016/j.phycom.2018.04.026",
url = "http://www.sciencedirect.com/science/article/pii/S1874490717306067",
author = "Ravinder Singh and Timothy O’Farrell and Mauro Biagi and John P.R. David",
keywords = "Visible light communications, Color shift keying, IEEE 802.15.7, Channel coding, Frequency domain equalization, Indoor VLC channels",
abstract = "The adaptive tri-chromatic color shift keying (CSK) modulation standardized in IEEE 802.15.7 and the advanced quad-chromatic CSK are unable to utilize their entire spectral efficiency range over dispersive indoor VLC channels and incur large power penalties without the use of channel equalization and forward error correction. This degrades the energy efficiency and limits the throughput capability of CSK schemes to approximately half of the data-rates specified in the IEEE standard for multimedia services in wireless personal area networks. To comply with the desire for low latency and minimal implementation complexity in the CSK standardization framework, we consider an industry standard binary convolutional code and frequency domain equalization (FDE) which provide one-shot data packet processing. Our results show that when operating over hybrid indoor visible light communication (VLC) links, the FDE based rate-adaptive coded modulation CSK scheme achieves a significant 11 dB SNR gain over an uncoded, unequalized system."
}
@article{CHAN20151859,
title = "Equating Visual Function Scales to Facilitate Reporting of Medicare Functional G-Code Severity/Complexity Modifiers for Low-Vision Patients",
journal = "Archives of Physical Medicine and Rehabilitation",
volume = "96",
number = "10",
pages = "1859 - 1865",
year = "2015",
issn = "0003-9993",
doi = "https://doi.org/10.1016/j.apmr.2015.06.013",
url = "http://www.sciencedirect.com/science/article/pii/S0003999315005420",
author = "Tiffany L. Chan and Monica S. Perlmutter and Melva Andrews and Janet S. Sunness and Judith E. Goldstein and Robert W. Massof",
keywords = "Medicare, Occupational therapy, Outcome assessment (health care), Rehabilitation, Vision, low",
abstract = "Objective
To present a method of estimating and equating scales across functional assessment instruments that appropriately represents changes in a patient's functional ability and can be meaningfully mapped to changes in Medicare G-code severity modifiers.
Design
Previously published measures of patients' overall visual ability, estimated from low-vision patient responses to 7 different visual function rating scale questionnaires, are equated and mapped onto Medicare G-code severity modifiers.
Setting
Outpatient low-vision rehabilitation clinics.
Participants
The analyses presented in this article were performed on raw or summarized low-vision patient ratings of visual function questionnaire (VFQ) items obtained from previously published research studies.
Interventions
Previously published visual ability measures from Rasch analysis of low-vision patient ratings of items in different VFQs (National Eye Institute Visual Functioning Questionnaire, Index of Visual Functioning, Activities of Daily Vision Scale, Visual Activities Questionnaire) were equated with the Activity Inventory (AI) scale. The 39 items in the Self-Report Assessment of Functional Visual Performance (SRAFVP) and the 48 items in the Veterans Affairs Low Vision Visual Functioning Questionnaire (VA LV VFQ) were paired with similar items in the AI in order to equate the scales.
Main Outcome Measures
Tests using different observation methods and indicators cannot be directly compared on the same scale. All test results would have to be transformed to measures of the same functional ability variable on a common scale as described here, before a single measure could be estimated from the multiple measures.
Results
Bivariate regression analysis was performed to linearly transform the SRAFVP and VA LV VFQ item measures to the AI item measure scale. The nonlinear relationship between person measures of visual ability on a logit scale and item response raw scores was approximated with a logistic function, and the 2 regression coefficients were estimated for each of the 7 VFQs. These coefficients can be used with the logistic function to estimate functional ability on the same interval scale for each VFQ and for transforming raw VFQ responses to Medicare's G-code severity modifier categories.
Conclusions
The principle of using equated interval scales allows for comparison across measurement instruments of low-vision functional status and outcomes, but can be applied to any area of rehabilitation."
}
@article{BAILLOT201656,
title = "Higher-order interpretations and program complexity",
journal = "Information and Computation",
volume = "248",
pages = "56 - 81",
year = "2016",
note = "Development on Implicit Computational Complexity (DICE 2013)",
issn = "0890-5401",
doi = "https://doi.org/10.1016/j.ic.2015.12.008",
url = "http://www.sciencedirect.com/science/article/pii/S0890540115001388",
author = "Patrick Baillot and Ugo Dal Lago",
keywords = "Implicit computational complexity, Term rewriting systems, Type systems, Lambda-calculus",
abstract = "Polynomial interpretations and their generalizations like quasi-interpretations have been used in the setting of first-order functional languages to design criteria ensuring statically some complexity bounds on programs [10]. This fits in the area of implicit computational complexity, which aims at giving machine-free characterizations of complexity classes. In this paper, we extend this approach to the higher-order setting. For that we consider simply-typed term rewriting systems [35], we define higher-order polynomial interpretations for them, and we give a criterion ensuring that a program can be executed in polynomial time. In order to obtain a criterion flexible enough to validate interesting programs using higher-order primitives, we introduce a notion of polynomial quasi-interpretations, coupled with a simple termination criterion based on linear types and path-like orders."
}
@article{THOMPSON2019100873,
title = "The added value of establishing a lexicon to help inform, compare, and better understand the implementation of policy, systems, and environmental change strategies in Supplemental Nutrition Assistance Program Education",
journal = "Preventive Medicine Reports",
volume = "14",
pages = "100873",
year = "2019",
issn = "2211-3355",
doi = "https://doi.org/10.1016/j.pmedr.2019.100873",
url = "http://www.sciencedirect.com/science/article/pii/S2211335519300555",
author = "Jack Thompson and Katherine Sutton and Tony Kuo",
keywords = "Obesity prevention, Public health, Policy, systems and environmental change, Lexicon, Program implementation, Program evaluation",
abstract = "Categorization of terms/concepts/constructs that allows for better understanding and comparison of public health interventions is often lacking in program implementation and evaluation. A classification system such as a lexicon, when used appropriately, can help address this need. The present narrative describes a lexicon of policy, systems, and environmental change strategies (PSEs) that was developed and prototyped to aid local implementation of Supplemental Nutrition Assistance Program Education (SNAP-Ed) interventions in obesity prevention. The lexicon was reviewed and refined by a panel of experts who provided iterative feedback on the system's scope and utility. To develop the lexicon, a team from the local health department: (i) conducted an inventory (community context scan) of SNAP-Ed PSEs implemented in Los Angeles County during 2010–2015; (ii) assessed commonalities among PSEs that were translated into “index factors” to contextualize terms/concepts/constructs relevant to SNAP-Ed services planning; and (iii) convened a panel of experts to review and test the classification system for quality and usability. In the latter activity, the panel reviewed the terms/concepts/constructs within the context of two geographical areas and by the selected PSEs. The final version of the lexicon organized the terms/concepts/constructs of the local SNAP-Ed PSEs into overarching categories, so they can be compared/assessed by type, content, and/or impact. The goal of the project was to create a classification system that can help facilitate meaningful communications among program implementers, evaluators, and community stakeholders. The lexicon has practical implications and potential applications for other jurisdictions interested in reducing obesity rates through SNAP-Ed PSEs."
}
@article{MOHAMED2013497,
title = "The Use of Cyclomatic Complexity Metrics in Programming Performance's Assessment",
journal = "Procedia - Social and Behavioral Sciences",
volume = "90",
pages = "497 - 503",
year = "2013",
note = "6th International Conference on University Learning and Teaching (InCULT 2012)",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2013.07.119",
url = "http://www.sciencedirect.com/science/article/pii/S1877042813020077",
author = "Noraini Mohamed and Raja Fitriyah Raja Sulaiman and Wan Rohana Wan Endut",
keywords = "Cyclomatic Complexity, programming, performance assessment",
abstract = "Programming course is usually difficult and complex as taken from a student's point of view. Complexity of a programming code may be determined by the number of linearly independent path of its source code. This paper will be discussing the consistencies of the complexity of the code supposedly produced by students based on the programming questions found in the programming assessment throughout a semester of studies. The comparisons of the level of complexity and the students’ grade for various assessments materials are done. Based on this a relationship is observed and the conclusion is drawn on whether the consistencies in producing a certain pattern of expected complexity of a programming code has an effect on students performance."
}
@article{BLONDEEL20141971,
title = "Complexity of fuzzy answer set programming under Łukasiewicz semantics",
journal = "International Journal of Approximate Reasoning",
volume = "55",
number = "9",
pages = "1971 - 2003",
year = "2014",
note = "Weighted Logics for Artificial Intelligence",
issn = "0888-613X",
doi = "https://doi.org/10.1016/j.ijar.2013.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S0888613X13002338",
author = "Marjon Blondeel and Steven Schockaert and Dirk Vermeir and Martine De Cock",
keywords = "Answer set programming, Łukasiewicz logic, Computational complexity",
abstract = "Fuzzy answer set programming (FASP) is a generalization of answer set programming (ASP) in which propositions are allowed to be graded. Little is known about the computational complexity of FASP and almost no techniques are available to compute the answer sets of a FASP program. In this paper, we analyze the computational complexity of FASP under Łukasiewicz semantics. In particular we show that the complexity of the main reasoning tasks is located at the first level of the polynomial hierarchy, even for disjunctive FASP programs for which reasoning is classically located at the second level. Moreover, we show a reduction from reasoning with such FASP programs to bilevel linear programming, thus opening the door to practical applications. For definite FASP programs we can show P-membership. Surprisingly, when allowing disjunctions to occur in the body of rules – a syntactic generalization which does not affect the expressivity of ASP in the classical case – the picture changes drastically. In particular, reasoning tasks are then located at the second level of the polynomial hierarchy, while for simple FASP programs, we can only show that the unique answer set can be found in pseudo-polynomial time. Moreover, the connection to an existing open problem about integer equations suggests that the problem of fully characterizing the complexity of FASP in this more general setting is not likely to have an easy solution."
}
@article{RANDO2012148,
title = "Combinatorial complexity in chromatin structure and function: revisiting the histone code",
journal = "Current Opinion in Genetics & Development",
volume = "22",
number = "2",
pages = "148 - 155",
year = "2012",
note = "Genome architecture and expression",
issn = "0959-437X",
doi = "https://doi.org/10.1016/j.gde.2012.02.013",
url = "http://www.sciencedirect.com/science/article/pii/S0959437X1200024X",
author = "Oliver J Rando",
abstract = "Covalent modifications of histone proteins play key roles in transcription, DNA repair, recombination, and other such processes. Over a hundred histone modifications have been described, and a popular idea in the field is that the function of a single histone mark cannot be understood without understanding its combinatorial co-occurrence with other marks, an idea generally called the ‘histone code hypothesis.’ This idea is hotly debated, with increasing biochemical evidence for chromatin regulatory factors that bind to specific histone modification combinations, but functional and localization studies finding minimal combinatorial complexity in histone modification patterns. This review will focus on these contrasting results, and will briefly touch on possible ways to reconcile these conflicting views."
}
@article{MUNOZ201524,
title = "Key Challenges in Software Application Complexity and Obsolescence Management within Aerospace Industry",
journal = "Procedia CIRP",
volume = "37",
pages = "24 - 29",
year = "2015",
note = "CIRPe 2015 - Understanding the life cycle implications of manufacturing",
issn = "2212-8271",
doi = "https://doi.org/10.1016/j.procir.2015.08.013",
url = "http://www.sciencedirect.com/science/article/pii/S2212827115008537",
author = "Raúl González Muñoz and Essam Shehab and Martin Weinitzke and Rachel Bence and Chris Fowler and Stephen Tothill and Paul Baguley",
keywords = "Software Obsolescence, System Complexity, Software Lifecycle, Aerospace",
abstract = "Software applications are becoming highly critical in aircraft development lifecycle. They provide the capability and functionalities required to design, manufacture and support aerospace assets of increasing complexity. As such criticality rises, there is a need to implement monitoring and management techniques on software application portfolios, in order to optimise service performance. These methodologies will support industry activities, avoiding operation disruption and mitigating the systemic risks that come with a higher complexity, as well as controlling obsolescence issues over time. This paper seeks to understand the key challenges faced when monitoring software lifecycle and obsolescence over large software application portfolios in aerospace industry. The paper describes results from a series of interviews and workshops with industry experts, as well as meetings with academic experts and literature reviews, to provide a discerning sight on the challenges faced when managing large application portfolios, in terms of complexity and obsolescence."
}
@article{CAI201862,
title = "A low-computation-complexity, energy-efficient, and high-performance linear program solver based on primal–dual interior point method using memristor crossbars",
journal = "Nano Communication Networks",
volume = "18",
pages = "62 - 71",
year = "2018",
issn = "1878-7789",
doi = "https://doi.org/10.1016/j.nancom.2018.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S1878778917301394",
author = "Ruizhe Cai and Ao Ren and Sucheta Soundarajan and Yanzhi Wang",
keywords = "Memristor, Memristor crossbar, Linear programming, Primal–dual interior point method",
abstract = "Linear programming is required in a wide variety of application including routing, scheduling, and various optimization problems. The primal–dual interior point (PDIP) method is state-of-the-art algorithm for solving linear programs, and can be decomposed to matrix–vector multiplication and solving systems of linear equations, both of which can be conducted by the emerging memristor crossbar technique in O(1) time complexity in the analog domain. This work is the first to apply memristor crossbar for linear program solving based on the PDIP method, which has been reformulated for memristor crossbars to compute in the analog domain. The proposed linear program solver can overcome limitations of memristor crossbars such as supporting only non-negative coefficients, and has been extended for higher scalability. The proposed solver is iterative and achieves O(N) computation complexity in each iteration. Experimental results demonstrate that reliable performance with high accuracy can be achieved under process variations."
}
@article{DOUTHWAITE201788,
title = "Towards a complexity-aware theory of change for participatory research programs working within agricultural innovation systems",
journal = "Agricultural Systems",
volume = "155",
pages = "88 - 102",
year = "2017",
issn = "0308-521X",
doi = "https://doi.org/10.1016/j.agsy.2017.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0308521X17303190",
author = "Boru Douthwaite and Elizabeth Hoffecker",
keywords = "Theory of change, Complexity, Agricultural innovation systems, Participatory action research, Empowerment, Evaluation",
abstract = "Agricultural innovation systems (AIS) are increasingly recognized as complex adaptive systems in which interventions cannot be expected to create predictable, linear impacts. Nevertheless, the logic models and theory of change (ToC) used by standard-setting international agricultural research agencies and donors assume that agricultural research will create impact through a predictable linear adoption pathway which largely ignores the complexity dynamics of AIS, and which misses important alternate pathways through which agricultural research can improve system performance and generate sustainable development impact. Despite a growing body of literature calling for more dynamic, flexible and “complexity-aware” approaches to monitoring and evaluation, few concrete examples exist of ToC that takes complexity dynamics within AIS into account, or provide guidance on how such theories could be developed. This paper addresses this gap by presenting an example of how an empirically-grounded, complexity-aware ToC can be developed and what such a model might look like in the context of a particular type of program intervention. Two detailed case studies are presented from an agricultural research program which was explicitly seeking to work in a “complexity-aware” way within aquatic agricultural systems in Zambia and the Philippines. Through an analysis of the outcomes of these interventions, the pathways through which they began to produce impacts, and the causal factors at play, we derive a “complexity-aware” ToC to model how the cases worked. This middle-range model, as well as an overarching model that we derive from it, offer an alternate narrative of how development change can be produced in agricultural systems, one which aligns with insights from complexity science and which, we argue, more closely represents the ways in which many research for development interventions work in practice. The nested ToC offers a starting point for asking a different set of evaluation and research questions which may be more relevant to participatory research efforts working from within a complexity-aware, agricultural innovation systems perspective."
}
@incollection{HANNA2017515,
title = "Chapter 7 - Designing Social Protection Programs: Using Theory and Experimentation to Understand How to Help Combat Poverty",
editor = "Abhijit Vinayak Banerjee and Esther Duflo",
series = "Handbook of Economic Field Experiments",
publisher = "North-Holland",
volume = "2",
pages = "515 - 553",
year = "2017",
booktitle = "Handbook of Economic Field Experiments",
issn = "2214-658X",
doi = "https://doi.org/10.1016/bs.hefe.2016.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S2214658X16300022",
author = "R. Hanna and D. Karlan",
keywords = "Social protection, Development, Antipoverty, O10, O12, H53",
abstract = "“Antipoverty” programs come in many varieties, ranging from multifaceted, complex programs to more simple cash transfers. Articulating and understanding the root problem motivating government and nongovernmental organization intervention are critical for choosing among many antipoverty policies or combinations thereof. Policies should differ depending on whether the underlying problem is about uninsured shocks, liquidity constraints, information failures, or some combination of all of the above. Experimental designs and thoughtful data collection can help diagnose the root problems better, thus providing better predictions for what antipoverty programs to employ in specific conditions and contexts. However, the more complex theories are likewise more challenging to test, requiring larger samples, and often more nuanced experimental designs, as well as detailed data on many aspects of household and community behavior and outcomes. We provide guidance on these design and testing issues for social protection programs, from how to target programs, to who should implement the program, and to whether and what conditions to require for program participation. In short, careful experimentation-designed testing can help provide a stronger conceptual understanding of why programs do or not work, thereby allowing one to ultimately make stronger policy prescriptions that further the goal of poverty reduction."
}
@article{HAO2012465,
title = "On the Concept of Trusted Computing and Software Watermarking: A Computational Complexity Treatise",
journal = "Physics Procedia",
volume = "25",
pages = "465 - 474",
year = "2012",
note = "International Conference on Solid State Devices and Materials Science, April 1-2, 2012, Macao",
issn = "1875-3892",
doi = "https://doi.org/10.1016/j.phpro.2012.03.112",
url = "http://www.sciencedirect.com/science/article/pii/S1875389212005287",
author = "Wu Hao and Wu Guoqing",
keywords = "Trusted computing, Trust measurement, one way function, software watermarking",
abstract = "Trusted computing is a new requirement of information security, after the successful modern cryptography. Although there are many developments of methods and techniques to address on the problems of trusted computing, the lack of rigorous theoretic treatment makes it hard to give further mathematical treatment. In this paper, we develop theory and concepts of trusted computing founded on computational complexity, which analogies to modern cryptography and find close connections between trusted computing and one way functions, one of the pillars of the modern cryptography. After that, we reveal that trusted computing has a subtle but deep relation with software watermarking, that concept can be found on the concept of trusted computing."
}
@article{HANG2017116,
title = "A low computational complexity bandwidth extension method for mobile audio coding",
journal = "Computers & Electrical Engineering",
volume = "60",
pages = "116 - 125",
year = "2017",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2017.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S0045790617312557",
author = "Bo Hang and Changqing Kang and Yi Wang",
keywords = "Mobile audio, Bandwidth extension, Computational complexity, Time-frequency transformation",
abstract = "Mobile devices always demand low computational complexity because computing resources, such as processor and battery, in mobile are always limited. The bandwidth extension (BWE) algorithm in mobile audio codec standard of China was proposed to improve audio quality in mobile communication. But the computational complexity of the algorithm is too high to implement in mobile devices. By analyzing the BWE algorithm, we discover that the main reason of high computational complexity is the frequently usage of time-frequency transformation. Then a low computational complexity scheme is proposed, which include algorithm optimization and code optimization. The experiment results show that computation time consumption ratio of BWE module in encoder and decoder are decreased by 4.5 and 14.3 percentage points respectively, without reducing the overall audio codec subjective quality, which is be conductive to the algorithm implement in mobile audio field."
}
@article{MI201860,
title = "Improving code readability classification using convolutional neural networks",
journal = "Information and Software Technology",
volume = "104",
pages = "60 - 71",
year = "2018",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.07.006",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301496",
author = "Qing Mi and Jacky Keung and Yan Xiao and Solomon Mensah and Yujin Gao",
keywords = "Code readability, Convolutional Neural Network, Deep learning, Program comprehension, Empirical software engineering, Open source software",
abstract = "Context
Code readability classification (which refers to classification of a piece of source code as either readable or unreadable) has attracted increasing concern in academia and industry. To construct accurate classification models, previous studies depended mainly upon handcrafted features. However, the manual feature engineering process is usually labor-intensive and can capture only partial information about the source code, which is likely to limit the model performance.
Objective
To improve code readability classification, we propose the use of Convolutional Neural Networks (ConvNets).
Method
We first introduce a representation strategy (with different granularities) to transform source codes into integer matrices as the input to ConvNets. We then propose DeepCRM, a deep learning-based model for code readability classification. DeepCRM consists of three separate ConvNets with identical architectures that are trained on data preprocessed in different ways. We evaluate our approach against five state-of-the-art code readability models.
Results
The experimental results show that DeepCRM can outperform previous approaches. The improvement in accuracy ranges from 2.4% to 17.2%.
Conclusions
By eliminating the need for manual feature engineering, DeepCRM provides a relatively improved performance, confirming the efficacy of deep learning techniques in the task of code readability classification."
}
@article{SMITH2011355,
title = "The Pattern Instance Notation: A simple hierarchical visual notation for the dynamic visualization and comprehension of software patterns",
journal = "Journal of Visual Languages & Computing",
volume = "22",
number = "5",
pages = "355 - 374",
year = "2011",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2011.03.003",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X11000188",
author = "Jason McC. Smith",
keywords = "Design patterns, Visualization, Education, Comprehension",
abstract = "Design patterns are a common tool for developers and architects to understand and reason about a software system. Visualization techniques for patterns tend to be either highly theoretical in nature or based on a structural view of a system's implementation. The Pattern Instance Notation is a simple notation technique for visualizing design patterns and other abstractions of software engineering. While based on a formal representation of design patterns, PIN is a tool for comprehension or reasoning which requires no formal training or study, and it is suitable for the programmer or designer without a theoretical background. PIN is hierarchical in nature and compactly encapsulates abstractions that may be spread widely across a system in a concise graphical format, while allowing for repeated unveiling of deeper layers of complexity and interaction on demand. It is designed to be used in either a dynamic visualization tool, or as a static representation for documentation and as a teaching aid."
}
@article{WEIJIA201626,
title = "Performance analysis of complexity reduction BP decoding of rateless codes by deleting low reliable symbols",
journal = "The Journal of China Universities of Posts and Telecommunications",
volume = "23",
number = "5",
pages = "26 - 31",
year = "2016",
issn = "1005-8885",
doi = "https://doi.org/10.1016/S1005-8885(16)60054-7",
url = "http://www.sciencedirect.com/science/article/pii/S1005888516600547",
author = "Lei Weijia and Chen Shengnan",
keywords = "rateless codes, symbols deletion, logarithm likelihood ratio, average mutual information, decoding complexity",
abstract = "This paper investigates the performance of the method used to reduce the decoding complexity of rateless codes through the deletion of the received symbols with low reliability. In the decoder, the received symbols whose absolute value of logarithm likelihood ratio (LLR) is lower than the threshold are removed, together with their corresponding edges, and thus not involved in the decoding process. The relationship between the deletion probability and the likelihood ratio deletion threshold is derived. The average mutual information per received symbol is analyzed in the case of deletion. The required number of symbols for the decoder to keep the same performance as regular decoding decreases since the average mutual information per symbol increases with the deletion, thus reducing the decoding complexity. This paper analyzes the reduction of decoding computations and the consequent transmission efficiency loss from the perspective of mutual information. The simulation results of decoding performance are consistent with those of the theoretical analysis, which show that the method can effectively reduce the decoding complexity at the cost of a slight loss of transmission efficiency."
}
@article{YEH2018342,
title = "Coding unit complexity-based predictions of coding unit depth and prediction unit mode for efficient HEVC-to-SHVC transcoding with quality scalability",
journal = "Journal of Visual Communication and Image Representation",
volume = "55",
pages = "342 - 351",
year = "2018",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2018.06.008",
url = "http://www.sciencedirect.com/science/article/pii/S1047320318301330",
author = "Chia-Hung Yeh and Wen-Yu Tseng and Li-Wei Kang and Cheng-Wei Lee and Kahlil Muchtar and Mei-Juan Chen",
keywords = "HEVC (high efficiency video coding), SHVC (scalability extension of HEVC), Video transcoding, Scalable video coding, Early termination, Coding unit complexity",
abstract = "To support good video quality of experiences in heterogeneous environments, transcoding an existed HEVC (high efficiency video coding) video bitstream to a SHVC (scalability extension of HEVC) bitstream with quality scalability is highly required. A straightforward way is to first fully decode the input HEVC bitstream and then fully re-encode it with the SHVC encoder, which requires a tremendous computational complexity. To solve the problem, in this paper, a coding unit complexity (CUC)-based prediction method for predictions of CU (coding unit) depth and PU (prediction unit) mode for efficient HEVC-to-SHVC transcoding with quality scalability is proposed to significantly reduce the transcoding complexity. The proposed method contains two prediction techniques, including (i) early termination and (ii) adaptive confidence interval, and predicts the CU depth and PU mode relying on the decoded information from the input HEVC bitstream. Experimental results have shown that the proposed method significantly outperforms the traditional HEVC-to-SHVC method by 74.14% on average in reductions of encoding time for SHVC enhancement layer."
}
@article{ALAOUI2018284,
title = "A low complexity soft decision decoder for linear block codes",
journal = "Procedia Computer Science",
volume = "127",
pages = "284 - 292",
year = "2018",
note = "PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2018.01.124",
url = "http://www.sciencedirect.com/science/article/pii/S1877050918301364",
author = "M.S. El Kasmi Alaoui and S. Nouh and A. Marzak",
keywords = "Error correcting codes, Soft Input Hard Output decoder, HSDec, Chase-2 algorithm",
abstract = "The Chase-2 decoding algorithm is an efficient soft input hard output decoder that uses a list of most likely error patterns. The main idea behind this decoder is to use a hard decision decoder HD in 2t times, where t is the error correcting capability of the used code. The complexity of Chase-2 decoder is then of order 2t.O(HD). The choice of the HD decoder impacts considerably the complexity. In this paper, we present the Chase-HSDec decoder where we have integrated the low complexity hard decision decoder HSDec that we have recently developed as component decoder. Given its low temporal complexity comparing to their competitors, the use of HSDec in the Chasing technique, decrease the temporal complexity. Chase-HSDec is applied to decode some BCH and QR codes; the simulation results show that the proposed solution yield to good error correcting performances. The comparisons between the proposed decoder and some competitors have shown that it exceeds them. The main advantage of this decoder is that it guarantees the same performances of Chase-Berlekamp Massey decoder with reduced temporal complexity. Contrary to the Chase-Berlekamp Massey decoder which is generally used for BCH codes; it is possible to apply Chase-HSDec on other linear codes defined with their generator matrix."
}
@article{DVORAK201291,
title = "On the Complexity and Optimization of Branching Programs for Decision Diagram Machines",
journal = "IFAC Proceedings Volumes",
volume = "45",
number = "7",
pages = "91 - 96",
year = "2012",
note = "11th IFAC,IEEE International Conference on Programmable Devices and Embedded Systems",
issn = "1474-6670",
doi = "https://doi.org/10.3182/20120523-3-CZ-3015.00020",
url = "http://www.sciencedirect.com/science/article/pii/S1474667015350692",
author = "V. Dvořák",
keywords = "Boolean functions, multi-terminal binary decision diagrams MTBDDs, branching programs, MTBDD complexity, decision diagram machines DDMs",
abstract = "Decision Diagram Machines (DDMs) are special purpose processors that evaluate decision diagrams. First, this paper derives upper bounds on the cost of multi-terminal binary decision diagrams (MTBDDs) for multiple-output logic functions. From these bounds we can estimate the size of branching programs running on various DDMs. Second, optimization of heterogeneous branching programs is undertaken that makes the area-time trade-off between the amount of memory required for a branching program and its execution time. As a case study, optimal architectures of branching programs are found for a set of benchmark tasks. Beside DDMs, the technique can also be used for micro-controllers with a support for multi-way branching running logic-intensive embedded firmware."
}
@article{TIWARI201722,
title = "Investigating comprehension and learnability aspects of use cases for software specification problems",
journal = "Information and Software Technology",
volume = "91",
pages = "22 - 43",
year = "2017",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2017.06.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584917304329",
author = "Saurabh Tiwari and Atul Gupta",
keywords = "Use cases, Use case templates, Usability aspects, , , Software specification problem, Experimental study",
abstract = "Context: Availability of multiple use case templates to document software requirements inevitably requires their characterization in terms of their relevance, usefulness, and the degree of the formality of the expressions. Objective: This paper reports two experimental studies that separately investigate two usability aspects, namely the comprehension and the learnability of use case templates for software specification problems. Method: We judged the comprehension aspect by evaluating the subjects’ understanding of the requirements, specified in eight different use case templates, and the ease with which the changes were made by them in the requirement specifications. The learnability aspect was judged by assessing the completeness, the correctness, and the redundancy of the use case specifications developed by the subjects using these eight use case templates for three software specification problems. Results: Our results suggested that the Kettenis’s use case template was found to be significantly more understandable, and the templates by Tiwari, Yue and Somé were found to be significantly more flexible to adapt to the changes. On the learnability aspect, the way we formulated it, we found different templates to be more complete (Kettenis), correct (Somé), and non-redundant (Tiwari). Conclusion: The specifications documented using a more detailed use case template with an intermediate degree of formality can be more comprehensible and flexible to adapt to the required changes to be made in the specification. A more formal template seems to enhance the learnability as well."
}
@article{VAHIDOV201750,
title = "The effects of interplay between negotiation tactics and task complexity in software agent to human negotiations",
journal = "Electronic Commerce Research and Applications",
volume = "26",
pages = "50 - 61",
year = "2017",
issn = "1567-4223",
doi = "https://doi.org/10.1016/j.elerap.2017.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S1567422317300789",
author = "Rustam Vahidov and Raafat Saade and Bo Yu",
keywords = "Concession-making, Electronic negotiations, Experimental studies, Mechanism design, Multi-issue negotiations, Negotiations, Software agents",
abstract = "Modern networked business environment enables design of flexible and effective mechanisms of exchange between economic parties. Online negotiations allow geographically and temporally separated participants to engage in exchange of offers in search for acceptable agreements. The digital medium enables development of software agents, which can assist with negotiation tasks while saving time and human effort. The current paper investigates the prospects of utilizing software agents in negotiations with the human counterparts. It presents the findings from experiment where human subjects acted as buyers negotiating with software agent sellers over a mobile phone plan. An electronic negotiation system incorporating software agents was used in the experiment. The agents employed various concession-making schedules while engaging in negotiation tasks involving one of two complexity levels. Negotiation task complexity was manipulated using different number of issues involved in the negotiations. Subjects were recruited among university students. Negotiations between the subjects and agents took place during a two-day period in an asynchronous mode through the web. The findings suggest that interaction between negotiation task complexity and negotiation tactic has significant effects on negotiation outcomes and subjective assessments by the human participants. In particular, task complexity had a higher impact on the agreement rate when agents employed a competitive tactic vs. when they used a conceding one."
}
@article{SLOWACK201094,
title = "Flexible distribution of complexity by hybrid predictive-distributed video coding",
journal = "Signal Processing: Image Communication",
volume = "25",
number = "2",
pages = "94 - 110",
year = "2010",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2009.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S0923596509001374",
author = "Jürgen Slowack and Jozef Škorupa and Stefaan Mys and Peter Lambert and Christos Grecos and Rik Van de Walle",
keywords = "Distributed video coding, Hybrid video coding, Wyner-Ziv coding",
abstract = "There is currently limited flexibility for distributing complexity in a video coding system. While rate-distortion-complexity (RDC) optimization techniques have been proposed for conventional predictive video coding with encoder-side motion estimation, they fail to offer true flexible distribution of complexity between encoder and decoder since the encoder is assumed to have always more computational resources available than the decoder. On the other hand, distributed video coding solutions with decoder-side motion estimation have been proposed, but hardly any RDC optimized systems have been developed. To offer more flexibility for video applications involving multi-tasking or battery-constrained devices, in this paper, we propose a codec combining predictive video coding concepts and techniques from distributed video coding and show the flexibility of this method in distributing complexity. We propose several modes to code frames, and provide complexity analysis illustrating encoder and decoder computational complexity for each mode. Rate distortion results for each mode indicate that the coding efficiency is similar. We describe a method to choose which mode to use for coding each inter frame, taking into account encoder and decoder complexity constraints, and illustrate how complexity is distributed more flexibly."
}
@article{STEFIK2011820,
title = "An empirical investigation into the design of auditory cues to enhance computer program comprehension",
journal = "International Journal of Human-Computer Studies",
volume = "69",
number = "12",
pages = "820 - 838",
year = "2011",
issn = "1071-5819",
doi = "https://doi.org/10.1016/j.ijhcs.2011.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S1071581911000814",
author = "Andreas Stefik and Christopher Hundhausen and Robert Patterson",
keywords = "Auditory programming, Programming, Debugging, Program comprehension",
abstract = "Decades of research have led to notable improvements in the representations used to aid human comprehension of computer programs. Much of this research has focused on visual representations, which leaves open the question of how best to design auditory representations of computer programs. While this question has particular relevance for visually impaired programmers, sighted programmers might also benefit from enhanced auditory representations of their programs. In order to investigate this question empirically, first, we introduce artifact encoding, a novel approach to rigorously measuring the comprehensibility of auditory representations of computer programs. Using this approach as a foundation, we present an experimental study that compared the comprehensibility of two alternative auditory program representations: one with lexical scoping cues that convey the nesting level of program statements, and another without such scoping cues. The results of our first experiment validate both artifact encoding and the scoping cues we used. To see whether auditory cues validated through our paradigm can aid program comprehension in a realistic task scenario, we experimentally compared programmers' ability to debug programs using three alternative environments: (1) an auditory execution environment with our empirically derived auditory cues; (2) an auditory execution environment with the current state-of-the-art auditory cues generated by a screen reader running on top of Microsoft Visual Studio; and (3) a visual version of the execution environment. The results of our second experiment showed that our comprehensible auditory cues are significantly better than the state-of-the-art, affording human performance approaching the effectiveness of visual representations within the statistical margin of error. This research contributes a novel methodology and foundational empirical data that can guide the design of effective auditory representations of computer programs."
}
@article{HUDRY201983,
title = "Unique (optimal) solutions: Complexity results for identifying and locating–dominating codes",
journal = "Theoretical Computer Science",
volume = "767",
pages = "83 - 102",
year = "2019",
issn = "0304-3975",
doi = "https://doi.org/10.1016/j.tcs.2018.09.034",
url = "http://www.sciencedirect.com/science/article/pii/S0304397518306054",
author = "Olivier Hudry and Antoine Lobstein",
keywords = "Complexity theory, Graph theory, Uniqueness of solution, Polynomial reduction, Locating–dominating codes, Identifying codes",
abstract = "We investigate the complexity of four decision problems dealing with the uniqueness of a solution in a graph: “Uniqueness of an r-Locating–Dominating Code with bounded size” (U-LDCr), “Uniqueness of an Optimal r-Locating–Dominating Code” (U-OLDCr), “Uniqueness of an r-Identifying Code with bounded size” (U-IdCr), “Uniqueness of an Optimal r-Identifying Code” (U-OIdCr), for any fixed integer r≥1. In particular, we describe a polynomial reduction from “Unique Satisfiability of a Boolean formula” (U-SAT) to U-OLDCr, and from U-SAT to U-OIdCr; for U-LDCr and U-IdCr, we can do even better and prove that their complexity is the same as that of U-SAT, up to polynomials. Consequently, all these problems are NP-hard, and U-LDCr and U-IdCr belong to the class DP."
}
@article{FABER2011278,
title = "Semantics and complexity of recursive aggregates in answer set programming",
journal = "Artificial Intelligence",
volume = "175",
number = "1",
pages = "278 - 298",
year = "2011",
note = "John McCarthy's Legacy",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2010.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S000437021000038X",
author = "Wolfgang Faber and Gerald Pfeifer and Nicola Leone",
keywords = "Nonmonotonic reasoning, Answer set programming, Aggregates, Computational complexity",
abstract = "The addition of aggregates has been one of the most relevant enhancements to the language of answer set programming (ASP). They strengthen the modelling power of ASP in terms of natural and concise problem representations. Previous semantic definitions typically agree in the case of non-recursive aggregates, but the picture is less clear for aggregates involved in recursion. Some proposals explicitly avoid recursive aggregates, most others differ, and many of them do not satisfy desirable criteria, such as minimality or coincidence with answer sets in the aggregate-free case. In this paper we define a semantics for programs with arbitrary aggregates (including monotone, antimonotone, and nonmonotone aggregates) in the full ASP language allowing also for disjunction in the head (disjunctive logic programming — DLP). This semantics is a genuine generalization of the answer set semantics for DLP, it is defined by a natural variant of the Gelfond–Lifschitz transformation, and treats aggregate and non-aggregate literals in a uniform way. This novel transformation is interesting per se also in the aggregate-free case, since it is simpler than the original transformation and does not need to differentiate between positive and negative literals. We prove that our semantics guarantees the minimality (and therefore the incomparability) of answer sets, and we demonstrate that it coincides with the standard answer set semantics on aggregate-free programs. Moreover, we carry out an in-depth study of the computational complexity of the language. The analysis pays particular attention to the impact of syntactical restrictions on programs in the form of limited use of aggregates, disjunction, and negation. While the addition of aggregates does not affect the complexity of the full DLP language, it turns out that their presence does increase the complexity of normal (i.e., non-disjunctive) ASP programs up to the second level of the polynomial hierarchy. However, we show that there are large classes of aggregates the addition of which does not cause any complexity gap even for normal programs, including the fragment allowing for arbitrary monotone, arbitrary antimonotone, and stratified (i.e., non-recursive) nonmonotone aggregates. The analysis provides some useful indications on the possibility to implement aggregates in existing reasoning engines."
}
@article{ZHANG2013313,
title = "Assisting comprehension of animation programs through interactive code visualization",
journal = "Journal of Visual Languages & Computing",
volume = "24",
number = "5",
pages = "313 - 326",
year = "2013",
issn = "1045-926X",
doi = "https://doi.org/10.1016/j.jvlc.2013.07.001",
url = "http://www.sciencedirect.com/science/article/pii/S1045926X13000402",
author = "Yan Zhang and Sheela Surisetty and Christopher Scaffidi",
keywords = "Animation programming, Visualization, Novice programmers",
abstract = "Visual languages have been widely used to help people create animation programs. However, current programming environments lack features supporting efficient code exploration and program comprehension, particularly for understanding relationships among parts of animation programs. In this paper, we present novel interactive visualizations aimed at helping people to understand animation programs. We conducted an empirical study to evaluate the impact of these visualizations on programmer comprehension of the code, showing that our approach enabled programmers to comprehend more information with less effort and in less time. This result is potentially significant because it demonstrates an approach for helping users to explore and understand animation code. We anticipate that this approach could be applied in a wide variety of animation programming tools, which could ease common animation programming tasks that require understanding code."
}
@article{ELANGO2016161,
title = "Design of complete software GPS signal simulator with low complexity and precise multipath channel model",
journal = "Journal of Electrical Systems and Information Technology",
volume = "3",
number = "2",
pages = "161 - 180",
year = "2016",
issn = "2314-7172",
doi = "https://doi.org/10.1016/j.jesit.2016.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S2314717216300320",
author = "G. Arul Elango and G.F Sudha",
keywords = "GPS signal simulator, Rayleigh fading, Non line of sight (NLOS), Sum of sinusoids (SOS)",
abstract = "The need for GPS data simulators have become important due to the tremendous growth in the design of versatile GPS receivers. Commercial hardware and software based GPS simulators are expensive and time consuming. In this work, a low cost simple novel GPS L1 signal simulator is designed for testing and evaluating the performance of software GPS receiver in a laboratory environment. A typical real time paradigm, similar to actual satellite derived GPS signal is created on a computer generated scenario. In this paper, a GPS software simulator is proposed that may offer a lot of analysis and testing flexibility to the researchers and developers as it is totally software based primarily running on a laptop/personal computer without the requirement of any hardware. The proposed GPS simulator allows provision for re-configurability and test repeatability and is developed in VC++ platform to minimize the simulation time. It also incorporates Rayleigh multipath channel fading model under non-line of sight (NLOS) conditions. In this work, to efficiently design the simulator, several Rayleigh fading models viz. Inverse Discrete Fourier Transform (IDFT), Filtering White Gaussian Noise (FWFN) and modified Sum of Sinusoidal (SOS) simulators are tested and compared in terms of accuracy of its first and second order statistical metrics, execution time and the later one is found to be as the best appropriate Rayleigh multipath model suitable for incorporating with GPS simulator. The fading model written in ‘MATLAB’ engine has been linked with software GPS simulator module enable to test GPS receiver’s functionality in different fading environments."
}
@article{FANG201634,
title = "Computational complexity allocation and control for inter-coding of high efficiency video coding with fast coding unit split decision",
journal = "Journal of Visual Communication and Image Representation",
volume = "40",
pages = "34 - 41",
year = "2016",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2016.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S1047320316300955",
author = "Jiunn-Tsair Fang and Zong-Yi Chen and Chang-Rui Lai and Pao-Chi Chang",
keywords = "High-efficiency video coding (HEVC), Complexity allocation, Complexity control, Coding unit, Motion vector",
abstract = "HEVC provides the quadtree structure of the coding unit (CU) with four coding-tree depths to facilitate high coding efficiency. However, compared with previous standards, the HEVC encoder increases computational complexity considerably, thus making it inappropriate for applications in power-constrained devices. This study therefore proposes a computational complexity allocation and control method for the low-delay P-frame configuration of the HEVC encoder. The complexity allocation includes the group of pictures (GOP) layer, the frame layer, and the CU layer in the HEVC encoder. Each layer involved uses individual method to distribute the complexity. In particular, motion vector estimation information is applied for CU complexity allocation and depth split determination. The total computational complexity can thus be reduced to 80% and 60% or even lower. Experiment results revealed that the average BD-PSNR exhibited a decrease of approximately 0.1dB and a BD-bitrate increment of 2% when the target complexity was reduced to 60%."
}
@article{WU2015367,
title = "Low-complexity code tracking loop with multipath diversity for GNSS over multipath fading channels",
journal = "Acta Astronautica",
volume = "115",
pages = "367 - 375",
year = "2015",
issn = "0094-5765",
doi = "https://doi.org/10.1016/j.actaastro.2015.05.037",
url = "http://www.sciencedirect.com/science/article/pii/S0094576515002325",
author = "Yating Wu and Y.S. Zhu and S.H. Leung and W.K. Wong and Tao Wang",
keywords = "Global navigation satellite system (GNSS), Code tracking loop, Multipath channels",
abstract = "A low-complexity noncoherent code tracking loop is proposed for global navigation satellite systems (GNSS) over multipath fading channels. Instead of trying to remove the multipath interference from the received signal or utilizing complex signal processing techniques such as maximum-likelihood estimation of the delay parameters, the proposed scheme collects useful component from the multipath signal by introducing assistant signals into the local reference signals. Cross-correlation from neighboring paths is combined constructively and forms an enhanced effective discriminator characteristic. Multipath diversity is thereby achieved with low computational complexity. The S-curve of the new loop is shown to be bias-free and odd-symmetric. Mean square tracking errors obtained by both linear and nonlinear analyses are used to assess the loop׳s tracking performance in terms of accuracy and robustness."
}
@article{BIAGI2010434,
title = "Anterior intraparietal cortex codes complexity of observed hand movements",
journal = "Brain Research Bulletin",
volume = "81",
number = "4",
pages = "434 - 440",
year = "2010",
issn = "0361-9230",
doi = "https://doi.org/10.1016/j.brainresbull.2009.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S0361923009003840",
author = "Laura Biagi and Giovanni Cioni and Leonardo Fogassi and Andrea Guzzetta and Michela Tosetti",
keywords = "Action observation, Mirror neuron, Manipulation, Parietal cortex, Hand identity, Grasping",
abstract = "Human and monkey studies clearly show that the anterior intraparietal area (AIP) is crucial for hand-related visuomotor transformations. Human AIP activates also during observation of hand actions, involving it in the mirror system. It is not known, however, whether its activation can also reflect a difference in the complexity of the observed action. In the present study we used functional magnetic resonance imaging (fMRI) to explore the activation of human area AIP during the observation of complex object-manipulation tasks (e.g. inserting a key in a lock and turning it) as compared to simple tasks (whole hand grasping of an object) executed with the left and the right hand in a first person perspective. The results show that, in general, both complex and simple tasks produced an activation of the fronto-parietal mirror system and that the activity of AIP in each hemisphere was higher during observation of the contralateral hand (hand identity effect). A Region-Of-Interest (ROI) analysis of the parietal activations responding to hand identity showed that each AIP was more active during the observation of complex with respect to simple tasks. In the right AIP this effect was stronger during observation of the contralateral hand, in the left AIP was strong during observation of both hands. This complexity-related property was not observed in the other activated areas. These findings support the concept that the observation of motor acts retrieves the internal representation of those same acts in the observer's motor system (direct-matching hypothesis based on the mirror neuron mechanism)."
}
@article{PAN201671,
title = "Low complexity Reed–Solomon-based low-density parity-check design for software defined optical transmission system based on adaptive puncturing decoding algorithm",
journal = "Optics Communications",
volume = "372",
pages = "71 - 75",
year = "2016",
issn = "0030-4018",
doi = "https://doi.org/10.1016/j.optcom.2016.03.086",
url = "http://www.sciencedirect.com/science/article/pii/S0030401816302577",
author = "Xiaolong Pan and Bo Liu and Jianglong Zheng and Qinghua Tian",
keywords = "Adaptive decoding algorithm, RS-LDPC codes, Low complexity, Optical transmission system",
abstract = "We propose and demonstrate a low complexity Reed–Solomon-based low-density parity-check (RS-LDPC) code with adaptive puncturing decoding algorithm for elastic optical transmission system. Partial received codes and the relevant column in parity-check matrix can be punctured to reduce the calculation complexity by adaptive parity-check matrix during decoding process. The results show that the complexity of the proposed decoding algorithm is reduced by 30% compared with the regular RS-LDPC system. The optimized code rate of the RS-LDPC code can be obtained after five times iteration."
}
@article{ABDULRAHMAN2014286,
title = "Learning programming via worked-examples: Relation of learning styles to cognitive load",
journal = "Computers in Human Behavior",
volume = "30",
pages = "286 - 298",
year = "2014",
issn = "0747-5632",
doi = "https://doi.org/10.1016/j.chb.2013.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S074756321300335X",
author = "Siti-Soraya Abdul-Rahman and Benedict du Boulay",
keywords = "Cognitive load, Learning styles, Worked-example strategies, Programming",
abstract = "This paper describes an experiment that compared learners with contrasting learning styles, Active vs. Reflective, using three different strategies for learning programming via worked-examples: Paired-method, Structure-emphasising, and Completion. The quality of the learners’ acquired cognitive schemata was assessed in terms of their post-test performance. The experiment investigated variations in learners’ cognitive load, taking both the learning strategies and the learners’ learning styles into account. Overall, the results of the experiment were inconsistent. In comparing the effects of the strategies during the learning phase, the study found significant differences in cognitive load. Unexpectedly, no differences were then detected either in cognitive load or in performance during the post-test (post-test). In comparing the effects of the learning styles during the learning phase and the transfer phase, medium effect sizes suggested that learning style may have had an effect on cognitive load. However, no significant difference was observed in performance during the post-test."
}
@article{STRANOVSKA2014936,
title = "Dynamics of Reading Comprehension Skills in Linguistic Intervention Programme",
journal = "Procedia - Social and Behavioral Sciences",
volume = "149",
pages = "936 - 942",
year = "2014",
note = "LUMEN 2014 - From Theory to Inquiry in Social Sciences, Iasi, Romania, 10-12 April 2014",
issn = "1877-0428",
doi = "https://doi.org/10.1016/j.sbspro.2014.08.304",
url = "http://www.sciencedirect.com/science/article/pii/S1877042814050174",
author = "Eva Stranovská and Daša Munková and Michal Munk",
keywords = "Linguistic intervention programme, intervention, reading comprehension skills, foreign language;",
abstract = "The aim of this study is an examination of dynamics of reading comprehension in a foreign language through the linguistic intervention programme. The point of that was to examine an influence of the linguistic intervention programme on reading comprehension skills. The linguistic intervention programme represents a method of active social learning, autonomous learning and a set of strategies and specific methods of foreign language learning. Two hundred and twenty one university students took part in this experiment, where we used as research methods - an observation and a test of foreign-language proficiency focusing on reading comprehension skills (English, German). The results have shown interesting findings in the direction of supporting or obstructing variable on the process of reading comprehension in a foreign language."
}
@article{BLONDINMASSE20116455,
title = "Palindromic complexity of codings of rotations",
journal = "Theoretical Computer Science",
volume = "412",
number = "46",
pages = "6455 - 6463",
year = "2011",
issn = "0304-3975",
doi = "https://doi.org/10.1016/j.tcs.2011.08.007",
url = "http://www.sciencedirect.com/science/article/pii/S0304397511006682",
author = "A. Blondin Massé and S. Brlek and S. Labbé and L. Vuillon",
keywords = "Codings of rotations, Sturmian, Rote, Return words, Full words",
abstract = "We study the palindromic complexity of infinite words obtained by coding rotations on partitions of the unit circle by inspecting the return words. The main result is that every coding of rotations on two intervals is full, that is, it realizes the maximal palindromic complexity. As a byproduct, a slight improvement about return words in codings of rotations is obtained: every factor of a coding of rotations on two intervals has at most 4 complete return words, where the bound is realized only for a finite number of factors. We also provide a combinatorial proof for the special case of complementary-symmetric Rote sequences by considering both palindromes and antipalindromes occurring in it."
}
@article{VIJAYANAGAR2014361,
title = "Low complexity distributed video coding",
journal = "Journal of Visual Communication and Image Representation",
volume = "25",
number = "2",
pages = "361 - 372",
year = "2014",
issn = "1047-3203",
doi = "https://doi.org/10.1016/j.jvcir.2013.12.006",
url = "http://www.sciencedirect.com/science/article/pii/S1047320313002204",
author = "Krishna Rao Vijayanagar and Joohee Kim and Yunsik Lee and Jong-bok Kim",
keywords = "Distributed video coding, Wyner–Ziv, GOP size control, Video surveillance, Encoder rate control, BCH code, Unidirectional, Low-complexity",
abstract = "Context
Conventional video encoding is a computationally intensive process that requires a lot of computing resources, power and memory. Such codecs cannot be deployed in remote sensors that are constrained in terms of power, memory and computational capabilities. For such applications, distributed video coding might hold the answer.
Objective
In this paper, we propose a distributed video coding (DVC) architecture that adheres to the principles of DVC by shifting the computational complexity from the encoder to the decoder and caters to low-motion scenarios like video conferencing and surveillance of hallways and buildings.
Method
The architecture presented is block-based and introduces a simple yet effective classification scheme that aims at maximizing the use of skip blocks to exploit temporal correlation between consecutive frames. In addition to the skip blocks, a dynamic GOP size control algorithm is proposed that instantaneously alters the GOP size in response to the video statistics without causing any latency and without the need to buffer additional frames at the encoder. To facilitate real-time video delivery and consumption, iterative channel codes like low density parity check codes and turbo codes are not used and in their place a Bose–Chaudhuri–Hocquenghem (BCH) code with encoder rate control is used.
Results
In spite of reducing the complexity and eliminating the feedback channel, the proposed architecture can match and even surpass the performance of current DVC systems making it a viable solution as a codec for low-motion scenarios.
Conclusion
We conclude that the proposed architecture is a suitable solution for applications that require real-time, low bit rate video transmission but have constrained resources and cannot support the complex conventional video encoding solutions.
Practical implications
The practical implications of the proposed DVC architecture include deployment in remote video sensors like hallway and building surveillance, video conferencing, video sensors that are deployed in remote regions (wildlife surveillance applications), and capsule endoscopy."
}
@article{MACKOWIAK2018206,
title = "On some end-user programming constructs and their understandability",
journal = "Journal of Systems and Software",
volume = "142",
pages = "206 - 222",
year = "2018",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2018.03.064",
url = "http://www.sciencedirect.com/science/article/pii/S0164121218300633",
author = "Michał Maćkowiak and Jerzy Nawrocki and Mirosław Ochodek",
keywords = "End-user programming, Code understanding, Spreadsheets",
abstract = "Context: End-user programming is becoming more and more important. However, existing programming paradigms and the languages based on them seem far-removed from what end-user programmers would need, especially in the area of Management Information. Objective: To evaluate the understandability of a set of programming constructs based on the spreadsheet metaphor from the point of view of end-user programmers in the context of Management Information. The examined set comprises single assignment with exemplary computations, data-driven iterations, selection by colours, and read-write heads (we refer to this as Board Programming). Method:A series of experiments was performed with students of Management Engineering, split into an experimental group and a control group. Each participant was given a piece of code expressed either with the proposed programming construct (experimental group) or its classical counterpart (control group). Their task was to predict the results. For the purpose of evaluation, the FACT indicators of understandability (First attempt failure rate, Attempt number, Cancellation ratio, prediction Time) were proposed and measured. Results:Three of the four examined features, i.e. single assignment with exemplary computations, data-driven iterations, and read-write heads, proved to increase understandability of the chosen programs with regard to three out of the four FACT indicators, and these results were statistically significant. Selection by colours was not as effective as expected: the FACT indicator values were improved by that feature, but the difference was not statistically significant. Conclusions:The described programming constructs appear to be an interesting option when designing an end-user programming language for domain experts in the field of Management Information."
}
@article{FARINA2018147,
title = "Ecoacoustic codes and ecological complexity",
journal = "Biosystems",
volume = "164",
pages = "147 - 154",
year = "2018",
note = "Code Biology",
issn = "0303-2647",
doi = "https://doi.org/10.1016/j.biosystems.2017.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0303264717302228",
author = "Almo Farina",
keywords = "Ecological complexity, Soundscape, Ecoacoustic codes, Eco-fields, Acoustic complexity index",
abstract = "Multi-layer communication and sensing network assures the exchange of relevant information between animals and their umwelten, imparting complexity to the ecological systems. Individual soniferous species, the acoustic community, and soundscape are the three main operational levels that comprise this multi-layer network. Acoustic adaptation and acoustic niche are two more important mechanisms that regulate the acoustic performances at the first level while the acoustic community model explains the complexity of the interspecific acoustic network at the second level. Acoustic habitat and ecoacoustic events are two of the most relevant mechanisms that operate at the third level. The exchange of ecoacoustic information on each of these levels is assured by ecoacoustic codes. At the level of individual sonifeorus species, a dyadic intraspecific exchange of information is established between an emitter and a receiver. Ecoacoustic codes discriminate, identify, and label specific signals that pertain to the theme, variation, motif repetition, and intensity of signals. At the acoustic community level, a voluntarily or involuntarily communication is established between networks of interspecific emitters and receivers. Ecoacoustic codes at this level transmit information (e.g., recognition of predators, location of food sources, availability and location of refuges) between one species and the acoustically interacting community and impart cohesion to interspecific assemblages. At the soundscape level, acoustic information is transferred from a mosaic of geophonies, biophonies, and technophonies to different species that discriminate meaningful ecoacoustic events and their temporal dynamics during habitat selection processes. Ecoacoustic codes at this level operate on a limited set of signals from the environmental acoustic dynamic that are heterogeneous in time and space, and these codes are interpreted differently according to the species during habitat selection and the completion of phenological cycles. The process of ecoacoustic coding can be interpreted according to the eco-field theory, which describes the procedures utilized by a receiver to intercept and optimize acoustic information. The acoustic codes may be detected and identified using mathematical models that simulate their performances. The Acoustic Complexity Indices are an appropriate tool to investigate the acoustic codes in action on all three levels. Ecoacoustic codes are powerful agencies used by sound-adapted species to cope with environmental novelties, and their efficiency may represent an important divide between whether a species perpetuates or becomes extinct."
}
@article{URQUIZAFUENTES2013178,
title = "Toward the effective use of educational program animations: The roles of student's engagement and topic complexity",
journal = "Computers & Education",
volume = "67",
pages = "178 - 192",
year = "2013",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2013.02.013",
url = "http://www.sciencedirect.com/science/article/pii/S0360131513000523",
author = "Jaime Urquiza-Fuentes and J. Ángel Velázquez-Iturbide",
keywords = "Evaluation of CAL systems, Interactive learning environments, Multimedia/hypermedia systems, Programming and programming languages, Simulations",
abstract = "Programming is one of the most complex subjects in computer science degrees. Program visualization is one of the approaches adopted to make programming concepts more accessible to students. In this work we study the educational impact of an active and highly engaging approach, namely the construction of program animations by students. We systematically compared this approach with two instructional scenarios, based on viewing animations and on the traditional instruction without systematic use of animations. A general conclusion of this work is that animations actually improve learning in terms of some educational aspects: short-term and long-term knowledge acquisition, and drop-out rates. Short-term improvements depend on the complexity level of the topic: while there is no impact for simple topics, there is a learning improvement in complex topics using the viewing and constructing approaches, and there is a learning improvement for highly complex topics using the viewing approach. In the long-term, drop-out rates were significantly decreased for students involved in the two most engaging approaches. In addition, both animation viewing and animation construction improved students' passing-rate in the term exam. Nevertheless, we were unable to prove in the long term that students involved in construction tasks yielded higher grades than those involved in viewing tasks."
}
@article{CHATTERJI2016128,
title = "Causal inferences on the effectiveness of complex social programs: Navigating assumptions, sources of complexity and evaluation design challenges",
journal = "Evaluation and Program Planning",
volume = "59",
pages = "128 - 140",
year = "2016",
issn = "0149-7189",
doi = "https://doi.org/10.1016/j.evalprogplan.2016.05.009",
url = "http://www.sciencedirect.com/science/article/pii/S0149718916301094",
author = "Madhabi Chatterji",
keywords = "Impact evaluations, Experimental designs, Mixed methods, Causal inferences, Complex social programs",
abstract = "This paper explores avenues for navigating evaluation design challenges posed by complex social programs (CSPs) and their environments when conducting studies that call for generalizable, causal inferences on the intervention’s effectiveness. A definition is provided of a CSP drawing on examples from different fields, and an evaluation case is analyzed in depth to derive seven (7) major sources of complexity that typify CSPs, threatening assumptions of textbook-recommended experimental designs for performing impact evaluations. Theoretically-supported, alternative methodological strategies are discussed to navigate assumptions and counter the design challenges posed by the complex configurations and ecology of CSPs. Specific recommendations include: sequential refinement of the evaluation design through systems thinking, systems-informed logic modeling; and use of extended term, mixed methods (ETMM) approaches with exploratory and confirmatory phases of the evaluation. In the proposed approach, logic models are refined through direct induction and interactions with stakeholders. To better guide assumption evaluation, question-framing, and selection of appropriate methodological strategies, a multiphase evaluation design is recommended."
}
@article{YUSHCHENKO2017538,
title = "Cost-effectiveness of energy efficiency programs: How to better understand and improve from multiple stakeholder perspectives?",
journal = "Energy Policy",
volume = "108",
pages = "538 - 550",
year = "2017",
issn = "0301-4215",
doi = "https://doi.org/10.1016/j.enpol.2017.06.015",
url = "http://www.sciencedirect.com/science/article/pii/S0301421517303695",
author = "Alisa Yushchenko and Martin Kumar Patel",
keywords = "Energy efficiency programs, Cost-effectiveness, Stakeholders, GDP, Employment",
abstract = "Cost-effectiveness analysis is one of the core elements of energy efficiency program evaluation, having important impact on energy policy, program design and budget allocation. In this context two questions arise. How to evaluate cost-effectiveness of energy efficiency programs in a more comprehensive manner? And how to improve the programs’ cost-effectiveness? We evaluate the cost-effectiveness of energy efficiency programs in Switzerland, based on three electricity-saving programs in Geneva. We review the existing practices of cost-effectiveness analysis and propose a somewhat modified methodology that allows considering perspectives of the following stakeholders: program participants, energy consumers, program administrator, utility, geographic jurisdiction territory and society as a whole. We also analyze the cost breakdown of energy efficiency programs and its evolution over time. We find that energy efficiency programs can contribute to energy efficiency target achievement while having positive economic and social impacts, including increased GDP and employment. However, energy efficiency programs can potentially lead to increased energy tariffs and higher costs for utilities. As policy recommendations for increasing cost-effectiveness of energy efficiency programs we propose a larger scale of energy efficiency programs, focus on education and training as well as on the development of long-term relationships with program participants and contractors."
}
@article{HEERING201582,
title = "Generative software complexity and software understanding",
journal = "Science of Computer Programming",
volume = "97",
pages = "82 - 85",
year = "2015",
note = "Special Issue on New Ideas and Emerging Results in Understanding Software",
issn = "0167-6423",
doi = "https://doi.org/10.1016/j.scico.2013.11.017",
url = "http://www.sciencedirect.com/science/article/pii/S0167642313003018",
author = "Jan Heering",
keywords = "Structural complexity, Software generation, Software understanding, Kolmogorov complexity",
abstract = "Taking generative software development as our point of departure, we introduce generative software complexity as a measure for quantifying the structural complexity of software. After explaining that it is the same as Kolmogorov complexity, we discuss its merits from the viewpoint of software understanding. The results obtained are in many ways unsatisfactory, but sufficiently intriguing to warrant further work."
}
@article{MOSHTARI20138,
title = "Using complexity metrics to improve software security",
journal = "Computer Fraud & Security",
volume = "2013",
number = "5",
pages = "8 - 17",
year = "2013",
issn = "1361-3723",
doi = "https://doi.org/10.1016/S1361-3723(13)70045-9",
url = "http://www.sciencedirect.com/science/article/pii/S1361372313700459",
author = "Sara Moshtari and Ashkan Sami and Mahdi Azimi",
abstract = "Information technology is quickly spreading across critical infrastructures and software has become an inevitable part of industries and organisations. At the same time, many cyberthreats are the result of poor software coding. Stuxnet, which was the most powerful cyber-weapon used against industrial control systems, exploited zero-day vulnerabilities in Microsoft Windows.1 The US Department of Homeland Security (DHS) also announced that software vulnerabilities are among the three most common cyber-security vulnerabilities in Industrial Control Systems (ICSs).2 Therefore, improving software security has an important role in increasing the security level of computer-based systems. Software vulnerability prediction is a tedious task, so automating vulnerability prediction would save a lot of time and resources. One recently used methodology in vulnerability prediction is based on automatic fault prediction using software metrics. Here, Sara Moshtari, Ashkan Sami and Mahdi Azimi of Shiraz University, Iran build on previous studies by providing more complete vulnerability information. They show what can be achieved using different classification techniques and more complete vulnerability information."
}
@article{FEREE201541,
title = "Characterizing polynomial time complexity of stream programs using interpretations",
journal = "Theoretical Computer Science",
volume = "585",
pages = "41 - 54",
year = "2015",
note = "Developments in Implicit Complexity",
issn = "0304-3975",
doi = "https://doi.org/10.1016/j.tcs.2015.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S0304397515002017",
author = "Hugo Férée and Emmanuel Hainry and Mathieu Hoyrup and Romain Péchoux",
keywords = "Stream programs, Type-2 functionals, Interpretations, Polynomial time, Basic feasible functionals, Computable analysis, Rewriting",
abstract = "This paper provides a criterion based on interpretation methods on term rewrite systems in order to characterize the polynomial time complexity of second order functionals. For that purpose it introduces a first order functional stream language that allows the programmer to implement second order functionals. This characterization is extended through the use of exp-poly interpretations as an attempt to capture the class of Basic Feasible Functionals (bff). Moreover, these results are adapted to provide a new characterization of polynomial time complexity in computable analysis. These characterizations give a new insight on the relations between the complexity of functional stream programs and the classes of functions computed by Oracle Turing Machine, where oracles are treated as inputs."
}
@article{LEWIS2015155,
title = "A predictive coding framework for rapid neural dynamics during sentence-level language comprehension",
journal = "Cortex",
volume = "68",
pages = "155 - 168",
year = "2015",
note = "Special issue: Prediction in speech and language processing",
issn = "0010-9452",
doi = "https://doi.org/10.1016/j.cortex.2015.02.014",
url = "http://www.sciencedirect.com/science/article/pii/S0010945215000714",
author = "Ashley G. Lewis and Marcel Bastiaansen",
keywords = "Language comprehension, Neural oscillations, Beta, Gamma, Predictive coding",
abstract = "There is a growing literature investigating the relationship between oscillatory neural dynamics measured using electroencephalography (EEG) and/or magnetoencephalography (MEG), and sentence-level language comprehension. Recent proposals have suggested a strong link between predictive coding accounts of the hierarchical flow of information in the brain, and oscillatory neural dynamics in the beta and gamma frequency ranges. We propose that findings relating beta and gamma oscillations to sentence-level language comprehension might be unified under such a predictive coding account. Our suggestion is that oscillatory activity in the beta frequency range may reflect both the active maintenance of the current network configuration responsible for representing the sentence-level meaning under construction, and the top-down propagation of predictions to hierarchically lower processing levels based on that representation. In addition, we suggest that oscillatory activity in the low and middle gamma range reflect the matching of top-down predictions with bottom-up linguistic input, while evoked high gamma might reflect the propagation of bottom-up prediction errors to higher levels of the processing hierarchy. We also discuss some of the implications of this predictive coding framework, and we outline ideas for how these might be tested experimentally."
}
@article{REIN201558,
title = "Evaluation of the wavelet image two-line coder: A low complexity scheme for image compression",
journal = "Signal Processing: Image Communication",
volume = "37",
pages = "58 - 74",
year = "2015",
issn = "0923-5965",
doi = "https://doi.org/10.1016/j.image.2015.07.010",
url = "http://www.sciencedirect.com/science/article/pii/S0923596515001150",
author = "Stephan A. Rein and Frank H.P. Fitzek and Clemens Gühmann and Thomas Sikora",
keywords = "Wavelet image two-line coder (Wi2l), Set-partitioning in hierarchical trees (SPIHT), Backward coding of wavelet trees (BCWT), Fractional wavelet filter, Fixed-point wavelet transform, Camera sensor node",
abstract = "This paper introduces the wavelet image two-line (Wi2l) coding algorithm for low complexity compression of images. The algorithm recursively encodes an image backwards reading only two lines of a wavelet subband, which are read in blocks of 512 bytes from flash memory. It thus only requires very little memory, i.e., a memory array for two wavelet subband lines, an array to store intermediate tree level data, and an array for writing binary data. A picture of 256×256 pixels would require 1152 bytes of memory. Computation time for the coding is derived analytically and measured on a real system. The times on a low-cost microcontroller for 256×256 grayscale pictures are measured as 0.25–0.6s for encoding and 0.22–0.77s for decoding. The algorithm can thus realize a low complexity system for compression of images when combined with a customized scheme for the wavelet transform; low complexity here refers to low memory, minimum write access to flash memory, usage of integer operations only, and low conceptual complexity (ease of implementation). As demonstrated in this paper, a compression performance similar to JPEG 2000 and the more recent Google WebP picture compression is achieved. The compression system uses flash memory (SD or MMC card) and a small camera sensor thus building an image communication system. It is also suitable for mobile devices or satellite communication. The underlying C source code is made publicly available."
}
@article{BUJAKI201928,
title = "Utilizing professional accounting concepts to understand and respond to academic dishonesty in accounting programs",
journal = "Journal of Accounting Education",
volume = "47",
pages = "28 - 47",
year = "2019",
issn = "0748-5751",
doi = "https://doi.org/10.1016/j.jaccedu.2019.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S0748575118301350",
author = "Merridee Bujaki and Camillo Lento and Naqi Sayed",
keywords = "Fraud triangle, Risk management techniques, Academic dishonesty, Controls for academic dishonesty",
abstract = "We apply professional accounting concepts to academic fraud in accounting education. First, we use the fraud triangle to understand professors’ perceptions of academic dishonesty and find two components to each fraud triangle corner. Specifically, the attitude and pressure corners have elements of faculty and student agency, while the opportunity corner is within a professor’s control. Second, risk mapping reveals plagiarism and exam cheating as more impactful than assessment protocols. We also find that faculty efforts to control academic dishonesty are mostly well-directed; however, there are opportunities to employ both preventive and detective controls more frequently."
}
@article{LIU2015165,
title = "A low complexity detection/decoding algorithm for NB-LDPC coded PRCPM system",
journal = "Physical Communication",
volume = "17",
pages = "165 - 171",
year = "2015",
issn = "1874-4907",
doi = "https://doi.org/10.1016/j.phycom.2015.08.011",
url = "http://www.sciencedirect.com/science/article/pii/S1874490715000464",
author = "Xiying Liu and Shancheng Zhao and Xiao Ma",
keywords = "BCJR algorithm, Iterative decoding, NB-LDPC codes, PRCPM, -EMS algorithms",
abstract = "This paper studies the combination of non-binary low-density parity-check (NB-LDPC) codes and M-ary partial response continuous phase modulation (PRCPM). A low-complexity joint detection/decoding algorithm is proposed, which is referred to as the Max-Log-MAP/X-EMS algorithm. In this joint algorithm, the CPM detector is implemented by the Max-Log-MAP algorithm while the LDPC decoder is implemented by the extended min-sum (EMS) algorithms. Three kinds of EMS algorithms, including D-EMS, T-EMS, and M-EMS algorithms, are compared, which are referred to as X-EMS algorithm for convenience. Simulation results show that the Max-Log-MAP/X-EMS algorithm performs as well as the traditional iterative detection/decoding algorithm based on the BCJR algorithm and the q-ary sum–product algorithm, but with lower complexity. In addition, comparison of the proposed NB-LDPC coded PRCPM system with the eBCH coded PRCPM system is given, which shows the performance advantages of our system."
}
@article{DURISIC20131275,
title = "Measuring the impact of changes to the complexity and coupling properties of automotive software systems",
journal = "Journal of Systems and Software",
volume = "86",
number = "5",
pages = "1275 - 1293",
year = "2013",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2012.12.021",
url = "http://www.sciencedirect.com/science/article/pii/S0164121212003330",
author = "Darko Durisic and Martin Nilsson and Miroslaw Staron and Jörgen Hansson",
keywords = "Automotive software system, Quality metric, Architectural change, Maintainability, Complexity, Coupling",
abstract = "Background
In the past few decades exponential increase in the amount of software used in cars has been recorded together with enhanced requirements for functional safety of their embedded software. As the evolution of software systems in cars often entails changes to software architecture, it is important to be able to monitor their impact.
Method
We conducted a case study on a distributed software system in cars at Volvo Car Corporation with the goal to develop, apply and evaluate measures of complexity and coupling which could support software architects in monitoring changes.
Results
The results showed that two metrics – structural complexity and coupling measures – can guide architectural work and turn attention of architects to most complex subsystems. The results were confirmed by monitoring a complete electrical system of a vehicle under two releases.
Conclusion
By applying the metrics after each significant change in the architecture, it is possible to verify that certain quality attributes have not deteriorated and to identify new testing areas. Using these metrics increases the product quality with respect to stability, reliability, and maintainability and also has potential to reduce long-term software development/maintenance costs."
}
@article{PONCE20121170,
title = "Instructional effectiveness of a computer-supported program for teaching reading comprehension strategies",
journal = "Computers & Education",
volume = "59",
number = "4",
pages = "1170 - 1183",
year = "2012",
issn = "0360-1315",
doi = "https://doi.org/10.1016/j.compedu.2012.05.013",
url = "http://www.sciencedirect.com/science/article/pii/S0360131512001340",
author = "Héctor R. Ponce and Mario J. López and Richard E. Mayer",
keywords = "Learning strategies, Subject areas, Elementary education, Interactive learning environments, Classroom teaching, Reading comprehension",
abstract = "This article examines the effectiveness of a computer-based instructional program (e-PELS) aimed at direct instruction in a collection of reading comprehension strategies. In e-PELS, students learn to highlight and outline expository passages based on various types of text structures (such as comparison or cause-and-effect) as well as to paraphrase, self-question, and summarize. The study involved 1041 fourth-grade elementary students from 21 schools distributed in three regions in central Chile. Participant teachers integrated this program into the Spanish language curriculum, instructing their students during thirty sessions of 90 min each during one school semester. Pretest-to-posttest gains in reading comprehension scores were significantly greater for students instructed with this program than for students who received traditional instruction (d = .5), with particularly strong effects for lower-achieving students (d = .7). The findings support the efficacy of direct instruction in specific learning strategies in a computer-based environment."
}
@article{DASILVA2015527,
title = "Using a multi-method approach to understand Agile software product lines",
journal = "Information and Software Technology",
volume = "57",
pages = "527 - 542",
year = "2015",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2014.06.004",
url = "http://www.sciencedirect.com/science/article/pii/S0950584914001438",
author = "Ivonei Freitas da Silva and Paulo Anselmo da Mota Silveira Neto and Pádraig O’Leary and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira",
keywords = "Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion",
abstract = "Context
Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
Objective
This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
Method
Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
Results
This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
Conclusion
Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies."
}
@article{GU20152,
title = "Low complexity Bi-Partition mode selection for 3D video depth intra coding",
journal = "Displays",
volume = "40",
pages = "2 - 8",
year = "2015",
note = "Next Generation TV Systems and Technologies",
issn = "0141-9382",
doi = "https://doi.org/10.1016/j.displa.2015.05.007",
url = "http://www.sciencedirect.com/science/article/pii/S0141938215000931",
author = "Zhouye Gu and Jianhua Zheng and Nam Ling and Philipp Zhang",
keywords = "Video coding, 3D-HEVC, Bi-Partition mode, Depth map, Intra coding, Low complexity",
abstract = "This paper proposes a fast mode decision algorithm for 3D High Efficiency Video Coding (3D-HEVC) depth intra coding. In the current 3D-HEVC design, it is observed that for most of the cases, full rate-distortion (RD) cost search of Bi-Partition mode could be skipped since most coding units (CUs) of depth map are very flat or smooth while Bi-Partition modes are designed for CUs with edge or sharp transition. Using the rough RD cost value calculated by HEVC Rough Mode Decision as a selection threshold, we propose a fast Bi-Partition modes selection algorithm to speed up the encoding process. The test result for the proposed fast algorithm reports a 34.4% encoding time saving with a 0.3% bitrate increase on synthesized views for the All-Intra test case and negligible impact under the random access test case. Moreover, by simply varying the selection threshold, we can make a tradeoff between encoding time saving and bitrate loss based on the requirement of different applications."
}
@article{KAHLER20161595,
title = "Using topic coding to understand the nature of change language in a motivational intervention to reduce alcohol and sex risk behaviors in emergency department patients",
journal = "Patient Education and Counseling",
volume = "99",
number = "10",
pages = "1595 - 1602",
year = "2016",
issn = "0738-3991",
doi = "https://doi.org/10.1016/j.pec.2016.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S0738399116301902",
author = "Christopher W. Kahler and Amy J. Caswell and M. Barton Laws and Justin Walthers and Molly Magill and Nadine R. Mastroleo and Chanelle J. Howe and Timothy Souza and Ira Wilson and Kendall Bryant and Peter M. Monti",
keywords = "Alcohol, Sex risk, HIV, Motivational interviewing, Dialogue coding",
abstract = "Objective
To elucidate patient language that supports changing a health behavior (change talk) or sustaining the behavior (sustain talk).
Methods
We developed a novel coding system to characterize topics of patient speech in a motivational intervention targeting alcohol and HIV/sexual risk in 90 Emergency Department patients. We further coded patient language as change or sustain talk.
Results
For both alcohol and sex, discussions focusing on benefits of behavior change or change planning were most likely to involve change talk, and these topics comprised a large portion of all change talk. Greater discussion of barriers and facilitators of change also was associated with more change talk. For alcohol use, benefits of drinking behavior was the most common topic of sustain talk. For sex risk, benefits of sexual behavior were rarely discussed, and sustain talk centered more on patterns and contexts, negations of drawbacks, and drawbacks of sexual risk behavior change.
Conclusions
Topic coding provided unique insights into the content of patient change and sustain talk.
Practice implications
Patients are most likely to voice change talk when conversation focuses on behavior change rather than ongoing behavior. Interventions addressing multiple health behaviors should address the unique motivations for maintaining specific risky behaviors."
}
@article{HENSEL2018105,
title = "Termination and complexity analysis for programs with bitvector arithmetic by symbolic execution",
journal = "Journal of Logical and Algebraic Methods in Programming",
volume = "97",
pages = "105 - 130",
year = "2018",
issn = "2352-2208",
doi = "https://doi.org/10.1016/j.jlamp.2018.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S2352220817300524",
author = "Jera Hensel and Jürgen Giesl and Florian Frohn and Thomas Ströder",
keywords = "Termination analysis, Bitvectors, Symbolic execution, , Runtime complexity",
abstract = "In earlier work, we developed an approach for automated termination analysis of C programs with explicit pointer arithmetic, which is based on symbolic execution. However, similar to many other termination techniques, this approach assumed the program variables to range over mathematical integers instead of bitvectors. This eases mathematical reasoning but is unsound in general. In this paper, we extend our approach in order to handle fixed-width bitvector integers. Thus, we present the first technique for termination analysis of C programs that covers both byte-accurate pointer arithmetic and bit-precise modeling of integers. Moreover, we show that our approach can also be used to analyze the runtime complexity of bitvector programs. We implemented our contributions in the automated termination prover AProVE and evaluate its power by extensive experiments."
}
@article{STUMM2018174,
title = "Raspberry-like supraparticles from nanoparticle building-blocks as code-objects for hidden signatures readable by terahertz rays",
journal = "Materials Today Communications",
volume = "16",
pages = "174 - 177",
year = "2018",
issn = "2352-4928",
doi = "https://doi.org/10.1016/j.mtcomm.2018.05.011",
url = "http://www.sciencedirect.com/science/article/pii/S2352492818301764",
author = "Christopher Stumm and Klaus Szielasko and Tim Granath and Claudia Stauch and Karl Mandel",
keywords = "supraparticles, codes, markers, anti-counterfeit, terahertz",
abstract = "Supraparticles, i.e., raspberry-like microparticles which are composed of nanoparticles (iron oxide), reveal specific interaction properties with terahertz (THz) rays. Depending on the density of the clustering of the nanoparticles within the raspberry-like supraparticle, characteristic THz components are altered upon transmission. The clustering can be adjusted upon supraparticle assembly via modification of the nanoparticles’ surfaces. By employing very densely and very loosely clustered supraparticles, a graphical coding system can be developed which allows creating signatures that are hidden in the bulk of a material (an object) and are easily and unambiguously decodable with THz rays."
}
@article{KAUR201931,
title = "Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe",
journal = "Information and Software Technology",
volume = "106",
pages = "31 - 48",
year = "2019",
issn = "0950-5849",
doi = "https://doi.org/10.1016/j.infsof.2018.09.002",
url = "http://www.sciencedirect.com/science/article/pii/S0950584918301903",
author = "Loveleen Kaur and Ashutosh Mishra",
keywords = "Cognitive complexity, Software change, Software metrics, Logistic regression analysis, Machine learning",
abstract = "Context
It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
Objective
This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
Method
For multiple successive releases of two Java-based software projects, where the source code of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's source code Java files. We construct eight datasets and build predictive models using statistical analysis and machine learning techniques.
Results
The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change."
}